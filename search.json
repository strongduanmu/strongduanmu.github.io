[{"title":"百倍提速！ShardingSphere 联邦查询批量 IN 查询深度优化","path":"/blog/speed-up-by-100x-shardingsphere-sql-federation-in-predicate-deep-optimization-practice.html","content":"问题背景 最近，用户在测试 ShardingSphere 联邦查询功能时，反馈了 1 条 2w6k 行的超长 SQL，执行时出现了 Communications link failure 异常，SQL 在 Proxy 接入端执行了 20 多分钟后最终超时断开。 笔者刚看到这个异常，怀疑是超长 SQL 过于复杂，导致在 SQL 解析、SQL 绑定或者生成执行计划的过程中出现问题，但是看到这条 SQL 的执行计划时，大概了解了问题的原因——Calcite 对于 IN 查询，会通过 SubQueryRemoveRule 规则，将 IN 查询转换为 JOIN。由于超长 SQL 中包含了大量的 IN 查询过滤条件，转换为 JOIN 方式执行，会导致下推的 SQL 缺失 IN 过滤条件，查询数据量相比预期要多很多，从而触发多轮磁盘文件交换，最终导致执行超时异常。 为了彻底解决 IN 查询的问题，笔者对 Calcite IN 查询内部处理逻辑进行了深入探究，通过对 SubQueryRemoveRule 规则进行优化，以及联邦查询谓词下推规则 PushFilterIntoScanRule 的适配，同时还重写了 Calcite RelToSqlConverter 中 IN 语法树转换 SQL 逻辑，最终将 2w6k 行超长 SQL 的查询耗时缩减到 1s 左右，实现了百倍以上的性能提升。 问题分析 为了深入分析这个问题，我们通过 EXPLAIN 语句来观察联邦查询的执行计划。如下展示了 2w6k 行超长 SQL 的执行计划，可以看到除了最外层的 DBPlusEngineLimitSort 算子，内部全部是由 UNION ALL 组合的多个查询语句，每个查询语句中包含了单列 IN 和多列 IN 查询条件。 原始 SQL 中的单列 IN 被转换为红色部分的 DBPlusEngineSortMergeJoin，通过关联 DBPlusEngineValues 中的常量实现 IN 过滤，而多列 IN 则被转换为蓝色部分的 DBPlusEngineHashJoin，同样通过关联 DBPlusEngineValues 中的常量实现多列 IN 过滤。这样的转换逻辑影响了 DBPlusEngineScan 中下推 SQL 的过滤条件，导致部分 IN 过滤条件没有下推下去，下推 SQL 查询的数据量过大。 为了方便问题分析，我们使用如下精简 SQL 进行探究，这条 SQL 和执行计划中的 SQL 结构一致，只是少了一些 UNION ALL 子查询，以及 IN 批量查询条件。 SELECT *FROM ( SELECT * FROM t_order WHERE creation_date = 2017-08-08 AND order_id IN (1000, 1001, 1100, 1101, 1200, 1201, 1300, 1301, 1400, 1401, 1500, 1501, 1600, 1601, 1700, 1701, 1800, 1801, 1900, 1901, 2000, 2001, 2100, 2101, 2200) AND (order_id, user_id, status, merchant_id) IN ((1000, 10, init, 1), (1001, 10, init, 2), (1100, 11, init, 5), (1101, 11, init, 6), (1200, 12, init, 9), (1201, 12, init, 10), (1300, 13, init, 13), (1301, 13, init, 14), (1400, 14, init, 17), (1401, 14, init, 18), (1500, 15, init, 1), (1501, 15, init, 2), (1600, 15, init, 5), (1601, 15, init, 6), (1700, 17, init, 9), (1701, 17, init, 10), (1800, 18, init, 13), (1801, 18, init, 14), (1900, 19, init, 17), (1901, 19, init, 18), (2000, 20, init, 3), (2001, 20, init, 4), (2100, 21, init, 7), (2101, 21, init, 8)) UNION ALL SELECT * FROM t_order WHERE creation_date = 2017-08-08 AND order_id IN (1000, 1001, 1100, 1101, 1200, 1201, 1300, 1301, 1400, 1401, 1500, 1501, 1600, 1601, 1700, 1701, 1800, 1801, 1900, 1901, 2000, 2001, 2100, 2101, 2200) AND (order_id, user_id, status, merchant_id) IN ((1000, 10, init, 1), (1001, 10, init, 2), (1100, 11, init, 5), (1101, 11, init, 6), (1200, 12, init, 9), (1201, 12, init, 10), (1300, 13, init, 13), (1301, 13, init, 14), (1400, 14, init, 17), (1401, 14, init, 18), (1500, 15, init, 1), (1501, 15, init, 2), (1600, 15, init, 5), (1601, 15, init, 6), (1700, 17, init, 9), (1701, 17, init, 10), (1800, 18, init, 13), (1801, 18, init, 14), (1900, 19, init, 17), (1901, 19, init, 18), (2000, 20, init, 3), (2001, 20, init, 4), (2100, 21, init, 7), (2101, 21, init, 8))) aORDER BY order_id ASCLIMIT 0, 10; 我们通过 EXPLAIN 语句观察这条 SQL 的执行计划，可以看到和用户反馈 SQL 的执行计划类似，2 个 IN 过滤条件依次被转换为 DBPlusEngineLookupJoin 和 DBPlusEngineSortMergeJoin，这些 JOIN 通过关联 DBPlusEngineValues 中的常量列表来实现数据过滤。此外，DBPlusEngineScan 算子中的下推 SQL，只包含了单列 IN 过滤条件，没有包含多列 IN 过滤条件，这会导致查询到内存的数据量比预期要大，通过内存过滤多列 IN，查询的性能要差很多。 +---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+| PLAN |+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+| DBPlusEngineLimitSort(sort0=[$0], dir0=[ASC-nulls-first], offset=[0], fetch=[10]) || DBPlusEngineUnion(all=[true]) || DBPlusEngineCalc(expr#0..10=[inputs], expr#11=[2017-08-08:CHAR(10)], expr#12=[=($t5, $t11)], proj#0..5=[exprs], $condition=[$t12]) || DBPlusEngineSortMergeJoin(condition=[AND(=($0, $7), =($1, $8), =($2, $9), =($3, $10))], joinType=[inner]) || DBPlusEngineSort(sort0=[$0], sort1=[$1], sort2=[$2], sort3=[$3], dir0=[ASC-nulls-first], dir1=[ASC-nulls-first], dir2=[ASC-nulls-first], dir3=[ASC-nulls-first]) || DBPlusEngineCalc(expr#0..6=[inputs], expr#7=[Sarg[1000L:BIGINT, 1001L:BIGINT, 1100L:BIGINT, 1101L:BIGINT, 1200L:BIGINT, 1201L:BIGINT, 1300L:BIGINT, 1301L:BIGINT, 1400L:BIGINT, 1401L:BIGINT, 1500L:BIGINT, 1501L:BIGINT, 1600L:BIGINT, 1601L:BIGINT, 1700L:BIGINT, 1701L:BIGINT, 1800L:BIGINT, 1801L:BIGINT, 1900L:BIGINT, 1901L:BIGINT, 2000L:BIGINT, 2001L:BIGINT, 2100L:BIGINT, 2101L:BIGINT]:BIGINT], expr#8=[SEARCH($t0, $t7)], expr#9=[Sarg[10, 11, 12, 13, 14, 15, 17, 18, 19, 20, 21]], expr#10=[SEARCH($t1, $t9)], expr#11=[init:VARCHAR(50)], expr#12=[=($t2, $t11)], expr#13=[Sarg[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 13, 14, 17, 18]], expr#14=[SEARCH($t3, $t13)], expr#15=[AND($t8, $t10, $t12, $t14)], proj#0..6=[exprs], $condition=[$t15]) || DBPlusEngineCalc(expr#0..6=[inputs], order_id=[$t1], user_id=[$t2], status=[$t3], merchant_id=[$t4], remark=[$t5], creation_date=[$t6], EXPR$0=[$t0]) || DBPlusEngineLookupJoin(condition=[=($0, $1)], joinType=[inner]) || DBPlusEngineValues(values=[[[1000:BIGINT], [1001:BIGINT], [1100:BIGINT], [1101:BIGINT], [1200:BIGINT], [1201:BIGINT], [1300:BIGINT], [1301:BIGINT], [1400:BIGINT], [1401:BIGINT], [1500:BIGINT], [1501:BIGINT], [1600:BIGINT], [1601:BIGINT], [1700:BIGINT], [1701:BIGINT], [1800:BIGINT], [1801:BIGINT], [1900:BIGINT], [1901:BIGINT], [2000:BIGINT], [2001:BIGINT], [2100:BIGINT], [2101:BIGINT], [2200:BIGINT]]]) || DBPlusEngineScan(sql=[SELECT * FROM `sharding_db`.`t_order` WHERE `order_id` IN (1000, 1001, 1100, 1101, 1200, 1201, 1300, 1301, 1400, 1401, 1500, 1501, 1600, 1601, 1700, 1701, 1800, 1801, 1900, 1901, 2000, 2001, 2100, 2101, 2200) AND `order_id` IN (...)]) || DBPlusEngineSort(sort0=[$0], sort1=[$1], sort2=[$2], sort3=[$3], dir0=[ASC-nulls-first], dir1=[ASC-nulls-first], dir2=[ASC-nulls-first], dir3=[ASC-nulls-first]) || DBPlusEngineCalc(expr#0..3=[inputs], expr#4=[Sarg[1000L:BIGINT, 1001L:BIGINT, 1100L:BIGINT, 1101L:BIGINT, 1200L:BIGINT, 1201L:BIGINT, 1300L:BIGINT, 1301L:BIGINT, 1400L:BIGINT, 1401L:BIGINT, 1500L:BIGINT, 1501L:BIGINT, 1600L:BIGINT, 1601L:BIGINT, 1700L:BIGINT, 1701L:BIGINT, 1800L:BIGINT, 1801L:BIGINT, 1900L:BIGINT, 1901L:BIGINT, 2000L:BIGINT, 2001L:BIGINT, 2100L:BIGINT, 2101L:BIGINT, 2200L:BIGINT]:BIGINT], expr#5=[SEARCH($t0, $t4)], proj#0..3=[exprs], $condition=[$t5]) || DBPlusEngineValues(values=[[[1000:BIGINT, 10, init:VARCHAR(50), 1], [1001:BIGINT, 10, init:VARCHAR(50), 2], [1100:BIGINT, 11, init:VARCHAR(50), 5], [1101:BIGINT, 11, init:VARCHAR(50), 6], [1200:BIGINT, 12, init:VARCHAR(50), 9], [1201:BIGINT, 12, init:VARCHAR(50), 10], [1300:BIGINT, 13, init:VARCHAR(50), 13], [1301:BIGINT, 13, init:VARCHAR(50), 14], [1400:BIGINT, 14, init:VARCHAR(50), 17], [1401:BIGINT, 14, init:VARCHAR(50), 18], [1500:BIGINT, 15, init:VARCHAR(50), 1], [1501:BIGINT, 15, init:VARCHAR(50), 2], [1600:BIGINT, 15, init:VARCHAR(50), 5], [1601:BIGINT, 15, init:VARCHAR(50), 6], [1700:BIGINT, 17, init:VARCHAR(50), 9], [1701:BIGINT, 17, init:VARCHAR(50), 10], [1800:BIGINT, 18, init:VARCHAR(50), 13], [1801:BIGINT, 18, init:VARCHAR(50), 14], [1900:BIGINT, 19, init:VARCHAR(50), 17], [1901:BIGINT, 19, init:VARCHAR(50), 18], [2000:BIGINT, 20, init:VARCHAR(50), 3], [2001:BIGINT, 20, init:VARCHAR(50), 4], [2100:BIGINT, 21, init:VARCHAR(50), 7], [2101:BIGINT, 21, init:VARCHAR(50), 8]]]) || DBPlusEngineCalc(expr#0..10=[inputs], expr#11=[2017-08-08:CHAR(10)], expr#12=[=($t5, $t11)], proj#0..5=[exprs], $condition=[$t12]) || DBPlusEngineCalc(expr#0..10=[inputs], order_id=[$t5], user_id=[$t6], status=[$t7], merchant_id=[$t8], remark=[$t9], creation_date=[$t10], EXPR$0=[$t0], EXPR$00=[$t1], EXPR$1=[$t2], EXPR$2=[$t3], EXPR$3=[$t4]) || DBPlusEngineSortMergeJoin(condition=[=($0, $5)], joinType=[inner]) || DBPlusEngineSort(sort0=[$0], dir0=[ASC-nulls-first]) || DBPlusEngineCalc(expr#0=[inputs], expr#1=[Sarg[1000L:BIGINT, 1001L:BIGINT, 1100L:BIGINT, 1101L:BIGINT, 1200L:BIGINT, 1201L:BIGINT, 1300L:BIGINT, 1301L:BIGINT, 1400L:BIGINT, 1401L:BIGINT, 1500L:BIGINT, 1501L:BIGINT, 1600L:BIGINT, 1601L:BIGINT, 1700L:BIGINT, 1701L:BIGINT, 1800L:BIGINT, 1801L:BIGINT, 1900L:BIGINT, 1901L:BIGINT, 2000L:BIGINT, 2001L:BIGINT, 2100L:BIGINT, 2101L:BIGINT]:BIGINT], expr#2=[SEARCH($t0, $t1)], EXPR$0=[$t0], $condition=[$t2]) || DBPlusEngineValues(values=[[[1000:BIGINT], [1001:BIGINT], [1100:BIGINT], [1101:BIGINT], [1200:BIGINT], [1201:BIGINT], [1300:BIGINT], [1301:BIGINT], [1400:BIGINT], [1401:BIGINT], [1500:BIGINT], [1501:BIGINT], [1600:BIGINT], [1601:BIGINT], [1700:BIGINT], [1701:BIGINT], [1800:BIGINT], [1801:BIGINT], [1900:BIGINT], [1901:BIGINT], [2000:BIGINT], [2001:BIGINT], [2100:BIGINT], [2101:BIGINT], [2200:BIGINT]]]) || DBPlusEngineSort(sort0=[$4], dir0=[ASC-nulls-first]) || DBPlusEngineLookupJoin(condition=[AND(=($0, $4), =($1, $5), =($2, $6), =($3, $7))], joinType=[inner]) || DBPlusEngineCalc(expr#0..3=[inputs], expr#4=[Sarg[1000L:BIGINT, 1001L:BIGINT, 1100L:BIGINT, 1101L:BIGINT, 1200L:BIGINT, 1201L:BIGINT, 1300L:BIGINT, 1301L:BIGINT, 1400L:BIGINT, 1401L:BIGINT, 1500L:BIGINT, 1501L:BIGINT, 1600L:BIGINT, 1601L:BIGINT, 1700L:BIGINT, 1701L:BIGINT, 1800L:BIGINT, 1801L:BIGINT, 1900L:BIGINT, 1901L:BIGINT, 2000L:BIGINT, 2001L:BIGINT, 2100L:BIGINT, 2101L:BIGINT, 2200L:BIGINT]:BIGINT], expr#5=[SEARCH($t0, $t4)], proj#0..3=[exprs], $condition=[$t5]) || DBPlusEngineValues(values=[[[1000:BIGINT, 10, init:VARCHAR(50), 1], [1001:BIGINT, 10, init:VARCHAR(50), 2], [1100:BIGINT, 11, init:VARCHAR(50), 5], [1101:BIGINT, 11, init:VARCHAR(50), 6], [1200:BIGINT, 12, init:VARCHAR(50), 9], [1201:BIGINT, 12, init:VARCHAR(50), 10], [1300:BIGINT, 13, init:VARCHAR(50), 13], [1301:BIGINT, 13, init:VARCHAR(50), 14], [1400:BIGINT, 14, init:VARCHAR(50), 17], [1401:BIGINT, 14, init:VARCHAR(50), 18], [1500:BIGINT, 15, init:VARCHAR(50), 1], [1501:BIGINT, 15, init:VARCHAR(50), 2], [1600:BIGINT, 15, init:VARCHAR(50), 5], [1601:BIGINT, 15, init:VARCHAR(50), 6], [1700:BIGINT, 17, init:VARCHAR(50), 9], [1701:BIGINT, 17, init:VARCHAR(50), 10], [1800:BIGINT, 18, init:VARCHAR(50), 13], [1801:BIGINT, 18, init:VARCHAR(50), 14], [1900:BIGINT, 19, init:VARCHAR(50), 17], [1901:BIGINT, 19, init:VARCHAR(50), 18], [2000:BIGINT, 20, init:VARCHAR(50), 3], [2001:BIGINT, 20, init:VARCHAR(50), 4], [2100:BIGINT, 21, init:VARCHAR(50), 7], [2101:BIGINT, 21, init:VARCHAR(50), 8]]]) || DBPlusEngineScan(sql=[SELECT * FROM `sharding_db`.`t_order` WHERE `order_id` IN (1000, 1001, 1100, 1101, 1200, 1201, 1300, 1301, 1400, 1401, 1500, 1501, 1600, 1601, 1700, 1701, 1800, 1801, 1900, 1901, 2000, 2001, 2100, 2101) AND `user_id` IN (10, 11, 12, 13, 14, 15, 17, 18, 19, 20, 21) AND `status` = init AND `merchant_id` IN (1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 13, 14, 17, 18) AND (`order_id`, `user_id`, `status`, `merchant_id`) IN (..., ..., ..., ...)]) |+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+24 rows in set (0.97 sec) 为了找到解决问题的方案，我们来跟踪下这条 SQL 的优化过程，观察从最开始的逻辑执行计划，依次经过 RBO 和 CBO 优化后，执行计划的变化情况。下图展示了 Calcite 生成的原始执行计划，可以看到原始 SQL 中的 2 个 IN 过滤条件，都包含在 LogicalFilter 算子的 condition 中。 经过 RBO 优化的执行计划如下图所示，此时 LogicalFilter 算子已经被优化， condition 中的 2 个 IN 过滤条件被转换为 JOIN，因此可以将问题的范围缩小到 RBO 优化过程。 由于 RBO 优化已经将 IN 转换为 JOIN，后续 CBO 继续处理时，只能基于 JOIN 的运算方式，选择代价最小的物理算子，无法再对执行方式进行修改。 现在我们已经将问题的范围缩小到 RBO 优化过程，但是 RBO 优化使用的优化规则很多，具体是哪一个优化规则导致的呢？我们首先来 Debug，观察原始逻辑执行计划生成的过程，了解 IN 算子在 RelNode 中是如何表示的，再根据 RelNode 去排查优化规则。 Calcite 在转换逻辑执行计划时，会从 WHERE 中首先提取出 IN 子查询，然后尝试将 IN 子查询进行替换，如下是 replaceSubQueries 实现逻辑： protected void replaceSubQueries(final Blackboard bb, final SqlNode expr, RelOptUtil.Logic logic) // 查找 expr 中的子查询，本案例中是查找 where 条件的 in 子查询 findSubQueries(bb, expr, logic, false); for (SubQuery node : bb.subQueryList) // 替换 in 子查询 substituteSubQuery(bb, node); 在 substituteSubQuery 替换逻辑中，会判断当前 IN 运算值的个数，如果个数小于 20（或者值列表中引用了列），则会将 IN 转换为 OR 拼接的条件。由于我们的查询 Case 模拟业务场景，IN 个数都超过了 20，感兴趣的朋友可以自行尝试，生成对应的执行计划。 由于我们的 IN 值个数不小于 20，逻辑会继续向下执行，将 IN 转换为 RexSubQuery 子查询。从源码注释中，我们也可以看到，后续这个对象会通过 SubQueryRemoveRule 规则进行优化。 如下图所示，在 Calcite 最终生成的逻辑执行计划中，IN 被表示为 RexSubQuery 对象。而在 RBO 优化中，和子查询转 JOIN 相关的规则是 CoreRules.FILTER_SUB_QUERY_TO_CORRELATE，内部实现对应的是 SubQueryRemoveRule 规则，负责将谓词中的子查询转换为其他结构。 我们来具体看下 CoreRules.FILTER_SUB_QUERY_TO_CORRELATE 的配置，这个优化规则匹配 Filter 中包含子查询的场景，通过 RexUtil.SubQueryFinder::containsSubQuery 来判断谓词中是否包含子查询。 Config FILTER = ImmutableSubQueryRemoveRule.Config.builder() .withMatchHandler(SubQueryRemoveRule::matchFilter) .build() .withOperandSupplier(b - b.operand(Filter.class) .predicate(RexUtil.SubQueryFinder::containsSubQuery).anyInputs()) .withDescription(SubQueryRemoveRule:Filter); 当规则匹配到谓词中的 IN 子查询时，会继续调用 SubQueryRemoveRule::matchFilter 对关系代数进行变换，方法内部会判断子查询的类型，本案例 Case 是 IN 子查询，最终会调用到 rewriteIn 方法。 如上图所示，IN 子查询会被改写为 JOIN 关联查询。这就是本案例查询慢的根本原因，对于 IN 常量集合，无需进行改写，只需要将 IN 条件完整地下推到底层数据库，就可以提前过滤掉大部分数据，从而提升查询性能。 (adsbygoogle = window.adsbygoogle || []).push({}); 赞助商 问题解决 研究清楚问题的根本原因后，我们的目标就很明确了——重写 SubQueryRemoveRule 规则，IN 常量集合子查询不改写为 JOIN，保留原始的 Filter，并将 Filter 下推到 DBPlusEngineScan 中。 如下图所示，我们先尝试重写 SubQueryRemoveRule 规则，并修改规则匹配条件，当 Filter 条件中完全是 LogicalValues 时，则不使用 SubQueryRemoveRule 将 IN 改写为 JOIN。确保执行计划中仍然包含 Filter 算子，能够实现 IN 谓词下推。 然后我们再修改 PushFilterIntoScanRule 谓词下推规则，当 Filter 过滤条件是 IN 常量值子查询时，支持将这类 Filter 下推到 Scan 中。 修改完成后，我们再次执行 EXPLAIN 语句，可以看到 IN 子查询已经被下推到 DBPlusEngineScan 中，但是 RelNode 转 SQL 时，将 LogicalValues 转换为了多个 UNION ALL 查询。在 MySQL 上测试了下，执行效率没有 IN 子查询高，需要再调整下 RelToSqlConverter#visit(Values e) 方法，转换为 MySQL 原始的 IN 语句。 +--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+| PLAN |+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+| DBPlusEngineLimitSort(sort0=[$0], dir0=[ASC-nulls-first], offset=[0], fetch=[10]) || DBPlusEngineUnion(all=[true]) || DBPlusEngineScan(sql=[SELECT * FROM `sharding_db`.`t_order` WHERE `creation_date` = 2017-08-08 AND `order_id` IN (SELECT 1000 UNION ALL SELECT 1001 UNION ALL SELECT 1100 UNION ALL SELECT 1101 UNION ALL SELECT 1200 UNION ALL SELECT 1201 UNION ALL SELECT 1300 UNION ALL SELECT 1301 UNION ALL SELECT 1400 UNION ALL SELECT 1401 UNION ALL SELECT 1500 UNION ALL SELECT 1501 UNION ALL SELECT 1600 UNION ALL SELECT 1601 UNION ALL SELECT 1700 UNION ALL SELECT 1701 UNION ALL SELECT 1800 UNION ALL SELECT 1801 UNION ALL SELECT 1900 UNION ALL SELECT 1901 UNION ALL SELECT 2000 UNION ALL SELECT 2001 UNION ALL SELECT 2100 UNION ALL SELECT 2101 UNION ALL SELECT 2200) AND (`order_id`, `user_id`, `status`, `merchant_id`) IN (SELECT 1000, 10, init, 1 UNION ALL SELECT 1001, 10, init, 2 UNION ALL SELECT 1100, 11, init, 5 UNION ALL SELECT 1101, 11, init, 6 UNION ALL SELECT 1200, 12, init, 9 UNION ALL SELECT 1201, 12, init, 10 UNION ALL SELECT 1300, 13, init, 13 UNION ALL SELECT 1301, 13, init, 14 UNION ALL SELECT 1400, 14, init, 17 UNION ALL SELECT 1401, 14, init, 18 UNION ALL SELECT 1500, 15, init, 1 UNION ALL SELECT 1501, 15, init, 2 UNION ALL SELECT 1600, 15, init, 5 UNION ALL SELECT 1601, 15, init, 6 UNION ALL SELECT 1700, 17, init, 9 UNION ALL SELECT 1701, 17, init, 10 UNION ALL SELECT 1800, 18, init, 13 UNION ALL SELECT 1801, 18, init, 14 UNION ALL SELECT 1900, 19, init, 17 UNION ALL SELECT 1901, 19, init, 18 UNION ALL SELECT 2000, 20, init, 3 UNION ALL SELECT 2001, 20, init, 4 UNION ALL SELECT 2100, 21, init, 7 UNION ALL SELECT 2101, 21, init, 8)]) || DBPlusEngineScan(sql=[SELECT * FROM `sharding_db`.`t_order` WHERE `creation_date` = 2017-08-08 AND `order_id` IN (SELECT 1000 UNION ALL SELECT 1001 UNION ALL SELECT 1100 UNION ALL SELECT 1101 UNION ALL SELECT 1200 UNION ALL SELECT 1201 UNION ALL SELECT 1300 UNION ALL SELECT 1301 UNION ALL SELECT 1400 UNION ALL SELECT 1401 UNION ALL SELECT 1500 UNION ALL SELECT 1501 UNION ALL SELECT 1600 UNION ALL SELECT 1601 UNION ALL SELECT 1700 UNION ALL SELECT 1701 UNION ALL SELECT 1800 UNION ALL SELECT 1801 UNION ALL SELECT 1900 UNION ALL SELECT 1901 UNION ALL SELECT 2000 UNION ALL SELECT 2001 UNION ALL SELECT 2100 UNION ALL SELECT 2101 UNION ALL SELECT 2200) AND (`order_id`, `user_id`, `status`, `merchant_id`) IN (SELECT 1000, 10, init, 1 UNION ALL SELECT 1001, 10, init, 2 UNION ALL SELECT 1100, 11, init, 5 UNION ALL SELECT 1101, 11, init, 6 UNION ALL SELECT 1200, 12, init, 9 UNION ALL SELECT 1201, 12, init, 10 UNION ALL SELECT 1300, 13, init, 13 UNION ALL SELECT 1301, 13, init, 14 UNION ALL SELECT 1400, 14, init, 17 UNION ALL SELECT 1401, 14, init, 18 UNION ALL SELECT 1500, 15, init, 1 UNION ALL SELECT 1501, 15, init, 2 UNION ALL SELECT 1600, 15, init, 5 UNION ALL SELECT 1601, 15, init, 6 UNION ALL SELECT 1700, 17, init, 9 UNION ALL SELECT 1701, 17, init, 10 UNION ALL SELECT 1800, 18, init, 13 UNION ALL SELECT 1801, 18, init, 14 UNION ALL SELECT 1900, 19, init, 17 UNION ALL SELECT 1901, 19, init, 18 UNION ALL SELECT 2000, 20, init, 3 UNION ALL SELECT 2001, 20, init, 4 UNION ALL SELECT 2100, 21, init, 7 UNION ALL SELECT 2101, 21, init, 8)]) |+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+4 rows in set (3.87 sec) 为了将 LogicalValues 转换为下推 SQL 中 MySQL 原生的 IN 语法，需要增加 DBPlusEngineRelToSQLConverter，并重写 visit(final Values values) 方法，当判断是 IN 常量子查询时，采用 ROW 语法生成 SqlNode。 调整完成后，再次执行 EXPLAIN 观察执行计划，可以看到在 IN 过滤条件下推 SQL 中，使用了 MySQL 原生支持的 (order_id, user_id, status, merchant_id) IN 语法，通过这种方式执行，SQL 可以快速查询出结果。 +------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+| PLAN |+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+| DBPlusEngineLimitSort(sort0=[$0], dir0=[ASC-nulls-first], offset=[0], fetch=[10]) || DBPlusEngineUnion(all=[true]) || DBPlusEngineScan(sql=[SELECT * FROM `sharding_db`.`t_order` WHERE `creation_date` = 2017-08-08 AND `order_id` IN ((1000), (1001), (1100), (1101), (1200), (1201), (1300), (1301), (1400), (1401), (1500), (1501), (1600), (1601), (1700), (1701), (1800), (1801), (1900), (1901), (2000), (2001), (2100), (2101), (2200)) AND (`order_id`, `user_id`, `status`, `merchant_id`) IN ((1000, 10, init, 1), (1001, 10, init, 2), (1100, 11, init, 5), (1101, 11, init, 6), (1200, 12, init, 9), (1201, 12, init, 10), (1300, 13, init, 13), (1301, 13, init, 14), (1400, 14, init, 17), (1401, 14, init, 18), (1500, 15, init, 1), (1501, 15, init, 2), (1600, 15, init, 5), (1601, 15, init, 6), (1700, 17, init, 9), (1701, 17, init, 10), (1800, 18, init, 13), (1801, 18, init, 14), (1900, 19, init, 17), (1901, 19, init, 18), (2000, 20, init, 3), (2001, 20, init, 4), (2100, 21, init, 7), (2101, 21, init, 8))]) || DBPlusEngineScan(sql=[SELECT * FROM `sharding_db`.`t_order` WHERE `creation_date` = 2017-08-08 AND `order_id` IN ((1000), (1001), (1100), (1101), (1200), (1201), (1300), (1301), (1400), (1401), (1500), (1501), (1600), (1601), (1700), (1701), (1800), (1801), (1900), (1901), (2000), (2001), (2100), (2101), (2200)) AND (`order_id`, `user_id`, `status`, `merchant_id`) IN ((1000, 10, init, 1), (1001, 10, init, 2), (1100, 11, init, 5), (1101, 11, init, 6), (1200, 12, init, 9), (1201, 12, init, 10), (1300, 13, init, 13), (1301, 13, init, 14), (1400, 14, init, 17), (1401, 14, init, 18), (1500, 15, init, 1), (1501, 15, init, 2), (1600, 15, init, 5), (1601, 15, init, 6), (1700, 17, init, 9), (1701, 17, init, 10), (1800, 18, init, 13), (1801, 18, init, 14), (1900, 19, init, 17), (1901, 19, init, 18), (2000, 20, init, 3), (2001, 20, init, 4), (2100, 21, init, 7), (2101, 21, init, 8))]) |+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+4 rows in set (0.80 sec) 测试 SQL 执行计划符合预期后，我们再用复杂的业务 SQL 对比测试，观察下优化前后的查询性能。 在优化之前，由于下推 SQL 过滤条件较少，部分 IN 常量集合子查询被转换为 JOIN 处理，导致 Scan 查询的数据量非常大，联邦查询引擎为了避免占用过多内存，会使用多次磁盘交换来完成 SQL 执行。本地 Mac 测试这条 SQL 需要 28s，用户环境受限于磁盘的读写性能，执行时间甚至超过了 20m。 经过本文优化之后，所有原始 SQL 中的 IN 过滤条件都能下推到 Scan 中，因此无需再进行磁盘交换，联邦查询引擎通过流式方式获取 Scan 中的数据，仅需要再对多个 UNION ALL 的查询进行数据合并，查询性能得到了极大的提升，本地 Mac 执行仅需 0.295s，相比原来的 28s，性能提升了 100 倍。 结语 本文介绍了复杂 SQL 包含 IN 常量子查询性能优化的过程，通过重写 SubQueryRemoveRule 和 DBPlusEngineRelToSQLConverter 逻辑，我们在联邦查询引擎中支持了 IN 常量子查询下推，最终将原始 SQL 的性能提升了 100 倍。 在优化的过程中，笔者也发现了很多联邦查询引擎和底层 Calcite 框架的不足，对于一些复杂的业务场景，还需要进行扩展和增强，才能完美地适配业务需求。后续规划中，联邦查询引擎将继续加强测试，尤其是结合业务场景和数据进行测试，尽可能多地发现问题，并进行深度优化，保证业务 SQL 准确、高性能地查询出结果。 本文整理的优化方案，对于其他使用 Calcite 框架的项目同样有借鉴意义，大家如有需要，可以参考进行优化。由于笔者水平经验有限，如果文章中有错误或者不足之处，欢迎大家留言指正。 写在最后 笔者因为工作原因接触到 Calcite，前期学习过程中，深感 Calcite 学习资料之匮乏，因此创建了 Calcite 从入门到精通知识星球，希望能够将学习过程中的资料和经验沉淀下来，为更多想要学习 Calcite 的朋友提供一些帮助。 欢迎关注 欢迎关注「端小强的博客」微信公众号，会不定期分享日常学习和工作经验，欢迎大家关注交流。","tags":["Calcite","ShardingSphere"],"categories":["Calcite"]},{"title":"使用 SQLancer 测试 ShardingSphere 联邦查询","path":"/blog/use-sqlancer-to-test-shardingsphere-sql-federation.html","content":"前言 在上一篇文章 ShardingSphere 联邦查询 GROUPING 聚合结果问题分析中，我们详细介绍了联邦查询引擎实现 GROUPING 聚合函数存在的问题，当时笔者曾提到 SQLancer 测试工具，它能够通过一些科学的方法来发现 SQL 逻辑问题，帮助提升联邦查询引擎的 SQL 支持度。本文将为大家详细介绍 SQLancer 测试工具，以及工具中内置的几种测试方法，然后我们会使用 SQLancer 工具，直接对联邦查询引擎进行测试，看看这个工具是否能够达到预期的测试效果，发现一些有价值的 SQL 漏洞。 什么是 SQLancer SQLancer 项目，是由 Manuel Rigger 教授创建的，旨在发现数据库 SQL 引擎的逻辑 BUG，Manuel Rigger 教授曾在 Andy 组织的线上分享中介绍过 SQLancer，感兴趣的朋友可以观看 Finding Logic Bugs in Database Management Systems 视频了解。 SQLancer is a tool to automatically test Database Management Systems (DBMSs) in order to find bugs in their implementation. That is, it finds bugs in the code of the DBMS implementation, rather than in queries written by the user. SQLancer has found hundreds of bugs in mature and widely-known DBMSs. 根据官方文档介绍，SQLancer 是一款用于自动测试数据库管理系统的工具，用于查找数据库实现逻辑中的错误。它查找的是 DBMS 实现代码中的错误，而不是用户编写 SQL 中的错误。目前，SQLancer 已在众多主流的 DBMS 中发现了数百个错误。 下图展示了一个具体的逻辑错误：当用户输入 SQL 语句查询数据时，原本数据库中存在 2 条匹配的数据，但由于数据库的 SQL 引擎存在逻辑错误，最终只返回了 1 条数据。除了少返回数据行外，逻辑错误还包含：错误返回过滤条件外的结果，返回的数据行内容错误等。 数据库逻辑错误相比于语法错误危害性更大，语法错误会在执行阶段通过异常码反馈出来，中断当前的 SQL 执行，逻辑错误则会返回不正确的查询结果，用户无法通过任何信息识别出当前的逻辑错误，最终可能会导致严重的业务错误。 使用 SQLancer 测试工具，可以快速发现 SQL 逻辑问题，帮助提升 SQL 引擎的正确性，下面我们将分别介绍 SQLancer 常用的几种测试方法，看看这些方法是如何检测 SQL 逻辑问题。 (adsbygoogle = window.adsbygoogle || []).push({}); 赞助商 PQS 测试方法 PQS 全称为 Pivoted Query Synthesis（枢轴查询合成），该方法详细的介绍可以参考论文——Testing Database Engines via Pivoted Query Synthesis。它的核心思想是：随机选择一条记录（即枢轴记录），然后基于这条记录生成过滤条件和查询语句，再去 DBMS 中执行查询，如果 DBMS 返回的结果集没有包含这条记录，则说明 DBMS 存在问题。 上图展示了 PQS 测试方法的详细步骤，总结下来包括如下 7 个步骤： 随机生成一些表（t0 和 t1 表）和数据行（t0 表 c0:3, c1:TRUE 数据行，t1 表 c0:-5 数据行）； 从每张表中随机的选择一行数据，将这行数据作为基准行； 基于选择的基准行，随机生成表达式，并根据基准行的值计算出表达式结果； 根据表达式的计算结果调整表达式，直到表达式的计算结果为 TRUE，例如：上图步骤 3 中表达式计算结果为 FALSE，步骤 4 中通过增加 NOT 将计算结果调整为 TRUE； 基于表达式随机生成查询语句，表达式使用在查询的 WHERE 或者 JOIN 子句中，查询语句会返回基准行对应的列（SELECT t0.c0, t0.c1, t1.c0）； 将查询语句提交到 DBMS 中执行，获取返回的结果集； 校验结果集是否包含最初选择的基准行，如果不包含，说明 DBMS 可能存在缺陷。 PQS 测试方法是 SQLancer 支持的第一个测试方法，它支持 SQLite（3.28）、MySQL（8.0.16） 及 PostgreSQL（11.4） 数据库，由于该测试方法实现的工作量巨大，需要为每个 DB 实现 AST 解释器，并且无法支持聚合函数、窗口函数测试，目前 SQLancer 已经不再维护，官方推荐使用其他测试方法。如果大家对这个测试方法感兴趣，仍然可以使用如下的命令执行 PQS 测试，--oracle pqs 属性用于指定测试预言机的类型。 java -jar sqlancer-*.jar --num-threads 4 --port 3306 --username root --password 123456 mysql --oracle pqs 下图展示了使用 SQLancer PQS 方法测试 MySQL 的截图，测试出的不支持 SQL 可以在 target/logs 目录下查看。 NoREC 测试方法 NoREC 是 SQLancer 支持的第二个测试方法，全称是 Non-Optimizing Reference Engine Construction（非优化参考引擎构造），该方法的详细内容可参考论文——Detecting Optimization Bugs in Database Engines via Non-Optimizing Reference Engine Construction。 NoREC 的核心思想是：通过对比优化查询与非优化查询的结果差异，来检测 SQL 优化可能存在的漏洞。优化查询具体指：SELECT * FROM t0 WHERE φ，这条 SQL 可能会被 SQL 引擎优化，NoREC 测试方法会将这条 SQL 转换为非优化查询——SELECT (φ IS TRUE) FROM t0，将过滤条件移动到投影列中。通过 NoREC 测试方法，可以测试出数据库管理系统中的优化错误。 上图展示了 NoREC 测试方法的详细步骤，测试逻辑非常简单，具体如下： 随机生成一条较高优化潜力的 SQL（数据库中大多数优化都和过滤相关，因此生成包含 WHERE 条件的 SQL，预期将会被数据库管理系统进行优化），例如：SELECT * FROM t0 WHERE φ； 将优化 SQL 转换为无法优化的形式，具体来说，是将 WHERE 条件中的表达式移动到投影列中，例如：SELECT (φ IS TRUE) FROM t0，这种查询缺乏 WHERE 条件，数据库管理系统必须检索所有记录； 执行优化 SQL 和未优化 SQL 并比较结果集，如果未优化 SQL 返回 TRUE 的行数不等于优化 SQL 返回行数，则说明存在 BUG。 NoREC 测试方法支持 SQLite、MariaDB、PostgreSQL 和 CockroachDB 数据库，支持 WHERE、JOIN、ORDER BY 等子句测试，暂不支持 DISTINCT、窗口函数 测试。相比于 PQS，NoREC 增加了对聚合函数的支持，并且可以检测重复记录错误。执行如下的命令测试 NoREC 方法，通过 --oracle norec 参数指定 NoREC 方法： java -jar sqlancer-*.jar --num-threads 4 --port 3344 --username root --password 123456 mariadb --oracle norec 下图展示了使用 SQLancer PQS 方法测试 MariaDB 的截图，测试出的不支持 SQL 可以在 target/logs 目录下查看。 TLP 测试方法 TLP 测试方法全称为 Ternary Logic Partitioning（三元逻辑分区），该方法的详细介绍可以参考论文——Finding bugs in database systems via query partitioning。 TLP 测试方法的核心思想是：将一个原始查询分解为多个分区查询，每个分区查询计算原始查询结果的一个子集，然后通过组合操作将这些子集合并，验证合并结果是否与原始查询结果一致，不一致则说明存在 BUG。从 TLP 的命名我们可以看出，在进行分区查询时，该方法采用了三元逻辑分区，基于 SQL 的三值逻辑（TRUE、FALSE、NULL），将原始查询分解为三个分区查询，分别对应谓词为 TRUE、FALSE 和 NULL 的情况。 上图展示了 TLP 测试方法的实现原理，具体测试流程如下： 随机生成数据库和查询语句； 根据原始查询生成分区查询，例如：WHERE p、WHERE NOT p、WHERE p IS NULL； 执行分区查询并合并查询结果； 与原始查询结果对比，不一致则发现 DB BUG。 如下表所示，TLP 测试方法相比 PQS（主要测试 WHERE）和 NoREC（主要测试 WHERE，部分聚合），能够支持更多的语法类型，包括：WHERE、GROUP BY、HAVING、聚合函数（如 MIN、MAX、SUM、COUNT、AVG）和 DISTINCT 查询。目前，TLP 测试方法已经支持了 SQLite、MySQL、PostgreSQL 等多种数据库，可以将 TLP 和其他测试方法结合，覆盖更多的测试场景。 执行如下的命令测试 TLP 方法，通过 --oracle tlp_where 参数指定 TLP 方法： java -jar sqlancer-*.jar --num-threads 4 --port 3306 --username root --password 123456 mysql --oracle tlp_where 下图展示了使用 SQLancer TLP 方法测试 MySQL 的截图，测试出的不支持 SQL 可以在 target/logs 目录下查看。 DQE 测试方法 DQE 测试方法全称为 Differential Query Execution（差分查询执行），该方法由国内研究团队提出，详细论文内容可以参考——Testing Database Systems via Differential Query Execution。 DQE 测试方法的核心思想是：使用相同谓词（WHERE 条件）生成 SELECT、UPDATE 和 DELETE 语句，然后分别执行这 3 条语句，观察他们操作的数据行，如果操作的数据行不一致，则可能存在逻辑错误。 上图展示了 DQE 测试方法的详细流程，具体测试细节如下： 生成随机的库（t1、t2）和表（c1、c2）； 生成随机谓词，例如：NOT t1.c1； 基于相同的谓词，生成一个查询元组，包含 SELECT、UPDATE 和 DELETE 语句； 在相同的数据库状态下，执行 SELECT、UPDATE 和 DELETE 语句； 获取 SELECT、UPDATE 和 DELETE 语句执行结果； 比较执行结果：SELECT 语句返回 rowId，确定访问的数据行范围。UPDATE 语句除了更新常规字段外，还额外更新 updated 为 1，执行后检查 updated = 1 的数据行及其 rowId，确认修改行是否和 SELECT 一致。DELETE 语句执行前记录所有 rowId，执行后比较出删除的 rowId 范围，并和 SELECT 语句对比是否一致。 DQE 相比前文提到的其他测试方法，首次提出了针对 UPDATE 和 DELETE 语句逻辑错误的检测方法，并且支持 MySQL、MariaDB、CockroachDB 等数据库，完善了 SQLancer 测试工具覆盖的 SQL 场景。但是差分测试也存在一定的局限性，例如：无法测试出 SELECT、UPDATE 和 DELETE 语句同时出现相同错误的场景，不支持 DISTINCT、GROUP BY 等语法，以及包含非确定函数的场景（例如：RAND 函数）。 执行如下的命令测试 DQE 方法，通过 --oracle dqe 参数指定 DQE 方法： java -jar sqlancer-*.jar --num-threads 4 --port 3306 --username root --password 123456 mysql --oracle dqe 下图展示了使用 SQLancer DQE 方法测试 MySQL 的截图，测试出的不支持 SQL 可以在 target/logs 目录下查看。 联邦查询测试实战 前文我们介绍了 SQLancer 常用的 4 种测试方法，通过这 4 种方法，可以较为全面地覆盖 SELECT、UPDATE 和 DELETE 语句，测试出 SQL 引擎的逻辑错误。本小节我们再来研究下，如何将 SQLancer 应用到 ShardingSphere 联邦查询功能中，帮助我们发现更多联邦查询的功能漏洞。 在开始测试联邦查询前，我们先了解下 SQLancer 工具如何使用，按照如下的命令克隆源码（Fork 仓库增加了对 ShardingSphere 的适配），然后进行编译打包，最后执行 java -jar sqlancer-*.jar --num-threads 4 sqlite3 --oracle NoREC 命令来开始测试。 git clone git@github.com:strongduanmu/sqlancer.gitcd sqlancermvn package -DskipTestscd targetjava -jar sqlancer-*.jar --num-threads 4 sqlite3 --oracle NoREC 执行 java -jar sqlancer-*.jar -h 命令可以查看 SQLancer 所有命令参数及说明，下面展示的内容进行了一些截取，重点展示 SQLancer 全局选项（options） 以及 MySQL 数据库的参数选项。从 Usage 格式可以看出（Usage: SQLancer [options] [command] [command options]），SQLancer 命令需要先指定全局选项 options（例如：--num-threads 4），然后再指定数据库专属命令 command（例如：sqlite3） 以及数据库专属选项 command options（例如：--oracle NoREC）。 Usage: SQLancer [options] [command] [command options] Options: --canonicalize-sql-strings # 自动给 SQL 末尾加分号，适配严格要求语句结束符的 DBMS Should canonicalize query string (add ; at the end Default: true --database-prefix # 生成数据库的名称前缀，便于识别测试库（如 database_1、database_2） The prefix used for each database created Default: database --help, -h Lists all supported options and commands --host # 数据库服务器地址 The host used to log into the DBMS --max-expression-depth # 随机生成表达式的最大嵌套深度，增大值（如 5）可测试复杂表达式 Specifies the maximum depth of randomly-generated expressions Default: 3 --max-num-inserts # 每个测试周期执行的 INSERT 语句数，控制测试数据量 Specifies how many INSERT statements should be issued Default: 30 --num-queries # 每个数据库执行的查询数，达到该数量后自动创建新数据库继续测试 Specifies the number of queries to be issued to a database before creating a new database Default: 100000 --num-threads # 并发测试的线程数，线程数越多测试效率越高，但需避免 DBMS 资源耗尽 How many threads should run concurrently to test separate databases Default: 16 --num-tries # 发现错误后停止测试的阈值，找到 100 个错误即终止，可设更大值（如 1000）延长测试 Specifies after how many found errors to stop testing Default: 100 --password # 登录数据库的密码 The password used to log into the DBMS Default: sqlancer --port # 数据库端口 The port used to log into the DBMS Default: -1 --pqs-test-aggregates # 仅当表只有 1 行时，测试聚合函数（非 PQS 测试方法特有属性） Partially test aggregate functions when all tables contain only a single row. Default: false --print-progress-information # 打印进度（生成的数据库数 / 执行的查询数），用于监控测试进度 Whether to print progress information such as the number of databases generated or queries issued Default: true --print-progress-summary # 退出时打印执行汇总（总查询数 / 错误数等），测试结束后统计结果 Whether to print an execution summary when exiting SQLancer Default: true --random-seed # 随机种子，设为非 -1 值（如 12345）可复现测试过程（确定性生成） A seed value != -1 that can be set to make the query and database generation deterministic Default: -1 --random-string-generation # 随机字符串生成策略，NUMERIC（纯数字）、ALPHANUMERIC（字母数字）、ALPHANUMERIC_SPECIALCHAR（含特殊字符）、SOPHISTICATED（复杂混合） Select the random-string eneration approach Default: SOPHISTICATED Possible Values: [NUMERIC, ALPHANUMERIC, ALPHANUMERIC_SPECIALCHAR, SOPHISTICATED] --serialize-reproduce-state # 序列化测试状态，用于测出 BUG 后复现问题 Serialize the state to reproduce Default: false --storage-unit-host # ShardingSphere 存储单元数据库服务器地址 Storage unit host for ShardingSphere Default: 127.0.0.1 --storage-unit-password # ShardingSphere 存储单元密码 Storage unit password for ShardingSphere Default: 123456 --storage-unit-port # ShardingSphere 存储单元端口 Storage unit port for ShardingSphere Default: 3306 --storage-unit-username # ShardingSphere 存储单元用户名 Storage unit username for ShardingSphere Default: root --string-constant-max-length # 生成字符串常量的最大长度，增大值（如 50）可测试长字符串场景 Specify the maximum-length of generated string constants Default: 10 --username # 数据库用户名 The user name used to log into the DBMS Default: sqlancer Commands: mysql MySQL (default port: 3306, default host: localhost) Usage: mysql [options] Options: # 指定 MySQL 测试使用的测试预言机，默认值 为 TLP_WHERE --oracle Default: [TLP_WHERE] 由于 ShardingSphere 有逻辑库和存储单元（物理库）的概念，因此需要对 SQLancer 进行一些改造，才能测试 ShardingSphere 联邦查询。从上面的属性可以看出，我们新增了 storage-unit-host、storage-unit-port、storage-unit-username 和 storage-unit-password 属性，用于指定 ShardingSphere 存储单元的连接信息。而 SQLancer 原有的属性 host、port、username、password，则用于连接 ShardingSphere Proxy 接入端。 当 SQLancer 工具创建测试库时，不仅会在 Proxy 接入端创建逻辑库，还会额外通过 DistSQL 注册物理存储单元，然后再进行建表、生成数据和 SQL 测试等操作。我们执行如下的命令测试 ShardingSphere 联邦查询，--storage-unit-* 属性用于指定存储单元的连接信息，--num-threads 用来指定测试线程数，--num-tries 则用于控制发现多少个错误则终止测试。 java -jar sqlancer-*.jar \\ --port 3307 \\ --username root \\ --password root \\ --storage-unit-port 3306 \\ --storage-unit-username root \\ --storage-unit-password 123456 \\ --num-threads 1 \\ --num-tries 1000 \\ mysql \\ --oracle tlp_where ShardingSphere 商业联邦查询的配置可以参考联邦查询-配置示例文档，首先在 Proxy 端执行以下的 DistSQL 开启联邦查询，ALL_QUERY_USE_SQL_FEDERATION 设置为 true，保证所有的 DML SQL 使用联邦查询引擎执行。 ALTER SQL_FEDERATION RULE ( SQL_FEDERATION_ENABLED=true, ALL_QUERY_USE_SQL_FEDERATION=true, EXECUTION_PLAN_CACHE(INITIAL_CAPACITY=2000, MAXIMUM_SIZE=65535, TTL_MILLI_SECONDS=86400000), MAX_USAGE_MEMORY_PER_QUERY=10M, SPILL_ENABLED=true, SPILL_PATH=file:///tmp/dbplusengine/spill, SPILL_COMPRESSION_ENABLED=true); 然后我们再开启统计信息收集功能，支持 SQLancer 对库、表、列等信息的查询，确保 SQLancer 测试逻辑能够正常运行。 ALTER STATISTICS_STORAGE RULE ( NAME_PREFIX=sphereex, STORAGE UNIT ( URL = jdbc:mysql://127.0.0.1:3306?serverTimezone=UTCuseSSL=falseallowPublicKeyRetrieval=true, USER = root, PASSWORD = 123456, PROPERTIES(minPoolSize=1, maxPoolSize=2) )); 开启统计信息收集，需要在 MySQL 中执行 Grant 语句给 STATISTICS_STORAGE Rule 中的账号赋权（如下数据库账号按照实际情况修改）。 GRANT CREATE, SELECT, INSERT, UPDATE, DELETE, DROP ON sphereex_information_schema.* TO root@%;GRANT CREATE, SELECT, INSERT, UPDATE, DELETE, DROP ON sphereex_shardingsphere.* TO root@%; 再执行 DistSQL 设置统计信息收集间隔，并开启统计信息收集功能。 SET DIST VARIABLE proxy_meta_data_collector_cron = 0 0/30 * * * ?;SET DIST VARIABLE proxy_meta_data_collector_enabled = true; 首次开启统计信息功能，我们可以主动执行 REFRESH STATISTICS METADATA; 更新统计信息。 REFRESH STATISTICS METADATA; 配置完联邦查询功能，并开启统计信息收集后，我们可以执行前面的 SQLancer 命令，来测试下 ShardingSphere 联邦查询功能，很快我们就测试出了第一个联邦查询不支持的 Case——IS UNKNOWN 语法（异常 Case 会输出到 target/logs/mysql 目录下）。 SQLancer 提供的报错信息很全面，不仅包含了异常堆栈信息，还提供了复现这个异常所需的 SQL 操作步骤，我们依次执行 SQL 语句，就可以快速复现异常。 java.lang.AssertionError: SELECT ALL t2.c1 AS ref0 FROM t2 WHERE NULL UNION ALL SELECT t2.c1 AS ref0 FROM t2 WHERE (NOT (NULL)) UNION ALL SELECT t2.c1 AS ref0 FROM t2 WHERE (NULL) IS UNKNOWN; at sqlancer.common.query.SQLQueryAdapter.checkException(SQLQueryAdapter.java:166) at sqlancer.common.query.SQLQueryAdapter.internalExecuteAndGet(SQLQueryAdapter.java:207) at sqlancer.common.query.SQLQueryAdapter.executeAndGet(SQLQueryAdapter.java:177) at sqlancer.common.query.SQLQueryAdapter.executeAndGet(SQLQueryAdapter.java:172) at sqlancer.ComparatorHelper.getResultSetFirstColumnAsString(ComparatorHelper.java:56) at sqlancer.ComparatorHelper.getCombinedResultSet(ComparatorHelper.java:151) at sqlancer.common.oracle.TLPWhereOracle.check(TLPWhereOracle.java:110) at sqlancer.ProviderAdapter.generateAndTestDatabase(ProviderAdapter.java:61) at sqlancer.Main$DBMSExecutor.run(Main.java:467) at sqlancer.Main$2.run(Main.java:684) at sqlancer.Main$2.runThread(Main.java:666) at sqlancer.Main$2.run(Main.java:657) at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) at java.base/java.lang.Thread.run(Thread.java:840)Caused by: java.sql.SQLSyntaxErrorException: Unsupported SQL operator: `IS` at com.mysql.cj.jdbc.exceptions.SQLError.createSQLException(SQLError.java:120) at com.mysql.cj.jdbc.exceptions.SQLExceptionsMapping.translateException(SQLExceptionsMapping.java:122) at com.mysql.cj.jdbc.StatementImpl.executeQuery(StatementImpl.java:1200) at sqlancer.common.query.SQLQueryAdapter.internalExecuteAndGet(SQLQueryAdapter.java:196) ... 13 more--java.lang.AssertionError: SELECT ALL t2.c1 AS ref0 FROM t2 WHERE NULL UNION ALL SELECT t2.c1 AS ref0 FROM t2 WHERE (NOT (NULL)) UNION ALL SELECT t2.c1 AS ref0 FROM t2 WHERE (NULL) IS UNKNOWN;-- at sqlancer.common.query.SQLQueryAdapter.checkException(SQLQueryAdapter.java:166)-- at sqlancer.common.query.SQLQueryAdapter.internalExecuteAndGet(SQLQueryAdapter.java:207)-- at sqlancer.common.query.SQLQueryAdapter.executeAndGet(SQLQueryAdapter.java:177)-- at sqlancer.common.query.SQLQueryAdapter.executeAndGet(SQLQueryAdapter.java:172)-- at sqlancer.ComparatorHelper.getResultSetFirstColumnAsString(ComparatorHelper.java:56)-- at sqlancer.ComparatorHelper.getCombinedResultSet(ComparatorHelper.java:151)-- at sqlancer.common.oracle.TLPWhereOracle.check(TLPWhereOracle.java:110)-- at sqlancer.ProviderAdapter.generateAndTestDatabase(ProviderAdapter.java:61)-- at sqlancer.Main$DBMSExecutor.run(Main.java:467)-- at sqlancer.Main$2.run(Main.java:684)-- at sqlancer.Main$2.runThread(Main.java:666)-- at sqlancer.Main$2.run(Main.java:657)-- at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)-- at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)-- at java.base/java.lang.Thread.run(Thread.java:840)--Caused by: java.sql.SQLSyntaxErrorException: Unsupported SQL operator: `IS`-- at com.mysql.cj.jdbc.exceptions.SQLError.createSQLException(SQLError.java:120)-- at com.mysql.cj.jdbc.exceptions.SQLExceptionsMapping.translateException(SQLExceptionsMapping.java:122)-- at com.mysql.cj.jdbc.StatementImpl.executeQuery(StatementImpl.java:1200)-- at sqlancer.common.query.SQLQueryAdapter.internalExecuteAndGet(SQLQueryAdapter.java:196)-- ... 13 more---- Time: 2025/12/28 20:31:14-- Database: database0-- Database version: 5.7.22-DBPlusEngine-Proxy 5.5.3-SNAPSHOT-82e26ad-- seed value: 1766925060265DROP DATABASE IF EXISTS database0;CREATE DATABASE database0;USE database0;CREATE TABLE t0(c0 VARCHAR(500) COMMENT asdf NULL COLUMN_FORMAT DEFAULT, c1 DECIMAL STORAGE MEMORY) ;INSERT INTO t0(c0) VALUES(0.5415928422036966);UPDATE t0 SET c1=0.6302674886178765, c0=0.6302674886178765;INSERT DELAYED INTO t0(c1, c0) VALUES(NULL, h|Ij#k);INSERT INTO t0(c1) VALUES(0.3379070670876593);UPDATE t0 SET c1=-947955752 WHERE CAST( *Y AS SIGNED);UPDATE t0 SET c1=(NOT ( EXISTS (SELECT 1 wHERE FALSE))), c0=t0.c0;INSERT INTO t0(c1) VALUES((g);INSERT IGNORE INTO t0(c1) VALUES(9);DELETE IGNORE FROM t0;INSERT IGNORE INTO t0(c1) VALUES(NULL), (NULL), (0.2584659624758967), (NULL), (i|);DELETE IGNORE FROM t0;ALTER TABLE t0 DELAY_KEY_WRITE 0, COMPRESSION ZLIB;SHOW TABLES;INSERT HIGH_PRIORITY IGNORE INTO t0(c1) VALUES(1634001425);INSERT LOW_PRIORITY INTO t0(c0) VALUES();SHOW TABLES;UPDATE t0 SET c0=-1148055694;UPDATE t0 SET c0=17125872;INSERT IGNORE INTO t0(c1, c0) VALUES(0.3283236604697395, NULL);INSERT INTO t0(c1) VALUES(0.048050841555574375);ALTER TABLE t0 DROP c0, FORCE, CHECKSUM 1;INSERT LOW_PRIORITY INTO t0(c1) VALUES(0.07787208116702882);DELETE LOW_PRIORITY QUICK IGNORE FROM t0;INSERT IGNORE INTO t0(c1) VALUES(829218842);INSERT LOW_PRIORITY INTO t0(c1) VALUES(NULL);INSERT IGNORE INTO t0(c1) VALUES(NULL);INSERT INTO t0(c1) VALUES(0.02242428818878528);DELETE QUICK IGNORE FROM t0;ALTER TABLE t0 INSERT_METHOD FIRST, PACK_KEYS 1, DISABLE KEYS, COMPRESSION NONE, ROW_FORMAT FIXED, CHECKSUM 0, STATS_PERSISTENT DEFAULT, FORCE, RENAME AS t0, DELAY_KEY_WRITE 0, ALGORITHM DEFAULT, STATS_AUTO_RECALC DEFAULT;ALTER TABLE t0 ENABLE KEYS, ALGORITHM INPLACE, STATS_PERSISTENT 0, CHECKSUM 0, PACK_KEYS 0, INSERT_METHOD LAST;ALTER TABLE t0 FORCE, ROW_FORMAT REDUNDANT, INSERT_METHOD NO, STATS_AUTO_RECALC DEFAULT, PACK_KEYS 0, ENABLE KEYS, STATS_PERSISTENT 1, RENAME AS t0, ALGORITHM DEFAULT;ALTER TABLE t0 PACK_KEYS 0, STATS_PERSISTENT DEFAULT, ENABLE KEYS, ALGORITHM COPY, ROW_FORMAT DYNAMIC, RENAME t2, CHECKSUM 0, COMPRESSION NONE, STATS_AUTO_RECALC 1, DELAY_KEY_WRITE 0, INSERT_METHOD NO, FORCE;INSERT HIGH_PRIORITY IGNORE INTO t2(c1) VALUES(-1656112204);UPDATE t2 SET c1= EXISTS (SELECT 1 wHERE FALSE);INSERT DELAYED IGNORE INTO t2(c1) VALUES(1e500);SHOW TABLES;INSERT HIGH_PRIORITY IGNORE INTO t2(c1) VALUES(0.9546301382857064); 另外，我们还可以根据 seed value: 1766925060265 来控制 SQLancer 生成的 Case，使用相同的种子值（通过参数 --random-seed 1766925060265 控制），可以生成相同的测试 Case，这样我们就可以稳定复现异常。 java -jar sqlancer-*.jar \\ --port 3307 \\ --username root \\ --password root \\ --storage-unit-port 3306 \\ --storage-unit-username root \\ --storage-unit-password 123456 \\ --random-seed 1766925060265 \\ --num-threads 1 \\ --num-tries 1000 \\ mysql \\ --oracle tlp_where SQLancer 测试工具功能强大，目前已经测试出一批 ShardingSphere 联邦查询不支持的 Case，由于篇幅限制，本文就不一一介绍了，笔者会根据这些异常信息，逐个进行分析和修复，不断提升 ShardingSphere 联邦查询的 SQL 支持度。大家如果有 SQL 引擎的测试需求，不妨也尝试下 SQLancer，相信它一定能够发现更多潜在的问题，帮助大家提升 SQL 引擎的稳定性。由于笔者也是初次探索和使用 SQLancer，如果文章有错误之处，或者其他 SQLancer 使用技巧，欢迎大家留言指导。 欢迎关注 欢迎关注「端小强的博客」微信公众号，会不定期分享日常学习和工作经验，欢迎大家关注交流。","tags":["ShardingSphere","SQLancer"],"categories":["ShardingSphere"]},{"title":"ShardingSphere 联邦查询 GROUPING 聚合结果问题分析","path":"/blog/analyze-wrong-result-for-shardingsphere-sql-federation-grouping-function.html","content":"问题背景 上周笔者所在公司客户反馈，使用商业版联邦查询进行复杂 SQL 聚合分析时，出现了 StringIndexOutOfBoundsException 异常。根据客户反馈的异常信息，起初笔者觉得这只是一个简单的下标越界问题，于是快速通过 E2E 测试程序复现问题（如下图所示），并对下标越界的代码进行了增强。 但是问题似乎没有这么简单，修改后 E2E 测试仍然没有通过，根据断言结果来看，查询结果同样不符合预期，为了彻底搞清楚这个问题，笔者在周末进行了一番探索研究，最终解决了这个问题。调查问题过程中，笔者对 GROUPING 函数的语义有了更深的理解，为了方便自己以及其他有需要的同学参考学习，本文将对问题调查过程进行记录总结，如有介绍不详细或者错误之处，还恳请大家留言指导。 问题分析 由于原始 SQL 非常复杂，为了方便问题描述，我们使用一个精简版的 SQL 来进行分析。首先，我们搭建一个 Proxy 集群，并参考商业文档开启联邦查询功能，然后在 Proxy 上配置分片规则，按照 data_batch_id 字段对 test_analysis_result 表进行哈希取模分片。 rules:- !SINGLE tables: - *.*- !SHARDING tables: test_analysis_result: actualDataNodes: ds_$0..9.test_analysis_result databaseStrategy: standard: shardingColumn: data_batch_id shardingAlgorithmName: data_batch_inline defaultTableStrategy: none: shardingAlgorithms: data_batch_inline: type: INLINE props: algorithm-expression: ds_$Math.abs(data_batch_id.hashCode() % 10) 再通过 Proxy 执行如下的建表语句，创建 test_analysis_result 测试表： CREATE TABLE `test_analysis_result` ( `creator_account` varchar(200) DEFAULT NULL, `is_deleted` varchar(1) DEFAULT NULL, `last_modified_time` datetime DEFAULT NULL, `data_batch_id` date DEFAULT NULL, `modifier_account` varchar(200) DEFAULT NULL, `analysis_id` varchar(200) NOT NULL, `analysis_description` varchar(1000) DEFAULT NULL, `last_modifier_id` bigint DEFAULT NULL, `creator_id` bigint DEFAULT NULL, `create_time` datetime DEFAULT NULL, `data_version` int DEFAULT 0, `test_identifier` varchar(200) NOT NULL, `test_number` varchar(40) NOT NULL, `plan_entity_code` varchar(40) NOT NULL, `plan_entity_name_cn` varchar(100) DEFAULT NULL, `plan_entity_name_en` varchar(100) DEFAULT NULL, `material_code` varchar(40) NOT NULL, `test_create_date` date NOT NULL, `analysis_version` int NOT NULL, `calculation_complete_date` date DEFAULT NULL, `calculation_method` varchar(10) DEFAULT NULL, `warranty_type` varchar(10) DEFAULT NULL, `standard_type` varchar(10) DEFAULT NULL, `product_line_code` varchar(50) DEFAULT NULL, `product_line_name_cn` varchar(1000) DEFAULT NULL, `product_line_name_en` varchar(1000) DEFAULT NULL, `product_series_code` varchar(30) DEFAULT NULL, `product_series_name_cn` varchar(512) DEFAULT NULL, `product_series_name_en` varchar(512) DEFAULT NULL, `marketing_name` text, `material_category_code` varchar(100) DEFAULT NULL, `price_rmb` decimal(18, 2) DEFAULT NULL, `price_usd` decimal(18, 2) DEFAULT NULL, `material_desc_cn` text, `material_desc_en` text, `market_launch_date` date DEFAULT NULL, `total` bigint DEFAULT NULL, `lifecycle_consumed_total` bigint DEFAULT NULL, `actual_consumption_qty` bigint DEFAULT NULL, `forecast_total_consumption` bigint DEFAULT NULL, `current_and_transit_inventory` bigint DEFAULT NULL, `forecast_future` bigint DEFAULT NULL, `distribution_center_stock` bigint DEFAULT NULL, `distribution_center_transit` bigint DEFAULT NULL, `service_center_stock` bigint DEFAULT NULL, `service_center_transit` bigint DEFAULT NULL, `return_good_stock` bigint DEFAULT NULL, `period_1` varchar(20) DEFAULT NULL, `period_2` varchar(20) DEFAULT NULL, `period_3` varchar(20) DEFAULT NULL, `period_4` varchar(20) DEFAULT NULL, `period_5` varchar(20) DEFAULT NULL, `value_1` varchar(20) DEFAULT NULL, `value_2` varchar(20) DEFAULT NULL, `value_3` varchar(20) DEFAULT NULL, `value_4` varchar(20) DEFAULT NULL, `value_5` varchar(20) DEFAULT NULL, `analysis_start_period` varchar(20) DEFAULT NULL, `analysis_end_period` varchar(20) DEFAULT NULL, `integration_task_no` varchar(120) DEFAULT NULL, `source_plan_entity_code` varchar(40) DEFAULT NULL, `test_completion_date` varchar(20) DEFAULT NULL, `source_test_number` varchar(40) DEFAULT NULL, `monitoring_month` date DEFAULT NULL, `analysis_task_type` varchar(1) DEFAULT NULL, `return_good_transit` bigint DEFAULT NULL, `summary_entity_stock` bigint DEFAULT NULL, `product_status` varchar(64) DEFAULT NULL, `peak_consumption_flag` varchar(1) DEFAULT NULL, `actual_consumption_current` bigint DEFAULT NULL, `actual_consumption_previous` bigint DEFAULT NULL, `total_actual_consumption` bigint DEFAULT NULL, `historical_test_consumption` bigint DEFAULT NULL, `regional_forecast` bigint DEFAULT NULL, `current_region_application` bigint DEFAULT NULL, `current_region_consumption` bigint DEFAULT NULL, `future_test_consumption` bigint DEFAULT NULL, `adjusted_regional` bigint DEFAULT NULL, `regional_adjustment_reason` varchar(240) DEFAULT NULL, `product_adjustment_reason` varchar(240) DEFAULT NULL, `regional_risk_assessment_reason` varchar(240) DEFAULT NULL, `product_risk_assessment_reason` varchar(240) DEFAULT NULL, `business_decision` varchar(240) DEFAULT NULL, `product_line_remaining_application` bigint DEFAULT NULL, `product_line_remaining_consumption` bigint DEFAULT NULL, `overall_adjusted` bigint DEFAULT NULL, `inventory_risk_quantity` bigint DEFAULT NULL, `monthly_consumption_risk_reason` varchar(240) DEFAULT NULL, `total_consumption_deviation_risk_reason` varchar(240) DEFAULT NULL, `new_forecast_consumption_risk_reason` varchar(240) DEFAULT NULL, `test_investment_ratio` decimal(10, 2) DEFAULT NULL, `test_peak_period` int DEFAULT NULL, `actual_peak_period` int DEFAULT NULL, `peak_period_deviation` int DEFAULT NULL, `peak_period_activation_rate` decimal(18, 4) DEFAULT NULL, `memory_spec` varchar(100) DEFAULT NULL, `material_type` varchar(100) DEFAULT NULL, `actual_vs_forecast_comparison` decimal(18, 2) DEFAULT NULL, `forecast_comparison_ratio` decimal(18, 2) DEFAULT NULL, `adjusted_model_future_consumption` bigint DEFAULT NULL, `adopted_model_future_consumption` bigint DEFAULT NULL, `color_en` varchar(120) DEFAULT NULL, `color_cn` varchar(120) DEFAULT NULL, PRIMARY KEY (`analysis_id`) ) ENGINE = InnoDB; 为了复现问题，我们再为这张表 Mock 10 条数据： INSERT INTO `test_analysis_result` VALUES (user001, N, 2023-05-15 10:30:00, 2023-05-15, user001 , ID000001, Initial record, 1001, 1001, 2023-05-15 10:00:00 , 1, LTB001, LTB-2023-001, CN1, 中国区 , China Region, ITEM001, 2023-01-10, 1, 2023-05-10 , MODEL1, ACTIVE, 3, SPDT001, 产品系列1 , Product Series 1, PS001, 产品系列1, Product Series 1, Marketing Name 1 , MAT001, 1000.00, 150.00, 产品描述中文1, Product Description 1 , 2023-03-01, 24, 12, 500, 1000 , 200, 300, 100, 50, 150 , 30, 70, 2023-06, 2023-07, 2023-08 , 2023-09, 2023-10, 100, 150, 200 , 180, 160, 2023-05, 2023-10, API001 , CN1, 2023-05-20, LTB-2023-001, 2023-05-01, 1 , 40, 250, New, Y, 120 , 150, 270, 400, 600, 200 , 180, 800, 650, Regional adjustment reason 1, Product adjustment reason 1 , Regional risk reason 1, Product risk reason 1, TRADE1, 300, 280 , 700, 50, Monthly risk reason 1, Total deviation reason 1, New predict reason 1 , 0.75, 18, 16, 2, 88.8889 , 16GB, Aluminum, 95.00, 105.00, 750 , 800, Black, 黑色), (user002, N, 2023-05-16 11:30:00, 2023-05-15, user002 , ID000002, Second record, 1002, 1002, 2023-05-16 11:00:00 , 1, LTB001, LTB-2023-001, Overseas_Regional_Summary, 海外区域汇总 , Overseas Regional Summary, ITEM001, 2023-01-10, 1, 2023-05-10 , MODEL1, ACTIVE, 3, SPDT001, 产品系列1 , Product Series 1, PS001, 产品系列1, Product Series 1, Marketing Name 1 , MAT001, 1000.00, 150.00, 产品描述中文1, Product Description 1 , 2023-03-01, 24, 12, 500, 1000 , 200, 300, 100, 50, 150 , 30, 70, 2023-06, 2023-07, 2023-08 , 2023-09, 2023-10, 100, 150, 200 , 180, 160, 2023-05, 2023-10, API001 , CN1, 2023-05-20, LTB-2023-001, 2023-05-01, 1 , 40, 250, New, Y, 120 , 150, 270, 400, 600, 200 , 180, 800, 650, Regional adjustment reason 1, Product adjustment reason 1 , Regional risk reason 1, Product risk reason 1, TRADE1, 300, 280 , 700, 50, Monthly risk reason 1, Total deviation reason 1, New predict reason 1 , 0.75, 18, 16, 2, 88.8889 , 16GB, Aluminum, 95.00, 105.00, 750 , 800, Black, 黑色), (user003, N, 2023-05-17 12:30:00, 2023-05-15, user003 , ID000003, Third record, 1003, 1003, 2023-05-17 12:00:00 , 1, LTB001, LTB-2023-001, Global_Summary, 全球汇总 , Global Summary, ITEM001, 2023-01-10, 1, 2023-05-10 , MODEL1, ACTIVE, 3, SPDT001, 产品系列1 , Product Series 1, PS001, 产品系列1, Product Series 1, Marketing Name 1 , MAT001, 1000.00, 150.00, 产品描述中文1, Product Description 1 , 2023-03-01, 24, 12, 500, 1000 , 200, 300, 100, 50, 150 , 30, 70, 2023-06, 2023-07, 2023-08 , 2023-09, 2023-10, 100, 150, 200 , 180, 160, 2023-05, 2023-10, API001 , CN1, 2023-05-20, LTB-2023-001, 2023-05-01, 1 , 40, 250, New, Y, 120 , 150, 270, 400, 600, 200 , 180, 800, 650, Regional adjustment reason 1, Product adjustment reason 1 , Regional risk reason 1, Product risk reason 1, TRADE1, 300, 280 , 700, 50, Monthly risk reason 1, Total deviation reason 1, New predict reason 1 , 0.75, 18, 16, 2, 88.8889 , 16GB, Aluminum, 95.00, 105.00, 750 , 800, Black, 黑色), (user004, N, 2023-05-18 13:30:00, 2023-05-15, user004 , ID000004, Fourth record, 1004, 1004, 2023-05-18 13:00:00 , 1, LTB001, LTB-2023-001, CN1, 中国区 , China Region, ITEM002, 2023-01-15, 1, 2023-05-12 , MODEL2, ACTIVE, 3, SPDT002, 产品系列2 , Product Series 2, PS002, 产品系列2, Product Series 2, Marketing Name 2 , MAT002, 1200.00, 180.00, 产品描述中文2, Product Description 2 , 2023-03-15, 36, 18, 600, 1200 , 250, 350, 120, 60, 180 , 40, 90, 2023-06, 2023-07, 2023-08 , 2023-09, 2023-10, 120, 160, 210 , 190, 170, 2023-05, 2023-10, API002 , CN1, 2023-05-22, LTB-2023-001, 2023-05-01, 1 , 50, 300, Mature, Y, 140 , 170, 310, 500, 700, 250 , 220, 900, 750, Regional adjustment reason 2, Product adjustment reason 2 , Regional risk reason 2, Product risk reason 2, TRADE2, 350, 320 , 800, 60, Monthly risk reason 2, Total deviation reason 2, New predict reason 2 , 0.80, 20, 18, 2, 90.0000 , 32GB, Plastic, 98.00, 108.00, 850 , 900, Silver, 银色), (user005, N, 2023-05-19 14:30:00, 2023-05-15, user005 , ID000005, Fifth record, 1005, 1005, 2023-05-19 14:00:00 , 1, LTB001, LTB-2023-001, Overseas_Regional_Summary, 海外区域汇总 , Overseas Regional Summary, ITEM002, 2023-01-15, 1, 2023-05-12 , MODEL2, ACTIVE, 3, SPDT002, 产品系列2 , Product Series 2, PS002, 产品系列2, Product Series 2, Marketing Name 2 , MAT002, 1200.00, 180.00, 产品描述中文2, Product Description 2 , 2023-03-15, 36, 18, 600, 1200 , 250, 350, 120, 60, 180 , 40, 90, 2023-06, 2023-07, 2023-08 , 2023-09, 2023-10, 120, 160, 210 , 190, 170, 2023-05, 2023-10, API002 , CN1, 2023-05-22, LTB-2023-001, 2023-05-01, 1 , 50, 300, Mature, Y, 140 , 170, 310, 500, 700, 250 , 220, 900, 750, Regional adjustment reason 2, Product adjustment reason 2 , Regional risk reason 2, Product risk reason 2, TRADE2, 350, 320 , 800, 60, Monthly risk reason 2, Total deviation reason 2, New predict reason 2 , 0.80, 20, 18, 2, 90.0000 , 32GB, Plastic, 98.00, 108.00, 850 , 900, Silver, 银色), (user006, N, 2023-05-20 15:30:00, 2023-05-15, user006 , ID000006, Sixth record, 1006, 1006, 2023-05-20 15:00:00 , 1, LTB001, LTB-2023-001, Global_Summary, 全球汇总 , Global Summary, ITEM002, 2023-01-15, 1, 2023-05-12 , MODEL2, ACTIVE, 3, SPDT002, 产品系列2 , Product Series 2, PS002, 产品系列2, Product Series 2, Marketing Name 2 , MAT002, 1200.00, 180.00, 产品描述中文2, Product Description 2 , 2023-03-15, 36, 18, 600, 1200 , 250, 350, 120, 60, 180 , 40, 90, 2023-06, 2023-07, 2023-08 , 2023-09, 2023-10, 120, 160, 210 , 190, 170, 2023-05, 2023-10, API002 , CN1, 2023-05-22, LTB-2023-001, 2023-05-01, 1 , 50, 300, Mature, Y, 140 , 170, 310, 500, 700, 250 , 220, 900, 750, Regional adjustment reason 2, Product adjustment reason 2 , Regional risk reason 2, Product risk reason 2, TRADE2, 350, 320 , 800, 60, Monthly risk reason 2, Total deviation reason 2, New predict reason 2 , 0.80, 20, 18, 2, 90.0000 , 32GB, Plastic, 98.00, 108.00, 850 , 900, Silver, 银色), (user007, N, 2023-05-21 16:30:00, 2023-05-15, user007 , ID000007, Seventh record, 1007, 1007, 2023-05-21 16:00:00 , 1, LTB002, LTB-2023-002, CN1, 中国区 , China Region, ITEM003, 2023-01-20, 1, 2023-05-14 , MODEL3, ACTIVE, 3, SPDT003, 产品系列3 , Product Series 3, PS003, 产品系列3, Product Series 3, Marketing Name 3 , MAT003, 1500.00, 220.00, 产品描述中文3, Product Description 3 , 2023-04-01, 24, 12, 700, 1400 , 300, 400, 150, 70, 200 , 50, 100, 2023-06, 2023-07, 2023-08 , 2023-09, 2023-10, 140, 180, 230 , 210, 190, 2023-05, 2023-10, API003 , CN1, 2023-05-24, LTB-2023-002, 2023-05-01, 1 , 60, 350, EOL, N, 160 , 190, 350, 600, 800, 300 , 260, 1000, 850, Regional adjustment reason 3, Product adjustment reason 3 , Regional risk reason 3, Product risk reason 3, TRADE3, 400, 360 , 900, 70, Monthly risk reason 3, Total deviation reason 3, New predict reason 3 , 0.85, 22, 20, 2, 90.9091 , 64GB, Glass, 102.00, 112.00, 950 , 1000, Gold, 金色), (user008, N, 2023-05-22 17:30:00, 2023-05-15, user008 , ID000008, Eighth record, 1008, 1008, 2023-05-22 17:00:00 , 1, LTB002, LTB-2023-002, Overseas_Regional_Summary, 海外区域汇总 , Overseas Regional Summary, ITEM003, 2023-01-20, 1, 2023-05-14 , MODEL3, ACTIVE, 3, SPDT003, 产品系列3 , Product Series 3, PS003, 产品系列3, Product Series 3, Marketing Name 3 , MAT003, 1500.00, 220.00, 产品描述中文3, Product Description 3 , 2023-04-01, 24, 12, 700, 1400 , 300, 400, 150, 70, 200 , 50, 100, 2023-06, 2023-07, 2023-08 , 2023-09, 2023-10, 140, 180, 230 , 210, 190, 2023-05, 2023-10, API003 , CN1, 2023-05-24, LTB-2023-002, 2023-05-01, 1 , 60, 350, EOL, N, 160 , 190, 350, 600, 800, 300 , 260, 1000, 850, Regional adjustment reason 3, Product adjustment reason 3 , Regional risk reason 3, Product risk reason 3, TRADE3, 400, 360 , 900, 70, Monthly risk reason 3, Total deviation reason 3, New predict reason 3 , 0.85, 22, 20, 2, 90.9091 , 64GB, Glass, 102.00, 112.00, 950 , 1000, Gold, 金色), (user009, N, 2023-05-23 18:30:00, 2023-05-15, user009 , ID000009, Ninth record, 1009, 1009, 2023-05-23 18:00:00 , 1, LTB002, LTB-2023-002, Global_Summary, 全球汇总 , Global Summary, ITEM003, 2023-01-20, 1, 2023-05-14 , MODEL3, ACTIVE, 3, SPDT003, 产品系列3 , Product Series 3, PS003, 产品系列3, Product Series 3, Marketing Name 3 , MAT003, 1500.00, 220.00, 产品描述中文3, Product Description 3 , 2023-04-01, 24, 12, 700, 1400 , 300, 400, 150, 70, 200 , 50, 100, 2023-06, 2023-07, 2023-08 , 2023-09, 2023-10, 140, 180, 230 , 210, 190, 2023-05, 2023-10, API003 , CN1, 2023-05-24, LTB-2023-002, 2023-05-01, 1 , 60, 350, EOL, N, 160 , 190, 350, 600, 800, 300 , 260, 1000, 850, Regional adjustment reason 3, Product adjustment reason 3 , Regional risk reason 3, Product risk reason 3, TRADE3, 400, 360 , 900, 70, Monthly risk reason 3, Total deviation reason 3, New predict reason 3 , 0.85, 22, 20, 2, 90.9091 , 64GB, Glass, 102.00, 112.00, 950 , 1000, Gold, 金色), (user010, N, 2023-05-24 19:30:00, 2023-05-15, user010 , ID000010, Tenth record, 1010, 1010, 2023-05-24 19:00:00 , 1, LTB003, LTB-2023-003, CN1, 中国区 , China Region, ITEM004, 2023-02-01, 1, 2023-05-16 , MODEL1, ACTIVE, 3, SPDT004, 产品系列4 , Product Series 4, PS004, 产品系列4, Product Series 4, Marketing Name 4 , MAT004, 1800.00, 260.00, 产品描述中文4, Product Description 4 , 2023-04-15, 36, 18, 800, 1600 , 350, 450, 180, 80, 220 , 60, 120, 2023-06, 2023-07, 2023-08 , 2023-09, 2023-10, 160, 200, 250 , 230, 210, 2023-05, 2023-10, API004 , CN1, 2023-05-26, LTB-2023-003, 2023-05-01, 1 , 70, 400, New, Y, 180 , 210, 390, 700, 900, 350 , 300, 1100, 950, Regional adjustment reason 4, Product adjustment reason 4 , Regional risk reason 4, Product risk reason 4, TRADE4, 450, 400 , 1000, 80, Monthly risk reason 4, Total deviation reason 4, New predict reason 4 , 0.90, 24, 22, 2, 91.6667 , 128GB, Ceramic, 105.00, 115.00, 1050 , 1100, Blue, 蓝色); 在 Proxy 上执行完成后，我们同样在 MySQL 上创建相同的单表，并初始化下相同的数据。然后我们在 Proxy 和 MySQL 上执行如下的查询 SQL，对比返回的结果差异。 SELECT plan_entity_code\t, CONCAT(GROUP_CONCAT(DISTINCT CONCAT(source_plan_entity_code, #, product_line_code, #, product_series_code, #, marketing_name, #, material_category_code, #, product_status, #, adjusted_regional) SEPARATOR ,), MAX(adjusted_regional)) AS regional_adjustment_summaryFROM test_analysis_result aWHERE 1 = 1GROUP BY test_number, plan_entity_code, product_line_code, product_series_code, marketing_name, material_category_code, product_status, calculation_method, warranty_typeORDER BY test_number ASC, CASE plan_entity_code\tWHEN CN1 THEN 1\tWHEN Overseas_Regional_Summary THEN 2\tWHEN Global_Summary THEN 3END ASC, product_line_code ASC, product_series_code ASC, marketing_name ASC, material_category_code ASC, CASE WHEN product_status IS NULL THEN 0\tWHEN product_status = New THEN 1\tWHEN product_status = Mature THEN 2\tWHEN product_status = EOL THEN 3END ASC; 如下是通过 Proxy 执行返回的结果，可以看到第二列 CONCAT 函数拼接返回结果为 NULL，这不符合预期，因为我们 Mock 的数据中，这些列都有值。 我们再通过 MySQL 执行相同的 SQL 语句，可以看到 CONCAT 函数拼接结果应该是有值的，Proxy 联邦查询的执行逻辑存在 BUG。 确定问题后，我们通过 EXPLAIN 语句来观察下联邦查询的执行计划，可以看到执行计划中包含了 2 次 DBPlusEngineHashAggregate，最底层的 DBPlusEngineHashAggregate 中除了常见的 group 分组条件外，还包含了 groups 和 GROUPING 函数，这块之前联邦查询并未进行兼容实现。 mysql EXPLAIN SELECT plan_entity_code - , CONCAT(GROUP_CONCAT(DISTINCT CONCAT(source_plan_entity_code, #, product_line_code, #, product_series_code, #, marketing_name, #, material_category_code, #, product_status, #, adjusted_regional) SEPARATOR ,), MAX(adjusted_regional)) AS regional_adjustment_summary - FROM test_analysis_result a - WHERE 1 = 1 - GROUP BY test_number, plan_entity_code, product_line_code, product_series_code, marketing_name, material_category_code, product_status, calculation_method, warranty_type - ORDER BY test_number ASC, CASE plan_entity_code - WHEN CN1 THEN 1 - WHEN Overseas_Regional_Summary THEN 2 - WHEN Global_Summary THEN 3 - END ASC, product_line_code ASC, product_series_code ASC, marketing_name ASC, material_category_code ASC, CASE - WHEN product_status IS NULL THEN 0 - WHEN product_status = New THEN 1 - WHEN product_status = Mature THEN 2 - WHEN product_status = EOL THEN 3 - END ASC;+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+| PLAN |+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+| DBPlusEngineSort(sort0=[$2], sort1=[$3], sort2=[$4], sort3=[$5], sort4=[$6], sort5=[$7], sort6=[$8], dir0=[ASC], dir1=[ASC], dir2=[ASC], dir3=[ASC], dir4=[ASC], dir5=[ASC], dir6=[ASC]) || DBPlusEngineCalc(expr#0..10=[inputs], expr#11=[CAST($t10):VARCHAR CHARACTER SET UTF-8], expr#12=[CONCAT($t9, $t11)], expr#13=[_UTF-8CN1:VARCHAR(40) CHARACTER SET UTF-8], expr#14=[=($t1, $t13)], expr#15=[_UTF-81:VARCHAR CHARACTER SET UTF-8], expr#16=[_UTF-8Overseas_Regional_Summary:VARCHAR(40) CHARACTER SET UTF-8], expr#17=[=($t1, $t16)], expr#18=[_UTF-82:VARCHAR CHARACTER SET UTF-8], expr#19=[_UTF-8Global_Summary:VARCHAR(40) CHARACTER SET UTF-8], expr#20=[=($t1, $t19)], expr#21=[_UTF-83:VARCHAR CHARACTER SET UTF-8], expr#22=[_UTF-8NULL:VARCHAR CHARACTER SET UTF-8], expr#23=[CASE($t14, $t15, $t17, $t18, $t20, $t21, $t22)], expr#24=[IS NULL($t6)], expr#25=[_UTF-80:VARCHAR CHARACTER SET UTF-8], expr#26=[_UTF-8New:VARCHAR(64) CHARACTER SET UTF-8], expr#27=[=($t6, $t26)], expr#28=[_UTF-8Mature:VARCHAR(64) CHARACTER SET UTF-8], expr#29=[=($t6, $t28)], expr#30=[_UTF-8EOL:VARCHAR(64) CHARACTER SET UTF-8], expr#31=[=($t6, $t30)], expr#32=[CASE($t24, $t25, $t27, $t15, $t29, $t18, $t31, $t21, $t22)], plan_entity_code=[$t1], regional_adjustment_summary=[$t12], test_number=[$t0], EXPR$3=[$t23], product_line_code=[$t2], product_series_code=[$t3], marketing_name=[$t4], material_category_code=[$t5], EXPR$8=[$t32]) || DBPlusEngineHashAggregate(group=[0, 1, 2, 3, 4, 5, 6, 7, 8], agg#0=[LISTAGG($9) FILTER $11], agg#1=[MIN($10) FILTER $12]) || DBPlusEngineCalc(expr#0..11=[inputs], expr#12=[0], expr#13=[=($t11, $t12)], expr#14=[1], expr#15=[=($t11, $t14)], proj#0..10=[exprs], $g_0=[$t13], $g_1=[$t15]) || DBPlusEngineHashAggregate(group=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9], groups=[[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4, 5, 6, 7, 8]], $f10=[MAX($10)], $g=[GROUPING($0, $1, $2, $3, $4, $5, $6, $7, $8, $9)]) || DBPlusEngineScan(table=[[sphereex_db_tbl_sql_federation_honor, test_analysis_result]], sql=[SELECT `test_number`, `plan_entity_code`, `product_line_code`, `product_series_code`, `marketing_name`, `material_category_code`, `product_status`, `calculation_method`, `warranty_type`, CONCAT(`source_plan_entity_code`, #, `product_line_code`, #, `product_series_code`, #, `marketing_name`, #, `material_category_code`, #, `product_status`, #, CAST(`adjusted_regional` AS CHAR)) AS `$f9`, `adjusted_regional` FROM `sphereex_db_tbl_sql_federation_honor`.`test_analysis_result`]) |+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+6 rows in set (1.60 sec) 为了排查具体是哪个运算符执行时数据出错，笔者尝试在执行器 current 方法中打印出数据行。首先观察最底层的 DBPlusEngineHashAggregate 执行结果，可以看到，除了返回分组条件 group=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9 对应的值之外，数据行中还额外包含了 2 列，分别对应执行计划中的 $f10=[MAX($10)] 和 $g=[GROUPING($0, $1, $2, $3, $4, $5, $6, $7, $8, $9)]。 DBPlusEngineHashAggregateExecutor: LTB-2023-001, Global_Summary, SPDT002, PS002, Marketing Name 2, MAT002, Mature, MODEL2, ACTIVE, CN1#SPDT002#PS002#Marketing Name 2#MAT002#Mature#750, 750, 1023DBPlusEngineHashAggregateExecutor: LTB-2023-002, CN1, SPDT003, PS003, Marketing Name 3, MAT003, EOL, MODEL3, ACTIVE, CN1#SPDT003#PS003#Marketing Name 3#MAT003#EOL#850, 850, 1023DBPlusEngineHashAggregateExecutor: LTB-2023-001, CN1, SPDT002, PS002, Marketing Name 2, MAT002, Mature, MODEL2, ACTIVE, CN1#SPDT002#PS002#Marketing Name 2#MAT002#Mature#750, 750, 1023DBPlusEngineHashAggregateExecutor: LTB-2023-003, CN1, SPDT004, PS004, Marketing Name 4, MAT004, New, MODEL1, ACTIVE, CN1#SPDT004#PS004#Marketing Name 4#MAT004#New#950, 950, 1023DBPlusEngineHashAggregateExecutor: LTB-2023-002, Global_Summary, SPDT003, PS003, Marketing Name 3, MAT003, EOL, MODEL3, ACTIVE, CN1#SPDT003#PS003#Marketing Name 3#MAT003#EOL#850, 850, 1023DBPlusEngineHashAggregateExecutor: LTB-2023-001, Overseas_Regional_Summary, SPDT002, PS002, Marketing Name 2, MAT002, Mature, MODEL2, ACTIVE, CN1#SPDT002#PS002#Marketing Name 2#MAT002#Mature#750, 750, 1023DBPlusEngineHashAggregateExecutor: LTB-2023-001, Overseas_Regional_Summary, SPDT001, PS001, Marketing Name 1, MAT001, New, MODEL1, ACTIVE, CN1#SPDT001#PS001#Marketing Name 1#MAT001#New#650, 650, 1023DBPlusEngineHashAggregateExecutor: LTB-2023-002, Overseas_Regional_Summary, SPDT003, PS003, Marketing Name 3, MAT003, EOL, MODEL3, ACTIVE, CN1#SPDT003#PS003#Marketing Name 3#MAT003#EOL#850, 850, 1023DBPlusEngineHashAggregateExecutor: LTB-2023-001, Global_Summary, SPDT001, PS001, Marketing Name 1, MAT001, New, MODEL1, ACTIVE, CN1#SPDT001#PS001#Marketing Name 1#MAT001#New#650, 650, 1023DBPlusEngineHashAggregateExecutor: LTB-2023-001, CN1, SPDT001, PS001, Marketing Name 1, MAT001, New, MODEL1, ACTIVE, CN1#SPDT001#PS001#Marketing Name 1#MAT001#New#650, 650, 1023 MAX 函数没什么特别，GROUPING 具体是用来做什么的呢？从网上查阅了一些资料，GROUPING 函数是用于在 GROUPING SETS 多维度分组中标识哪些列被聚合（即不在当前分组中），GROUPING 函数返回一个位掩码，其中每位对应了一个分组列，若该列被聚合（即不在当前分组中），则位值为 1，否则为 0。按照 GROUPING 函数的定义，当前分组为 group=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]，所有列都在分组中，应当全部为 0，而数据行中返回的则是 1023，显然是 GROUPING 函数的计算逻辑出错了。 (adsbygoogle = window.adsbygoogle || []).push({}); 赞助商 问题解决 搞清楚问题后，我们尝试修改 GroupingAggregateFunctionEvaluator 计算逻辑，如下图所示，左侧逻辑是之前参考 Calcite 内置的 GroupingImplementor 执行逻辑实现的，该逻辑似乎和 GROUPING 函数的语义相反，如果当前列不被聚合（即在当前分组中），则位值为 1，否则为 0。我们暂且先不深究 Calcite 的实现逻辑，按照 GROUPING 函数语义，笔者对函数逻辑进行了修改，严格按照函数语义实现，只有当该列被聚合（即不在当前分组中），才将当前位赋值为 1。 修改完成后，通过 IDEA Debug 观察 GROUPING 函数的计算结果，可以看到这次得到了符合预期的结果 0。 不过意外情况又出现了，修改完 GROUPING 函数逻辑后，SQL 的执行结果仍然不正确。 问题又是出在哪里呢？无奈继续观察前面输出的数据行日志，排查 DBPlusEngineHashAggregate 上层的 DBPlusEngineCalc 执行结果，如下是具体的数据： DBPlusEngineCalcExecutor: LTB-2023-001, Overseas_Regional_Summary, SPDT001, PS001, Marketing Name 1, MAT001, New, MODEL1, ACTIVE, CN1#SPDT001#PS001#Marketing Name 1#MAT001#New#650, 650, true, falseDBPlusEngineCalcExecutor: LTB-2023-003, CN1, SPDT004, PS004, Marketing Name 4, MAT004, New, MODEL1, ACTIVE, CN1#SPDT004#PS004#Marketing Name 4#MAT004#New#950, 950, true, falseDBPlusEngineCalcExecutor: LTB-2023-002, Overseas_Regional_Summary, SPDT003, PS003, Marketing Name 3, MAT003, EOL, MODEL3, ACTIVE, CN1#SPDT003#PS003#Marketing Name 3#MAT003#EOL#850, 850, true, falseDBPlusEngineCalcExecutor: LTB-2023-001, Global_Summary, SPDT002, PS002, Marketing Name 2, MAT002, Mature, MODEL2, ACTIVE, CN1#SPDT002#PS002#Marketing Name 2#MAT002#Mature#750, 750, true, falseDBPlusEngineCalcExecutor: LTB-2023-001, Global_Summary, SPDT001, PS001, Marketing Name 1, MAT001, New, MODEL1, ACTIVE, CN1#SPDT001#PS001#Marketing Name 1#MAT001#New#650, 650, true, falseDBPlusEngineCalcExecutor: LTB-2023-002, Global_Summary, SPDT003, PS003, Marketing Name 3, MAT003, EOL, MODEL3, ACTIVE, CN1#SPDT003#PS003#Marketing Name 3#MAT003#EOL#850, 850, true, falseDBPlusEngineCalcExecutor: LTB-2023-001, Overseas_Regional_Summary, SPDT002, PS002, Marketing Name 2, MAT002, Mature, MODEL2, ACTIVE, CN1#SPDT002#PS002#Marketing Name 2#MAT002#Mature#750, 750, true, falseDBPlusEngineCalcExecutor: LTB-2023-002, CN1, SPDT003, PS003, Marketing Name 3, MAT003, EOL, MODEL3, ACTIVE, CN1#SPDT003#PS003#Marketing Name 3#MAT003#EOL#850, 850, true, falseDBPlusEngineCalcExecutor: LTB-2023-001, CN1, SPDT001, PS001, Marketing Name 1, MAT001, New, MODEL1, ACTIVE, CN1#SPDT001#PS001#Marketing Name 1#MAT001#New#650, 650, true, falseDBPlusEngineCalcExecutor: LTB-2023-001, CN1, SPDT002, PS002, Marketing Name 2, MAT002, Mature, MODEL2, ACTIVE, CN1#SPDT002#PS002#Marketing Name 2#MAT002#Mature#750, 750, true, false 可以看到最后 2 列的值固定为 true 和 false，而这 2 列对应的是执行计划中（如下图）的 expr#13=[=($t11, $t12)]（即：$t11 = 0），以及 expr#15=[=($t11, $t14)]（即：$t11 = 1），由于 DBPlusEngineHashAggregate 返回的结果都是 0，因此这 2 列计算结果为 true 和 false。DBPlusEngineCalc 上层的 DBPlusEngineHashAggregate 会根据这 2 列来过滤数据，由于 MIN($10) FILTER $12 对应的状态都是 false，因此导致 SQL 聚合列为 NULL。 为什么 DBPlusEngineCalc 中的 expr#13=[=($t11, $t12)] 和 expr#15=[=($t11, $t14)] 会引用相同的列 $t11 呢？笔者回想到最开始实现 GROUPING 函数时，参考了 Calcite 内置的 GroupingImplementor，既然已经提供了函数实现，Calcite 大概率是支持 GROUPING 函数的。 为了彻底搞清楚 GROUPING 函数的含义，笔者切换到 ShardingSphere 开源版联邦查询功能进行测试，由于 Calcite 内置的执行器是通过 Linq4j 生成并使用 Janino 编译，如果想要调试这部分逻辑，需要在 JVM 参数中增加如下内容。 -Dorg.codehaus.janino.source_debugging.enable=true-Dorg.codehaus.janino.source_debugging.dir=/Users/duanzhengqiang/softs/calcite/janio 首先，我们查看这条 SQL 在开源版联邦查询中的执行计划，除了物理运算符的类型不同外，执行计划整体上都是相同的，执行计划中也包含了 GROUPING 函数，以及多个 groups 分组。 EnumerableSort(sort0=[$2], sort1=[$3], sort2=[$4], sort3=[$5], sort4=[$6], sort5=[$7], sort6=[$8], dir0=[ASC], dir1=[ASC], dir2=[ASC], dir3=[ASC], dir4=[ASC], dir5=[ASC], dir6=[ASC]) EnumerableCalc(expr#0..10=[inputs], expr#11=[CAST($t10):VARCHAR CHARACTER SET UTF-8], expr#12=[CONCAT($t9, $t11)], expr#13=[CAST($t1):VARCHAR CHARACTER SET UTF-8], expr#14=[_UTF-8CN1:VARCHAR CHARACTER SET UTF-8], expr#15=[=($t13, $t14)], expr#16=[_UTF-81:VARCHAR CHARACTER SET UTF-8], expr#17=[_UTF-8Overseas_Regional_Summary:VARCHAR CHARACTER SET UTF-8], expr#18=[=($t13, $t17)], expr#19=[_UTF-82:VARCHAR CHARACTER SET UTF-8], expr#20=[_UTF-8Global_Summary:VARCHAR CHARACTER SET UTF-8], expr#21=[=($t13, $t20)], expr#22=[_UTF-83:VARCHAR CHARACTER SET UTF-8], expr#23=[_UTF-8NULL:VARCHAR CHARACTER SET UTF-8], expr#24=[CASE($t15, $t16, $t18, $t19, $t21, $t22, $t23)], expr#25=[IS NULL($t6)], expr#26=[_UTF-80:VARCHAR CHARACTER SET UTF-8], expr#27=[CAST($t6):VARCHAR CHARACTER SET UTF-8], expr#28=[_UTF-8New:VARCHAR CHARACTER SET UTF-8], expr#29=[=($t27, $t28)], expr#30=[_UTF-8Mature:VARCHAR CHARACTER SET UTF-8], expr#31=[=($t27, $t30)], expr#32=[_UTF-8EOL:VARCHAR CHARACTER SET UTF-8], expr#33=[=($t27, $t32)], expr#34=[CASE($t25, $t26, $t29, $t16, $t31, $t19, $t33, $t22, $t23)], plan_entity_code=[$t1], regional_adjustment_summary=[$t12], test_number=[$t0], EXPR$3=[$t24], product_line_code=[$t2], product_series_code=[$t3], marketing_name=[$t4], material_category_code=[$t5], EXPR$8=[$t34]) EnumerableAggregate(group=[0, 1, 2, 3, 4, 5, 6, 7, 8], agg#0=[LISTAGG($9) FILTER $11], agg#1=[MIN($10) FILTER $12]) EnumerableCalc(expr#0..11=[inputs], expr#12=[0], expr#13=[=($t11, $t12)], expr#14=[1], expr#15=[=($t11, $t14)], proj#0..10=[exprs], $g_0=[$t13], $g_1=[$t15]) EnumerableAggregate(group=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9], groups=[[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4, 5, 6, 7, 8]], $f10=[MAX($10)], $g=[GROUPING($0, $1, $2, $3, $4, $5, $6, $7, $8, $9)]) EnumerableScan(table=[[sphereex_db_tbl_sql_federation_honor, test_analysis_result]], sql=[SELECT `test_number`, `plan_entity_code`, `product_line_code`, `product_series_code`, `marketing_name`, `material_category_code`, `product_status`, `calculation_method`, `warranty_type`, CONCAT(`source_plan_entity_code`, #, `product_line_code`, #, `product_series_code`, #, `marketing_name`, #, `material_category_code`, #, `product_status`, #, CAST(`adjusted_regional` AS CHAR)) AS `$f9`, `adjusted_regional` FROM `sphereex_db_tbl_sql_federation_honor`.`test_analysis_result`], dynamicParameters=[null]) 另外，在 Calcite 生成的执行代码中，最底层的 EnumerableAggregate 运算符对应的执行方法是 EnumerableDefaults#groupByMultiple，该方法第一个参数是 enumerable，第二个参数是 keySelectors，生成代码包含了 2 个 keySelector，第一个 keySelector 包含了 0-9 列，第二个 keySelector 包含了 0-8 列，第 9 列为 null。每一个 keySelector 后面还追加了一串 true、false 的状态值，应该是每个 groups 中分组的标记位，表示当前分组中包含哪些聚合列。 我们断点跟踪 groupByMultiple 方法，可以看到在遍历每行数据时，内部会再遍历 keySelectors，并将计算结果存储到 map 中， 因为我们原始数据共 10 行，经过 2 个 keySelectors 计算后共返回 20 行数据。 获取到 map 结果集后，Calcite 生成代码会继续调用 resultSelector 方法，最后一行就是在计算 GROUPING 函数结果，它从每行数据的第 10 列获取结果，判断状态是否为 true，为 true 则将当前位设置为 1（第 1 位为 512），依此类推计算下去，求出 GROUPING 函数结果。最终，20 行数据中，有 10 行计算结果为 0，另外 10 行计算结果为 1，再经过上层的 DBPlusEngineHashAggregate 过滤，最终 LISTAGG 和 MIN 都可以正确计算出结果。 到这里，我们终于搞清楚了 Calcite 中 GROUPING 函数的执行逻辑，结合以下的执行计划，GROUPING 函数主要使用在多维度分组中（本案例对应的是 groups=[[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4, 5, 6, 7, 8]] 中的 2 个维度），在聚合计算遍历每行数据时，会先按照主分组条件进行处理（即：group=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]），然后再按照子分组条件 groups 循环，分别生成每个 group 分组对应的数据行。需要注意：如果当前子分组，相对于主分组缺少了部分列，则该数据行对应列需要设置为 null，避免影响结果正确性。生成所有子分组的数据行后，再进行 GROUPING 聚合计算，此时计算 BIT 位时，需要根据子分组 childGroupSet 计算函数结果，不同的子分组会生成不同的 GROUPING 函数值。 +---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+| PLAN |+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+| DBPlusEngineSort(sort0=[$2], sort1=[$3], sort2=[$4], sort3=[$5], sort4=[$6], sort5=[$7], sort6=[$8], dir0=[ASC], dir1=[ASC], dir2=[ASC], dir3=[ASC], dir4=[ASC], dir5=[ASC], dir6=[ASC]) || DBPlusEngineCalc(expr#0..10=[inputs], expr#11=[CAST($t10):VARCHAR CHARACTER SET UTF-8], expr#12=[CONCAT($t9, $t11)], expr#13=[_UTF-8CN1:VARCHAR(40) CHARACTER SET UTF-8], expr#14=[=($t1, $t13)], expr#15=[_UTF-81:VARCHAR CHARACTER SET UTF-8], expr#16=[_UTF-8Overseas_Regional_Summary:VARCHAR(40) CHARACTER SET UTF-8], expr#17=[=($t1, $t16)], expr#18=[_UTF-82:VARCHAR CHARACTER SET UTF-8], expr#19=[_UTF-8Global_Summary:VARCHAR(40) CHARACTER SET UTF-8], expr#20=[=($t1, $t19)], expr#21=[_UTF-83:VARCHAR CHARACTER SET UTF-8], expr#22=[_UTF-8NULL:VARCHAR CHARACTER SET UTF-8], expr#23=[CASE($t14, $t15, $t17, $t18, $t20, $t21, $t22)], expr#24=[IS NULL($t6)], expr#25=[_UTF-80:VARCHAR CHARACTER SET UTF-8], expr#26=[_UTF-8New:VARCHAR(64) CHARACTER SET UTF-8], expr#27=[=($t6, $t26)], expr#28=[_UTF-8Mature:VARCHAR(64) CHARACTER SET UTF-8], expr#29=[=($t6, $t28)], expr#30=[_UTF-8EOL:VARCHAR(64) CHARACTER SET UTF-8], expr#31=[=($t6, $t30)], expr#32=[CASE($t24, $t25, $t27, $t15, $t29, $t18, $t31, $t21, $t22)], plan_entity_code=[$t1], regional_adjustment_summary=[$t12], test_number=[$t0], EXPR$3=[$t23], product_line_code=[$t2], product_series_code=[$t3], marketing_name=[$t4], material_category_code=[$t5], EXPR$8=[$t32]) || DBPlusEngineHashAggregate(group=[0, 1, 2, 3, 4, 5, 6, 7, 8], agg#0=[LISTAGG($9) FILTER $11], agg#1=[MIN($10) FILTER $12]) || DBPlusEngineCalc(expr#0..11=[inputs], expr#12=[0], expr#13=[=($t11, $t12)], expr#14=[1], expr#15=[=($t11, $t14)], proj#0..10=[exprs], $g_0=[$t13], $g_1=[$t15]) || DBPlusEngineHashAggregate(group=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9], groups=[[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4, 5, 6, 7, 8]], $f10=[MAX($10)], $g=[GROUPING($0, $1, $2, $3, $4, $5, $6, $7, $8, $9)]) || DBPlusEngineScan(table=[[sphereex_db_tbl_sql_federation_honor, test_analysis_result]], sql=[SELECT `test_number`, `plan_entity_code`, `product_line_code`, `product_series_code`, `marketing_name`, `material_category_code`, `product_status`, `calculation_method`, `warranty_type`, CONCAT(`source_plan_entity_code`, #, `product_line_code`, #, `product_series_code`, #, `marketing_name`, #, `material_category_code`, #, `product_status`, #, CAST(`adjusted_regional` AS CHAR)) AS `$f9`, `adjusted_regional` FROM `sphereex_db_tbl_sql_federation_honor`.`test_analysis_result`]) |+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+6 rows in set (1.60 sec) 笔者在联邦查询中对 GROUPING 函数的修改，并调整了聚合逻辑，聚合计算时会遍历 groups，为每个分组生成对应的数据行。 修改完成后，我们再次通过联邦查询执行业务 SQL，可以看到查询结果和 MySQL 返回的结果一致，终于搞定了 GROUPING 函数的适配。 结语 本文结合真实的客户案例，为大家详细介绍了联邦查询中 GROUPING 函数的问题，笔者从问题复现开始，一步步深入探究问题的根本原因，最终通过对 Calcite 生成代码的深入分析，正确理解了 GROUPING 函数的语义和执行逻辑，从而完善了联邦查询功能对 GROUPING 函数的支持。未来，在 GROUPING 函数的基础上，联邦查询引擎还可以进一步支持 GROUP BY WITH ROLLUP 等高级 SQL 用法。 另外，通过这个问题的调查，笔者再次感受到了 SQL 引擎开发的难度，开发者需要对 SQL 执行的每个细节都深入分析，做到刨根问底，一丝不苟，不能留有任何的疑问。除了开发者主观上的重视外，还需要引入更多的测试工具，对 SQL 语义进行持续测试，在 SQL 引擎测试领域，SQLacner 是一个不错的工具，笔者后续将探索使用 SQLacner 测试 ShardingSphere 联邦查询，不断提升联邦查询的 SQL 兼容度。 写在最后 笔者因为工作原因接触到 Calcite，前期学习过程中，深感 Calcite 学习资料之匮乏，因此创建了 Calcite 从入门到精通知识星球，希望能够将学习过程中的资料和经验沉淀下来，为更多想要学习 Calcite 的朋友提供一些帮助。 欢迎关注 欢迎关注「端小强的博客」微信公众号，会不定期分享日常学习和工作经验，欢迎大家关注交流。","tags":["Calcite"],"categories":["Calcite"]},{"title":"Apache Calcite Catalog 拾遗之类型系统实现","path":"/blog/apache-calcite-catalog-type-system-implementation.html","content":"注意：本文基于 Calcite main 分支 34989b0 版本源码进行学习研究，其他版本可能会存在实现逻辑差异，对源码感兴趣的读者请注意版本选择。 前言 在之前发布的深度探究 Apache Calcite SQL 校验器实现原理一文中，我们详细介绍了 Calcite 校验器的实现原理，在 SQL 校验的过程中，Calcite 会不断调用 deriveType 进行类型推断，当时由于篇幅的原因，我们在文章中没有进行过多介绍。今天，让我们继续刨根问底，专门从 Calcite 类型系统的角度，深入探究 Calcite 的类型体系，了解在校验过程时如何进行类型推导和类型转换的。 类型推导 类型转换 显示转换 隐式转换 结语 写在最后 笔者因为工作原因接触到 Calcite，前期学习过程中，深感 Calcite 学习资料之匮乏，因此创建了 Calcite 从入门到精通知识星球，希望能够将学习过程中的资料和经验沉淀下来，为更多想要学习 Calcite 的朋友提供一些帮助。 欢迎关注 欢迎关注「端小强的博客」微信公众号，会不定期分享日常学习和工作经验，欢迎大家关注交流。","tags":["Calcite","ShardingSphere"],"categories":["Calcite"]},{"title":"以 Calcite 为例探究 Join 算子的常用实现","path":"/blog/using-calcite-as-an-example-to-explore-the-common-implementation-of-join-operators.html","content":"TODO 写在最后 笔者因为工作原因接触到 Calcite，前期学习过程中，深感 Calcite 学习资料之匮乏，因此创建了 Calcite 从入门到精通知识星球，希望能够将学习过程中的资料和经验沉淀下来，为更多想要学习 Calcite 的朋友提供一些帮助。 欢迎关注 欢迎关注「端小强的博客」微信公众号，会不定期分享日常学习和工作经验，欢迎大家关注交流。","tags":["Calcite"],"categories":["Calcite"]},{"title":"Apache Calcite 在 MyCat2 中的实践探究","path":"/blog/explore-the-practice-of-apache-calcite-in-mycat2.html","content":"注意：本文基于 MyCat2 main 分支 ced134b 版本源码进行学习研究，其他版本可能会存在实现逻辑差异，对源码感兴趣的读者请注意版本选择。 前言 MyCat 是曾经较为流行的一款分库分表中间件，能够支持海量数据的水平分片，以及读写分离、分布式事务等功能。MyCat2 在原有功能的基础上增加了分布式查询引擎，该引擎基于 Calcite 项目实现，能够将 SQL 编译为关系代数表达式，并基于规则优化引擎和代价优化引擎，生成物理执行计划，实现对跨库、跨实例的分布式 SQL 的支持。虽然 MyCat 项目已经停止维护，但是分布式查询引擎功能仍然值得我们学习，本文将带领大家一起探索 Apache Calcite 在 MyCat2 中的实践，学习如何基于 Calcite 构建分布式查询引擎。 MyCat2 环境搭建 首先，我们需要本地启动 MyCat2 服务，参考入门 MyCat2，MyCat2 配置分为服务器配置和 Schema 配置。服务器配置 server.json 中可以指定 MyCat2 对外提供服务的 IP 和端口，serverVersion 用于模拟 MySQL 版本，此处我们将 serverVersion 调整为 8.0.40-mycat-2.0。 server: ip: 127.0.0.1, mycatId: 1, port: 8066, // 注意设置模拟的 MySQL 版本，与后端客户端版本对应 serverVersion: 8.0.40-mycat-2.0 Schema 对应了 MySQL 中的库，MyCat2 Schema 配置包含了库与表的配置，它是建立在集群的基础上，而集群则是建立在数据源的基础上。因此，我们在配置时，需要自下而上进行配置，先配置数据源，再加数据源构建为集群，然后在集群上配置库与表。 MyCat2 中将 Schema 划分为 2 类：原型库和业务库，原型库 prototype 用于支持 MySQL 的兼容性 SQL 和系统表 SQL，这些 SQL 通常是由客户端或者 DAO 框架请求，普通用户一般不会使用。业务库顾名思义，就是指用户业务数据存储的库，通常会对这些库表进行水平分片、读写分离的配置。原型库和业务库都遵循上面的 Schema 配置方式，都可以配置在集群之上。 原型库配置 按照前文所属，我们先配置下 prototype 原型库的数据源，修改 prototypeDs.datasource.json 文件，将 MySQL 中的系统库 mysql 注册进来，数据源的名称为 prototypeDs。 dbType: mysql, idleTimeout: 60000, initSqls: [], initSqlsGetConnection: true, instanceType: READ_WRITE, maxCon: 1000, maxConnectTimeout: 30000, maxRetryCount: 5, minCon: 1, name: prototypeDs, password: 123456, type: JDBC, url: jdbc:mysql://localhost:3306/mysql?useUnicode=trueserverTimezone=Asia/ShanghaicharacterEncoding=UTF-8, user: root, weight: 0 然后修改 prototype.cluster.json，将 prototypeDs 数据源构建为原型库集群，prototype.cluster.json 配置文件如下： clusterType: MASTER_SLAVE, heartbeat: heartbeatTimeout: 1000, maxRetry: 3, minSwitchTimeInterval: 300, slaveThreshold: 0 , masters: [ prototypeDs ], maxCon: 200, name: prototype, readBalanceType: BALANCE_ALL, switchType: SWITCH 配置完成后，我们搜索 MycatCore 类进行本地启动，出现如下的日志表示启动成功。 启动成功后，可以使用 mysql -h127.0.0.1 -uroot -p -P8066 --binary-as-hex=0 -c -A 命令连接 MyCat2，密码为 123456。通过 SHOW DATABASES 可以看到，MyCat2 通过原型库默认提供了 3 个系统库。 mysql -h127.0.0.1 -uroot -p -P8066 --binary-as-hex=0 -c -A  1 ✘ │ 13sEnter password:Welcome to the MySQL monitor. Commands end with ; or \\g.Your MySQL connection id is 0Server version: 8.0.40-mycat-2.0 MySQL Community Server - GPLCopyright (c) 2000, 2024, Oracle and/or its affiliates.Oracle is a registered trademark of Oracle Corporation and/or itsaffiliates. Other names may be trademarks of their respectiveowners.Type help; or \\h for help. Type \\c to clear the current input statement.mysql SHOW DATABASES;+--------------------+| `Database` |+--------------------+| information_schema || mysql || performance_schema |+--------------------+3 rows in set (0.14 sec) 业务库配置 完成原型库配置后，我们再来配置业务库。和原型库的配置类似，我们同样需要先注册 MySQL 数据源，然后将数据源构建为集群。MyCat2 提供了一种注释 SQL，用来注册数据源和集群。我们使用 mysql -h127.0.0.1 -uroot -p -P8066 --binary-as-hex=0 -c -A 连接 MyCat2 服务，并执行以下 SQL 注册数据源。为了模拟 MySQL 主从同步，我们将从库的数据库设置为和主库相同。 /*+ mycat:createDataSource\tname:ds_write_0,\turl:jdbc:mysql://127.0.0.1:3306/ds_write_0,\tuser:root,\tpassword:123456 */;/*+ mycat:createDataSource\tname:ds_read_0,\turl:jdbc:mysql://127.0.0.1:3306/ds_write_0,\tuser:root,\tpassword:123456 */;/*+ mycat:createDataSource\tname:ds_write_1,\turl:jdbc:mysql://127.0.0.1:3306/ds_write_1,\tuser:root,\tpassword:123456 */;/*+ mycat:createDataSource\tname:ds_read_1,\turl:jdbc:mysql://127.0.0.1:3306/ds_write_1,\tuser:root,\tpassword:123456 */; 然后，我们基于创建的数据源构建集群，执行以下 SQL 创建集群： /*! mycat:createCluster\tname:c0,\tmasters:[ds_write_0],\treplicas:[ds_read_0] */;/*! mycat:createCluster\tname:c1,\tmasters:[ds_write_1],\treplicas:[ds_read_1] */; 创建完集群后，我们就可以创建一些不同维度的分片表，并通过这些表来观察 MyCat2 是如何支持分布式 SQL 的，执行如下 SQL 创建分片表。 CREATE DATABASE sharding_db;USE sharding_db;-- 创建 3 张不同维度的分片表CREATE TABLE `sbtest_sharding_id` ( `id` int(11) NOT NULL AUTO_INCREMENT, `k` int(11) NOT NULL DEFAULT 0, `c` char(120) NOT NULL DEFAULT , `pad` char(60) NOT NULL DEFAULT , PRIMARY KEY (`id`)) DBPARTITION BY HASH(id) DBPARTITIONS 2; CREATE TABLE `sbtest_sharding_k` ( `id` int(11) NOT NULL AUTO_INCREMENT, `k` int(11) NOT NULL DEFAULT 0, `c` char(120) NOT NULL DEFAULT , `pad` char(60) NOT NULL DEFAULT , PRIMARY KEY (`id`)) DBPARTITION BY HASH(k) DBPARTITIONS 2;CREATE TABLE `sbtest_sharding_c` ( `id` int(11) NOT NULL AUTO_INCREMENT, `k` int(11) NOT NULL DEFAULT 0, `c` char(120) NOT NULL DEFAULT , `pad` char(60) NOT NULL DEFAULT , PRIMARY KEY (`id`)) DBPARTITION BY HASH(c) DBPARTITIONS 2; 初始化数据 创建好分片表后，我们再使用 sysbench 工具向 sbtest1 表插入 10w 条数据，执行如下脚本初始化数据： sysbench /opt/homebrew/Cellar/sysbench/1.0.20_6/share/sysbench/oltp_read_write.lua --tables=1 --table_size=100000 --mysql-user=root --mysql-password=123456 --mysql-host=127.0.0.1 --mysql-port=8066 --mysql-db=sharding_db prepare 由于 MyCat2 不支持 INSERT ... SELECT ... 语句，因此需要先使用 mysqldump 将 sbtest1 中的数据导出到文件。 mysqldump -h127.0.0.1 -uroot -p -P8066 sharding_db sbtest1 sbtest1.sql 然后修改 sbtest1.sql 文件，注释掉文件中除了 INSERT 外的语句，并将 sbtest1 分别修改为 sbtest_sharding_id、sbtest_sharding_k 和 sbtest_sharding_c，然后执行 mysql source ~/sbtest1.sql 导入数据到目标表。使用 SELECT COUNT(1) 检查各个表的数据量，都是 10w 条记录，符合我们的预期。 mysql SELECT COUNT(1) FROM sbtest_sharding_id;+----------+| COUNT(1) |+----------+| 100000 |+----------+mysql SELECT COUNT(1) FROM sbtest_sharding_k;+----------+| COUNT(1) |+----------+| 100000 |+----------+mysql SELECT COUNT(1) FROM sbtest_sharding_c;+----------+| COUNT(1) |+----------+| 100000 |+----------+ (adsbygoogle = window.adsbygoogle || []).push({}); 赞助商 MyCat2 Calcite 实践探究 执行流程概览 参考 MyCat2 SQL 编写指导，MyCat2 SQL 执行流程如下，服务端接收到 SQL 字符串或模板化 SQL 后，会先将 SQL 解析为 SQL AST，然后使用 Hack Router 进行路由判断，如果是一些简单的单节点 SQL，Hack Router 会直接将 SQL 路由到 DB 中执行，其他复杂的 SQL 则会进入 DRDS 处理流程。DRDS 处理流程中，会使用 Calcite 对 SQL 语句进行编译，然后生成关系代数树，并经过逻辑优化和物理优化两步，最终执行返回 SQL 结果。 初看 SQL 执行 由于本文主要关注 MyCat2 对于 Calcite 的应用，因此后续介绍中其他流程不会过多探究，感兴趣的朋友可以下载源码自行研究。我们执行如下的 SQL 示例，来跟踪 MyCat2 的执行流程，并探索在 SQL 执行过程中，Calcite 查询引擎都进行了哪些优化。 SELECT * FROM sbtest_sharding_id i INNER JOIN sbtest_sharding_k k ON i.id = k.id INNER JOIN sbtest_sharding_c c ON k.id = c.id LIMIT 10; 首先，我们可以执行 EXPLAIN 语句，先观察下这条语句的执行计划（省略了执行计划中生成的执行代码 Code 部分）。对于这 3 张表的 JOIN 处理，MyCat2 优化器选择了 SortMergeJoin 的方式，从 MySQL 中查询 sharding_db.sbtest_sharding_id 和 sharding_db.sbtest_sharding_k 表时，会使用 Join Key 进行排序，对于已经排序的结果集，再拉取到内存中进行 SortMergeJoin。处理完 Join 后，会对结果集进行一次内存排序，然后和 sharding_db.sbtest_sharding_c 表再进行一次 SortMergeJoin，最终的结果集经过内存排序后获取出前 10 条结果。 可以看到，MyCat2 中将分片的逻辑表封装为 MycatView，MycatView 在内部下推执行时，会根据分片的规则改写为不同的真实 SQL，执行计划中的 Each 部分展示了下推 DB 执行的 SQL 语句，由于使用了 SortMergeJoin，因此下推语句中包含了 ORDER BY 排序处理。 mysql EXPLAIN SELECT * FROM sbtest_sharding_id i INNER JOIN sbtest_sharding_k k ON i.id = k.id INNER JOIN sbtest_sharding_c c ON k.id = c.id LIMIT 10;+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+| plan |+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+| Plan: || MycatMemSort(fetch=[?0]) || MycatSortMergeJoin(condition=[=($4, $8)], joinType=[inner]) || MycatMemSort(sort0=[$4], dir0=[ASC]) || MycatSortMergeJoin(condition=[=($0, $4)], joinType=[inner]) || MycatView(distribution=[[sharding_db.sbtest_sharding_id]], mergeSort=[true]) || MycatView(distribution=[[sharding_db.sbtest_sharding_k]], mergeSort=[true]) || MycatView(distribution=[[sharding_db.sbtest_sharding_c]], mergeSort=[true]) || Each(targetName=c0, sql=SELECT * FROM sharding_db_0.sbtest_sharding_id_0 AS `sbtest_sharding_id` ORDER BY (`sbtest_sharding_id`.`id` IS NULL), `sbtest_sharding_id`.`id`) || Each(targetName=c1, sql=SELECT * FROM sharding_db_1.sbtest_sharding_id_0 AS `sbtest_sharding_id` ORDER BY (`sbtest_sharding_id`.`id` IS NULL), `sbtest_sharding_id`.`id`) || Each(targetName=c0, sql=SELECT * FROM sharding_db_0.sbtest_sharding_k_0 AS `sbtest_sharding_k` ORDER BY (`sbtest_sharding_k`.`id` IS NULL), `sbtest_sharding_k`.`id`) || Each(targetName=c1, sql=SELECT * FROM sharding_db_1.sbtest_sharding_k_0 AS `sbtest_sharding_k` ORDER BY (`sbtest_sharding_k`.`id` IS NULL), `sbtest_sharding_k`.`id`) || Each(targetName=c0, sql=SELECT * FROM sharding_db_0.sbtest_sharding_c_0 AS `sbtest_sharding_c` ORDER BY (`sbtest_sharding_c`.`id` IS NULL), `sbtest_sharding_c`.`id`) || Each(targetName=c1, sql=SELECT * FROM sharding_db_1.sbtest_sharding_c_0 AS `sbtest_sharding_c` ORDER BY (`sbtest_sharding_c`.`id` IS NULL), `sbtest_sharding_c`.`id`) |+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+170 rows in set (0.46 sec) 从 SQL 到执行计划 通过 MyCat2 的执行计划，我们对于分片表的多表关联查询有了初步的认识，下面我们来探究下 MyCat2 的代码实现，看看一条 SQL 是如何转换为执行计划的。我们执行如下的 SQL 语句： SELECT * FROM sbtest_sharding_id i INNER JOIN sbtest_sharding_k k ON i.id = k.id INNER JOIN sbtest_sharding_c c ON k.id = c.id LIMIT 10; MyCat2 SQL 执行的入口在 MycatdbCommand 类中，它会根据 SQL 语句的类型生成不同的 Handler 类，SQLSelectStatement 查询语句对应的是 ShardingSQLHandler。 获取到 ShardingSQLHandler 后，会调用 AbstractSQLHandler#execute 方法，最终会调用到 ShardingSQLHandler#onExecute 方法中，方法内部会使用 hackRouter 的 analyse 方法进行分析，用来决定 SQL 直接执行还是走 DRDS 执行。analyse 方法内部会先提取出语句中的表，然后调用 checkVaildNormalRoute 方法，对不同表的路由进行 check 并记录数据分布结果，最后根据数据分布结果决定执行方式。 public class ShardingSQLHandler extends AbstractSQLHandlerSQLSelectStatement @Override protected FutureVoid onExecute(SQLRequestSQLSelectStatement request, MycatDataContext dataContext, Response response) OptionalFutureVoid op = Optional.empty(); ... // SQL 模板化处理，转换为 select * from `sharding_db`.sbtest_sharding_id i inner join `sharding_db`.sbtest_sharding_k k on i.id = k.id inner join `sharding_db`.sbtest_sharding_c c on k.id = c.id limit ? 和参数 10 DrdsSqlWithParams drdsSqlWithParams = DrdsRunnerHelper.preParse(request.getAst(), dataContext.getDefaultSchema()); HackRouter hackRouter = new HackRouter(drdsSqlWithParams.getParameterizedStatement(), dataContext); try // 分析 SQL 中表的数据分布，然后决定透传执行还是走 DRDS 执行 if (hackRouter.analyse()) PairString, String plan = hackRouter.getPlan(); return response.proxySelect(Collections.singletonList(plan.getKey()), plan.getValue(), drdsSqlWithParams.getParams()); else return DrdsRunnerHelper.runOnDrds(dataContext, drdsSqlWithParams, response); catch (Throwable throwable) LOGGER.error(request.getAst().toString(), throwable); return Future.failedFuture(throwable); DrdsRunnerHelper#runOnDrds 方法逻辑如下，getPlan 用于获取 SQL 对应的执行计划，然后再调用 getPlanImplementor 获取执行计划的执行器，并执行 SQL 语句，然后返回 Future 对象等待返回结果。 public static FutureVoid runOnDrds(MycatDataContext dataContext, DrdsSqlWithParams drdsSqlWithParams, Response response) PlanImpl plan = getPlan(drdsSqlWithParams); PlanImplementor planImplementor = getPlanImplementor(dataContext, response, drdsSqlWithParams); return impl(plan, planImplementor);@NotNullpublic static PlanImpl getPlan(DrdsSqlWithParams drdsSqlWithParams) QueryPlanner planner = MetaClusterCurrent.wrapper(QueryPlanner.class); PlanImpl plan; ParamHolder paramHolder = ParamHolder.CURRENT_THREAD_LOCAL.get(); try paramHolder.setData(drdsSqlWithParams.getParams(), drdsSqlWithParams.getTypeNames()); CodeExecuterContext codeExecuterContext = planner.innerComputeMinCostCodeExecuterContext(drdsSqlWithParams); plan = new PlanImpl(codeExecuterContext.getMycatRel(), codeExecuterContext, drdsSqlWithParams.getAliasList()); finally return plan; 我们先重点关注 getPlan 方法是如何生成执行计划的，该方法内部调用的是 QueryPlanner#innerComputeMinCostCodeExecuterContext 方法，它负责从缓存中获取 MyCatRelList 执行计划，如果缓存中不存在则调用 add 方法生成执行计划，并将执行计划添加到缓存中。 public CodeExecuterContext innerComputeMinCostCodeExecuterContext(DrdsSql sqlSelectStatement) // 创建 RelOptCluster，内部注册 HintStrategyTable 处理 Hint 语法 RelOptCluster relOptCluster = DrdsSqlCompiler.newCluster(); // 从缓存中获取 MyCatRelList，如果不存在，则生成 MyCatRelList ListCodeExecuterContext codeExecuterContexts = getAcceptedMycatRelList(sqlSelectStatement); int size = codeExecuterContexts.size(); // 比较 Cost 获取最小的 CodeExecuterContextpublic ListCodeExecuterContext getAcceptedMycatRelList(DrdsSql drdsSql) ListCodeExecuterContext acceptedMycatRelList = planCache.getAcceptedMycatRelList(drdsSql); if (acceptedMycatRelList.isEmpty()) synchronized (this) // 从缓存中获取 MyCatRelList，存在直接返回 acceptedMycatRelList = planCache.getAcceptedMycatRelList(drdsSql); if (!acceptedMycatRelList.isEmpty()) return acceptedMycatRelList; else // 不存在则调用 add 方法生成 MyCatRelList，并添加到缓存中 PlanResultSet add = planCache.add(false, drdsSql); return Collections.singletonList(add.getContext()); else return acceptedMycatRelList; add 方法内部首先会获取 SQL 执行计划的基线，用于提供稳定的执行计划，然后调用 drdsSqlCompiler#dispatch 方法，内部包含了 CBO 和 RBO 优化，会生成 MycatRel 执行计划树。生成的执行计划树通过 RelJsonWriter 工具类转换为字符串，存储到新的执行计划基线中，最终调用 saveBaselinePlan 保存下来。 public synchronized PlanResultSet add(boolean fix, DrdsSql drdsSql) Long baselineId = null; // 获取 SQL 执行计划基线，用于提供稳定的执行计划 Baseline baseline = this.getBaseline(drdsSql); DrdsSqlCompiler drdsSqlCompiler = MetaClusterCurrent.wrapper(DrdsSqlCompiler.class); OptimizationContext optimizationContext = new OptimizationContext(); // 生成 MycatRel 执行计划树，内部包含了 RBO 和 CBO 优化 MycatRel mycatRel = drdsSqlCompiler.dispatch(optimizationContext, drdsSql); RelJsonWriter relJsonWriter = new RelJsonWriter(); mycatRel.explain(relJsonWriter); long hash = planIds.nextPlanId(); // 生成新的执行计划基线 BaselinePlan newBaselinePlan = new BaselinePlan(drdsSql.getParameterizedSQL(), relJsonWriter.asString(), hash, baselineId = baseline.getBaselineId(), null); getCodeExecuterContext(baseline,newBaselinePlan,optimizationContext, mycatRel); return saveBaselinePlan(fix, false, baseline, newBaselinePlan); drdsSqlCompiler#dispatch 方法负责将不同的 SQL 语句进行转换处理，如果是 SQLSelectStatement，则会调用 compileQuery 方法，方法实现逻辑如下： public MycatRel compileQuery(OptimizationContext optimizationContext, SchemaPlus plus, DrdsSql drdsSql) RelNode logPlan; RelNodeContext relNodeContext = null; // 创建 SqlToRelConverter 将 SQL AST 转换为 RelNode 关系代数树 relNodeContext = getRelRoot(plus, drdsSql); logPlan = relNodeContext.getRoot().rel; optimizationContext.relNodeContext = relNodeContext; RelDataType finalRowType = logPlan.getRowType(); // RBO 优化 RelNode rboLogPlan = optimizeWithRBO(logPlan); // CBO 优化 MycatRel mycatRel = optimizeWithCBO(rboLogPlan, Collections.emptyList()); if (!RelOptUtil.areRowTypesEqual(mycatRel.getRowType(), finalRowType, true)) Project relNode = (Project) relNodeContext.getRelBuilder().push(mycatRel).rename(finalRowType.getFieldNames()).build(); mycatRel = MycatProject.create(relNode.getInput(), relNode.getProjects(), relNode.getRowType()); return mycatRel; optimizeWithRBO 主要进行逻辑优化，包括：子查询优化、聚合查询优化、JOIN 顺序优化、其他优化（包括 MyCat2 自定义的优化），逻辑优化基本都采用了 HepPlanner 优化器，通过 HepProgramBuilder 添加的优化规则，builder 中可以调用 addMatchLimit 设置最大匹配次数。 private RelNode optimizeWithRBO(RelNode logPlan) // 子查询优化 Program subQueryProgram = getSubQueryProgram(); RelNode unSubQuery = subQueryProgram.run(null, logPlan, null, Collections.emptyList(), Collections.emptyList()); // 聚合查询优化 RelNode unAvg = resolveAggExpr(unSubQuery); ... // JOIN 顺序优化 RelNode joinClustering = toMultiJoin(unAvg).map(relNode - HepProgramBuilder builder = new HepProgramBuilder(); builder.addMatchLimit(1024); builder.addGroupBegin(); builder.addRuleInstance(MycatHepJoinClustering.Config.DEFAULT.toRule()); builder.addGroupEnd(); builder.addMatchLimit(64); builder.addGroupBegin(); builder.addRuleInstance(CoreRules.MULTI_JOIN_OPTIMIZE); builder.addGroupEnd(); HepPlanner planner = new HepPlanner(builder.build()); planner.setRoot(relNode); RelNode bestExp = planner.findBestExp(); return bestExp; ...; // 其他优化规则 HepProgramBuilder builder = new HepProgramBuilder(); builder.addMatchLimit(1024); builder.addGroupBegin().addRuleCollection(ImmutableList.of(AggregateExpandDistinctAggregatesRule.Config.DEFAULT.toRule(), CoreRules.AGGREGATE_ANY_PULL_UP_CONSTANTS, CoreRules.PROJECT_MERGE, CoreRules.PROJECT_CORRELATE_TRANSPOSE, CoreRules.PROJECT_SET_OP_TRANSPOSE, CoreRules.PROJECT_JOIN_TRANSPOSE, CoreRules.PROJECT_WINDOW_TRANSPOSE, CoreRules.PROJECT_FILTER_TRANSPOSE, ProjectRemoveRule.Config.DEFAULT.toRule())).addGroupEnd().addMatchOrder(HepMatchOrder.BOTTOM_UP); builder.addMatchLimit(1024); builder.addGroupBegin().addRuleCollection(FILTER).addGroupEnd().addMatchOrder(HepMatchOrder.BOTTOM_UP); builder.addMatchLimit(1024); builder.addGroupBegin().addRuleInstance(CoreRules.PROJECT_MERGE).addGroupEnd().addMatchOrder(HepMatchOrder.ARBITRARY); builder.addMatchLimit(1024); // MyCat2 自定义规则，包括：单表、广播表下推，JOIN ER 表下推等 builder.addGroupBegin().addRuleCollection(LocalRules.RBO_RULES).addRuleInstance(MycatAggDistinctRule.Config.DEFAULT.toRule()).addGroupEnd().addMatchOrder(HepMatchOrder.BOTTOM_UP); builder.addMatchLimit(1024); HepPlanner planner = new HepPlanner(builder.build()); planner.setRoot(joinClustering); RelNode bestExp = planner.findBestExp(); return bestExp; optimizeWithCBO 主要进行物理优化，它根据这种方案的 Cost 选择最优的执行计划。optimizeWithCBO 逻辑如下，如果 logPlan 已经是 MycatRel，则直接返回，否则继续执行进行优化。MyCat2 物理优化中使用了许多 Calcite 内置的优化规则，同时也扩展了一些适合于 MyCat2 的规则，例如：MycatTableLookupSemiJoinRule、MycatJoinTableLookupTransposeRule，感兴趣的朋友可以构造相关的 SQL 研究具体的优化规则逻辑。 optimizeWithCBO 方法最后使用 MatierialRewriter 对执行计划树进行改写，MatierialRewriter 主要用于处理计算过程中需要消耗较多内存、网络调用的场景，例如：MycatNestedLoopJoin，会将右表 right 替换为 MycatMatierial，然后在执行时 MycatMatierial 会一次读取右表（Inner）的数据，并写入到本地文件中，这样计算 MycatNestedLoopJoin 时，MyCat2 就无需频繁地去 MySQL 中获取 Inner 表中的数据，直接从本地文件就可以快速获取。 public MycatRel optimizeWithCBO(RelNode logPlan, CollectionRelOptRule relOptRules) if (logPlan instanceof MycatRel) return (MycatRel) logPlan; else RelOptCluster cluster = logPlan.getCluster(); RelOptPlanner planner = cluster.getPlanner(); planner.clear(); MycatConvention.INSTANCE.register(planner); ImmutableList.BuilderRelOptRule listBuilder = ImmutableList.builder(); listBuilder.addAll(MycatExtraSortRule.RULES); listBuilder.addAll(LocalRules.CBO_RULES); // 算子交换 // Filter/Join, TopN/Join, Agg/Join, Filter/Agg, Sort/Project, Join/TableLookup listBuilder.add(CoreRules.JOIN_PUSH_EXPRESSIONS); listBuilder.add(CoreRules.FILTER_INTO_JOIN); // TopN/Join listBuilder.add(CoreRules.SORT_JOIN_TRANSPOSE.config.withOperandFor(MycatTopN.class, Join.class).toRule()); listBuilder.add(CoreRules.FILTER_SET_OP_TRANSPOSE.config.toRule()); listBuilder.add(CoreRules.AGGREGATE_JOIN_TRANSPOSE.config.withOperandFor(Aggregate.class, Join.class, false).toRule()); // Sort/Project listBuilder.add(CoreRules.SORT_PROJECT_TRANSPOSE.config.withOperandFor(Sort.class, Project.class).toRule()); // index listBuilder.add(MycatViewIndexViewRule.DEFAULT_CONFIG.toRule()); if (DrdsSqlCompiler.RBO_BKA_JOIN) // TABLELOOKUP listBuilder.add(MycatTableLookupSemiJoinRule.INSTANCE); listBuilder.add(MycatTableLookupCombineRule.INSTANCE); listBuilder.add(MycatJoinTableLookupTransposeRule.LEFT_INSTANCE); listBuilder.add(MycatJoinTableLookupTransposeRule.RIGHT_INSTANCE); listBuilder.add(MycatValuesJoinRule.INSTANCE); listBuilder.build().forEach(c - planner.addRule(c)); MycatConvention.INSTANCE.register(planner); if (relOptRules != null) for (RelOptRule relOptRule : relOptRules) planner.addRule(relOptRule); ... logPlan = planner.changeTraits(logPlan, cluster.traitSetOf(MycatConvention.INSTANCE)); planner.setRoot(logPlan); RelNode bestExp = planner.findBestExp(); // MatierialRewriter 对需要消耗内存、网络调用的算子进行改写，将数据物化存储到本地文件 RelNode accept = bestExp.accept(new MatierialRewriter()); return (MycatRel) accept; 完成 CBO 优化后，最后会判断逻辑计划和优化后的执行计划 RowType 是否相同，如果不同则使用 RelBuilder 对优化后的执行计划进行改名，保证最终返回的列名和逻辑计划中的一致。执行到这里，就完成了从 SQL 到执行计划的全过程，其中有一些涉及优化规则的细节，由于文章的篇幅就不再一一研究，感兴趣的读者可以对照代码进行调试学习。 if (!RelOptUtil.areRowTypesEqual(mycatRel.getRowType(), finalRowType, true)) Project relNode = (Project) relNodeContext.getRelBuilder().push(mycatRel).rename(finalRowType.getFieldNames()).build(); mycatRel = MycatProject.create(relNode.getInput(), relNode.getProjects(), relNode.getRowType()); 执行代码生成 有了物理执行计划后，最后一步就是根据执行计划生成可执行代码，并执行 SQL 返回结果。执行代码生成的逻辑在 DrdsExecutorCompiler#getCodeExecuterContext 方法中，MyCat2 生成代码的方式和 Calcite 一致，都是采用 Linq4j 进行生成，MyCat2 根据自身项目需求，调整了一些算子生成代码的逻辑。 public static CodeExecuterContext getCodeExecuterContext(MapRexNode, RexNode constantMap, MycatRel relNode, boolean forUpdate) HashMapString, Object varContext = new HashMap(2); StreamMycatEnumerableRelImplementor mycatEnumerableRelImplementor = new StreamMycatEnumerableRelImplementor(varContext); HashMapString, MycatRelDatasourceSourceInfo stat = new HashMap(); ... ClassDeclaration classDeclaration = mycatEnumerableRelImplementor.implementHybridRoot(relNode, EnumerableRel.Prefer.ARRAY); String code = Expressions.toString(classDeclaration.memberDeclarations, , false); CodeContext codeContext = new CodeContext(classDeclaration.name, code); CodeExecuterContext executerContext = CodeExecuterContext.of(constantMap, stat, varContext, relNode, codeContext); return executerContext; StreamMycatEnumerableRelImplementor 是代码生成的入口类，调用 implementHybridRoot 方法遍历执行计划树。生成代码过程中通过 visitChild 方法访问子节点，从而调用子节点 implement 方法（每个物理算子都实现了 implement 方法），最终实现整个执行计划树的代码生成。MycatSortMergeJoin 的 implement 方法如下： @Overridepublic Result implement(MycatEnumerableRelImplementor implementor, Prefer pref) BlockBuilder builder = new BlockBuilder(); // visitChild 访问 left 节点，并生成代码，存储在 Result 对象中 final Result leftResult = implementor.visitChild(this, 0, (EnumerableRel) left, pref); final Expression leftExpression = toEnumerate(builder.append(left, leftResult.block)); final ParameterExpression left_ = Expressions.parameter(leftResult.physType.getJavaRowType(), left); final Result rightResult = implementor.visitChild(this, 1, (EnumerableRel) right, pref); final Expression rightExpression = toEnumerate(builder.append(right, rightResult.block)); final ParameterExpression right_ = Expressions.parameter(rightResult.physType.getJavaRowType(), right); final JavaTypeFactory typeFactory = implementor.getTypeFactory(); final PhysType physType = PhysTypeImpl.of(typeFactory, getRowType(), pref.preferArray()); final ListExpression leftExpressions = new ArrayList(); final ListExpression rightExpressions = new ArrayList(); for (PairInteger, Integer pair : Pair.zip(joinInfo.leftKeys, joinInfo.rightKeys)) final RelDataType keyType = typeFactory.leastRestrictive(ImmutableList.of(left.getRowType().getFieldList().get(pair.left).getType(), right.getRowType().getFieldList().get(pair.right).getType())); final Type keyClass = typeFactory.getJavaClass(keyType); leftExpressions.add(EnumUtils.convert(leftResult.physType.fieldReference(left_, pair.left), keyClass)); rightExpressions.add(EnumUtils.convert(rightResult.physType.fieldReference(right_, pair.right), keyClass)); Expression predicate = Expressions.constant(null); if (!joinInfo.nonEquiConditions.isEmpty()) final RexNode nonEquiCondition = RexUtil.composeConjunction(getCluster().getRexBuilder(), joinInfo.nonEquiConditions, true); if (nonEquiCondition != null) predicate = EnumUtils.generatePredicate(implementor, getCluster().getRexBuilder(), left, right, leftResult.physType, rightResult.physType, nonEquiCondition); final PhysType leftKeyPhysType = leftResult.physType.project(joinInfo.leftKeys, JavaRowFormat.LIST); final PhysType rightKeyPhysType = rightResult.physType.project(joinInfo.rightKeys, JavaRowFormat.LIST); // Generate the appropriate key Comparator (keys must be sorted in ascending order, nulls last). final int keysSize = joinInfo.leftKeys.size(); final ListRelFieldCollation fieldCollations = new ArrayList(keysSize); for (int i = 0; i keysSize; i++) fieldCollations.add(new RelFieldCollation(i, RelFieldCollation.Direction.ASCENDING, RelFieldCollation.NullDirection.LAST)); final RelCollation collation = RelCollations.of(fieldCollations); final Expression comparator = leftKeyPhysType.generateComparator(collation); // 生成调用 MERGE_JOIN 方法的代码，内部调用 EnumerableDefaults#mergeJoin 方法 return implementor.result(physType, builder.append(Expressions.call(BuiltInMethod.MERGE_JOIN.method, Expressions.list(leftExpression, rightExpression, Expressions.lambda(leftKeyPhysType.record(leftExpressions), left_), Expressions.lambda(rightKeyPhysType.record(rightExpressions), right_), predicate, EnumUtils.joinSelector(joinType, physType, ImmutableList.of(leftResult.physType, rightResult.physType)), Expressions.constant(EnumUtils.toLinq4jJoinType(joinType)), comparator))).toBlock()); MycatSortMergeJoin 生成代码的逻辑，主要都是围绕 BuiltInMethod.MERGE_JOIN.method 方法进行的，该方法是排序合并连接的核心逻辑，它的具体实现是 EnumerableDefaults#mergeJoin。调用 mergeJoin 方法需要传递左表（outer）、右表（inner）的结果集遍历器 Enumerable，以及左表和右表关联 Key 的选择器，通过选择器能够从行记录中选择出关联 Key，comparator 则是判断关联条件是否成立的比较器。 @API(since = 1.23, status = API.Status.EXPERIMENTAL)public static TSource, TInner, TKey extends ComparableTKey, TResult EnumerableTResult mergeJoin(final EnumerableTSource outer, final EnumerableTInner inner, final Function1TSource, TKey outerKeySelector, final Function1TInner, TKey innerKeySelector, final Predicate2TSource, TInner extraPredicate, final Function2TSource, TInner, TResult resultSelector, final JoinType joinType, final ComparatorTKey comparator) if (!isMergeJoinSupported(joinType)) throw new UnsupportedOperationException(MergeJoin unsupported for join type + joinType); return new AbstractEnumerableTResult() public EnumeratorTResult enumerator() return new MergeJoinEnumerator(outer, inner, outerKeySelector, innerKeySelector, extraPredicate, resultSelector, joinType, comparator); ; 最终生成的可执行代码如下，笔者个人是不太推荐 Calcite 代码生成的方式，因为不管是生成代码的逻辑，还是生成后的执行代码，他们的可阅读性都比较差，后期维护的压力会比较大。 public Object bindObservable(final org.apache.calcite.runtime.NewMycatDataContext root) final java.util.Comparator comparator = new java.util.Comparator() public int compare(Object[] v0, Object[] v1) int c; return 0; public int compare(Object o0, Object o1) return this.compare((Object[]) o0, (Object[]) o1); ; return org.apache.calcite.linq4j.EnumerableDefaults.orderBy(org.apache.calcite.util.RxBuiltInMethodImpl.toEnumerable(org.apache.calcite.linq4j.EnumerableDefaults.mergeJoin(org.apache.calcite.util.RxBuiltInMethodImpl.toEnumerable(org.apache.calcite.linq4j.EnumerableDefaults.orderBy(org.apache.calcite.util.RxBuiltInMethodImpl.toEnumerable(org.apache.calcite.linq4j.EnumerableDefaults.mergeJoin(org.apache.calcite.util.RxBuiltInMethodImpl.toEnumerable(root.getObservable(MycatView.MYCAT2.[0](relNode=LocalSort#107,distribution=[sharding_db.sbtest_sharding_id],mergeSort=true), new org.apache.calcite.linq4j.function.Function1() public long apply(Object[] v) return org.apache.calcite.runtime.SqlFunctions.toLong(v[0]); public Object apply(Object v) return apply((Object[]) v); , org.apache.calcite.linq4j.function.Functions.nullsComparator(false, false), 0, 2147483647)), org.apache.calcite.util.RxBuiltInMethodImpl.toEnumerable(root.getObservable(MycatView.MYCAT2.[0](relNode=LocalSort#111,distribution=[sharding_db.sbtest_sharding_k],mergeSort=true), new org.apache.calcite.linq4j.function.Function1() public long apply(Object[] v) return org.apache.calcite.runtime.SqlFunctions.toLong(v[0]); public Object apply(Object v) return apply((Object[]) v); , org.apache.calcite.linq4j.function.Functions.nullsComparator(false, false), 0, 2147483647)), new org.apache.calcite.linq4j.function.Function1() public long apply(Object[] left) return org.apache.calcite.runtime.SqlFunctions.toLong(left[0]); public Object apply(Object left) return apply((Object[]) left); , new org.apache.calcite.linq4j.function.Function1() public long apply(Object[] right) return org.apache.calcite.runtime.SqlFunctions.toLong(right[0]); public Object apply(Object right) return apply((Object[]) right); , null, new org.apache.calcite.linq4j.function.Function2() public Object[] apply(Object[] left, Object[] right) return new Object[]left[0], left[1], left[2], left[3], right[0], right[1], right[2], right[3]; public Object[] apply(Object left, Object right) return apply((Object[]) left, (Object[]) right); , org.apache.calcite.linq4j.JoinType.INNER, new java.util.Comparator() public int compare(Long v0, Long v1) final int c; c = org.apache.calcite.runtime.Utilities.compare(v0, v1); if (c != 0) return c; return 0; public int compare(Object o0, Object o1) return this.compare((Long) o0, (Long) o1); )), new org.apache.calcite.linq4j.function.Function1() public long apply(Object[] v) return org.apache.calcite.runtime.SqlFunctions.toLong(v[4]); public Object apply(Object v) return apply((Object[]) v); , org.apache.calcite.linq4j.function.Functions.nullsComparator(false, false), 0, 2147483647)), org.apache.calcite.util.RxBuiltInMethodImpl.toEnumerable(root.getObservable(MycatView.MYCAT2.[0](relNode=LocalSort#140,distribution=[sharding_db.sbtest_sharding_c],mergeSort=true), new org.apache.calcite.linq4j.function.Function1() public long apply(Object[] v) return org.apache.calcite.runtime.SqlFunctions.toLong(v[0]); public Object apply(Object v) return apply((Object[]) v); , org.apache.calcite.linq4j.function.Functions.nullsComparator(false, false), 0, 2147483647)), new org.apache.calcite.linq4j.function.Function1() public long apply(Object[] left) return org.apache.calcite.runtime.SqlFunctions.toLong(left[4]); public Object apply(Object left) return apply((Object[]) left); , new org.apache.calcite.linq4j.function.Function1() public long apply(Object[] right) return org.apache.calcite.runtime.SqlFunctions.toLong(right[0]); public Object apply(Object right) return apply((Object[]) right); , null, new org.apache.calcite.linq4j.function.Function2() public Object[] apply(Object[] left, Object[] right) return new Object[]left[0], left[1], left[2], left[3], left[4], left[5], left[6], left[7], right[0], right[1], right[2], right[3]; public Object[] apply(Object left, Object right) return apply((Object[]) left, (Object[]) right); , org.apache.calcite.linq4j.JoinType.INNER, new java.util.Comparator() public int compare(Long v0, Long v1) final int c; c = org.apache.calcite.runtime.Utilities.compare(v0, v1); if (c != 0) return c; return 0; public int compare(Object o0, Object o1) return this.compare((Long) o0, (Long) o1); )), org.apache.calcite.linq4j.function.Functions.identitySelector(), comparator, 0, (Integer) root.get(?0));public boolean isObservable() return false;public Class getElementType() return java.lang.Object[].class; 生成了可执行代码字符串后，MyCat2 会获取 PlanImplementor，此处获取的实现类是 ObservableColocatedImplementor。由于本文的 Case 是查询语句，执行时会调用 executeQuery 方法，内部调用 executorProvider#prepare 方法时会通过 janino 编译并创建 ArrayBindable 实例。 获取到 ArrayBindable 后，MyCat2 会调用 PrepareExecutor#getMysqlPayloadObjectObservable 方法，为 ArrayBindable 绑定可观察对象 AsyncMycatDataContextImpl。AsyncMycatDataContextImpl 类内部提供了 getObservables 方法，如下图所示，getObservables 方法会在对应的数据源上执行下推的 SQL，并封装为 Observable 对象。 获取到 observable 对象后，MyCat2 会增加执行结果订阅，将 observable 内部执行返回的结果数据存储到 MysqlObjectArrayRow 对象中，再作为新的 ObservableMysqlPayloadObject 可观察对象。最终查询结果通过 MysqlPayloadObjectObserver 类进行输出，根据 MySQL 协议的要求进行封包，然后返回给客户端程序。 observable.subscribe(objects - emitter.onNext(new MysqlObjectArrayRow(objects)), throwable - newMycatDataContext.endFuture().onComplete(event - emitter.onError(throwable));, () - CompositeFuture compositeFuture = newMycatDataContext.endFuture(); compositeFuture.onSuccess(event - emitter.onComplete()); compositeFuture.onFailure(event - emitter.onError(event));); 结语 本文主要介绍了 Apache Calcite 在 MyCat2 分布式查询引擎中的实践应用。我们首先搭建了一个 MyCat2 开发环境，了解了 MyCat2 中原型库、业务库等基础概念，然后借助 sysbench 工具初始化了 10w 条数据，并创建了 3 张不同维度的分片表，方便后续探究 SQL 内部实现。 实践探究部分，我们参考 MyCat2 官方文档，学习了 MyCat2 中 SQL 的执行流程，MyCat2 的执行流程和传统的关系型数据库类似，需要经过 SQL 编译、逻辑优化、物理优化、SQL 执行等关键步骤。我们通过 SQL 执行计划，初步了解了 SQL 是如何执行的，然后结合一个 SQL 示例，探究了 MyCat2 如何使用 Apache Calcite 生成执行计划，MyCat2 也结合项目特点，扩展了很多优化规则。获取到执行计划后，MyCat2 最后会通过 Linq4j 生成可执行代码，并通过 Janio 编译创建实例对象，生成的代码中使用 RxJava 异步处理数据流。 尽管 MyCat2 项目已停止维护，但其基于 Calcite 的优化器设计仍具借鉴意义。通过规则优化与代价模型的结合，MyCat2 实现了对跨库复杂查询的支持，尤其在分片键路由、多表 Join 优化等场景中展现了 Calcite 的灵活性。大家如果工作中有类似的 Calcite 实践需求，也可以参考 MyCat2 的实现进行增强，欢迎大家一起研究探讨。 写在最后 笔者因为工作原因接触到 Calcite，前期学习过程中，深感 Calcite 学习资料之匮乏，因此创建了 Calcite 从入门到精通知识星球，希望能够将学习过程中的资料和经验沉淀下来，为更多想要学习 Calcite 的朋友提供一些帮助。 欢迎关注 欢迎关注「端小强的博客」微信公众号，会不定期分享日常学习和工作经验，欢迎大家关注交流。","tags":["Calcite"],"categories":["Calcite"]},{"title":"Calcite UDF 实战之 ShardingSphere 联邦查询适配 MySQL BIT_COUNT","path":"/blog/calcite-udf-in-action-shardingsphere-sql-federation-adapte-to-mysql-bit-count.html","content":"前言 熟悉 Apache ShardingSphere 的朋友们，可能听说过 SQL Federation 功能，它主要适用于海量数据水平分片场景下，提供对跨节点关联查询、子查询、分页、排序、聚合查询等复杂查询语句的支持。SQL Federation 功能内部使用了 Apache Calcite 项目，来实现 SQL 优化和执行。随着 Calcite 1.38.0 版本的发布，Calcite 对于不同数据库的函数支持度进一步提升，为了提升 SQL Federation 功能支持度，升级 Calcite 至 1.38.0 版本也成为必然的选择。 由于升级前 ShardingSphere 使用的是 Caclite 1.35.0 版本，该版本和 1.38.0 相差了 1 年多，Calcite 内部进行了大量的优化和增强，因此升级后出现了 BIT_COUNT 函数无法执行的问题，下图展示了 ShardingSphere E2E 中出现异常的 BIT_COUNT Case。 BIT_COUNT 异常初探 根据 ShardingSphere E2E 中抛出的异常信息，主要可以分为两类：NumberFormatException 和 CalciteContextException，下面我们分别来看下这两类异常出现的原因，并探究下 1.38.0 版本对于 MySQL BIT_COUNT 函数的支持情况。 NumberFormatException 首先，我们来看下 NumberFormatException，根据异常信息可以看出，Calcite 会将 BIT_COUNT 函数的参数，转换为 BigDecimal 类型，然后在初始化 BigDecimal 对象时，遇到了不支持的字符 a。检查联邦查询的测试 Case，确实存在包含字符 a 的 SQL。 java.lang.NumberFormatException: Character a is neither a decimal digit number, decimal point, nor e notation exponential mark.\tat java.base/java.math.BigDecimal.init(BigDecimal.java:522)\tat java.base/java.math.BigDecimal.init(BigDecimal.java:405)\tat java.base/java.math.BigDecimal.init(BigDecimal.java:838)\tat org.apache.calcite.linq4j.tree.Primitive.charToDecimalCast(Primitive.java:433)\tat Baz$1$1.current(Unknown Source)\tat org.apache.shardingsphere.sqlfederation.resultset.SQLFederationResultSet.next(SQLFederationResultSet.java:105)\tat com.zaxxer.hikari.pool.HikariProxyResultSet.next(HikariProxyResultSet.java)\tat org.apache.shardingsphere.test.e2e.engine.type.dql.BaseDQLE2EIT.assertRows(BaseDQLE2EIT.java:157)\tat org.apache.shardingsphere.test.e2e.engine.type.dql.BaseDQLE2EIT.assertResultSet(BaseDQLE2EIT.java:107)\tat org.apache.shardingsphere.test.e2e.engine.type.dql.GeneralDQLE2EIT.assertExecuteQueryForStatement(GeneralDQLE2EIT.java:99)\tat org.apache.shardingsphere.test.e2e.engine.type.dql.GeneralDQLE2EIT.assertExecuteQueryWithExpectedDataSource(GeneralDQLE2EIT.java:85)\tat org.apache.shardingsphere.test.e2e.engine.type.dql.GeneralDQLE2EIT.assertExecuteQuery(GeneralDQLE2EIT.java:62)\tat org.apache.shardingsphere.test.e2e.engine.type.dql.GeneralDQLE2EIT.assertExecuteQuery(GeneralDQLE2EIT.java:55) 在 MySQL 中执行可以发现，当 BIT_COUNT 函数的参数，包含了 abcdefg 等非数值字符时，BIT_COUNT 函数会返回 0，而非抛出异常。因此，我们需要为 Calcite 函数进行增强，来支持 BIT_COUNT 函数包含非法字符的 SQL 场景。 mysql SELECT bit_count(123456), bit_count(123456), bit_count(abcdefg);+-------------------+---------------------+----------------------+| bit_count(123456) | bit_count(123456) | bit_count(abcdefg) |+-------------------+---------------------+----------------------+| 6 | 6 | 0 |+-------------------+---------------------+----------------------+1 row in set, 1 warning (0.00 sec) CalciteContextException 我们再来看下 CalciteContextException 异常，根据异常堆栈可以发现，该异常是 Calcite 进行元数据校验时抛出的，checkOperandTypes 方法在进行操作数类型判断时，发现当前 Case 中的 BIT_COUNT(JAVATYPE(CLASS JAVA.LANG.BOOLEAN)) 还不支持，因此抛出了异常，我们需要为 Calcite BIT_COUNT 函数适配 Boolean 类型的参数。 Caused by: org.apache.calcite.runtime.CalciteContextException: At line 0, column 0: Cannot apply BIT_COUNT to arguments of type BIT_COUNT(JAVATYPE(CLASS JAVA.LANG.BOOLEAN)). Supported form(s): BIT_COUNT(NUMERIC)BIT_COUNT(BINARY)\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)\tat org.apache.calcite.runtime.Resources$ExInstWithCause.ex(Resources.java:511)\tat org.apache.calcite.sql.SqlUtil.newContextException(SqlUtil.java:952)\tat org.apache.calcite.sql.SqlUtil.newContextException(SqlUtil.java:937)\tat org.apache.calcite.sql.validate.SqlValidatorImpl.newValidationError(SqlValidatorImpl.java:5899)\tat org.apache.calcite.sql.SqlCallBinding.newValidationSignatureError(SqlCallBinding.java:399)\tat org.apache.calcite.sql.type.FamilyOperandTypeChecker.checkSingleOperandType(FamilyOperandTypeChecker.java:137)\tat org.apache.calcite.sql.type.FamilyOperandTypeChecker.checkOperandTypes(FamilyOperandTypeChecker.java:172)\tat org.apache.calcite.sql.type.CompositeOperandTypeChecker.check(CompositeOperandTypeChecker.java:345)\tat org.apache.calcite.sql.type.CompositeOperandTypeChecker.checkOperandTypes(CompositeOperandTypeChecker.java:275)\tat org.apache.calcite.sql.SqlOperator.checkOperandTypes(SqlOperator.java:754)\tat org.apache.calcite.sql.SqlOperator.validateOperands(SqlOperator.java:496)\tat org.apache.calcite.sql.SqlFunction.deriveType(SqlFunction.java:350)\tat org.apache.calcite.sql.SqlFunction.deriveType(SqlFunction.java:232)\tat org.apache.calcite.sql.validate.SqlValidatorImpl$DeriveTypeVisitor.visit(SqlValidatorImpl.java:6967)\tat org.apache.calcite.sql.validate.SqlValidatorImpl$DeriveTypeVisitor.visit(SqlValidatorImpl.java:6954)\tat org.apache.calcite.sql.SqlCall.accept(SqlCall.java:168)\tat org.apache.calcite.sql.validate.SqlValidatorImpl.deriveTypeImpl(SqlValidatorImpl.java:2006)\tat org.apache.calcite.sql.validate.SqlValidatorImpl.deriveType(SqlValidatorImpl.java:1993)\tat org.apache.calcite.sql.validate.SqlValidatorImpl.expandSelectItem(SqlValidatorImpl.java:505)\tat org.apache.calcite.sql.validate.SqlValidatorImpl.validateSelectList(SqlValidatorImpl.java:5015)\tat org.apache.calcite.sql.validate.SqlValidatorImpl.validateSelect(SqlValidatorImpl.java:4096)\tat org.apache.calcite.sql.validate.SelectNamespace.validateImpl(SelectNamespace.java:62)\tat org.apache.calcite.sql.validate.AbstractNamespace.validate(AbstractNamespace.java:95)\tat org.apache.calcite.sql.validate.SqlValidatorImpl.validateNamespace(SqlValidatorImpl.java:1206)\tat org.apache.calcite.sql.validate.SqlValidatorImpl.validateQuery(SqlValidatorImpl.java:1177)\tat org.apache.calcite.sql.SqlSelect.validate(SqlSelect.java:282)\tat org.apache.calcite.sql.validate.SqlValidatorImpl.validateScopedExpression(SqlValidatorImpl.java:1143)\tat org.apache.calcite.sql.validate.SqlValidatorImpl.validate(SqlValidatorImpl.java:849)\tat org.apache.calcite.sql2rel.SqlToRelConverter.convertQuery(SqlToRelConverter.java:624)\tat org.apache.shardingsphere.sqlfederation.optimizer.statement.SQLStatementCompiler.compile(SQLStatementCompiler.java:55)\tat org.apache.shardingsphere.sqlfederation.optimizer.statement.SQLStatementCompilerEngine.compile(SQLStatementCompilerEngine.java:45)\tat org.apache.shardingsphere.sqlfederation.optimizer.SQLFederationCompilerEngine.compile(SQLFederationCompilerEngine.java:44)\tat org.apache.shardingsphere.sqlfederation.engine.SQLFederationEngine.compileQuery(SQLFederationEngine.java:227)\tat org.apache.shardingsphere.sqlfederation.engine.SQLFederationEngine.executeQuery(SQLFederationEngine.java:208)\tat org.apache.shardingsphere.driver.executor.engine.DriverExecuteQueryExecutor.executeQuery(DriverExecuteQueryExecutor.java:85)\tat org.apache.shardingsphere.driver.executor.engine.facade.DriverExecutorFacade.executeQuery(DriverExecutorFacade.java:104)\tat org.apache.shardingsphere.driver.jdbc.core.statement.ShardingSpherePreparedStatement.executeQuery(ShardingSpherePreparedStatement.java:180) (adsbygoogle = window.adsbygoogle || []).push({}); 赞助商 MySQL BIT_COUNT 调研 初步分析了 ShardingSphere 联邦查询中的 BIT_COUNT 函数异常后，我们再来调研下 MySQL BIT_COUNT 函数，看下该函数的实际作用，以及它支持的参数类型。 根据 MySQL BIT_COUNT 函数文档说明，函数语法格式为 BIT_COUNT(N)，用于计算参数 N 的二进制形式中 1 的个数，如果参数为 NULL，BIT_COUNT 函数也会返回 NULL。 Returns the number of bits that are set in the argument N as an unsigned 64-bit integer, or NULL if the argument is NULL.以无符号 64 位整数形式返回参数 N 中设置的位数，如果参数为 NULL，则返回 NULL。 MySQL 文档中并未具体说明 BIT_COUNT 具体支持哪些参数，我们使用 MySQL 来实际测试下 BIT_COUNT 函数。如下是一些常用类型的测试，包括数值类型、字符串类型，数值表达式，Boolean 类型以及 NULL。可以看到，当字符串中包含非 0-9 数字时，BIT_COUNT 函数会直接返回 0，而对于 Boolean 类型，会将 true、false 转换为 1 和 0，然后再进行 BIT_COUNT 计算。 mysql SELECT bit_count(123456), bit_count(123456), bit_count(abcdefg), BIT_COUNT(abcdef1234), bit_count(), bit_count(1 + 1), bit_count(true), bit_count(null);+-------------------+---------------------+----------------------+-------------------------+---------------+------------------+-----------------+-----------------+| bit_count(123456) | bit_count(123456) | bit_count(abcdefg) | BIT_COUNT(abcdef1234) | bit_count() | bit_count(1 + 1) | bit_count(true) | bit_count(null) |+-------------------+---------------------+----------------------+-------------------------+---------------+------------------+-----------------+-----------------+| 6 | 6 | 0 | 0 | 0 | 1 | 1 | NULL |+-------------------+---------------------+----------------------+-------------------------+---------------+------------------+-----------------+-----------------+1 row in set, 3 warnings (0.00 sec) 除了数值类型外，BIT_COUNT 函数还支持日期/时间类型，MySQL BIT_COUNT 对于日期和时间的处理也比较特别，它会删除日期和时间格式中的非数字字符，例如：BIT_COUNT(TIMESTAMP '1996-08-03 16:22:34') 会转换为 BIT_COUNT('19960803162234') 进行计算。 mysql SELECT BIT_COUNT(DATE 1996-08-03), BIT_COUNT(TIME 16:22:34), BIT_COUNT(TIMESTAMP 1996-08-03 16:22:34) UNION ALL - SELECT BIT_COUNT(DATE 2001-01-01), BIT_COUNT(TIME 12:20:00), BIT_COUNT(TIMESTAMP 2001-01-01 12:20:00) UNION ALL - SELECT BIT_COUNT(DATE 2002-05-03), BIT_COUNT(TIME 13:12:14), BIT_COUNT(TIMESTAMP 2002-05-03 13:12:14) UNION ALL - SELECT BIT_COUNT(DATE 2005-09-07), BIT_COUNT(TIME 06:02:04), BIT_COUNT(TIMESTAMP 2005-09-07 06:02:04) UNION ALL - SELECT BIT_COUNT(DATE 2007-01-01), BIT_COUNT(TIME 23:09:59), BIT_COUNT(TIMESTAMP 2007-01-01 23:09:59);+------------------------------+----------------------------+--------------------------------------------+| BIT_COUNT(DATE 1996-08-03) | BIT_COUNT(TIME 16:22:34) | BIT_COUNT(TIMESTAMP 1996-08-03 16:22:34) |+------------------------------+----------------------------+--------------------------------------------+| 12 | 11 | 24 || 12 | 8 | 22 || 14 | 5 | 22 || 16 | 9 | 25 || 14 | 10 | 21 |+------------------------------+----------------------------+--------------------------------------------+5 rows in set (0.03 sec) 此外，BIT_COUNT 函数还支持 BINARY 和 VARBINARY 类型以及负数等特殊类型和数值，下面展示了 BINARY 类型和 -1 计算 BIT_COUNT 值的结果。 mysql SELECT BIT_COUNT(CAST(xad AS BINARY(1))), BIT_COUNT(-1);+-------------------------------------+---------------+| BIT_COUNT(CAST(xad AS BINARY(1))) | BIT_COUNT(-1) |+-------------------------------------+---------------+| 5 | 64 |+-------------------------------------+---------------+1 row in set (0.01 sec) 了解了 MySQL 中 BIT_COUNT 函数的含义，以及支持的类型后，下面我们再来探究下 Calcite 目前对 BIT_COUNT 函数的适配，以及我们如何扩展 BIT_COUNT 函数，能让它适配更多的 MySQL 数据类型，从而解决 ShardingSphere 联邦查询中出现的问题。 Calcite BIT_COUNT 适配 Calcite BIT_COUNT 现状梳理 参考 CALCITE-3697，在 Calcite 1.38.0 版本中，Norman Jordan 支持了 MySQL BIT_COUNT 函数。MySQL BIT_COUNT 函数除了支持标准 BITCOUNT 函数中的数值类型和二进制类型外，它还支持小数类型的参数，会使用参数的整数部分进行计算，下面展示了 MySQL BIT_COUNT 函数的一些示例。 Expression Result bit_count(5.23) 2 bit_count(18446744073709551615) 64 bit_count(18446744073709551616) 63 bit_count(18446744073709551617) 63 bit_count(-9223372036854775808) 1 bit_count(-9223372036854775809) 1 Norman Jordan 在 PR#3927 中增加了标准 BITCOUNT 函数，以及 BIT_COUNT_BIG_QUERY 和 BIT_COUNT_MYSQL 方言函数，函数实现都是调用的 BuiltInMethod.BITCOUNT.method 方法，NullPolicy.STRICT 表示参数为 NULL 时，函数直接返回 NULL 结果。 // bitwisedefineMethod(BITCOUNT, BuiltInMethod.BITCOUNT.method, NullPolicy.STRICT);defineMethod(BIT_COUNT_BIG_QUERY, BuiltInMethod.BITCOUNT.method, NullPolicy.STRICT);defineMethod(BIT_COUNT_MYSQL, BuiltInMethod.BITCOUNT.method, NullPolicy.STRICT); MySQL BIT_COUNT 函数声明接受 NUMERIC 或者 BINARY 类型的参数，因此 ShardingSphere 联邦查询传递字符 a 以及 Boolean 类型时，会出现报错。此外，MySQL 支持的 DATE/TIME 类型，Calcite BIT_COUNT 函数也没有进行实现。 @LibraryOperator(libraries = MYSQL)public static final SqlFunction BIT_COUNT_MYSQL = new SqlFunction( BIT_COUNT, SqlKind.OTHER_FUNCTION, ReturnTypes.BIGINT_NULLABLE, null, OperandTypes.NUMERIC.or(OperandTypes.BINARY), SqlFunctionCategory.NUMERIC); BIT_COUNT 函数的具体实现逻辑在 SqlFunctions 类中，目前支持 long、BigDecimal 及 ByteString 3 种参数类型，long 类型入参用于支持常规的整数类型参数，直接调用 Long#bitCount 实现计算。 /** * Helper function for implementing codeBITCOUNT/code. Counts the number * of bits set in an integer value. */public static long bitCount(long b) return Long.bitCount(b); 如果参数类型是 BigDecimal，则会比较参数值是否在 (-2^63, 2^64 - 1) 范围内，如果等于最大值，则返回 64，如果大于最大值，则返回 63，如果小于等于最小值，则固定返回 1，如果在区间范围内，则会丢弃小数部分，然后按照整数计算 BIT_COUNT 结果。 private static final BigDecimal BITCOUNT_MAX = new BigDecimal(2).pow(64).subtract(new BigDecimal(1));private static final BigDecimal BITCOUNT_MIN = new BigDecimal(2).pow(63).negate();/** * Helper function for implementing codeBITCOUNT/code. Counts the number * of bits set in the integer portion of a decimal value. */public static long bitCount(BigDecimal b) final int comparison = b.compareTo(BITCOUNT_MAX); if (comparison 0) if (b.compareTo(BITCOUNT_MIN) = 0) return 1; else return bitCount(b.setScale(0, RoundingMode.DOWN).longValue()); else if (comparison == 0) return 64; else return 63; 对于二进制类型，BIT_COUNT 函数会使用 0xff 逐字节去计算，然后将 BIT 位为 1 的个数进行累加，得到最终 BIT_COUNT 结果。 /** * Helper function for implementing codeBITCOUNT/code. Counts the number * of bits set in a ByteString value. */public static long bitCount(ByteString b) long bitsSet = 0; for (int i = 0; i b.length(); i++) bitsSet += Integer.bitCount(0xff b.byteAt(i)); return bitsSet; 从 Calcite BIT_COUNT 实现逻辑可以了解到，目前，Calcite 还没有适配非数值字符、Boolean 以及日期/时间类型，下面我们来探究下如何增强 BIT_COUNT 实现逻辑，来支持 MySQL 的这些特殊类型。 Calcite BIT_COUNT 增强适配 初步尝试 想要为 BIT_COUNT 函数适配更多的数据类型，首先需要在 SqlLibraryOperators 中为 BIT_COUNT 函数声明更多的参数类型，我们增加如下的 OperandTypes.BOOLEAN、OperandTypes.CHARACTER、OperandTypes.DATETIME、OperandTypes.DATE、OperandTypes.TIME 和 OperandTypes.TIMESTAMP 类型。 @LibraryOperator(libraries = MYSQL)public static final SqlFunction BIT_COUNT_MYSQL = new SqlFunction( BIT_COUNT, SqlKind.OTHER_FUNCTION, ReturnTypes.BIGINT_NULLABLE, null, OperandTypes.or(OperandTypes.NUMERIC, OperandTypes.BINARY, OperandTypes.BOOLEAN, OperandTypes.CHARACTER, OperandTypes.DATETIME, OperandTypes.DATE, OperandTypes.TIME, OperandTypes.TIMESTAMP), SqlFunctionCategory.NUMERIC); 然后我们需要在 RexImpTable 类中定义函数实现，将上面的函数声明对象 BIT_COUNT_MYSQL 和具体的函数实现逻辑关联上。下面展示了定义函数实现的具体逻辑，Calcite 动态生成的代码会根据这个映射关系，找到函数的具体实现，即：SqlFunctions#bitCountMySQL，声明中最后一个参数 Object.class 代表的函数的参数类型，由于 MySQL BIT_COUNT 函数支持多种类型，因此我们先将参数类型定义为 Object.class。 // 定义函数实现defineMethod(BIT_COUNT_MYSQL, BuiltInMethod.BIT_COUNT_MYSQL.method, NullPolicy.STRICT);// 声明 MySQL BIT_COUNT 函数实现方法BIT_COUNT_MYSQL(SqlFunctions.class, bitCountMySQL, Object.class), SqlFunctions#bitCountMySQL 具体实现逻辑如下，除了支持原有的数值和二进制类型，我们还增加了 Boolean 和 String 类型，并且当 String 类型中出现非数字字符时，捕获异常然后返回 0。对于 Date、Time 和 Timestamp，实现逻辑中模拟了 MySQL 的行为，将这几个类型中的非数字字符去除，然后计算 BIT_COUNT 值。 /** * Helper function for implementing codeBIT_COUNT_MYSQL/code. Counts the number * of bits set in an Object value. */public static long bitCountMySQL(Object b) // Boolean 转换为 1 或 0 计算 if (b instanceof Boolean) return Long.bitCount((Boolean) b ? 1L : 0L); // 当出现非数字字符时，捕获异常返回 0 if (b instanceof String) try return bitCount(new BigDecimal((String) b)); catch (Exception ignore) return 0; if (b instanceof Number) return bitCount(new BigDecimal(b.toString())); if (b instanceof ByteString) return bitCount((ByteString) b); // Date 去除非数字字符后计算 if (b instanceof java.sql.Date) return bitCountMySQL(new SimpleDateFormat(yyyyMMdd).format(((java.sql.Date) b))); // Time 去除非数字字符后计算 if (b instanceof Time) return bitCountMySQL(new SimpleDateFormat(HHmmss).format(((Time) b))); // Timestamp 去除非数字字符后计算 if (b instanceof Timestamp) return bitCountMySQL(new SimpleDateFormat(yyyyMMddHHmmss).format(((Timestamp) b))); return 0; BIT_COUNT 实现逻辑增强后，我们在 Calcite functions.iq 文件中增加相关类型的测试用例，执行后发现 Date、Time 和 Timestamp 类型计算的结果和 MySQL 不一致，这又是为什么呢？ 跟踪 BIT_COUNT 函数的执行逻辑可以发现，Calcite 在生成执行代码前，会调用 implement 方法生成可执行代码，生成的过程中会调用 RexToLixTranslator 将日期、时间类型转换为自 1970 年以来的整数值，因此传入 bitCountMySQL 方法的日期、时间参数被转换为整数，此时计算的 BIT_COUNT 结果出现了错误。 解决这个问题最直接的想法是，去除将日期/时间转换为整数值的逻辑，直接将原始对象传递到 bitCountMySQL 方法中，但由于笔者缺乏对 Calcite 执行逻辑的研究，调整日期/时间转换可能影响到其他功能，因此打算先提交 PR，看看社区大佬是否能提供一些思路。 社区交流 提交 PR 后，很快 mihaibudiu 大佬就 Review 了 PR（非常感谢大佬多次指导），建议通过传递一个额外的类型参数，来解决这个问题。笔者按照这个思路，又研究了下 Calcite 生成代码的逻辑，发现调用 SqlFunctions.bitCountMySQL 之前，先调用了 SqlFunctions.toInt（如下所示），如果能够将 toInt 转换为 toDate，是否就能实现 MySQL DATE 相关的处理逻辑呢？我们先来研究下 toInt 函数生成代码的逻辑，然后参考 toInt 实现方式，应该就能实现 Date/Time 类型传递到 BIT_COUNT 函数中。 // 执行 SELECT BIT_COUNT(joinedate) FROM EMPS_DATE_TIME LIMIT 1 生成代码public org.apache.calcite.linq4j.Enumerable bind(final org.apache.calcite.DataContext root) final org.apache.calcite.linq4j.Enumerable _inputEnumerable = org.apache.calcite.linq4j.Linq4j.asEnumerable(new Object[] new Object[] 100, Fred, 10, null, null, 40, Integer.valueOf(25), Boolean.valueOf(true), false, // DATE 1996-08-03 转换为 9711 9711, new Object[] 110, Eric, 20, M, San Francisco, 3, Integer.valueOf(80), null, false, // DATE 2001-01-01 转换为 11323 11323, new Object[] 110, John, 40, M, Vancouver, 2, null, Boolean.valueOf(false), true, // DATE 2002-05-03 转换为 11810 11810, new Object[] 120, Wilma, 20, F, null, 1, Integer.valueOf(5), null, true, // DATE 2005-09-07 转换为 13033 13033, new Object[] 130, Alice, 40, F, Vancouver, 2, null, Boolean.valueOf(false), true, // DATE 2007-01-01 转换为 13514 13514).take(1); return new org.apache.calcite.linq4j.AbstractEnumerable() public org.apache.calcite.linq4j.Enumerator enumerator() return new org.apache.calcite.linq4j.Enumerator() public final org.apache.calcite.linq4j.Enumerator inputEnumerator = _inputEnumerable.enumerator(); public void reset() inputEnumerator.reset(); public boolean moveNext() return inputEnumerator.moveNext(); public void close() inputEnumerator.close(); public Object current() // 先调用 SqlFunctions.toInt，将 long 转为 int，再调用 SqlFunctions.bitCountMySQL return org.apache.calcite.runtime.SqlFunctions.bitCountMySQL(org.apache.calcite.runtime.SqlFunctions.toInt(((Object[]) inputEnumerator.current())[9])); ; ;public Class getElementType() return long.class; 跟踪 implement 方法内部的执行代码生成逻辑，Calcite 在生成投影列中的函数执行逻辑时，会调用 translateProjects 方法，该方法会记录 storageTypes 表示存储数据的类型，由于在 EnumerableValues 取值时，将 DATE 类型转换成了 long 类型，因此 storageTypes 记录的存储类型为 long 类型。 然后 translateProjects 方法会调用到前文我们提到的 RexToLixTranslator 类，具体调用的方法是 translateList，跟踪 translateList 方法内部逻辑，参数 operandList 此时为 $t10，desiredType（期望类型）为 long。 每个操作数 operand 都会调用 translate 方法，方法内部会将 $t10 转换为 BIT_COUNT($t9)，再根据 BIT_COUNT 函数从 RexImpTable.INSTANCE 中获取函数实现器 Implementor，具体实现逻辑如下。 /** * Visit @code RexCall. For most @code SqlOperators, we can get the implementor * from @code RexImpTable. Several operators (e.g., CaseWhen) with special semantics * need to be implemented separately. */@Overridepublic Result visitCall(RexCall call) if (rexResultMap.containsKey(call)) return rexResultMap.get(call); ... // 从 RexImpTable.INSTANCE 中获取函数实现器 Implementor final RexImpTable.RexCallImplementor implementor = RexImpTable.INSTANCE.get(operator); if (implementor == null) throw new RuntimeException(cannot translate call + call); // 获取函数操作数 final ListRexNode operandList = call.getOperands(); // 转换为内部类型 final List@Nullable Type storageTypes = EnumUtils.internalTypes(operandList); final ListResult operandResults = new ArrayList(); // 遍历操作数，实现函数逻辑 for (int i = 0; i operandList.size(); i++) final Result operandResult = implementCallOperand(operandList.get(i), storageTypes.get(i), this); operandResults.add(operandResult); callOperandResultMap.put(call, operandResults); final Result result = implementor.implement(this, call, operandResults); rexResultMap.put(call, result); return result; EnumUtils.internalTypes 会将操作数类型转换为内部类型，可以看到 DATE、TIME 类型会被转换为 Integer 或 int 类型，而 TIMESTAMP 类型则会被转换为 Long 或 long 类型。由于我们测试的 Date 类型，内部最终会使用 Integer 或 int 类型，因此在生成代码时需要调用 toInt 方法进行转换。 // EnumUtils#toInternal 方法static @Nullable Type toInternal(RelDataType type, boolean forceNotNull) switch (type.getSqlTypeName()) case DATE: case TIME: return type.isNullable() !forceNotNull ? Integer.class : int.class; case TIMESTAMP: return type.isNullable() !forceNotNull ? Long.class : long.class; default: return null; // we dont care; use the default storage type 了解完 toInt 函数生成的逻辑，笔者根据 mihaibudiu 建议，在 bitCountMySQL 方法中增加额外的类型参数，但此时出现了报错，Calcite 会在 RexImpTable 中查找函数，并根据方法名和参数进行调用，由于参数个数不匹配，此时会出现找不到函数异常。难道这样的方式就没法实现吗？让我们继续探究函数的实现逻辑。 最终实现 增加额外的类型参数执行出现了异常，笔者又进行了一番深入思考，想到之前贡献的 SYSDATE 函数——https://github.com/apache/calcite/pull/4029，在调用函数实现时，传递了额外的 DataContext 参数，bitCountMySQL 可以参考 sysDate 函数的实现方式。Calcite 调用 sysDate 执行时，是通过 SystemFunctionImplementor 进行实现，该类不根据参数进行查找，直接判断 op 的类型，然后通过 Expressions.call 生成 sysDate(DateContext) 调用，root 就是额外传递的 DataContext 参数。 而 BIT_COUNT 函数目前则是通过 MethodImplementor 类进行处理，该类实现函数逻辑时会根据参数个数进行查找。因此，可以考虑自行实现一个 BitCountMySQLImplementor，内部判断参数的类型，如果是 DATE、TIME 或 TIMESTAMP 类型，则将内部表示的整数值，转换为对应的日期、时间类型，然后调用 bitCountMySQL 函数实现。 BitCountMySQLImplementor 最终的实现逻辑如下，内部会根据 SqlTypeName 判断不同类型的处理方式，例如二进制类型，会调用原有的 bitCount 函数实现，而 DATE、TIME、TIMESTAMP 类型，则会先调用转换方法，再调用 bitCountMySQL 函数实现。 /** * Implementor for MYSQL @code BIT_COUNT function. */private static class BitCountMySQLImplementor extends AbstractRexCallImplementor BitCountMySQLImplementor() super(bitCount, NullPolicy.STRICT, false); @Override Expression implementSafe(final RexToLixTranslator translator, final RexCall call, final ListExpression argValueList) Expression expr = argValueList.get(0); RelDataType relDataType = call.getOperands().get(0).getType(); if (SqlTypeUtil.isNull(relDataType)) return argValueList.get(0); // In MySQL, BIT_COUNT(TIMESTAMP 1996-08-03 16:22:34) is converted to // BIT_COUNT(19960803162234) for calculation, so the internal int value // needs to be converted to DATE/TIME and TIMESTAMP. SqlTypeName type = relDataType.getSqlTypeName(); switch (type) case VARBINARY: case BINARY: return Expressions.call(SqlFunctions.class, bitCount, expr); case DATE: expr = Expressions.call(BuiltInMethod.INTERNAL_TO_DATE.method, expr); break; case TIME: expr = Expressions.call(BuiltInMethod.INTERNAL_TO_TIME.method, expr); break; case TIMESTAMP: expr = Expressions.call(BuiltInMethod.INTERNAL_TO_TIMESTAMP.method, expr); break; default: break; return Expressions.call(SqlFunctions.class, bitCountMySQL, Expressions.box(expr)); 新增 BitCountMySQLImplementor 后，我们需要调整函数实现注册逻辑，自定义的 Implementor 需要通过 define 方法进行注册。 define(BIT_COUNT_MYSQL, new BitCountMySQLImplementor()); 最后，我们再对日期类型进行测试，可以发现此时的执行结果已经正确。 结语 本文介绍了笔者在升级 Calcite 版本过程中，遇到的 BIT_COUNT 函数不支持 Case，经过一步步地调研探索，以及与 Calcite 社区大佬交流，最终解决了 MySQL BIT_COUNT 函数支持不完善的问题。在探索过程中，加深了笔者对于 Calcite 生成可执行代码的理解，也对 Calcite 时间、日期类型的实现有了更深入的认识。 笔者本地打包 Calcite 1.39.0-SNAPSHOT，使用 ShardingSphere 联邦查询 E2E 重新进行了测试，之前不支持的 Boolean，字符串以及日期、时间等类型都可以正确执行，目前还遗留了一个 MySQL UNSIGNED 类型不支持问题，笔者将继续探索 Calcite 支持 UNSIGNED 类型的方案，争取让 ShardingSphere 联邦查询更完美地适配各种数据类型，欢迎大家持续关注。 写在最后 笔者因为工作原因接触到 Calcite，前期学习过程中，深感 Calcite 学习资料之匮乏，因此创建了 Calcite 从入门到精通知识星球，希望能够将学习过程中的资料和经验沉淀下来，为更多想要学习 Calcite 的朋友提供一些帮助。 欢迎关注 欢迎关注「端小强的博客」微信公众号，会不定期分享日常学习和工作经验，欢迎大家关注交流。","tags":["Calcite","ShardingSphere"],"categories":["Calcite"]},{"title":"ShardingSphere Proxy 适配 MySQL addBatch/executeBatch 数组结果实战","path":"/blog/shardingsphere-proxy-adapt-mysql-add-batch-execute-batch-return-int-array-in-action.html","content":"问题背景 MySQL JDBC 驱动支持使用 addBatch 和 executeBatch 方法，进行批量写入操作。MySQL 提供了 allowMultiQueries 和 rewriteBatchedStatements 两个属性用于控制是否开启批量写入，如果用户在 JDBC URL 上开启 allowMultiQueries=truerewriteBatchedStatements=true 属性，那么 MySQL 驱动会将多条 INSERT 语句改写成多组 VALUES，将 DELETE 和 UPDATE 语句改写成 ; 分隔的多语句。如果用户不配置 allowMultiQueries 和 rewriteBatchedStatements 属性，MySQL 驱动则会以单条 SQL 方式逐一请求。 ShardingSphere Proxy 实现了完整的 MySQL 协议，因此对 MySQL 批量写入也进行了兼容，但是笔者在开发 DBPlusEngine 全局索引功能时，发现新增的批量写入 Case 断言报错（如下图），executeBatch 返回的 int[] 不正确，需要进一步分析和适配。 问题分析 最小化 Demo 复现 首先，由于 E2E 程序不方便调试，我们编写一个最小化 Demo 复现这个异常，如下是最小化 Demo 的源码，使用了 PreparedStatement 方式创建预编译 SQL，然后再通过 addBatch() 添加多组参数，此处需要注意，只有当 batchCount 3 时，MySQL 驱动才会将 SQL 改写为批量 SQL。 @Testvoid assertGlobalIndex() throws SQLException try ( // Connection connection = DriverManager.getConnection(jdbc:shardingsphere:classpath:config/driver/foo-driver-fixture-global-index.yaml); Connection connection = DriverManager.getConnection(jdbc:mysql://127.0.0.1:3307/sphereex_global_index?useSSL=falseuseServerPrepStmts=trueuseLocalSessionState=truecharacterEncoding=utf-8allowMultiQueries=truerewriteBatchedStatements=true, root, root); // Connection connection = DriverManager.getConnection(jdbc:mysql://127.0.0.1:3306/global_index?useSSL=falseuseServerPrepStmts=trueuseLocalSessionState=truecharacterEncoding=utf-8allowMultiQueries=truerewriteBatchedStatements=true, root, 123456); PreparedStatement preparedStatement = connection.prepareStatement(DELETE FROM t_order WHERE order_id = ?)) // PreparedStatement preparedStatement = connection.prepareStatement(DELETE FROM t_order_1 WHERE order_id = ?)) connection.setAutoCommit(false); preparedStatement.setObject(1, 1001); preparedStatement.addBatch(); preparedStatement.setObject(1, 1101); preparedStatement.addBatch(); preparedStatement.setObject(1, 999); preparedStatement.addBatch(); preparedStatement.setObject(1, 998); preparedStatement.addBatch(); final int[] ints = preparedStatement.executeBatch(); System.out.println(ints); preparedStatement.clearBatch(); preparedStatement.setObject(1, 1000); preparedStatement.addBatch(); preparedStatement.setObject(1, 1100); preparedStatement.addBatch(); preparedStatement.setObject(1, 999); preparedStatement.addBatch(); preparedStatement.setObject(1, 998); preparedStatement.addBatch(); final int[] ints2 = preparedStatement.executeBatch(); System.out.println(ints2); preparedStatement.clearBatch(); connection.rollback(); Demo 中我们连接的是 3307 端口，该端口指向的是 Proxy 服务，我们需要将如下的配置添加到 Proxy 配置文件中（如下展示的全局索引配置为 ShardingSphere 商业版功能，开源版本需要删除全局索引配置）： databaseName: sphereex_global_indexdataSources: global_index: url: jdbc:mysql://localhost:3306/global_index?serverTimezone=UTCuseSSL=falsecharacterEncoding=utf-8allowPublicKeyRetrieval=true username: root password: 123456 connectionTimeoutMilliseconds: 30000 idleTimeoutMilliseconds: 60000 maxLifetimeMilliseconds: 1800000 maxPoolSize: 50 minPoolSize: 2rules:- !SINGLE tables: - *.*- !SHARDING tables: t_order: actualDataNodes: global_index.t_order_$0..9 tableStrategy: standard: shardingColumn: order_id shardingAlgorithmName: t_order_inline globalIndexStrategy: globalIndexNames: - t_order_user_id_idx - t_order_merchant_id_idx consistencyLevel: STRONG globalIndexes: t_order_user_id_idx: actualDataNodes: global_index.t_order_user_id_idx_$0..9 databaseStrategy: none: tableStrategy: standard: shardingColumn: user_id shardingAlgorithmName: t_order_user_id_idx_inline coveringColumns: - order_id t_order_merchant_id_idx: actualDataNodes: global_index.t_order_merchant_id_idx_$0..9 databaseStrategy: none: tableStrategy: standard: shardingColumn: merchant_id shardingAlgorithmName: t_order_merchant_id_idx_inline coveringColumns: - order_id - creation_date shardingAlgorithms: t_order_inline: type: INLINE props: algorithm-expression: t_order_$order_id % 10 t_order_user_id_idx_inline: type: INLINE props: algorithm-expression: t_order_user_id_idx_$user_id % 10 t_order_merchant_id_idx_inline: type: INLINE props: algorithm-expression: t_order_merchant_id_idx_$merchant_id % 10 启动 Demo 程序，可以复现和 E2E 中相同的异常，下面我们就来分析下异常的具体原因。 返回多个 MySQLOKPacket 我们使用最小化 Demo 进行 Debug，可以发现 Proxy 多语句的入口类是 MySQLMultiStatementsHandler，该类目前返回的结果为 UpdateResponseHeader，UpdateResponseHeader 会被封装为单个 MySQLOKPacket，目前执行多语句时，只会简单地将 updated 进行了累加，因此断言时结果不正确。 private UpdateResponseHeader executeBatchedStatements(final ExecutionGroupContextJDBCExecutionUnit executionGroupContext) throws SQLException boolean isExceptionThrown = SQLExecutorExceptionHandler.isExceptionThrown(); ResourceMetaData resourceMetaData = metaDataContexts.getMetaData().getDatabase(connectionSession.getUsedDatabaseName()).getResourceMetaData(); JDBCExecutorCallbackint[] callback = new BatchedJDBCExecutorCallback(resourceMetaData, sqlStatementSample, isExceptionThrown); Listint[] executeResults = jdbcExecutor.execute(executionGroupContext, callback); int updated = 0; for (int[] eachResult : executeResults) for (int each : eachResult) updated += each; // TODO Each logic SQL should correspond to an OK Packet. return new UpdateResponseHeader(sqlStatementSample, Collections.singletonList(new UpdateResult(updated, 0L))); 根据此处的 TODO 标记可以看出，MySQL 执行多语句时，需要返回批量的 MySQLOKPacket 集合，分别对应每条语句的执行结果。为了解决这个问题，需要增加一个 MultiStatementsUpdateResponseHeader 类进行封装，将多个 MySQLOKPacket 集合封装到其中： /** * Multi statements update response header. */@RequiredArgsConstructor@Getterpublic final class MultiStatementsUpdateResponseHeader implements ResponseHeader private final CollectionUpdateResponseHeader updateResponseHeaders; 然后在 MySQLComQueryPacketExecutor 执行器类中，对 MultiStatementsUpdateResponseHeader 进行处理，具体处理逻辑如下，根据 MultiStatementsUpdateResponseHeader 中维护的 UpdateResponseHeader 集合，将其组装为多个 MySQLOKPacket。 @Overridepublic CollectionDatabasePacket execute() throws SQLException ResponseHeader responseHeader = proxyBackendHandler.execute(); if (responseHeader instanceof QueryResponseHeader) return processQuery((QueryResponseHeader) responseHeader); responseType = ResponseType.UPDATE; if (responseHeader instanceof MultiStatementsUpdateResponseHeader) return processMultiStatementsUpdate((MultiStatementsUpdateResponseHeader) responseHeader); return processUpdate((UpdateResponseHeader) responseHeader);private CollectionDatabasePacket processMultiStatementsUpdate(final MultiStatementsUpdateResponseHeader responseHeader) CollectionDatabasePacket result = new LinkedList(); int index = 0; for (UpdateResponseHeader each : responseHeader.getUpdateResponseHeaders()) boolean lastPacket = ++index == responseHeader.getUpdateResponseHeaders().size(); result.addAll(ResponsePacketBuilder.buildUpdateResponsePackets(each, ServerStatusFlagCalculator.calculateFor(connectionSession, lastPacket))); return result; 此时，我们再次进行测试，但是发现结果仍然不正确，这又是为什么呢？想要搞清楚 MySQL 内部协议的交互逻辑，我们需要通过 WireShark 进行抓包，对比原生 MySQL 批量语句执行和 Proxy 批量语句执行之间的差异。 ServerStatusFlag 增加 SERVER_MORE_RESULTS_EXISTS 为了搞清楚 Proxy 和 MySQL 之间的差异，我们分别执行 Demo 程序中的 Proxy 示例和 MySQL 示例，并使用 WireShark 进行抓包（WireShark 使用可参考使用 Wireshark 解决 BenchmarkSQL 压测 Proxy 异常）。首先，我们执行 MySQL 批量写入并进行抓包，如下记录了抓包的内容，包括了 1 次 Request 和 4 次 Response。 然后我们再执行 Proxy 批量写入，并使用 WireShark 抓包，如下记录了 Proxy 抓包的内容，只有 1 次 Request 和 1 次 Response。 对比 MySQL 和 Proxy 抓包的差异，可以发现 MySQL 直到最后一个 MySQLOKPacket Server Status 才变为 1，前三个 MySQLOKPacket Server Status 都为 9（8 多语句 SERVER_MORE_RESULTS_EXISTS + 1 事务中 SERVER_STATUS_IN_TRANS），而 Proxy 第一个 Response 就返回了 1，并且后续不再返回 Response。 排查 MySQL 驱动可以发现，如果 SQL Response 中的 ServerStatusFlag 不包含 MySQLStatusFlag.SERVER_MORE_RESULTS_EXISTS，MySQL 驱动就只会读取第一个 MySQLOKPacket，并填充到客户端 int[] 数组中。因此可以考虑在封装多语句 MySQLOKPacket 时，根据多语句是否为最后一条，决定该标记的设置，当 MySQLOKPacket 未遍历到最后一条记录时，应设置 SERVER_MORE_RESULTS_EXISTS 标记。 我们调整 ServerStatusFlagCalculator#calculateFor 方法的实现逻辑，根据传入的 lastPacket 标记，决定是否设置 SERVER_MORE_RESULTS_EXISTS，具体实现逻辑如下： /** * Calculate server status flag for specified connection. * * @param connectionSession connection session * @param lastPacket last packet * @return server status flag */public static int calculateFor(final ConnectionSession connectionSession, final boolean lastPacket) int result = 0; result |= connectionSession.isAutoCommit() ? MySQLStatusFlag.SERVER_STATUS_AUTOCOMMIT.getValue() : 0; result |= connectionSession.getTransactionStatus().isInTransaction() ? MySQLStatusFlag.SERVER_STATUS_IN_TRANS.getValue() : 0; result |= lastPacket ? 0 : MySQLStatusFlag.SERVER_MORE_RESULTS_EXISTS.getValue(); return result; 修改完成后，我们再次运行测试程序，发现此时直接出现了 NPE，需要进一步分析 NPE 的原因。 capabilityFlags 增加 CLIENT_MULTI_RESULTS/CLIENT_PS_MULTI_RESULTS 根据出现 NPE 的位置，我们大致可以定位到 NativeProtocol#readNextResultset 方法，通过 Debug 可以发现，在 MySQL 驱动获取下一个结果集时，currentProtocolEntity 为空导致了 NPE。排查 currentProtocolEntity 赋值的地方，发现是 serverSession.useMultiResults() 返回 false，导致 currentProtocolEntity 未赋值，而 useMultiResults 方法的判断逻辑如下，会从 clientParam 标记中获取 CLIENT_MULTI_RESULTS 和 CLIENT_PS_MULTI_RESULTS。 // /Users/duanzhengqiang/.m2/repository/com/mysql/mysql-connector-j/8.0.31/mysql-connector-j-8.0.31-sources.jar!/com/mysql/cj/protocol/a/NativeServerSession.java:220@Overridepublic boolean useMultiResults() return (this.clientParam CLIENT_MULTI_RESULTS) != 0 || (this.clientParam CLIENT_PS_MULTI_RESULTS) != 0; 可以看到该判断主要依赖 clientParam 变量，NativeAuthenticationProvider 方法会在登录认证通过后，调用 setClientParam 方法初始化该变量，具体代码逻辑位置如下。 // /Users/duanzhengqiang/.m2/repository/com/mysql/mysql-connector-j/8.0.31/mysql-connector-j-8.0.31-sources.jar!/com/mysql/cj/protocol/a/NativeAuthenticationProvider.java:201long clientParam = capabilityFlags NativeServerSession.CLIENT_LONG_PASSWORD //| (this.propertySet.getBooleanProperty(PropertyKey.useAffectedRows).getValue() ? // 0 : capabilityFlags NativeServerSession.CLIENT_FOUND_ROWS) //| capabilityFlags NativeServerSession.CLIENT_LONG_FLAG //| (this.useConnectWithDb ? capabilityFlags NativeServerSession.CLIENT_CONNECT_WITH_DB : 0) //| (this.propertySet.getBooleanProperty(PropertyKey.useCompression).getValue() ? // capabilityFlags NativeServerSession.CLIENT_COMPRESS : 0) //| (this.propertySet.getBooleanProperty(PropertyKey.allowLoadLocalInfile).getValue() || this.propertySet.getStringProperty(PropertyKey.allowLoadLocalInfileInPath).isExplicitlySet() ? // capabilityFlags NativeServerSession.CLIENT_LOCAL_FILES : 0) //| capabilityFlags NativeServerSession.CLIENT_PROTOCOL_41 //| (this.propertySet.getBooleanProperty(PropertyKey.interactiveClient).getValue() ? // capabilityFlags NativeServerSession.CLIENT_INTERACTIVE : 0) //| (this.propertySet.SslModegetEnumProperty(PropertyKey.sslMode).getValue() != SslMode.DISABLED ? // capabilityFlags NativeServerSession.CLIENT_SSL : 0) //| capabilityFlags NativeServerSession.CLIENT_TRANSACTIONS // Required to get server status values.| NativeServerSession.CLIENT_SECURE_CONNECTION //| (this.propertySet.getBooleanProperty(PropertyKey.allowMultiQueries).getValue() ? // capabilityFlags NativeServerSession.CLIENT_MULTI_STATEMENTS : 0) //| capabilityFlags NativeServerSession.CLIENT_MULTI_RESULTS // Always allow multiple result sets.| capabilityFlags NativeServerSession.CLIENT_PS_MULTI_RESULTS // Always allow multiple result sets for SSPS.| NativeServerSession.CLIENT_PLUGIN_AUTH //| (NONE.equals(this.propertySet.getStringProperty(PropertyKey.connectionAttributes).getValue()) ? // 0 : capabilityFlags NativeServerSession.CLIENT_CONNECT_ATTRS) //| capabilityFlags NativeServerSession.CLIENT_PLUGIN_AUTH_LENENC_CLIENT_DATA //| (this.propertySet.getBooleanProperty(PropertyKey.disconnectOnExpiredPasswords).getValue() ? // 0 : capabilityFlags NativeServerSession.CLIENT_CAN_HANDLE_EXPIRED_PASSWORD) //| (this.propertySet.getBooleanProperty(PropertyKey.trackSessionState).getValue() ? // capabilityFlags NativeServerSession.CLIENT_SESSION_TRACK : 0) //| capabilityFlags NativeServerSession.CLIENT_DEPRECATE_EOF //| capabilityFlags NativeServerSession.CLIENT_QUERY_ATTRIBUTES //| capabilityFlags NativeServerSession.CLIENT_MULTI_FACTOR_AUTHENTICATION;sessState.setClientParam(clientParam); Proxy 端通过 MySQLAuthenticationEngine 处理 MySQL 登录认证，会将握手结果封装在 MySQLHandshakePacket 中，其中包含了 capabilityFlags 服务端能力标志位的信息。 @Overridepublic int handshake(final ChannelHandlerContext context) int result = ConnectionIdGenerator.getInstance().nextId(); connectionPhase = MySQLConnectionPhase.AUTH_PHASE_FAST_PATH; boolean sslEnabled = ProxySSLContext.getInstance().isSSLEnabled(); if (sslEnabled) context.pipeline().addFirst(MySQLSSLRequestHandler.class.getSimpleName(), new MySQLSSLRequestHandler()); context.writeAndFlush(new MySQLHandshakePacket(result, sslEnabled, authPluginData)); MySQLStatementIdGenerator.getInstance().registerConnection(result); return result; 参考 MySQL Client/Server Protocol 文档 - Capabilities Flags，能力标志位共 32 个 bit 位，每个 bit 位代表协议的一个可选功能，客户端和服务端的交集，共同决定了将使用协议的哪些可选部分。 按照功能属于高 16 位，还是低 16 位，需要分别将功能设置到 capabilityFlagsLower 和 capabilityFlagsUpper 中。查看 CLIENT_MULTI_RESULTS 和 CLIENT_PS_MULTI_RESULTS，它们属于高位功能，因此在 calculateHandshakeCapabilityFlagsUpper 方法中增加 Flags 即可，如下是具体设置代码。 CLIENT_MULTI_RESULTS(0x00020000),CLIENT_PS_MULTI_RESULTS(0x00040000),/** * Get handshake capability flags upper bit. * * @return handshake capability flags upper bit */public static int calculateHandshakeCapabilityFlagsUpper() return calculateCapabilityFlags(CLIENT_MULTI_STATEMENTS, CLIENT_PLUGIN_AUTH, CLIENT_MULTI_RESULTS, CLIENT_PS_MULTI_RESULTS) 16; 修改完成后，再次使用 Demo 程序测试，发现已经能够返回正确的结果，通过 JDBC 可以正常执行 addBatch/executeBatch 并返回 int[] 数组。 (adsbygoogle = window.adsbygoogle || []).push({}); 赞助商 功能测试 最后，我们使用全局索引功能 E2E 再次进行测试，原先断言失败的 Case 现在终于可以通过，大家终于可以放心使用商业版全局索引功能。在此，也真心向大家推荐 SphereEx 的 DBPlusEngine，相比开源的 ShardingSphere，它具有更完善的企业级功能，不仅能够进行海量数据的分片管理，还可以用于数据安全加密和数据库替换等场景，更多信息可以查看 SphereEx 官网。 结语 本文介绍了 E2E 测试 Proxy 发现批量写入返回结果错误后，如何一步步梳理 Proxy 代码，使用 Wireshark 抓包对比分析，以及排查 MySQL 驱动源码，最终完美解决了问题。提升 Proxy 对 MySQL 协议的兼容度，很直接的方法就是同测试用例比对，通过强大的 Wireshark 工具，我们可以很清晰地观测到请求过程中的差异，进而快速找到解决问题的方案。本案例的排查思路也适合其他 Proxy 接入端的问题，希望对大家有用，由于本人对 Wireshark 使用经验有限，如果问题也欢迎指正。 欢迎关注 欢迎关注「端小强的博客」微信公众号，会不定期分享日常学习和工作经验，欢迎大家关注交流。","tags":["ShardingSphere","MySQL"],"categories":["ShardingSphere"]},{"title":"Apache Calcite Catalog 拾遗之 UDF 函数实现和扩展","path":"/blog/apache-calcite-catalog-udf-function-implementation-and-extension.html","content":"注意：本文基于 Calcite main 分支 60e0a3f 版本源码进行学习研究，其他版本可能会存在实现逻辑差异，对源码感兴趣的读者请注意版本选择。 前言 最近，很多朋友咨询关于 Calcite UDF 实现和扩展的问题，在之前 Apache Calcite System Catalog 实现探究一文中，我们简单介绍过 Catalog 中的 Function 对象，也了解到 Calcite 内置了很多函数实现，但在实际使用中内置函数往往无法满足要求，用户需要能够根据自己的需求，灵活地注册新的函数。Caclite 允许用户动态注册 UDF 函数，从而实现更加复杂的 SQL 逻辑，下面本文将深入探讨 Calcite 函数的实现原理，以及 UDF 函数的扩展方式，帮助大家更好地在项目中使用 Calcite UDF。 Calcite 函数简介 在日常开发、数据分析工作中，我们除了会使用常用的 SQL 语句外，还会经常用到函数来实现一些特殊功能，函数功能的强弱直接会影响我们的开发效率。Calcite 作为当前流行的计算引擎，对函数功能也有较好的支持，它内置了不同数据库的上百种常用函数，可以直接调用执行。此外，Calcite 也提供了 UDF 自定义函数能力，用户可以通过 Schema 注册 UDF，从而实现更灵活地 SQL 运算逻辑。 在了解 UDF 函数实现和扩展前，我们先来了解下 Calcite 函数的基本概念。Calcite 对函数的定义是：接受参数并返回结果的命名表达式，函数一般通过 Schema 进行注册，然后使用 Schema#getFunctions 获取函数，获取函数时会根据参数类型进行过滤。下面是 Schema 中 Function 接口声明： public interface Function ListFunctionParameter getParameters(); Function 接口提供了 getParameters 获取函数参数的方法，它包含了 ScalarFunction、AggregateFunction、TableFunction 和 TableMarco 等几个主要的子接口。ScalarFunction 对应标量函数，也就是函数返回的结果为一个标量，AggregateFunction 对应聚合函数，会将多个值聚合计算为一个标量返回。 TableFunction 和 TableMacro 都对应了表函数，会返回一个表，他们的区别是 TableMacro 会在编译期间进行调用，编译期展开表达式允许 Calcite 实现更加强大的查询优化，例如我们可以对视图在编译期进行展开。相比于 TableMacro，TableFunction 则需要在执行阶段才能知道表的结果。 下图展示了 Function 的继承体系，Function 接口的 4 个子接口 ScalarFunction、AggregateFunction、TableFunction 和 TableMarco，他们都有对应的 Impl 实现类，实现类中定义了很多函数处理相关的方法，下面小节我们将分别对这几类函数的内部实现进行探究。 (adsbygoogle = window.adsbygoogle || []).push({}); 赞助商 内置函数实现探究 标量函数 标量函数（ScalarFunction）是指将输入数据转换为输出数据的函数，通常用于对单个字段值进行计算和转换。例如：ABS(num) 函数，它负责将每行输入的 num 字段值转换为绝对值再输出。 下图展示了标量函数在 Schema 对象中的继承体系，核心的实现逻辑在 ScalarFunctionImpl 类中，它实现了 ScalarFunction 和 ImplementableFunction 接口，并继承了 ReflectiveFunctionBase 抽象类，下面我们分别来介绍下这些接口和类的作用。 ScalarFunction 接口： ScalarFunction 接口继承了 Function 接口，并在接口中声明了 getReturnType 方法，用于表示标量函数返回值的类型。 /** * Function that returns a scalar result. */public interface ScalarFunction extends Function /** * Returns the return type of this function, constructed using the given * type factory. * * @param typeFactory Type factory */ RelDataType getReturnType(RelDataTypeFactory typeFactory); ImplementableFunction 接口： ImplementableFunction 接口用于声明该函数可以转换为 Java 代码进行执行，接口中提供了 getImplementor 方法，可以返回一个函数实现器 CallImplementor。 /** * Function that can be translated to java code. * * @see ScalarFunction * @see TableFunction */public interface ImplementableFunction extends Function /** * Returns implementor that translates the function to linq4j expression. * * @return implementor that translates the function to linq4j expression. */ CallImplementor getImplementor(); CallImplementor 接口中声明了 implement 方法，可以将函数转换为 linq4j 表达式，用于函数逻辑的调用（linq4j 参考了 .NET 中的 LINQ（Language-Integrated Query） 功能，可以实现类似于 SQL 的声明式语法，后续我们专门写一篇文章介绍 linq4j）。 public interface CallImplementor /** * Implements a call. * * @param translator Translator for the call * @param call Call that should be implemented * @param nullAs The desired mode of @code null translation * @return Translated call */ Expression implement(RexToLixTranslator translator, RexCall call, RexImpTable.NullAs nullAs); ReflectiveFunctionBase 抽象类： ReflectiveFunctionBase 抽象类用于处理基于方法实现的函数，负责将方法参数映射为 ListFunctionParameter。在初始化 ReflectiveFunctionBase 时，会传入函数逻辑对应的 Method 对象，ParameterListBuilder 类会根据 method 对象构造 ListFunctionParameter。 /** * Creates a ReflectiveFunctionBase. * * @param method Method that is used to get type information from */protected ReflectiveFunctionBase(Method method) this.method = method; this.parameters = builder().addMethodParameters(method).build(); ParameterListBuilder 类的核心逻辑为 addMethodParameters 方法，内部会遍历方法参数，通过 ReflectUtil 工具类获取参数名称（优先从 Parameter 注解中获取名称，无注解则使用参数名）和参数是否可选（优先从 Parameter 注解中获取是否可选，无注解则为 false），然后将 type、name 和 optional 参数传入 add 方法，用于创建 FunctionParameter 对象。 public ParameterListBuilder addMethodParameters(Method method) final Class?[] types = method.getParameterTypes(); for (int i = 0; i types.length; i++) add(types[i], ReflectUtil.getParameterName(method, i), ReflectUtil.isParameterOptional(method, i)); return this; add 方法实现逻辑如下，主要将传入的 type 参数通过 typeFactory 构建为 RelDataType 类型，将 name 和 optional 封装到对应的 FunctionParameter 接口方法中。此外，还根据参数的个数生成了 ordinal 序号，并封装到 getOrdinal 方法中。 public ParameterListBuilder add(final Class? type, final String name, final boolean optional) final int ordinal = builder.size(); builder.add(new FunctionParameter() @Override public String toString() return ordinal + : + name + + type.getSimpleName() + (optional ? ? : ); // 基于 0 的参数序号 @Override public int getOrdinal() return ordinal; // 参数名称 @Override public String getName() return name; // 参数类型 @Override public RelDataType getType(RelDataTypeFactory typeFactory) return typeFactory.createJavaType(type); // 参数是否可选，可选参数可以在函数调用时省略 @Override public boolean isOptional() return optional; ); return this; 除了 FunctionParameter 构建逻辑外，ReflectiveFunctionBase 还提供了 classHasPublicZeroArgsConstructor 和 classHasPublicFunctionContextConstructor 方法，用于判断函数逻辑类是否提供了无关构造方法，以及包含 FunctionContext（提供函数调用的相关信息，可以使函数在构造期间提前执行，无需每次调用执行，具体可以参考 FunctionContext）的构造方法，这些构造方法会在函数初始化时进行调用，不包含可能会抛出异常。 ScalarFunctionImpl 类： ScalarFunctionImpl 类实现了 ScalarFunction 和 ImplementableFunction 接口中的相关方法，内部方法通过调用如下的私有构造方法进行初始化。如下展示了 ScalarFunctionImpl 了的构造方法，首先会调用 super(method) 初始化函数参数 ListFunctionParameter，然后将函数实现器 CallImplementor 存储在成员变量中。 /** * Private constructor. */private ScalarFunctionImpl(Method method, CallImplementor implementor) super(method); this.implementor = implementor; ScalarFunctionImpl 核心的创建逻辑是由公共的 create 方法触发的，外部调用将函数方法 Method 对象传递给 create 方法。方法内部会先判断 Method 是否为静态方法，非静态方法如果没有无参构造方法，或者没有包含 FunctionContext 的构造方法，则会抛出异常。 如果检查通过，则根据 Method 对象创建 CallImplementor 函数实现器，然后调用私有的 ScalarFunctionImpl 构造方法，将 Method 对象和 CallImplementor 函数实现器传递给构造方法。 public static ScalarFunction create(Method method) if (!isStatic(method)) Class? clazz = method.getDeclaringClass(); if (!classHasPublicZeroArgsConstructor(clazz) !classHasPublicFunctionContextConstructor(clazz)) throw RESOURCE.requireDefaultConstructor(clazz.getName()).ex(); CallImplementor implementor = createImplementor(method); return new ScalarFunctionImpl(method, implementor); 创建 CallImplementor 函数实现器的逻辑如下，首先会调用 getNullPolicy 方法，返回 NullPolicy 枚举类型用于描述函数（或运算符）何时返回 NULL。 private static CallImplementor createImplementor(final Method method) final NullPolicy nullPolicy = getNullPolicy(method); return RexImpTable.createImplementor(new ReflectiveCallNotNullImplementor(method), nullPolicy, false); NullPolicy 枚举类包含了 ALL、STRICT、SEMI_STRICT、ANY、ARG0 和 NONE。ALL 表示只有所有的参数为 NULL，函数结果采返回 NULL。STRICT 表示只有一个参数为 NULL 使，函数结果返回 NULL。SEMI_STRICT 表示有 1 个或多个参数为 NULL 时，函数结果返回 NULL。ANY 表示只要有任意一个参数为 NULL，则函数结果返回 NULL，ANY 和 STRICT 比较类似，Caclite 更推荐使用 STRICT 类型。ARG0 表示第一个参数为 NULL 时，函数结果返回 NULL。NONE 表示不指定 NULL 策略，由函数逻辑进行处理。 这些枚举类型中，STRICT、SEMI_STRICT 比较常用，Calcite 分别为他们提供了 Strict 和 SemiStrict 注解，可以标注在函数方法或类上，用来声明 NULL 值策略。 public enum NullPolicy /** * Returns null if and only if all of the arguments are null; * If all of the arguments are false return false otherwise true. */ ALL, /** * Returns null if and only if one of the arguments are null. */ STRICT, /** * Returns null if one of the arguments is null, and possibly other times. */ SEMI_STRICT, /** * If any of the arguments are null, return null. */ ANY, /** * If the first argument is null, return null. */ ARG0, NONE 获取到 NullPolicy 后，调用 RexImpTable.createImplementor() 方法创建函数实现器，由于函数实现器中的 implement 方法在执行阶段才会调用，我们将在后面的 ScalarFunction 案例中进行详细介绍。 聚合函数 聚合函数（AggregateFunction）是指将多个值组合转换为标量值输出的函数。例如：SUM(num) 函数，它负责将每行输入的 num 字段值进行累加，最终输出累加总和。 Calcite 聚合函数内部包含了一个累加过程，如下面的伪代码所示，Accumulator 累加器内部维护了一个 sum 变量，用于存储 SUM 函数计算的累加值。聚合函数调用 init 方法进行初始化，此时会创建一个累加器对象，并将 sum 初始化为 0，然后通过 add 方法将当前行的值添加到累加器中进行计算。如果有多个累加器，则可以使用 merge 方法将两个累加器中的值合二为一，最后计算完成可以通过 result 方法返回结果。 // 聚合函数累加器struct Accumulator final int sum;// 聚合函数初始化方法Accumulator init() return new Accumulator(0);// 聚合函数累加方法Accumulator add(Accumulator a, int x) return new Accumulator(a.sum + x);// 聚合函数合并方法Accumulator merge(Accumulator a, Accumulator a2) return new Accumulator(a.sum + a2.sum);// 聚合函数获取结果方法int result(Accumulator a) return a.sum; 下图展示了聚合函数在 Schema 对象中的继承体系，核心的实现逻辑在 AggregateFunctionImpl 类中，它实现了 AggregateFunction 和 ImplementableAggFunction 接口，下面我们分别来介绍下这些接口和类的作用。 AggregateFunction 接口： AggregateFunction 接口继承了 Function 接口，并在接口中声明了 getReturnType 方法，用于表示聚合函数返回值的类型。 /** * Function that combines several values into a scalar result. */public interface AggregateFunction extends Function /** * Returns the return type of this function, constructed using the given * type factory. * * @param typeFactory Type factory */ RelDataType getReturnType(RelDataTypeFactory typeFactory); ImplementableAggFunction 接口： ImplementableAggFunction 接口用于声明该函数可以转换为 Java 代码进行执行，接口中提供了 getImplementor 方法，可以返回一个聚合函数实现器 AggImplementor，windowContext 用于标记当前聚合函数是否包含在窗口运算中。 public interface ImplementableAggFunction extends AggregateFunction /** * Returns implementor that translates the function to linq4j expression. * * @param windowContext true when aggregate is used in window context * @return implementor that translates the function to linq4j expression. */ AggImplementor getImplementor(boolean windowContext); AggImplementor 接口可以实现聚合函数所需的初始化、累加以及获取结果方法，如下展示了 AggImplementor 接口中的方法，getStateType 方法可以返回聚合函数实现时，中间变量的类型，例如：字符串连接函数，它的中间变量类型可以是 StringBuilder。implementReset、implementAdd 和 implementResult 分别对应了聚合函数的初始化、累加和获取结果方法，AggImplementor 接口的实现类，会根据不同的聚合函数类型实现其逻辑。 public interface AggImplementor // 返回聚合函数实现时，中间变量的类型 // 例如：字符串连接函数，它的中间变量类型可以是 StringBuilder ListType getStateType(AggContext info); // 将中间变量重置为初识状态 // 应使用 AggResetContext.accumulator() 来引用状态变量 void implementReset(AggContext info, AggResetContext reset); // 将新增加的当前值，累加到中间变量 void implementAdd(AggContext info, AggAddContext add); // 根据中间变量计算结果值 Expression implementResult(AggContext info, AggResultContext result); AggregateFunctionImpl 类： AggregateFunctionImpl 类实现了 AggregateFunction 和 ImplementableAggFunction 接口，通过私有的构造方法进行初始化，构造方法如下，declaringClass 表示聚合函数对应的实现类，params 表示聚合函数的参数，valueTypes 表示聚合函数参数的类型，accumulatorType 表示聚合器的类型，resultType 表示函数结果类型。 initMethod、addMethod、mergeMethod 和 resultMethod 分别表示聚合函数初始化方法、累加方法、合并方法和结果方法，isStatic 表示 initMethod 是否为静态方法。 private AggregateFunctionImpl(Class? declaringClass, ListFunctionParameter params, ListClass? valueTypes, Class? accumulatorType, Class? resultType, Method initMethod, Method addMethod, @Nullable Method mergeMethod, @Nullable Method resultMethod) this.declaringClass = declaringClass; this.valueTypes = ImmutableList.copyOf(valueTypes); this.parameters = params; this.accumulatorType = accumulatorType; this.resultType = resultType; this.initMethod = requireNonNull(initMethod, initMethod); this.addMethod = requireNonNull(addMethod, addMethod); this.mergeMethod = mergeMethod; this.resultMethod = resultMethod; this.isStatic = isStatic(initMethod); assert resultMethod != null || accumulatorType == resultType; 由于 AggregateFunctionImpl 构造方法是私有的，通常 Calcite 内部都是通过 create 方法来创建 AggregateFunctionImpl 对象，create 方法实现逻辑如下： /** * Creates an aggregate function, or returns null. */public static @Nullable AggregateFunctionImpl create(Class? clazz) // 获取函数类中的 init 方法 final Method initMethod = ReflectiveFunctionBase.findMethod(clazz, init); // 获取函数类中的 add 方法 final Method addMethod = ReflectiveFunctionBase.findMethod(clazz, add); // merge 方法暂未实现 final Method mergeMethod = null; // TODO: // 获取函数类中的 result 方法 final Method resultMethod = ReflectiveFunctionBase.findMethod(clazz, result); if (initMethod != null addMethod != null) // A is return type of init by definition // A 表示 init 方法的返回类型 final Class? accumulatorType = initMethod.getReturnType(); // R is return type of result by definition // R 表示 result 方法的返回类型 final Class? resultType = resultMethod != null ? resultMethod.getReturnType() : accumulatorType; // V is remaining args of add by definition // V 表示 add 方法第一个参数之外的类型 final ListClass addParamTypes = ImmutableList.copyOf(addMethod.getParameterTypes()); // 检查 add 方法第一个参数和累加器类型是否一致 if (addParamTypes.isEmpty() || addParamTypes.get(0) != accumulatorType) throw RESOURCE.firstParameterOfAdd(clazz.getName()).ex(); final ReflectiveFunctionBase.ParameterListBuilder params = ReflectiveFunctionBase.builder(); final ImmutableList.BuilderClass? valueTypes = ImmutableList.builder(); // 跳过第一个构造器参数，从第二个参数开始遍历 for (int i = 1; i addParamTypes.size(); i++) final Class type = addParamTypes.get(i); // 调用 ReflectUtil 获取参数名称和参数是否可选标记，优先选择 Parameter 注解声明的信息 final String name = ReflectUtil.getParameterName(addMethod, i); final boolean optional = ReflectUtil.isParameterOptional(addMethod, i); params.add(type, name, optional); valueTypes.add(type); // 如下展示了聚合函数中的方法和类型 // A init() // A add(A, V) // A merge(A, A) // R result(A) return new AggregateFunctionImpl(clazz, params.build(), valueTypes.build(), accumulatorType, resultType, initMethod, addMethod, mergeMethod, resultMethod); return null; create 方法首先会从 clazz 类中获取 init、add、merge 和 result 对应的 Method 对象，目前 merge 方法仍然没有实现，init 和 add 方法是必须的，否则 create 方法会直接返回 null，外部调用的地方会判断是否允许函数为 null。 如果 init 和 add 方法不为 null，则会从 Method 对象中获取聚合函数相关的类型及参数信息，代码中使用 A 表示 init 方法的返回类型，使用 R 表示 result 方法的返回类型，使用 V 表示 add 方法第一个参数之外的类型，并检查 add 方法中的第一个参数是否和累加器类型一致，不一致则抛出异常。 然后循环 add 方法的参数，循环时会跳过第一个累加器参数，从第二个参数开始遍历，然后使用 ReflectUtil 优先从 Parameter 注解中获取参数名称和参数是否可选标记，循环过程中会把参数和参数类型存储在 params 和 valueTypes 中，最终所有的参数会传递给 AggregateFunctionImpl 私有构造方法，用于创建 AggregateFunctionImpl 对象。 @Overridepublic AggImplementor getImplementor(boolean windowContext) return new RexImpTable.UserDefinedAggReflectiveImplementor(this); AggregateFunctionImpl#getImplementor 方法用于获取聚合函数实现器，参数 windowContext 表示当前是否是位于窗口运算中，聚合函数实现器直接调用了 RexImpTable.UserDefinedAggReflectiveImplementor 构造方法，并将当前的聚合函数实现类对象传递进去，实现器内部具体的逻辑，我们后文再详细分析，此处暂时跳过。 表函数 表宏 表函数（TableFunction）是指在执行阶段将某些数据转换为表的函数，表宏（TableMacro）是指在编译阶段将某些数据转换为表的函数。表函数或者表宏，通常会使用在 FROM 子句中，作为一张表进行使用，例如：SELECT * FROM XML_EXTRACT(/opt/csv/test.csv)，XML_EXTRACT 就是一个表函数，它负责从 test.csv 文件中获取数据，并返回一张表。 下图展示了表函数和表宏在 Schema 对象中的继承体系，表函数的核心实现逻辑在 TableFunctionImpl 类中，它实现了 ImplementableFunction 和 TableFunction 接口，并继承了 ReflectiveFunctionBase 抽象类。表宏的核心实现逻辑在 TableMarcoImpl 类中，它实现了 TableMarco 接口，并继承了 ReflectiveFunctionBase 抽象类，下面我们分别来介绍下这些接口和类的作用。 TableFunction 接口： TableFunction 接口继承了 Function 接口，并在接口中声明了 getRowType 方法，用于表示表函数返回表的数据行类型。 public interface TableFunction extends Function // 使用指定参数产生的表对应的数据行类型 RelDataType getRowType(RelDataTypeFactory typeFactory, List? extends @Nullable Object arguments); // 获取表对应的数据行类型（Java 类型） Type getElementType(List? extends @Nullable Object arguments); TableFunctionImpl 类： TableFunctionImpl 类实现了 TableFunction 和 ImplementableFunction 接口，并继承了 ReflectiveFunctionBase 抽象类，私有构造方法允许传入函数实现的 Method 对象，以及调用实现逻辑 CallImplementor，super(method) 会调用 ReflectiveFunctionBase 抽象类的方法，对函数参数进行处理。 private TableFunctionImpl(Method method, CallImplementor implementor) super(method); this.implementor = implementor; 私有构造方法会被 create 方法调用，create 方法会传入一个 Class 对象，以及函数方法名 methodName，根据方法名会查找 Method 对象，如果对象为 null，则返回 TableFunction 为 null。 /** * Creates a @link TableFunctionImpl from a class, looking for a method * with a given name. Returns null if there is no such method. */public static @Nullable TableFunction create(Class? clazz, String methodName) final Method method = findMethod(clazz, methodName); if (method == null) return null; return create(method); 如果 Method 对象存在，则继续判断方法是否为静态，如果是非静态方法，则需要提供无参的构造方法，否则抛出异常。此外，表函数返回值需要是 QueryableTable 或者 ScannableTable 类型，否则表函数直接返回 null。Method 对象检查满足条件后，则调用 createImplementor 创建方法实现器，再调用私有的 TableFunctionImpl 构造方法创建表函数对象。 /** * Creates a @link TableFunctionImpl from a method. */public static @Nullable TableFunction create(final Method method) // 判断是否为非静态方法 if (!isStatic(method)) Class clazz = method.getDeclaringClass(); // 非静态方法，如果没有公有的无参构造方法，则抛出异常 if (!classHasPublicZeroArgsConstructor(clazz)) throw RESOURCE.requireDefaultConstructor(clazz.getName()).ex(); final Class? returnType = method.getReturnType(); // 如果表函数返回值不为 QueryableTable 或者 ScannableTable，则直接返回 null if (!QueryableTable.class.isAssignableFrom(returnType) !ScannableTable.class.isAssignableFrom(returnType)) return null; // 创建调用实现器 CallImplementor implementor = createImplementor(method); // 创建 TableFunctionImpl 对象 return new TableFunctionImpl(method, implementor); TableMacro 接口： TableMacro 和 TableFunction 接口类似，都继承了 Function 接口，但是它是在编译期间进行调用，如下展示了 TableMacro 接口提供的 apply 方法，它可以根据传入的参数转换为 TranslatableTable，通过 TranslatableTable#toRel 方法，可以得到 RelNode。 public interface TableMacro extends Function /** * Applies arguments to yield a table. * * @param arguments Arguments * @return Table */ TranslatableTable apply(List? extends @Nullable Object arguments); TableMacroImpl 类： TableMacroImpl 类实现了 TableMacro 接口，并继承了 ReflectiveFunctionBase 抽象类，私有构造方法接受一个 Method 参数，并会调用 super(method) 方法，处理函数参数。 /** * Private constructor; use @link #create. */private TableMacroImpl(Method method) super(method); Calcite 内部通过 create 方法创建 TableMacro 对象，和 TableFunction 一样，在创建之前会判断方法是否为非静态方法，非静态方法必须提供公有无参构造方法。然后检查 TableMacro 函数返回值，必须为 TranslatableTable 类型，否则返回 null，最后调用 TableMacroImpl 构造方法创建表宏对象。 /** * Creates a @code TableMacro from a method. */public static @Nullable TableMacro create(final Method method) Class clazz = method.getDeclaringClass(); // 非静态方法，必须提供公有无参构造方法 if (!isStatic(method)) if (!classHasPublicZeroArgsConstructor(clazz)) throw RESOURCE.requireDefaultConstructor(clazz.getName()).ex(); final Class? returnType = method.getReturnType(); // 检查表宏返回值是否为 TranslatableTable if (!TranslatableTable.class.isAssignableFrom(returnType)) return null; return new TableMacroImpl(method); TableMacroImpl 类中的 apply 方法负责调用 method 方法，并将参数 arguments 传递给该方法，最终会返回 TranslatableTable 对象。 @Overridepublic TranslatableTable apply(List? extends @Nullable Object arguments) try Object o = null; if (!isStatic(method)) final Constructor? constructor = method.getDeclaringClass().getConstructor(); o = constructor.newInstance(); return (TranslatableTable) requireNonNull(method.invoke(o, arguments.toArray()), () - got null from + method + with arguments + arguments); catch (IllegalArgumentException e) throw new RuntimeException(Expected + Arrays.toString(method.getParameterTypes()) + actual + arguments, e); catch (IllegalAccessException | InvocationTargetException | NoSuchMethodException | InstantiationException e) throw new RuntimeException(e); 函数执行流程 前文我们介绍了标量函数、聚合函数、表函数和表宏中的核心类，下面我们将结合 Calcite 中的 CoreQuidemTest（该测试使用了 quidem 测试框架，可以像编写脚本一样编写测试程序），一起来看看 functions.iq 中的函数是如何执行的。下面的 SQL 取自 functions.iq 文件，它包含了 concat_ws 和 cast 两个函数，会将参数按照分隔符进行组装。 select concat_ws(,, a, cast(null as varchar), b); 测试程序的入口在 QuidemTest#test 方法，通过 getPath 获取 functions.iq 文件路径，然后调用 checkRun 中的 new Quidem(config).execute() 执行测试用例。我们可以在 net/hydromatic/quidem/Quidem.java:285 位置打断点，然后设置条件断点——sqlCommand instanceof SqlCommand ((SqlCommand) sqlCommand).sql.equalsIgnoreCase(select concat_ws(',', 'a', cast(null as varchar), 'b'))，这样可以跟踪上面测试 SQL 的 checkResult 断言逻辑。 SqlValidator 函数注册 SQL 语句在 Calcite 中执行，首先会调用 Avatica 提供的 JDBC 接口，内部会对 SQL 语句进行解析，得到 SqlNode 对象。然后创建 SqlValidator 进行校验，校验器创建逻辑具体参考 CalcitePrepareImpl#createSqlValidator，首先会根据 context.config() 中配置的 fun 属性，获取对应方言中的函数或运算符，具体调用的方法是 SqlLibraryOperatorTableFactory.INSTANCE.getOperatorTable()，返回方言对应的函数、运算符表对象。 @Overridepublic T @PolyNull T fun(ClassT operatorTableClass, @PolyNull T defaultOperatorTable) final String fun = CalciteConnectionProperty.FUN.wrap(properties).getString(); if (fun == null || fun.equals() || fun.equals(standard)) return defaultOperatorTable; // Parse the libraries final ListSqlLibrary libraryList = SqlLibrary.parse(fun); // Load standard plus the specified libraries. If all is among the // specified libraries, it is expanded to all libraries (except standard, // spatial, all). final ListSqlLibrary libraryList1 = SqlLibrary.expand(ConsList.of(SqlLibrary.STANDARD, libraryList)); // 根据 libraries 获取对应的 SqlLibraryOperatorTable final SqlOperatorTable operatorTable = SqlLibraryOperatorTableFactory.INSTANCE.getOperatorTable(libraryList1); return operatorTableClass.cast(operatorTable); SqlLibraryOperatorTableFactory.INSTANCE.getOperatorTable() 方法内部会遍历 SqlLibraryOperators 中的成员变量，获取类型为 SqlOperator 的变量，并判断 LibraryOperator 指定的方言和当前 fun 指定的方言是否一致，如果不匹配则继续比较方言继承的父类是否一致，一致则将 SqlOperator 添加到集合中，并组装为 SqlOperatorTable 返回。 for (Class aClass : classes) for (Field field : aClass.getFields()) try if (SqlOperator.class.isAssignableFrom(field.getType())) final SqlOperator op = (SqlOperator) requireNonNull(field.get(this), () - null value of + field + for + this); // 判断 Library 是否一致 if (operatorIsInLibrary(op.getName(), field, librarySet)) list.add(op); catch (IllegalArgumentException | IllegalAccessException e) throw Util.throwAsRuntime(Util.causeOrSelf(e)); 如果未配置或配置为 standard，则使用 SqlStdOperatorTable.instance() 创建 SqlOperatorTable 标准函数、运算符表对象。然后会将 CalciteCatalogReader 对象添加到集合中，CalciteCatalogReader 对象也实现了 SqlOperatorTable 接口，提供了 lookupOperatorOverloads 查找 Schema 中注册的函数和运算符。 private static SqlValidator createSqlValidator(Context context, CalciteCatalogReader catalogReader, UnaryOperatorSqlValidator.Config configTransform) // 根据 context.config() 中配置的 fun 属性，获取对应方言中的函数或运算符 // 未配置或配置为 standard，则使用 SqlStdOperatorTable.instance() 创建的标准函数、运算符表对象 final SqlOperatorTable opTab0 = context.config().fun(SqlOperatorTable.class, SqlStdOperatorTable.instance()); final ListSqlOperatorTable list = new ArrayList(); list.add(opTab0); // 添加 catalogReader list.add(catalogReader); // 组装为调用链 Chain final SqlOperatorTable opTab = SqlOperatorTables.chain(list); final JavaTypeFactory typeFactory = context.getTypeFactory(); final CalciteConnectionConfig connectionConfig = context.config(); final SqlValidator.Config config = configTransform.apply(SqlValidator.Config.DEFAULT.withLenientOperatorLookup(connectionConfig.lenientOperatorLookup()).withConformance(connectionConfig.conformance()).withDefaultNullCollation(connectionConfig.defaultNullCollation()).withIdentifierExpansion(true)); return new CalciteSqlValidator(opTab, catalogReader, typeFactory, config); SqlOperatorTable 对象实例是一个单例对象，只有在第一次执行 ReflectiveSqlOperatorTable#init 方法时才会创建，init 方法会使用反射获取 SqlStdOperatorTable 类中的成员变量，遍历时会判断变量是否使用 LibraryOperator 注解标注，对于有注解但是没有包含 STANDARD 类型的方言函数，直接跳过。处理完成后，会将 SqlOperator 构建为 Map 结果，key 为 SqlOperator 大写名称，value 为 SqlOperator 对象，并存储到 SqlStdOperatorTable 中的 operators 对象中，方便后续使用。 public final SqlOperatorTable init() // Use reflection to register the expressions stored in public fields. final ListSqlOperator list = new ArrayList(); for (Field field : getClass().getFields()) try final Object o = field.get(this); if (o instanceof SqlOperator) // Fields do not need the LibraryOperator tag, but if they have it, // we index them only if they contain STANDARD library. // 获取 LibraryOperator 注解，有注解但是没有 STANDARD 类型的函数为方言函数，直接跳过 LibraryOperator libraryOperator = field.getAnnotation(LibraryOperator.class); if (libraryOperator != null) if (Arrays.stream(libraryOperator.libraries()).noneMatch(library - library == SqlLibrary.STANDARD)) continue; // 标准函数大多不用 LibraryOperator 注解标记，直接添加到集合中 list.add((SqlOperator) o); catch (IllegalArgumentException | IllegalAccessException e) throw Util.throwAsRuntime(Util.causeOrSelf(e)); // 将 SqlOperator 构建为 Map 结果，key 为 SqlOperator 大写名称，value 为 SqlOperator 对象 // 存储到 SqlStdOperatorTable 中的 operators 对象中 setOperators(buildIndex(list)); return this; 至此就完成了 CalciteSqlValidator 函数注册流程，函数和运算符会存储在 SqlOperatorTable 对象中，通过 CalciteSqlValidator#getOperatorTable 方法，可以快速获取到 SqlOperatorTable 进行函数和运算符查找。 函数解析和校验 函数注册完成后，按照 Calcite SQL 执行的流程，会先对 SQL 语句进行解析，下图展示了 SQL 中函数解析的结果，CONCAT_WS 函数被解析为 SqlUnresolvedFunction，它表示该函数在 SQL 解析阶段无法处理，需要通过校验器从 SqlOperatorTable 中查找对应的函数，并将其改写为对应的函数类型，CAST 函数被解析为 SqlCastFunction，在后续的校验阶段无需再进行处理。 那么，哪些函数会被解析为 SqlUnresolvedFunction 类型呢？根据 Julian 在 Calcite does not return correct SqlKind 中的回答，所有函数在解析阶段都会被处理为 SqlUnresolvedFunction 类型，这样能够保持解析器简单、高效、可预测，在 Calcite 校验阶段，会从 SqlOperatorTable 中查找函数，并将 SqlUnresolvedFunction 转换为具体的 Function 对象。 但是实际上，从 Calcite Parser.jj 文件来看，不论是标准函数，还是方言函数，都有直接处理为具体 Function 对象的情况，SqlUnresolvedFunction 只是作为最后的处理方式。此外，SqlUnresolvedFunction 也适用于自定义函数场景，这些自定义函数，在 SQL 解析时是无法知道具体的函数对象。 完成 SQL 解析后，Calcite 会调用校验器对 SqlNode 语法树进行校验，参考深度探究 Apache Calcite SQL 校验器实现原理一文，我们知道，在校验阶段执行 performUnconditionalRewrites 方法时，会判断当前运算符是否为 SqlUnresolvedFunction，如果是则调用 SqlOperatorTable#lookupOperatorOverloads 方法，从 SqlOperatorTable 中查找函数，查找的范围包括内置函数以及元数据中注册的函数，然后替换 SqlNode 中的函数对象。 // 判断当前 SqlNode 是否为 SqlCallif (node instanceof SqlCall) ... SqlCall call = (SqlCall) node; // 获取 SqlKind 类型 final SqlKind kind = call.getKind(); // 获取 SqlNode 中包含的运算符 final ListSqlNode operands = call.getOperandList(); for (int i = 0; i operands.size(); i++) SqlNode operand = operands.get(i); ... // 每一个运算法调用 performUnconditionalRewrites 并设置到 SqlCall 中 SqlNode newOperand = performUnconditionalRewrites(operand, childUnderFrom); if (newOperand != null newOperand != operand) call.setOperand(i, newOperand); // 当前运算符为未解析函数 SqlUnresolvedFunction if (call.getOperator() instanceof SqlUnresolvedFunction) final SqlUnresolvedFunction function = (SqlUnresolvedFunction) call.getOperator(); final ListSqlOperator overloads = new ArrayList(); // 从 SqlOperatorTable 中查找函数，查找的范围包括内置函数以及元数据中注册的函数 opTab.lookupOperatorOverloads(function.getNameAsId(), function.getFunctionType(), SqlSyntax.FUNCTION, overloads, catalogReader.nameMatcher()); if (overloads.size() == 1) // 查找到函数则设置新的运算符 ((SqlBasicCall) call).setOperator(overloads.get(0)); 校验完成后，可以看到 SqlUnresolvedFunction 被替换为 SqlBasicFunction，它是 SqlFunction 类的一个具体实现，主要用来处理常规的函数格式，例如：NVL(value, value)、LENGTH(string)、LTRIM(string) 等。而 CAST(NULL AS VARCHAR) 由于包含了类型信息，SqlBasicFunction 无法处理，因此需要额外定义一个 SqlCastFunction。 刚好聊到 SqlFunction 对象，我们结合下图，一起来看看 SqlOperator 和 SqlFunction 继承体系。下图展示了部分 SqlFunction 实现类，除了我们刚刚介绍的 SqlBasicFunction、SqlCastFunction 和 SqlUnresolvedFunction 外，还有 SqlUserDefinedFunction、SqlUserDefinedAggFunction、SqlUserDefinedTableFunction 和 SqlUserDefinedTableMacro，他们用于处理用户自定义的函数。 SqlOperator 抽象类： SqlOperator 代表了 SQL 运算符，具体包括函数、运算符（例如：=）和语法结构（例如：CASE 语句），运算符可以表示查询级表达式（例如：SqlSelectOperator），也可以表示行级表达式（例如：SqlBetweenOperator）。 SqlOperator 不是 SQL 解析树中的 SqlNode 节点，它具体表示了 SqlNode 所对应的运算符类型。运算符中通常包含了若干个操作数，例如：除法运算符中包含两个操作数，分别是分子和分母，而在 SqlFunction 函数中，参数则是操作数。 如下展示了 SqlOperator 抽象类的核心字段和方法，包含了 name、kind 等表示名称、类型的基础字段，以及用于区分计算优先级的 leftPrec 和 rightPrec，此外，还包含了 returnTypeInference、operandTypeInference 字段，分别用于推断返回类型、操作数类型。 public abstract class SqlOperator // 运算符/函数名称 private final String name; // SqlNode 类型，例如：EQUALS，可以使用 exp.isA(EQUALS) 判断 public final SqlKind kind; // 运算符绑定到左侧表达式的优先级，如果运算符是左结合，则此优先级低于右侧优先级 private final int leftPrec; // 运算符绑定到右侧表达式的优先级，如果运算符是左结合，则此优先级高于右侧优先级 private final int rightPrec; // 推断运算符调用后的返回类型 private final @Nullable SqlReturnTypeInference returnTypeInference; // 推断操作数的类型 private final @Nullable SqlOperandTypeInference operandTypeInference; // 用于校验操作数类型 private final @Nullable SqlOperandTypeChecker operandTypeChecker; // 返回运算符的语法类型 public abstract SqlSyntax getSyntax(); // 推断运算符调用后的返回类型，仅在操作数的数量和类型校验后调用 public RelDataType inferReturnType(SqlOperatorBinding opBinding) ... SqlFunction 类： SqlFunction 继承了 SqlOperator，是用来表示函数调用语法的运算符，SqlFunction 中额外维护了 category 和 sqlIdentifier 字段，category 表示函数的分类，主要包括：STRING、NUMERIC、TIMEDATE、SYSTEM 等函数类型，完整函数类型可查看 SqlFunctionCategory，sqlIdentifier 表示函数的全限定名称，内置函数为 null。 public class SqlFunction extends SqlOperator // SQL 函数分类 private final SqlFunctionCategory category; // 函数的全限定名称，内置函数为 null private final @Nullable SqlIdentifier sqlIdentifier; protected SqlFunction(String name, @Nullable SqlIdentifier sqlIdentifier, SqlKind kind, @Nullable SqlReturnTypeInference returnTypeInference, @Nullable SqlOperandTypeInference operandTypeInference, @Nullable SqlOperandTypeChecker operandTypeChecker, SqlFunctionCategory category) // SqlFunction 默认传递给父类 SqlOperator 的 leftPrec 和 rightPrec 为 100 super(name, kind, 100, 100, returnTypeInference, operandTypeInference, operandTypeChecker); this.sqlIdentifier = sqlIdentifier; this.category = requireNonNull(category, category); // 获取语法类型，SqlSyntax.FUNCTION 表示 Foo(x, y) 格式的函数 @Override public SqlSyntax getSyntax() return SqlSyntax.FUNCTION; SqlBasicFunction 类： SqlBasicFunction 类继承了 SqlFunction，它是常规函数（例如：NVL(value, value)、LENGTH(string)、LTRIM(string)）的具体实现类，由于该类字段都是 final 类型，因此 SqlFunction 对象是不可变的，只允许通过 with 方法修改字段，然后创建一个新的对象。 SqlBasicFunction 字段中记录了 SqlSyntax，允许注册不同类型的函数语法，全部语法类型可以参考 SqlSyntax。此外，还提供了 deterministic 和 dynamic 两个属性，deterministic 表示是否是确定函数，即：是否传入相同操作数时返回相同结果，默认为 true，dynamic 则表示是否是动态函数，即：是否传入相同操作数时返回不同结果，默认为 false，动态函数不能被缓存。monotonicityInference 是一个函数式接口，用于推测函数单调性的策略，入参是 SqlOperatorBinding，表示 SqlOperator 与操作数之间的绑定关系，出参则是 SqlMonotonicity，表示 SQL 中值的单调性。 public class SqlBasicFunction extends SqlFunction // 语法类型，可以使用 withSyntax 修改 private final SqlSyntax syntax; // 是否是确定函数，即：是否传入相同操作数时返回相同结果，默认为 true private final boolean deterministic; // 操作数处理器 private final SqlOperandHandler operandHandler; // 校验调用的策略，目前没有发现使用场景 private final int callValidator; // 推断函数单调性的策略 // 入参是 SqlOperatorBinding，表示 SqlOperator 与操作数之间的绑定关系 // 出参是 SqlMonotonicity，表示 SQL 中值的单调性 private final FunctionSqlOperatorBinding, SqlMonotonicity monotonicityInference; // 是否是动态函数，即：是否传入相同操作数时返回不同结果，默认为 false，动态函数不能被缓存 private final boolean dynamic; SqlCastFunction 类： SqlCastFunction 类和 SqlBasicFunction 平级，都继承了 SqlFunction，它表示了常规函数之外的特殊函数，此处表示 CAST(NULL AS VARCHAR) 函数，其他特殊函数也有相应的函数类表示。 如下展示了 SqlCastFunction 中的核心方法，getSignatureTemplate 方法会返回一个模板，描述如何构建运算符签名，以 CAST 函数为例，0 表示运算符 CAST，1、2、3 是操作数。getSyntax 方法此处返回的是 SqlSyntax.SPECIAL 语法类型，它表示特殊语法，例如：CASE、CAST 运算符。getMonotonicity 方法用于获取函数的单调性，会根据函数操作数中配置的排序规则进行比较，如果排序规则的比较器不同，则返回 NOT_MONOTONIC，如果操作数的类型族包含在 nonMonotonicCasts 中，同样会返回 NOT_MONOTONIC 单调性，如果都不满足，则会返回第一个操作数的单调性。 public class SqlCastFunction extends SqlFunction // 所有不保持单调性的类型转换映射，getMonotonicity 方法根据 map 判断，包含在其中的，则返回 NOT_MONOTONIC private final SetMultimapSqlTypeFamily, SqlTypeFamily nonMonotonicCasts = ImmutableSetMultimap.SqlTypeFamily, SqlTypeFamilybuilder().put(SqlTypeFamily.EXACT_NUMERIC, SqlTypeFamily.CHARACTER).put(SqlTypeFamily.NUMERIC, SqlTypeFamily.CHARACTER).put(SqlTypeFamily.APPROXIMATE_NUMERIC, SqlTypeFamily.CHARACTER).put(SqlTypeFamily.DATETIME_INTERVAL, SqlTypeFamily.CHARACTER).put(SqlTypeFamily.CHARACTER, SqlTypeFamily.EXACT_NUMERIC).put(SqlTypeFamily.CHARACTER, SqlTypeFamily.NUMERIC).put(SqlTypeFamily.CHARACTER, SqlTypeFamily.APPROXIMATE_NUMERIC).put(SqlTypeFamily.CHARACTER, SqlTypeFamily.DATETIME_INTERVAL).put(SqlTypeFamily.DATETIME, SqlTypeFamily.TIME).put(SqlTypeFamily.TIMESTAMP, SqlTypeFamily.TIME).put(SqlTypeFamily.TIME, SqlTypeFamily.DATETIME).put(SqlTypeFamily.TIME, SqlTypeFamily.TIMESTAMP).build(); // 构造方法传入 name 和 kind，SAFE_CAST 和 CAST 类似，它转换失败时会返回 NULL，而不是异常 public SqlCastFunction(String name, SqlKind kind) super(name, kind, returnTypeInference(kind == SqlKind.SAFE_CAST), InferTypes.FIRST_KNOWN, null, SqlFunctionCategory.SYSTEM); checkArgument(kind == SqlKind.CAST || kind == SqlKind.SAFE_CAST, kind); // 返回一个模板，描述如何构建运算符签名，以 CAST 函数为例，0 表示运算符 CAST，1、2、3 是操作数 @Override public String getSignatureTemplate(final int operandsCount) assert operandsCount = 3; return 0(1 AS 2 [FORMAT 3]); // 获取语法类型，SqlSyntax.SPECIAL 表示特殊语法，例如：CASE、CAST 运算符 @Override public SqlSyntax getSyntax() return SqlSyntax.SPECIAL; @Override public SqlMonotonicity getMonotonicity(SqlOperatorBinding call) // 获取第一个操作数类型，CAST 原始类型 final RelDataType castFromType = call.getOperandType(0); // 获取第一个操作数类型族 final RelDataTypeFamily castFromFamily = castFromType.getFamily(); // 获取第一个操作数排序规则对应的排序器 final Collator castFromCollator = castFromType.getCollation() == null ? null : castFromType.getCollation().getCollator(); // 获取第二个操作数类型，CAST 目标类型 final RelDataType castToType = call.getOperandType(1); // 获取第二个操作数类型族 final RelDataTypeFamily castToFamily = castToType.getFamily(); // 获取第二个操作数排序规则对应的排序器 final Collator castToCollator = castToType.getCollation() == null ? null : castToType.getCollation().getCollator(); // 如果两个操作数的排序器不同，则返回 NOT_MONOTONIC if (!Objects.equals(castFromCollator, castToCollator)) // Cast between types compared with different collators: not monotonic. return SqlMonotonicity.NOT_MONOTONIC; // 如果两个操作数的类型族包含在 nonMonotonicCasts 中，则返回 NOT_MONOTONIC else if (castFromFamily instanceof SqlTypeFamily castToFamily instanceof SqlTypeFamily nonMonotonicCasts.containsEntry(castFromFamily, castToFamily)) return SqlMonotonicity.NOT_MONOTONIC; else // 否则返回第一个操作数的单调性策略 return call.getOperandMonotonicity(0); 除了以上介绍的 SqlOperator、SqlFunction、SqlBasicFunction 以及 SqlCastFunction，其他的函数运算符，大家可以根据兴趣进行探索，本文就不再一一介绍，下面的小节，我们将继续跟踪函数语句的执行。 SQL 优化和函数执行 完成 SQL 语句的解析和校验后，我们就得到了一个包含不同 SqlFunction 运算符的 SqlNode 语法树，Caclite JDBC 流程会调用 sqlToRelConverter.convertQuery(sqlQuery, needsValidation, true)，将 SqlNode 转换为关系代数表达式 RelNode。 由于示例 SQL 中的函数位于投影列中，因此我们只需要关注 Projection 转换逻辑即可，下面展示了 SqlToRelConverter#convertSelectList 的部分转换逻辑，投影列会调用 Blackboard 对象的 convertExpression 方法，该类实现了 SqlVisitor 接口，可以用于遍历 AST 处理对应 SqlNode 转换。 // Project select clause.int i = -1;for (SqlNode expr : selectList) ++i; final SqlNode measure = SqlValidatorUtil.getMeasure(expr); final RexNode e; if (measure != null) ... else // 调用 Blackboard 对象 convertExpression 方法，该类实现了 SqlVisitor，用于遍历 AST 处理对应 SqlCall 转换 e = bb.convertExpression(expr); exprs.add(e); fieldNames.add(deriveAlias(expr, aliases, i)); 由于 expr 是一个 SqlBasicCall，遍历语法树时会调用 visit(SqlCall call) 方法，该方法内部会调用 exprConverter.convertCall 方法，exprConverter 对应的类是 SqlNodeToRexConverter 用于将 SqlNode 转换为 RexNode 行表达式。 public RexNode visit (SqlCall call) if (agg != null) final SqlOperator op = call.getOperator(); if (window == null (op.isAggregator() || op.getKind() == SqlKind.FILTER || op.getKind() == SqlKind.WITHIN_DISTINCT || op.getKind() == SqlKind.WITHIN_GROUP)) return requireNonNull(agg.lookupAggregates(call), () - agg.lookupAggregates for call + call); // 调用 SqlNodeToRexConverter#convertCall 方法，SqlNodeToRexConverter 用于将 SqlNode 转换为 RexNode 行表达式 return exprConverter.convertCall(this, new SqlCallBinding(validator(), scope, call).permutedCall()); SqlNodeToRexConverter#convertCall 内部则是调用 convertletTable 获取 SqlRexConvertlet，然后调用 SqlRexConvertlet#convertCall 转换为 RexNode。 SqlRexConvertlet#convertCall 通过反射调用 StandardConvertletTable#convertFunction 方法，CONCAT_WS 函数就会调用该方法进行转换，CAST 函数则会调用 StandardConvertletTable#convertCast 方法。Calcite 支持的运算符、函数转换逻辑，可以参考 StandardConvertletTable 构造方法。 public RexNode convertFunction(SqlRexContext cx, SqlFunction fun, SqlCall call) // 转换操作数为 RexNode final ListRexNode exprs = convertOperands(cx, call, SqlOperandTypeChecker.Consistency.NONE); if (fun.getFunctionType() == SqlFunctionCategory.USER_DEFINED_CONSTRUCTOR) return makeConstructorCall(cx, fun, exprs); RelDataType returnType = cx.getValidator().getValidatedNodeTypeIfKnown(call); if (returnType == null) returnType = cx.getRexBuilder().deriveReturnType(fun, exprs); // 调用 RexBuilder#makeCall 转换 return cx.getRexBuilder().makeCall(call.getParserPosition(), returnType, fun, exprs); 转换完成后，我们就得到了逻辑执行计划树 RelNode Tree，通过 RelOptUtil.toString() 方法输出，可以得到如下结构，CONCAT_WS 函数被转换为 LogicalProject(EXPR$0=[CONCAT_WS(',', 'a', null:VARCHAR, 'b')])，而 CAST 函数则转换为 null:VARCHAR。 LogicalProject(EXPR$0=[CONCAT_WS(,, a, null:VARCHAR, b)]) LogicalValues(tuples=[[ 0 ]]) 经过 Calcite 优化后，逻辑执行计划会被转换为物理执行计划，如下展示了逻辑执行计划的结构，CONCAT_WS 被转换为 expr#5=[CONCAT_WS($t1, $t2, $t3, $t4)，CAST 函数被转换为 expr#3=[null:VARCHAR]。 EnumerableCalc(expr#0=[inputs], expr#1=[,], expr#2=[a], expr#3=[null:VARCHAR], expr#4=[b], expr#5=[CONCAT_WS($t1, $t2, $t3, $t4)], EXPR$0=[$t5]) EnumerableValues(tuples=[[ 0 ]]) 由于 Calcite JDBC 默认使用 Enumerable 调用约定，生成的物理运算符都是 EnumerableRel，调用 implement 方法执行时，会调用 EnumerableInterpretable#toBindable 方法生成 Bindable 对象，Bindable 类可以绑定 DataContext 生成 Enumerable 进行执行。EnumerableInterpretable#toBindable 方法实现逻辑如下，它会调用 implementRoot 生成执行代码，然后使用 Janio 将代码库编译为 Bindable 实现类。 public static Bindable toBindable(MapString, Object parameters, CalcitePrepare.@Nullable SparkHandler spark, EnumerableRel rel, EnumerableRel.Prefer prefer) EnumerableRelImplementor relImplementor = new EnumerableRelImplementor(rel.getCluster().getRexBuilder(), parameters); // 调用 implementRoot 生成执行代码 final ClassDeclaration expr = relImplementor.implementRoot(rel, prefer); String s = Expressions.toString(expr.memberDeclarations, , false); try if (spark != null spark.enabled()) return spark.compile(expr, s); else // 使用 Janio 编译代码为 Bindable 实现类 return getBindable(expr, s, rel.getRowType().getFieldCount()); catch (Exception e) throw Helper.INSTANCE.wrap(Error while compiling generated Java code: + s, e); implementRoot 方法根据物理计划树进行遍历，首先调用 EnumerableValues#implement 方法，生成的代码如下： return org.apache.calcite.linq4j.Linq4j.asEnumerable(new Integer[] 0 ); 然后会回到 EnumerableCalc#implement 继续执行，EnumerableValues 生成的结果会作为 EnumerableCalc 执行代码的一部分，EnumerableCalc 核心生成投影列函数执行代码的逻辑如下，会调用 RexToLixTranslator#translateProjects 方法进行转换： // EnumerableValues#implement() 方法ListExpression expressions = RexToLixTranslator.translateProjects(program, typeFactory, conformance, builder3, null, physType, DataContext.ROOT, new RexToLixTranslator.InputGetterImpl(input, result.physType), implementor.allCorrelateVariables);builder3.add(Expressions.return_(null, physType.record(expressions))); RexToLixTranslator#translateProjects 内部则会继续调用 RexToLixTranslator#translateList 方法，translateList 方法会遍历运算符的操作数，此案例中操作数对象为 RexLocalRef 类型，内容为 $t5，表示引用 expr#5=[CONCAT_WS($t1, $t2, $t3, $t4)]，获取到 RexNode 后，会继续内部的调用 translate 方法，该方法内会使用 RexToLixTranslator（实现了 RexVisitor 接口）访问 RexNode，该方法会访问 RexToLixTranslator#visitLocalRef 方法。 public ListExpression translateList(List? extends RexNode operandList, @Nullable List? extends @Nullable Type storageTypes) final ListExpression list = new ArrayList(operandList.size()); // 遍历投影列操作数 for (int i = 0; i operandList.size(); i++) // 返回 RexLocalRef 类型，内容为 $t5，表示引用 expr#5=[CONCAT_WS($t1, $t2, $t3, $t4)] RexNode rex = operandList.get(i); Type desiredType = null; if (storageTypes != null) desiredType = storageTypes.get(i); // 继续调用 translate 方法转换函数 final Expression translate = translate(rex, desiredType); list.add(translate); ... return list;Expression translate(RexNode expr, RexImpTable.NullAs nullAs, @Nullable Type storageType) currentStorageType = storageType; // 使用 RexToLixTranslator 遍历 RexNode，该方法会访问 RexToLixTranslator#visitLocalRef 方法 final Result result = expr.accept(this); final Expression translated = requireNonNull(EnumUtils.toInternal(result.valueVariable, storageType)); // When we asked for not null input that would be stored as box, avoid unboxing if (RexImpTable.NullAs.NOT_POSSIBLE == nullAs translated.type.equals(storageType)) return translated; return nullAs.handle(translated); visitLocalRef 方法会根据 RexLocalRef 中记录的引用下标，将对象转换为 RexCall，然后继续使用 RexToLixTranslator 进行访问，最终逻辑会执行到 RexToLixTranslator#visitCall 方法。正如下面 Calcite java doc 中说明的那样，大部分的运算符实现时都会从 RexImpTable 中获取实现器，但也有些特殊的函数需要单独实现，例如 CASE 语句。 /** * Visit @code RexCall. For most @code SqlOperators, we can get the implementor * from @code RexImpTable. Several operators (e.g., CaseWhen) with special semantics * need to be implemented separately. */@Overridepublic Result visitCall(RexCall call) ... // 从 RexImpTable 中获取运算符实现器 final RexImpTable.RexCallImplementor implementor = RexImpTable.INSTANCE.get(operator); if (implementor == null) throw new RuntimeException(cannot translate call + call); // 获取操作数 final ListRexNode operandList = call.getOperands(); final List@Nullable Type storageTypes = EnumUtils.internalTypes(operandList); final ListResult operandResults = new ArrayList(); // 遍历操作数，调用各自的 implement 方法生成代码 for (int i = 0; i operandList.size(); i++) final Result operandResult = implementCallOperand(operandList.get(i), storageTypes.get(i), this); operandResults.add(operandResult); callOperandResultMap.put(call, operandResults); final Result result = implementor.implement(this, call, operandResults); rexResultMap.put(call, result); return result; 下图展示了从 RexImpTable 中获取的运算符实现器，可以看到实现器中包含了 CONCAT_WS 函数对应的实现方法 SqlFunctions.concatMultiWithSeparator()。 最终，Caclite 会生成如下的代码，对函数的调用逻辑位于 current() 方法中，CONCAT_WS 函数会调用 SqlFunctions.concatMultiWithSeparator() 方法，并将 SQL 中的参数传递给函数方法。CAST 函数由于入参为 null，因此无需调用具体的函数，可以直接执行 (String) null 进行转换。getElementType 用于返回 SQL 执行结果的类型，示例 SQL 返回结果为 String 类型。 public org.apache.calcite.linq4j.Enumerable bind(final org.apache.calcite.DataContext root) final org.apache.calcite.linq4j.Enumerable _inputEnumerable = org.apache.calcite.linq4j.Linq4j.asEnumerable(new Integer[] 0 ); return new org.apache.calcite.linq4j.AbstractEnumerable() public org.apache.calcite.linq4j.Enumerator enumerator() return new org.apache.calcite.linq4j.Enumerator() public final org.apache.calcite.linq4j.Enumerator inputEnumerator = _inputEnumerable.enumerator(); public void reset() inputEnumerator.reset(); public boolean moveNext() return inputEnumerator.moveNext(); public void close() inputEnumerator.close(); public Object current() return org.apache.calcite.runtime.SqlFunctions.concatMultiWithSeparator(new String[] ,, a, (String) null, b ); ; ;public Class getElementType() return java.lang.String.class; 最后 Caclite JDBC 会调用 Bindable#bind 方法执行，会返回一个 Enumerable 枚举对象，使用枚举对象就可以很容易地获取到查询结果集。 UDF 函数扩展实践 前文我们介绍了 Caclite 函数相关的实现类，带领大家一起探究了标量函数、聚合函数、表函数 表宏在 Schema 中的实现，并结合 Calcite 函数测试 Case，一起跟踪了函数的执行流程。UDF 函数和 Calcite 内置函数类似，只是实现逻辑由用户提供并注册到 Schema 中，Calcite 在执行时会从 Schema 中找到函数实现类，并生成可执行代码。下面小节，我们结合一些 UDF 案例，为大家介绍下不同类型的 UDF 函数如何扩展。 UDF 标量函数扩展 首先，我们来实现一个自定义的标量函数 INDEX_OF，用于从 content 中查找 target 字符串，并返回其位置。我们知道，UDF 函数最重要的是函数实现逻辑，所以我们先来实现如下的 indexOf 方法，内部实现逻辑很简单，调用 String#indexOf 实现字符串位置查找。 @SuppressWarnings(unused)public class UDFRegistry // 从 content 中查找 target 字符串，并返回其位置 public static int indexOf(String content, String target) return content.indexOf(target); 实现函数逻辑后，第二步需要注册函数实现到 Schema 中，我们编写如下的逻辑，从 calciteConnection 中获取 SchemaPlus 对象，然后通过 add 方法将 Function 对象添加到 Schema 中。ScalarFunctionImpl#create 方法负责 Function 对象负责函数对象的创建，它的内部实现逻辑我们前文已经介绍，内部使用反射获取 Method 对象，然后创建 CallImplementor 存储在 ScalarFunctionImpl 中，用于后续的函数执行。 @Slf4jpublic final class UDFExample public static void main(String[] args) throws Exception Class.forName(org.apache.calcite.jdbc.Driver); try (Connection connection = DriverManager.getConnection(jdbc:calcite:, initProps())) CalciteConnection calciteConnection = connection.unwrap(CalciteConnection.class); SchemaPlus rootSchema = calciteConnection.getRootSchema(); Schema schema = createSchema(rootSchema); rootSchema.add(calcite_function, schema); rootSchema.add(INDEX_OF, Objects.requireNonNull(ScalarFunctionImpl.create(UDFRegistry.class, indexOf))); executeQuery(calciteConnection, SELECT INDEX_OF(user_name, san) FROM calcite_function.t_user); 那么，自定义函数在执行阶段和内置函数又有什么不同呢？我们来跟踪下这段代码，一起来看看内部实现有哪些差异。首先，Schema 中注册的 Function 对象，会在 Calcite JDBC 执行时，封装到 CalciteCatalogReader 对象中，并提供了 lookupOperatorOverloads 用于查找内置和自定义函数。然后会创建 SqlValidator 对象，此时会将 CalciteCatalogReader 对象封装到 SqlOperatorTable 对象中，用于后续运算符和函数的查找，也包含了自定义函数的查找。 Calcite SQL 解析器会将 SELECT INDEX_OF(user_name, 'san') FROM calcite_function.t_user 解析为 SqlNode 抽象语法树，此时可以看到 INDEX_OF 函数被解析为 SqlUnresolvedFunction 对象。 解析完成后，会继续执行 SQL 校验，此时会调用 SqlValidatorImpl#performUnconditionalRewrites 方法，此时会判断运算符是否为 SqlUnresolvedFunction，是则从 opTab 中的 CalciteCatalogReader 查找自定义函数。 if (call.getOperator() instanceof SqlUnresolvedFunction) assert call instanceof SqlBasicCall; final SqlUnresolvedFunction function = (SqlUnresolvedFunction) call.getOperator(); // This function hasnt been resolved yet. Perform // a half-hearted resolution now in case its a // builtin function requiring special casing. If its // not, well handle it later during overload resolution. final ListSqlOperator overloads = new ArrayList(); opTab.lookupOperatorOverloads(function.getNameAsId(), function.getFunctionType(), SqlSyntax.FUNCTION, overloads, catalogReader.nameMatcher()); if (overloads.size() == 1) ((SqlBasicCall) call).setOperator(overloads.get(0)); 最终从 Schema 中查找到了如下的自定义函数对象： 然后调用 CalciteCatalogReader#toOp 将 Function 对象转换为 SQL 运算符，内部会根据 function 类型转换为不同函数运算符，标量函数会转换为 SqlUserDefinedFunction 对象。 if (function instanceof ScalarFunction) final SqlReturnTypeInference returnTypeInference = infer((ScalarFunction) function); // 标量函数转换为 SqlUserDefinedFunction return new SqlUserDefinedFunction(name, kind, returnTypeInference, operandTypeInference, operandMetadata, function); else if (function instanceof AggregateFunction) final SqlReturnTypeInference returnTypeInference = infer((AggregateFunction) function); // 聚合函数转换为 SqlUserDefinedAggFunction return new SqlUserDefinedAggFunction(name, kind, returnTypeInference, operandTypeInference, operandMetadata, (AggregateFunction) function, false, false, Optionality.FORBIDDEN); else if (function instanceof TableMacro) // 表宏转换为 SqlUserDefinedTableMacro return new SqlUserDefinedTableMacro(name, kind, ReturnTypes.CURSOR, operandTypeInference, operandMetadata, (TableMacro) function); else if (function instanceof TableFunction) // 表函数转换为 SqlUserDefinedTableFunction return new SqlUserDefinedTableFunction(name, kind, ReturnTypes.CURSOR, operandTypeInference, operandMetadata, (TableFunction) function); else throw new AssertionError(unknown function type + function); 校验完成后，Calcite 会调用 SqlToRelConverter 将 SqlNode 转换为 RelNode，转换的流程和内置函数类似，最终会调用到 StandardConvertletTable#convertFunction 方法，将函数运算符转换为 RexCall，完整的 RelNode 树如下： LogicalProject(EXPR$0=[INDEX_OF($1, san)]) JdbcTableScan(table=[[calcite_function, t_user]]) 经过 Calcite 内置的优化器和优化规则优化，可以得到如下的物理执行计划，最底层使用了 JdbcTableScan 运算符扫描表中的数据，然后使用 JdbcToEnumerableConverter 将 JdbcConvention 转换为 EnumerableConvention，转换后数据就可以传递给 EnumerableCalc 运算符进行处理。 EnumerableCalc(expr#0..3=[inputs], expr#4=[san], expr#5=[INDEX_OF($t1, $t4)], EXPR$0=[$t5]) JdbcToEnumerableConverter JdbcTableScan(table=[[calcite_function, t_user]]) 最后会调用 CalcitePrepareImpl#implement 方法生成执行逻辑，内部生成自定义函数逻辑时会调用 ReflectiveCallNotNullImplementor#implement 方法，并返回一个 MethodCallExpression 对象，里面包含了对 UDF 函数的调用。 UDF 函数生成的代码逻辑如下，该逻辑会嵌入到最终的执行逻辑中，可以看到每行记录获取时，都会调用一次 UDF 函数，因此数据量越大，UDF 函数执行消耗的时间越多。 final Object[] current = (Object[]) inputEnumerator.current(); return com.strongduanmu.udf.UDFRegistry.indexOf(current[1] == null ? null : current[1].toString(), san); 最终 SQL 执行结果如下： # 表中的 user_name 记录19:46:54.104 [main] INFO com.strongduanmu.udf.UDFExample - ColumnLabel: USER_NAME, ColumnValue: zhangsan19:46:54.104 [main] INFO com.strongduanmu.udf.UDFExample - ColumnLabel: USER_NAME, ColumnValue: lisi19:46:54.104 [main] INFO com.strongduanmu.udf.UDFExample - ColumnLabel: USER_NAME, ColumnValue: wangwu# user_name index_of 执行结果19:43:32.790 [main] INFO com.strongduanmu.udf.UDFExample - ColumnLabel: EXPR$0, ColumnValue: 519:43:32.790 [main] INFO com.strongduanmu.udf.UDFExample - ColumnLabel: EXPR$0, ColumnValue: -119:43:32.790 [main] INFO com.strongduanmu.udf.UDFExample - ColumnLabel: EXPR$0, ColumnValue: -1 除了前面介绍的 UDF 标量函数外，Calcite 社区的同学之前还咨询关于 Oracle ROWNUM、SYSDATE 这些特殊函数，在 Oracle 中无参数的函数不能写括号，无参数的函数通常也叫做 Niladic Function，在 Calcite 中使用 allowNiladicParentheses 属性控制无参函数是否可以使用括号，像 MySQL、Apache Phoenix 就允许使用括号，而 Oracle、PostgreSQL、SQL Server 则不支持括号。 我们将上面的示例进行一些改造，设置 Calcite JDBC 中的 conformance 属性为 ORACLE_12，它对应的 allowNiladicParentheses 实现为 false。然后在 UDFRegistry 类中增加一个 sysDate 函数实现，并使用 rootSchema.add(SYS_DATE, Objects.requireNonNull(ScalarFunctionImpl.create(UDFRegistry.class, sysDate))); 将 SYS_DATE 函数注册到 Schema 中。 public static LocalDate sysDate() return LocalDate.now(); 然后执行 SELECT SYS_DATE FROM calcite_function.t_user 语句，按照预期 Calcite 应当能够正确执行该 SQL，并返回结果。但是执行之后，Calcite 却抛出了异常，具体异常堆栈信息如下： Caused by: org.apache.calcite.runtime.CalciteContextException: From line 1, column 8 to line 1, column 15: Column SYS_DATE not found in any table\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\tat java.lang.reflect.Constructor.newInstance(Constructor.java:423)\tat org.apache.calcite.runtime.Resources$ExInstWithCause.ex(Resources.java:511)\tat org.apache.calcite.sql.SqlUtil.newContextException(SqlUtil.java:952)\tat org.apache.calcite.sql.SqlUtil.newContextException(SqlUtil.java:937)\tat org.apache.calcite.sql.validate.SqlValidatorImpl.newValidationError(SqlValidatorImpl.java:5899)\tat org.apache.calcite.sql.validate.DelegatingScope.fullyQualify(DelegatingScope.java:293)\tat org.apache.calcite.sql.validate.SqlValidatorImpl$Expander.visit(SqlValidatorImpl.java:7105)\tat org.apache.calcite.sql.validate.SqlValidatorImpl$SelectExpander.visit(SqlValidatorImpl.java:7276)\tat org.apache.calcite.sql.validate.SqlValidatorImpl$SelectExpander.visit(SqlValidatorImpl.java:7261)\tat org.apache.calcite.sql.SqlIdentifier.accept(SqlIdentifier.java:324)\tat org.apache.calcite.sql.validate.SqlValidatorImpl$Expander.go(SqlValidatorImpl.java:7094)\tat org.apache.calcite.sql.validate.SqlValidatorImpl.expandSelectExpr(SqlValidatorImpl.java:6665)\tat org.apache.calcite.sql.validate.SqlValidatorImpl.expandSelectItem(SqlValidatorImpl.java:481)\tat org.apache.calcite.sql.validate.SqlValidatorImpl.validateSelectList(SqlValidatorImpl.java:5015)\tat org.apache.calcite.sql.validate.SqlValidatorImpl.validateSelect(SqlValidatorImpl.java:4096)\tat org.apache.calcite.sql.validate.SelectNamespace.validateImpl(SelectNamespace.java:62)\tat org.apache.calcite.sql.validate.AbstractNamespace.validate(AbstractNamespace.java:95)\tat org.apache.calcite.sql.validate.SqlValidatorImpl.validateNamespace(SqlValidatorImpl.java:1206)\tat org.apache.calcite.sql.validate.SqlValidatorImpl.validateQuery(SqlValidatorImpl.java:1177)\tat org.apache.calcite.sql.SqlSelect.validate(SqlSelect.java:282)\tat org.apache.calcite.sql.validate.SqlValidatorImpl.validateScopedExpression(SqlValidatorImpl.java:1143)\tat org.apache.calcite.sql.validate.SqlValidatorImpl.validate(SqlValidatorImpl.java:849)\tat org.apache.calcite.sql2rel.SqlToRelConverter.convertQuery(SqlToRelConverter.java:624)\tat org.apache.calcite.prepare.Prepare.prepareSql(Prepare.java:257)\tat org.apache.calcite.prepare.Prepare.prepareSql(Prepare.java:220)\tat org.apache.calcite.prepare.CalcitePrepareImpl.prepare2_(CalcitePrepareImpl.java:673)\tat org.apache.calcite.prepare.CalcitePrepareImpl.prepare_(CalcitePrepareImpl.java:524)\tat org.apache.calcite.prepare.CalcitePrepareImpl.prepareSql(CalcitePrepareImpl.java:492)\tat org.apache.calcite.jdbc.CalciteConnectionImpl.parseQuery(CalciteConnectionImpl.java:237)\tat org.apache.calcite.jdbc.CalciteMetaImpl.prepareAndExecute(CalciteMetaImpl.java:702)\tat org.apache.calcite.avatica.AvaticaConnection.prepareAndExecuteInternal(AvaticaConnection.java:677)\tat org.apache.calcite.avatica.AvaticaStatement.executeInternal(AvaticaStatement.java:157)\t... 3 more 我们在 SqlValidatorImpl.java:7105 位置设置断点，可以发现在校验 SYS_DATE 函数标识符时，会调用 validator.makeNullaryCall(id) 方法，该方法内部加载完函数后，会判断运算符的语法类型是否为 SqlSyntax.FUNCTION_ID，而 FUNCTION_ID 代表的就是类似于 CURRENTTIME 和 SYS_DATE 这样无括号的函数。 // SqlValidatorImpl#Expander 类 visit 方法@Overridepublic @Nullable SqlNode visit(SqlIdentifier id) // First check for builtin functions which dont have // parentheses, like LOCALTIME. final SqlCall call = validator.makeNullaryCall(id); if (call != null) return call.accept(this); final SqlIdentifier fqId = getScope().fullyQualify(id).identifier; SqlNode expandedExpr = expandDynamicStar(id, fqId); validator.setOriginal(expandedExpr, id); return expandedExpr;// SqlValidatorImpl 类 makeNullaryCall 方法@Overridepublic @Nullable SqlCall makeNullaryCall(SqlIdentifier id) if (id.names.size() == 1 !id.isComponentQuoted(0)) final ListSqlOperator list = new ArrayList(); opTab.lookupOperatorOverloads(id, null, SqlSyntax.FUNCTION, list, catalogReader.nameMatcher()); for (SqlOperator operator : list) if (operator.getSyntax() == SqlSyntax.FUNCTION_ID) // Even though this looks like an identifier, it is a // actually a call to a function. Construct a fake // call to this function, so we can use the regular // operator validation. return new SqlBasicCall(operator, ImmutableList.of(), id.getParserPosition(), null).withExpanded(true); return null; 那么问题的根源就变成了，为什么自定义函数转换为运算符后，它的语法类型不是 FUNCTION_ID？我们回顾下前面介绍的 INDEX_OF 函数执行过程，在 SQL 校验阶段，会将 ScalarFunction 转换为 SqlUserDefinedFunction 运算符，而它则继承了 SqlFunction 类，SqlFunction#getSyntax 方法默认返回 SqlSyntax.FUNCTION。笔者排查了 SqlUserDefinedFunction 类，发现 Calcite 没有提供用户自定义语法类型的 API，如果想要支持这种不带括号的写法，需要修改源码调整 CalciteCatalogReader 类中的 toOp 方法。 本地尝试修改了下 Calcite CalciteCatalogReader 类源码，在 toOp 方法转换时，判断参数个数以及 allowNiladicParentheses 属性，如果无参数并且 allowNiladicParentheses 设置为 false，则将语法类型设置为 FUNCTION_ID。 if (function instanceof ScalarFunction) final SqlReturnTypeInference returnTypeInference = infer((ScalarFunction) function); SqlSyntax syntax = function.getParameters().isEmpty() config.conformance().allowNiladicParentheses() ? SqlSyntax.FUNCTION : SqlSyntax.FUNCTION_ID; return new SqlUserDefinedFunction(name, kind, returnTypeInference, operandTypeInference, operandMetadata, function, syntax); 修改完成后，使用 ./gradlew publishToMavenLocal 发布到本地 Maven 仓库进行测试，然后引入新版本测试 SELECT SYS_DATE FROM calcite_function.t_user，此时可以正确执行并返回结果。 08:49:17.625 [main] INFO com.strongduanmu.udf.UDFExample - ColumnLabel: SYS_DATE, ColumnValue: 2024-10-1808:49:17.648 [main] INFO com.strongduanmu.udf.UDFExample - ColumnLabel: SYS_DATE, ColumnValue: 2024-10-1808:49:17.668 [main] INFO com.strongduanmu.udf.UDFExample - ColumnLabel: SYS_DATE, ColumnValue: 2024-10-18 目前，该 PR 还存在部分单测异常，笔者稍后修复后会提交到 Calcite 仓库，争取下个版本能够支持自定义函数不带括号的写法。 UDAF 聚合函数扩展 聚合函数的扩展和标量函数稍有不同，聚合函数会将多个值组合转换为标量值，因此在聚合函数内部需要维护一个累加器，负责将数据行的值进行累加，当所有数据行遍历完成后，输出聚合函数的结果。因此，扩展聚合函数需要实现 init、add 和 result 方法，如下展示了一个将记录聚合为集合的函数实现： @SuppressWarnings(unused)public class UDAFRegistry public CollectionObject init() return new LinkedList(); public CollectionObject add(final CollectionObject accumulator, final Object newElement) accumulator.add(newElement); return accumulator; public CollectionObject result(final CollectionObject accumulator) return accumulator; 在 init 方法中初始化了一个 CollectionObject 容器，执行时每行数据都会调用 add 方法，并将 CollectionObject 容器和当前行的元素传递进来，此时可以将新元数据添加到容器中，然后返回容器对象，最后所有行遍历完成后，会调用 result 方法，此时会返回容器对象，容器中记录了所有元素。 实现完 UDAF 后，我们需要通过 AggregateFunctionImpl#create 方法，将其注册到 Scehma 中： rootSchema.add(CONCAT_TO_LIST, Objects.requireNonNull(AggregateFunctionImpl.create(UDAFRegistry.class)));CalciteJDBCUtils.executeQuery(calciteConnection, SELECT CONCAT_TO_LIST(user_name) FROM calcite_function.t_user); 然后我们执行 SELECT CONCAT_TO_LIST(user_name) FROM calcite_function.t_user 语句进行测试，执行结果如下，可以发现 user_name 列中的值被转换为数组进行输出。 10:50:18.709 [main] INFO com.strongduanmu.udaf.UDAFExample - ColumnLabel: EXPR$0, ColumnValue: [zhangsan, lisi, wangwu] 查看执行过程中的执行计划，可以发现聚合函数位于 LogicalAggregate 运算符中，并且没有指定分组条件，会对所有数据行进行聚合计算。 # 逻辑执行计划LogicalAggregate(group=[], EXPR$0=[CONCAT_TO_LIST($0)]) LogicalProject(user_name=[$1]) JdbcTableScan(table=[[calcite_function, t_user]])# 物理执行计划EnumerableAggregate(group=[], EXPR$0=[CONCAT_TO_LIST($1)]) JdbcToEnumerableConverter JdbcTableScan(table=[[calcite_function, t_user]]) 跟踪 Calcite 生成的执行代码，会依次调用 apply() 方法初始化，内部调用的是 UDAFRegistry#init 方法，然后调用 accumulatorAdder() 方法执行聚合逻辑，每行数据都会调用 UDAFRegistry#add 方法，最后调用 singleGroupResultSelector 获取结果，内部调用的是 UDAFRegistry#result 方法。 java.util.List accumulatorAdders = new java.util.LinkedList();accumulatorAdders.add(new org.apache.calcite.linq4j.function.Function2() public Record3_0 apply(Record3_0 acc, Object[] in ) if (!( in [1] == null || in [1].toString() == null)) acc.f2 = true; // 调用 add 方法，增加新元数据 acc.f0 = acc.f1.add(acc.f0, in [1] == null ? null : in [1].toString()); return acc; public Record3_0 apply(Object acc, Object in ) return apply( (Record3_0) acc, (Object[]) in ); );org.apache.calcite.adapter.enumerable.AggregateLambdaFactory lambdaFactory = new org.apache.calcite.adapter.enumerable.BasicAggregateLambdaFactory( new org.apache.calcite.linq4j.function.Function0() public Object apply() java.util.Collection a0s0; com.strongduanmu.udaf.UDAFRegistry a0s1; boolean a0s2; a0s2 = false; // 创建 UDAFRegistry 对象 a0s1 = new com.strongduanmu.udaf.UDAFRegistry(); // 调用 init 方法初始化 a0s0 = a0s1.init(); Record3_0 record0; // 将聚合函数的累加器、UDF 对象、是否计算完成标记记录到 Record3_0 的 f0、f1 和 f2 中 record0 = new Record3_0(); record0.f0 = a0s0; record0.f1 = a0s1; record0.f2 = a0s2; return record0; , accumulatorAdders);// 先调用 apply() 初始化，然后调用 accumulatorAdder() 执行聚合逻辑，最后调用 singleGroupResultSelector 获取结果return org.apache.calcite.linq4j.Linq4j.singletonEnumerable(enumerable.aggregate(lambdaFactory.accumulatorInitializer().apply(), lambdaFactory.accumulatorAdder(), lambdaFactory.singleGroupResultSelector(new org.apache.calcite.linq4j.function.Function1() public Object apply(Record3_0 acc) // 判断是否计算完成，完成则调用 result 方法 return acc.f2 ? acc.f1.result(acc.f0) : null; public Object apply(Object acc) return apply( (Record3_0) acc); UDTF 表函数 表宏扩展 根据前文介绍，UDTF 表函数是指在执行阶段将某些数据转换为表的函数，而表宏则是指在编译阶段将某些数据转换为表的函数，两者都可以用于 FROM 子句中，作为一张表进行使用。本小节重点探究下 UDTF 表函数的扩展，来实现一个将 java=1,go=2,scala=3 结构数据，拆分为两列三行表结构的表函数，表宏的扩展方式类似，留给大家自行探究。 首先，我们需要实现表函数的逻辑，根据前面介绍，表函数需要实现 QueryableTable 或者 ScannableTable 接口，然后才可以创建出 TableFunction 实现对象，在前面很多文章中，我们介绍过 ScannableTable，本次我们使用 QueryableTable 来实现表函数逻辑。 UDTF 函数的实现逻辑如下，eval 方法是函数的主体逻辑，它接收 content 字符串以及 columnSize 数值两个参数，并返回 QueryableTable 实现对象。eval 方法内部我们创建了一个 AbstractQueryableTable 对象，构造方法中传递了函数的返回类型，是一个对象数组 Object[].class。 AbstractQueryableTable 中需要实现 asQueryable 和 getRowType 方法，asQueryable 方法负责将数据转换为可以遍历的对象，内部可以覆盖 enumerator 方法，该方法内部函数会对传入的 content 字段进行切割处理，并根据 columnSize 组装结果集的列数，moveNext 方法会首先被调用，将游标移动到结果集的第一位，然后调用 current 方法获取当前记录，然后循环调用 moveNext 和 current 直到所有记录遍历完成，最后会调用 reset 和 close 方法重置和关闭资源。getRowType 方法用于返回 UDTF 计算结果集的类型，本案例中表函数返回 key、value 两列，分别为 String 和 int 类型。 @SuppressWarnings(unused, unchecked)public class UDTFRegistry public static final Method UDTF_METHOD = Types.lookupMethod(UDTFRegistry.class, eval, String.class, int.class); public QueryableTable eval(final String content, final int columnSize) return new AbstractQueryableTable(Object[].class) @Override public T QueryableT asQueryable(final QueryProvider queryProvider, final SchemaPlus schema, final String tableName) return (QueryableT) new BaseQueryableObject[](queryProvider, String.class, null) @Override public EnumeratorObject[] enumerator() return new EnumeratorObject[]() String[] rows = content.split(,); int index = -1; @Override public Object[] current() String row = rows[index]; Object[] result = new Object[columnSize]; String[] columns = row.split(=); Preconditions.checkArgument(columns.length == 2, String.format(Invalid row: %s, row must constains %d columns., row, columnSize)); return columns; @Override public boolean moveNext() if (index rows.length - 1) index++; return true; return false; @Override public void reset() @Override public void close() ; ; @Override public RelDataType getRowType(final RelDataTypeFactory typeFactory) return typeFactory.createStructType(Arrays.asList(typeFactory.createJavaType(String.class), typeFactory.createJavaType(String.class)), Arrays.asList(key, value)); ; 实现完 UDTF 逻辑后，我们需要使用 TableFunctionImpl#create 方法，将 UDTF 函数注册到 Schema 中。 rootSchema.add(EXTRACT_FROM_CSV, Objects.requireNonNull(TableFunctionImpl.create(UDTFRegistry.UDTF_METHOD)));CalciteJDBCUtils.executeQuery(calciteConnection, SELECT * FROM EXTRACT_FROM_CSV(java=1,go=2,scala=3, 2)); 然后我们执行 SELECT * FROM EXTRACT_FROM_CSV('java=1,go=2,scala=3', 2) 语句，可以得到如下结果，可以看到原先的字符串，被表函数按照 , 拆分为 3 行，每行又按照 = 拆分为 2 列，列名也是我们指定的 key 和 value。 08:33:14.584 [main] INFO com.strongduanmu.common.CalciteJDBCUtils - ColumnLabel: key, ColumnValue: java08:33:14.584 [main] INFO com.strongduanmu.common.CalciteJDBCUtils - ColumnLabel: value, ColumnValue: 108:33:14.584 [main] INFO com.strongduanmu.common.CalciteJDBCUtils - ColumnLabel: key, ColumnValue: go08:33:14.584 [main] INFO com.strongduanmu.common.CalciteJDBCUtils - ColumnLabel: value, ColumnValue: 208:33:14.584 [main] INFO com.strongduanmu.common.CalciteJDBCUtils - ColumnLabel: key, ColumnValue: scala08:33:14.584 [main] INFO com.strongduanmu.common.CalciteJDBCUtils - ColumnLabel: value, ColumnValue: 3 跟踪 UDTF 函数的执行过程，我们可以发现 Calcite 解析后增加了一个 TABLE 函数嵌套在 EXTRACT_FROM_CSV 外部，用于标记这是一个表函数，内部的函数和其他自定义函数一样，是 SqlUnresolvedFunction 类型。 在校验阶段，TABLE 函数会被转换为 SqlCollectionTableOperator，根据 Calcite 注释说明，它表示表函数派生表运算符，用于将表函数的值转换为关系 Relation，EXTRACT_FROM_CSV 函数被转换为 SqlUserDefinedTableFunction 对象。 校验完成后，Calcite 又生成了如下的逻辑执行计划和物理执行计划，可以看到最终物理执行计划只包含了 EnumerableTableFunctionScan，这是因为我们使用 * 查询，最终的投影列和 EnumerableTableFunctionScan 一致，因此无需再进行投影计算。EnumerableTableFunctionScan 中还包含了函数调用信息，以及返回结果类型信息。 # 逻辑执行计划LogicalProject(key=[$0], value=[$1]) LogicalTableFunctionScan(invocation=[EXTRACT_FROM_CSV(java=1,go=2,scala=3, 2)], rowType=[RecordType(JavaType(class java.lang.String) key, JavaType(class java.lang.String) value)], elementType=[class [Ljava.lang.Object;])# 物理执行计划EnumerableTableFunctionScan(invocation=[EXTRACT_FROM_CSV(java=1,go=2,scala=3, 2)], rowType=[RecordType(JavaType(class java.lang.String) key, JavaType(class java.lang.String) value)], elementType=[class [Ljava.lang.Object;]) 最终通过 Calcite 代码生成，生成的可执行逻辑如下，在调用 bind 方法时，内部会调用到 UDTFRegistry#eval 方法，并转换为可枚举的 Enumerable 对象，最终通过 Calcite JDBC 提供查询结果。 public org.apache.calcite.linq4j.Enumerable bind(final org.apache.calcite.DataContext root) final com.strongduanmu.udtf.UDTFRegistry f = new com.strongduanmu.udtf.UDTFRegistry(); return f.eval(java=1,go=2,scala=3, 2).asQueryable(root.getQueryProvider(), (org.apache.calcite.schema.SchemaPlus) null, EXTRACT_FROM_CSV).asEnumerable();public Class getElementType() return java.lang.Object[].class; 结语 本文首先介绍了 Calcite 函数相关的概念和构成体系，然后按照函数的分类：标量函数、聚合函数以及表函数/表宏，为大家介绍了这些函数内部的实现细节，并通过 Calcite functions.iq 测试用例中的一个典型案例，和大家一起跟踪了函数的执行流程。 Calcite 函数执行的过程中，首先会创建 SqlValidator 对象，此时会将 Calcite 内置函数以及用户自定义函数注册进来。然后会依次进行 SQL 解析和 SQL 校验，通常函数都会被解析为 SqlUnresolvedFunction 对象，并在 SQL 校验时，通过查找函数注册表将 SqlUnresolvedFunction 转换为 SqlBasicFunction 或其他函数运算符对象。SQL 校验完成后，Calcite 会继续进行 SQL 优化和 SQL 执行，优化阶段会调用 StandardConvertletTable#convertFunction 方法，将函数运算符转换为 RexNode，并和其他 SQL 部分一起构成一个 RelNode 执行计划树。最后 Calcite 会调用 implement 方法生成可执行代码，此时会将内置函数或自定义函数的实现逻辑，嵌入到整个可执行代码中，通过 Janio 将代码编译为可执行的对象，这样就完成了整个 SQL 执行逻辑。 在文章的最后部分，我们又介绍了不同类型函数的扩展方式。Calcite 扩展函数非常简单，用户只需按照规范编写函数逻辑，然后通过 Schema API 注册到 Calcite 中，就可以成功执行函数逻辑，感兴趣的同学可以参考文中的代码示例，尝试实现不同逻辑的自定义函数。 根据 Apache Calcite System Catalog 实现探究中介绍，Calcite Catalog 中除了函数外，还包含了类型系统、视图、物化视图等对象，这些对象在日常数据处理中也都非常重要，我们将在后续文章中继续探究学习，欢迎感兴趣的朋友持续关注。 写在最后 笔者因为工作原因接触到 Calcite，前期学习过程中，深感 Calcite 学习资料之匮乏，因此创建了 Calcite 从入门到精通知识星球，希望能够将学习过程中的资料和经验沉淀下来，为更多想要学习 Calcite 的朋友提供一些帮助。 欢迎关注 欢迎关注「端小强的博客」微信公众号，会不定期分享日常学习和工作经验，欢迎大家关注交流。","tags":["Calcite"],"categories":["Calcite"]},{"title":"GraalVM 编译动态链接库之 MySQL UDF 实现","path":"/blog/graalvm-compilation-of-dynamic-link-library-mysql-udf-implementation.html","content":"前言 在之前发布的 Java AOT 编译框架 GraalVM 快速入门一文中，我们介绍了 GraalVM 编译器的基础知识，对比了 GraalVM 和传统 JVM 之间的优势和劣势，并通过 Demo 示例展示了如何将 JVM 程序编译为原生可执行程序。GraalVM 除了编译原生可执行程序外，还可以用于编译动态链接库，提供给 C、C++ 等原生语言调用，GraalVM 编译动态链接库的能力大大提升了 Java 和原生语言之间的互操作性。本文将为大家介绍如何使用 GraalVM 编译动态链接库，并使用 C 语言调用动态链接库，从而实现基于 SM4 加解密的 MySQL UDF。 GraalVM 动态链接库规范 首先，我们来了解如何使用 GraalVM 编译动态链接库，参考官方文档 Build a Native Shared Library，GraalVM 编译动态链接库，需要将 --shared 参数传递给 native-image 工具，默认会将 main 方法作为动态链接库的入口点方法，具体的编译命令如下： # 指定 class namenative-image class name --shared# 指定 jar 文件native-image -jar jarfile --shared 如果类中不包含 mian 方法，则需要通过 -o 参数指定库名称，并且在 Java 类中通过 @CEntryPoint 注解，指定需要导出的入口点方法，使用 -o 参数指定库名称的命令如下： native-image --shared -o libraryname class namenative-image --shared -jar jarfile -o libraryname 使用 @CEntryPoint 注解导出某个方法为动态链接库，需要满足以下条件： 方法必须声明为静态方法； 在要导出的方法上，使用 @CEntryPoint 注解进行标记； 方法参数需要增加额外的 IsolateThread 或 Isolate 类型参数，该参数用于提供当前线程的执行上下文； 方法返回类型只能是 Java 基础类型，以及 org.graalvm.nativeimage.c.type 包中的类型； 导出方法的名称必须保证唯一，否则 native-image 构建将会失败，如果未在 @CEntryPoint 注解中通过 name 指定名称，则必须在构建时提供 -o libraryName 选项。 下面展示了动态链接库入口方法示例，通过 name 指定了动态链接库函数名为 add，该函数有 3 个参数：thread、a 和 b，thread 用于提供当前线程的执行上下文，a 和 b 都是基础类型，因此可以直接使用在函数中。函数逻辑比较简单，将入参 a 和 b 的值相加求和，然后返回 int 类型的结果。 @CEntryPoint(name = add)static int add(IsolateThread thread, int a, int b) return a + b; 当我们使用 native-image 工具编译动态链接库时，它会生成一个动态链接库文件，以及一个 C 头文件，头文件中声明了导出的函数，可以直接在 C 代码中引入头文件，并调用 add 函数。 int add(graal_isolatethread_t* thread, int a, int b); (adsbygoogle = window.adsbygoogle || []).push({}); 赞助商 MySQL UDF 规范 介绍完 GraalVM 编译动态链接库，我们再来了解下 MySQL UDF 的使用规范。MySQL 8.0 官方文档 Adding a Loadable Function 详细介绍了如何使用 C/C++ 编写 MySQL UDF，并将其编译为动态链接库部署到 MySQL plugin 目录中，然后通过 CREATE FUNCTION ... RETURNS ... SONAME 'xxx.so' 语句创建 UDF 函数。 编写 MySQL UDF 需要实现 3 个函数，分别是：初始化函数 xxx_init()、主函数 xxx() 和析构函数 xxx_deinit()。MySQL 会先调用 xxx_init() 进行初始化，如果 xxx_init() 返回异常，则主函数 xxx() 和析构函数 xxx_deinit() 都不会被调用，整个语句会抛出异常信息。如果 xxx_init() 执行成功，MySQL 会调用主函数 xxx() 执行函数逻辑，通常情况下每行数据都会调用一次。当所有数据行调用完主函数后，最后会调用 xxx_deinit() 对资源进行清理。 xxx_init 函数 xxx_init() 函数是 MySQL UDF 的初始化函数，可以用来完成如下的初始化工作： 检查传入函数的参数个数； 检验传入函数的参数数据类型，是否可为空，以及对参数类型进行类型转换； 分配函数所需的内存； 指定返回值的最大长度； 指定返回值的精度（针对返回值是 REAL 类型的函数）； 指定返回值是否可以为 NULL。 初始化函数的声明如下： bool xxx_init(UDF_INIT *initid, UDF_ARGS *args, char *message); initid 参数中存储了初始化信息，它会传递给 UDF 涉及的 3 个函数，UDF_INIT 结构包含如下成员： bool maybe_null：用于标识当前 UDF 函数是否可以返回 NULL，如果可以返回 NULL，则需要在初始化时设置成 true。如果函数中的任意参数设置了 maybe_null 为 true，则函数 maybe_null 的默认值也为 true； unsigned int decimals：指定小数点右侧的小数位数。默认值是传递给函数参数中最大的小数位数，例如，函数参数 1.34、1.345 和 1.3，则默认值为 3，因为 1.345 有 3 个小数数字。对于没有固定小数位数的参数，decimals 值设置为 31，这比 DECIMAL、FLOAT 和 DOUBLE 数据类型允许的最大小数位数多 1； unsigned int max_length：指定返回值的最大长度。对于不同的返回值类型，max_length 的默认值不同，对于 STRING 类型，默认值和最长的函数参数相等。对于 INTEGER 类型， 默认值为 21。而对于 BLOB 类型的，可以将它设置成 65KB 或 16MB； char *ptr：可实现 UDF 特定需求的指针，一般在 xxx_init() 中申请内存，在 xxx_deinit() 中释放内存，例如：用来存储 UDF 函数过长（长度超过 255 个字符）的字符串结果，默认情况下 UDF 主函数中的 result 只能存储 255 个字符，超过 255 个字符需要自行通过 ptr 指针申请内存，用于存储字符串结果； initid - ptr = allocated_memory; bool const_item：如果 xxx() 函数总是返回相同值，xxx_init() 中可以把该值设置成 true。 args 参数用于存储 UDF 函数的参数信息，UDF_ARGS 结构包含如下成员： unsigned int arg_count：UDF 函数参数个数，可以在 xxx_init() 函数中检查参数个数是否符合预期，例如： if (args - arg_count != 2) strcpy(message,XXX() requires two arguments); return 1; enum Item_result *arg_type：用于定义参数类型的数组，可选值包括：STRING_RESULT、INT_RESULT、REAL_RESULT 和 DECIMAL_RESULT，可以在 xxx_init 函数中检查参数类型，也可以将 arg_type 元素设置为所需的类型，MySQL 在每次调用 xxx 函数时都会进行强制类型转换； // 类型检查if (args - arg_type[0] != STRING_RESULT || args - arg_type[1] != INT_RESULT) strcpy(message,XXX() requires a string and an integer); return 1;// 强制类型转换args - arg_type[0] = STRING_RESULT;args - arg_type[1] = INT_RESULT;args - arg_type[2] = REAL_RESULT; char **args：对于初始化函数 xxx_init()，当参数是常量时，例如：3、4 * 7 - 2 或 SIN(3.14)，args - args[i] 指向参数值，当参数是非常量时 args - args[i] 为 NULL。对于主函数 xxx()，args - args[i] 总是指向参数的值，如果参数 i 为 NULL，则 args - args[i] 为 NULL。对于 STRING_RESULT 类型，args - args[i] 指向对应的字符串，args - lengths[i] 代表字符串长度。对于 INT_RESULT 类型，需要强制转型为 long long，对于 REAL_RESULT 类型，需要转型为 double； // INT_RESULT 类型参数强转为 long longlong long int_val = *(long long *) args - args[i];// REAL_RESULT 类型参数强转为 doubledouble real_val = *(double *) args - args[i]; unsigned long *lengths：对于初始化函数 xxx_init()，lengths 数组表示每个参数的最大长度，对于主函数 xxx()，lengths 数组表示每个参数的实际长度； char *maybe_null：对于初始化函数 xxx_init()，maybe_null 表示对应的参数是否可以为 NULL； char **attributes：表示传入参数的参数名，参数名对应的长度存储在 args - attribute_lengths[i] 中，以 SELECT my_udf(expr1, expr2 AS alias1, expr3 alias2); 为例，对应的参数名和参数名长度如下； args - attributes[0] = expr1args - attribute_lengths[0] = 5args - attributes[1] = alias1args - attribute_lengths[1] = 6args - attributes[2] = alias2args - attribute_lengths[2] = 6 unsigned long *attribute_lengths：表示传入参数名称的长度。 xxx 函数 xxx 函数是 MySQL UDF 的主函数，在 SQL 中调用对应的 UDF 函数时，每行记录都会调用 xxx 函数，SQL 的数据类型和 C/C++ 的数据类型对应关系如下，这些数据类型可以用于函数参数和返回值。 SQL 类型 C/C++ 类型 STRING char * INTEGER long long REAL double 注意：也可以声明 DECIMAL 函数，但该值只能以字符串形式返回，因此你应该将函数编写为 STRING 函数。 对于不同的返回值类型，xxx 函数的定义不同，下面分别列举了返回值 STRING、INTEGER 和 REAL 类型的 UDF 函数定义： // 返回值是 STRING 或 DECIMALchar *xxx(UDF_INIT *initid, UDF_ARGS *args, char *result, unsigned long *length, char *is_null, char *error);// 返回值是 INTEGERlong long xxx(UDF_INIT *initid, UDF_ARGS *args, char *is_null, char *error);// 返回值是 REALdouble xxx(UDF_INIT *initid, UDF_ARGS *args, char *is_null, char *error); 主函数 xxx 如果出现了异常，需要将 *error 设置为 1： *error = 1; 此时，MySQL 执行到主函数时，会直接返回 NULL 结果，并忽略后续的数据行。和 xxx_init() 初始化函数不同的是，xxx 函数无法通过 message 返回错误，只能返回 NULL 值，并通过 fprintf 函数将错误日志输出到 MySQL ERROR 日志中（如果笔者理解错误，欢迎 MySQL 大佬指正）。 xxx_deinit 函数 xxx_deinit() 函数是 UDF 的析构函数，用于释放初始化函数分配的内存，以及进行其他的清理工作。xxx_deinit() 函数是可选的，如果 UDF 函数无需释放资源，则可以不实现该函数。 xxx_deinit() 函数的定义如下： void xxx_deinit(UDF_INIT *initid); GraalVM 实现 MySQL UDF 实战 前文我们已经介绍了如何使用 GraalVM 编译动态链接库，并了解了实现 MySQL UDF 的规范，下面我们通过一个实战案例，具体介绍下如何使用 GraalVM 实现基于 SM4 的加解密 UDF 函数。 基于 SM4 实现加解密 SM4 加密算法是国家密码管理局于 2012 年发布的，它是一种分组加密算法，目前 SM4 系列算法已经广泛应用于国内的安全加密领域。由于 SM4 系列算法主要应用于国内，JDK 的标准库中并未提供支持，因此我们需要通过第三方库 BouncyCastle 来实现 SM4 加解密。 使用 BouncyCastle 比较简单，首先需要在项目中依赖 BouncyCastle 库的 jar 包，我们可以通过如下的 Maven 坐标进行依赖： dependency groupIdorg.bouncycastle/groupId artifactIdbcprov-jdk18on/artifactId version1.78.1/version/dependency 然后借助 java.security 提供的扩展机制，使用 Security.addProvider(new BouncyCastleProvider()); 注册 BouncyCastle 提供器。下面展示了一个简单的 SM4 算法加解密示例，通过 Cipher.getInstance(SM4/ECB/PKCS5Padding 可以获取一个采用 ECB 模式的 SM4 算法，然后调用 init 方法分别对加密和解密模式进行初始化，最后调用 doFinal 方法执行加密和解密的逻辑。 public final class SM4CryptographicDemo @SneakyThrows public static void main(String[] args) // 注册 BouncyCastle Security.addProvider(new BouncyCastleProvider()); byte[] securityKey = Hex.decodeHex(4D744E003D713D054E7E407C350E447E); String originalValue = GraalVM SM4 Test.; System.out.println(Original Value: + originalValue); // 加密处理 Cipher encryptCipher = Cipher.getInstance(SM4/ECB/PKCS5Padding, BouncyCastleProvider.PROVIDER_NAME); encryptCipher.init(Cipher.ENCRYPT_MODE, new SecretKeySpec(securityKey, SM4)); String encryptValue = Hex.encodeHexString((encryptCipher.doFinal(originalValue.getBytes()))); System.out.println(Encrypt Value: + encryptValue); // 解密处理 Cipher decryptCipher = Cipher.getInstance(SM4/ECB/PKCS5Padding, BouncyCastleProvider.PROVIDER_NAME); decryptCipher.init(Cipher.DECRYPT_MODE, new SecretKeySpec(securityKey, SM4)); String decryptValue = new String((decryptCipher.doFinal(Hex.decodeHex(encryptValue)))); System.out.println(Decrypt Value: + decryptValue); SM4 算法加密和解密的执行结果如下图： 编译 SM4 加解密动态链接库 使用 BouncyCastle 库实现基础的 SM4 加密和解密后，我们尝试使用 GraalVM 来编译一个动态链接库。根据前文的介绍，GraalVM 编译动态链接库需要通过 @CEntryPoint 注解，由于我们需要实现加密和解密两个函数，因此需要分别定义加密和解密方法，并使用 @CEntryPoint 进行标记。如下的 SM4CryptographicNativeLibrary 类，展示了 SM4 加密和解密动态链接库的逻辑： @SuppressWarnings(unused)public final class SM4CryptographicNativeLibrary static // 注册 BouncyCastle Security.addProvider(new BouncyCastleProvider()); @SneakyThrows @CEntryPoint(name = sm4_encrypt) private static CCharPointer sm4Encrypt(final IsolateThread thread, final CCharPointer plainValue, final CCharPointer cipherValue) String encryptValue = Hex.encodeHexString(getCipher(Cipher.ENCRYPT_MODE).doFinal(CTypeConversion.toJavaString(plainValue).getBytes())); // C 语言中会在字符串结尾自动添加 \\0 转义符，所以这里需要 +1 int encryptValueLength = encryptValue.getBytes().length + 1; CTypeConversion.toCString(encryptValue, cipherValue, WordFactory.unsigned(encryptValueLength)); return cipherValue; @SneakyThrows @CEntryPoint(name = sm4_decrypt) private static CCharPointer sm4Decrypt(final IsolateThread thread, final CCharPointer cipherValue, final CCharPointer plainValue) String decryptValue = new String(getCipher(Cipher.DECRYPT_MODE).doFinal(Hex.decodeHex(CTypeConversion.toJavaString(cipherValue)))); // C 语言中会在字符串结尾自动添加 \\0 转义符，所以这里需要 +1 int decryptValueLength = decryptValue.getBytes().length + 1; CTypeConversion.toCString(decryptValue, plainValue, WordFactory.unsigned(decryptValueLength)); return plainValue; @SneakyThrows private static Cipher getCipher(final int cipherMode) Cipher result = Cipher.getInstance(SM4/ECB/PKCS5Padding, BouncyCastleProvider.PROVIDER_NAME); result.init(cipherMode, new SecretKeySpec(Hex.decodeHex(4D744E003D713D054E7E407C350E447E), SM4)); return result; 首先，SM4CryptographicNativeLibrary 类 static 静态代码块中注册 BouncyCastle，从而保证 java.security 扩展机制能够识别到 BouncyCastle。然后在类中我们定义了 sm4Encrypt 和 sm4Decrypt 两个方法，并使用 @CEntryPoint 进行了方法导出，分别命名为 sm4_encrypt 和 sm4_decrypt。方法内部会调用 getCipher 获取 SM4 加解密方法，并通过 GraalVM 提供的 CTypeConversion.toCString() 方法，将加密、解密后的结果存储到 CCharPointer 类型的缓冲区对象中，存储到缓冲区过程中需要指定 bufferSize，由于 C 语言中会在字符串结尾自动添加 \\0 转义符，所以这里需要 +1 处理。 我们执行如下的命令进行编译： ./mvnw -Pnative clean package -f mysql-udf 编译成功后，target 目录下会生成动态链接库 libsm4_udf.dylib，以及头文件 libsm4_udf.h，后续我们可以使用 C 语言编写 MySQL UDF，并调用 GraalVM 生成的动态链接库。 使用 C 实现 MySQL UDF 根据前文介绍我们知道，使用 C/C++ 编写 MySQL UDF，需要实现 xxx_init、xxx 和 xxx_deinit 3 个函数，其中 xxx_init 和 init 函数是必须实现的，而 xxx_deinit 则是可选的，如果没有资源需要释放，则可以不实现。笔者由于水平有限，简单使用 C 语言实现了 sm4_encrypt_udf 和 sm4_decrypt_udf 两个函数，分别用于 SM4 加密和解密操作。thread 对象的创建参考了 GraalVM 动态链接库文档，需要在调用动态链接库函数时传入 thread 参数，并且在最后通过 graal_tear_down_isolate 函数清理 thread 对象。 #include stdio.h#include string.h#include mysql.h#include libsm4_udf.hbool sm4_encrypt_udf_init(UDF_INIT *initid, UDF_ARGS *args, char *message) return false;char * sm4_encrypt_udf(UDF_INIT *initid, UDF_ARGS *args, char *result, unsigned long *length, char *is_null, char *error) graal_isolate_t *isolate = NULL; graal_isolatethread_t *thread = NULL; if (0 != graal_create_isolate(NULL, isolate, thread)) fprintf(stderr, initialization error ); return ; sm4_encrypt(thread, args - args[0], result); *length = strlen(result); graal_tear_down_isolate(thread); return result;bool sm4_decrypt_udf_init(UDF_INIT *initid, UDF_ARGS *args, char *message) return false;char * sm4_decrypt_udf(UDF_INIT *initid, UDF_ARGS *args, char *result, unsigned long *length, char *is_null, char *error) graal_isolate_t *isolate = NULL; graal_isolatethread_t *thread = NULL; if (0 != graal_create_isolate(NULL, isolate, thread)) fprintf(stderr, initialization error ); return ; sm4_decrypt(thread, args - args[0], result); *length = strlen(result); graal_tear_down_isolate(thread); return result; 我们尝试使用 GCC 工具将 C 源码编译为动态链接库，通过 -I 指定编译时头文件位置，-L/path/to 用于指定编译时库文件的位置，-l 则用于指定库的名称，通常库名称不包含前面的 lib 前缀。-Wl,-rpath=/path/to 指定了运行时的库搜索路径，经笔者测试 MacOS 指定该参数会编译报错，Linux 下不指定该参数，运行时会找不到依赖库报错（具体原因有待进一步探索）。-shared 表示当前编译结果是动态链接库，-o 指定的输出的位置及动态链接库名称，最后则指定了 C 源码的位置。 # -I 指定编译时头文件位置# -L/path/to 指定编译时库文件的位置# -Wl,-rpath=/path/to 指定了运行时的库搜索路径（MacOS 指定会编译报错，Linux 下需要指定）gcc -I/opt/homebrew/opt/mysql/include/mysql -I./mysql-udf/target -L./mysql-udf/target -lsm4_udf -fPIC -g -shared -o ./mysql-udf/target/sm4_encrypt_udf.so ./mysql-udf/src/main/native/sm4_encrypt_udf.c 编译完成后，我们在 target 目录中可以看到 sm4_encrypt_udf.so 文件，这个就是我们需要的 MySQL UDF 动态链接库。 使用如下的命令将 libsm4_udf.dylib 和 sm4_encrypt_udf.so 库复制到 MySQL /lib/plugin 目录下： cp mysql-udf/target/libsm4_udf.dylib /opt/homebrew/opt/mysql/lib/plugin/cp mysql-udf/target/sm4_encrypt_udf.so /opt/homebrew/opt/mysql/lib/plugin/ 然后执行 CREATE FUNCTION 语句，创建 sm4_encrypt_udf 和 sm4_decrypt_udf 函数。 CREATE FUNCTION sm4_encrypt_udf RETURNS STRING SONAME sm4_encrypt_udf.so;CREATE FUNCTION sm4_decrypt_udf RETURNS STRING SONAME sm4_encrypt_udf.so; 我们使用 SELECT sm4_encrypt_udf('123'); 测试 UDF 查询语句，但是出现了如下的异常信息，MySQL 服务由于 UDF 函数异常出现了重启。 mysql SELECT sm4_encrypt_udf(123);No connection. Trying to reconnect...Connection id: 8Current database: testERROR 2013 (HY000): Lost connection to MySQL server during queryNo connection. Trying to reconnect...ERROR 2003 (HY000): Cant connect to MySQL server on 127.0.0.1:3306 (61)ERROR:Cant connect to the server 为了搞清楚异常的原因，我们使用 tail -400f /opt/homebrew/var/mysql/duanzhengqiangdeMacBook-Pro.local.err 命令查看 MySQL Error 日志。如下展示了完整的异常信息，可以看出 GraalVM 生成的动态链接库没有找到 SM4/ECB/PKCS5Padding 算法，异常的根本原因是 GraalVM 只能在编译期注册 Security Provider，无法在运行期注册，因此我们需要手动注册 Security Provider。 参考从零放弃学习 Spring - 在 Native Image 中使用 Bouncy Castle，可以使用 GraalVM 的 Feature 机制注册 BouncyCastleProvider，我们编写如下的 SM4BouncyCastleFeature 类，实现 org.graalvm.nativeimage.hosted.Feature 接口，并通过 RuntimeClassInitialization 初始化 org.bouncycastle 相关的动态类。 @SuppressWarnings(unused)public final class SM4BouncyCastleFeature implements Feature @Override public void afterRegistration(final AfterRegistrationAccess access) RuntimeClassInitialization.initializeAtBuildTime(org.bouncycastle); RuntimeClassInitialization.initializeAtRunTime(org.bouncycastle.jcajce.provider.drbg.DRBG$Default); RuntimeClassInitialization.initializeAtRunTime(org.bouncycastle.jcajce.provider.drbg.DRBG$NonceAndIV); Security.addProvider(new BouncyCastleProvider()); 然后将 SM4BouncyCastleFeature 类路径配置到 native-maven-plugin 插件的 buildArgs 中，完整配置为 --features=com.strongduanmu.SM4BouncyCastleFeature。 build plugins plugin groupIdorg.graalvm.buildtools/groupId artifactIdnative-maven-plugin/artifactId version$native.maven.plugin.version/version extensionstrue/extensions configuration imageName$native.image.name/imageName buildArgs buildArg--no-fallback/buildArg buildArg--features=com.strongduanmu.SM4BouncyCastleFeature/buildArg /buildArgs sharedLibrarytrue/sharedLibrary /configuration executions execution idbuild-native/id goals goalcompile-no-fork/goal /goals phasepackage/phase /execution execution idtest-native/id goals goaltest/goal /goals phasetest/phase /execution /executions /plugin /plugins/build 然后执行 ./mvnw -Pnative clean package -f mysql-udf 以及 gcc -I/opt/homebrew/opt/mysql/include/mysql -I./mysql-udf/target -L./mysql-udf/target -lsm4_udf -fPIC -g -shared -o ./mysql-udf/target/sm4_encrypt_udf.so ./mysql-udf/src/main/native/sm4_encrypt_udf.c 重新编译生成动态链接库，打包完成后将动态链接库复制到 /opt/homebrew/opt/mysql/lib/plugin/ 目录下，并使用 brew services restart mysql 重启 MySQL 服务。 重启完成后，执行以下的 SQL 脚本，删除之前创建的 UDF 函数，并重新创建新的 UDF： # 删除已创建 UDFDROP FUNCTION sm4_encrypt_udf;DROP FUNCTION sm4_decrypt_udf;# 重新创建 UDFCREATE FUNCTION sm4_encrypt_udf RETURNS STRING SONAME sm4_encrypt_udf.so;CREATE FUNCTION sm4_decrypt_udf RETURNS STRING SONAME sm4_encrypt_udf.so; 然后我们再次测试 SM4 UDF 加解密功能，可以看到现在加密和解密都能够完美支持，完整的示例程序请参考 mysql-udf。 mysql SELECT sm4_encrypt_udf(123);+----------------------------------+| sm4_encrypt_udf(123) |+----------------------------------+| 2e5d924b4e9f26831c5cbcb087bd3439 |+----------------------------------+1 row in set (0.00 sec)mysql SELECT sm4_decrypt_udf(2e5d924b4e9f26831c5cbcb087bd3439);+-----------------------------------------------------+| sm4_decrypt_udf(2e5d924b4e9f26831c5cbcb087bd3439) |+-----------------------------------------------------+| 123 |+-----------------------------------------------------+1 row in set (0.01 sec) 结语 本文首先介绍了如何使用 GraalVM 编译动态链接库，需要在 GraalVM 编译时增加 --shared 参数，并将需要导出的动态链接库方法，使用 @CEntryPoint 注解进行标记，方法参数中需要增加额外的 IsolateThread 或 Isolate 参数，用于提供当前线程的执行上下文。 然后又结合 MySQL 官方文档，为大家介绍了 MySQL UDF 函数的实现规范，UDF 函数中通常包含了 xxx_init()、xxx() 和 xxx_deinit() 3 个主要函数，大家可以结合 UDF 函数的逻辑需要，在不同的函数中进行参数校验、UDF 逻辑实现等处理。 最后一个部分，我们使用 GraalVM 实现了一个基于 SM4 国密算法的加解密 UDF，核心的算法逻辑部分使用 GraalVM 编译，MySQL UDF 部分使用 C 语言实现，并调用 GraalVM 编译的动态链接库，最终我们完美地实现了加密和解密功能。 GraalVM 为 Java 生态带来了全新的应用场景，如何使用 GraalVM 将 Java 生态连接到原生应用中，这些都需要大家不断地思考和探索，欢迎大家积极留言交流 GraalVM 应用场景。另外，由于笔者水平有限，本文如有问题，也欢迎留言指正。 欢迎关注 欢迎关注「端小强的博客」微信公众号，会不定期分享日常学习和工作经验，欢迎大家关注交流。","tags":["JVM","GraalVM"],"categories":["GraalVM"]},{"title":"PolarDB-X 开发环境搭建笔记","path":"/blog/polardb-x-dev-environment-setup-note.html","content":"注意：本文基于 PolarDB-X main 分支 6309889 版本源码进行学习研究，其他版本可能会存在实现逻辑差异，对源码感兴趣的读者请注意版本选择。 前言 笔者为了学习 Calcite 相关的技术，最近尝试在本地搭建 PolarDB-X 开发环境，从而可以深入探索 PolarDB-X 是如何基于 Calcite 构建 HTAP 数据库。本文记录了完整的搭建过程，并简单介绍了 PolarDB-X 如何使用以及如何跟踪代码，希望能够帮助到对 Calcite 以及 PolarDB-X 感兴趣的朋友。 PolarDB-X 简介 首先，我们先来了解下 PolarDB-X 数据库，根据官方文档介绍，PolarDB-X 是一款面向超高并发、海量存储、复杂查询场景设计的云原生分布式数据库系统。其采用 Shared Nothing 与存储计算分离架构，支持水平扩展、分布式事务、混合负载等能力，具备企业级、云原生、高可用、高度兼容 MySQL 系统及生态等特点。 如上图所示，PolarDB-X 采用 Shared Nothing 与存储计算分离架构进行设计，系统由 CN、DN、GMS 和 CDC 4 个核心组件组成，下面我们简单介绍下不同组件的功能职责。 计算节点（CN, Compute Node，代码仓库：polardbx-sql）： 计算节点是系统的入口，采用无状态设计，包括 SQL 解析器、优化器、执行器等模块。负责数据分布式路由、计算及动态调度，负责分布式事务 2PC 协调、全局二级索引维护等，同时提供 SQL 限流、三权分立等企业级特性。 存储节点（DN, Data Node，代码仓库：polardbx-engine）： 存储节点负责数据的持久化，基于多数派 Paxos 协议提供数据高可靠、强一致保障，同时通过 MVCC 维护分布式事务可见性。 元数据服务（GMS, Global Meta Service，代码仓库：polardbx-engine）： 元数据服务负责维护全局强一致的 Table/Schema、Statistics 等系统 Meta 信息，维护账号、权限等安全信息，同时提供全局授时服务（即 TSO）。 日志节点（CDC, Change Data Capture，代码仓库：polardbx-cdc）： 日志节点提供完全兼容 MySQL Binlog 格式和协议的增量订阅能力，提供兼容 MySQL Replication 协议的主从复制能力。 (adsbygoogle = window.adsbygoogle || []).push({}); 赞助商 PolarDB-X 开发环境搭建 源码下载编译 前文我们介绍了 PolarDB-X 的基本信息，了解到 PolarDB-X 从架构层面主要可以分为：计算节点 CN、存储节点 DN、元数据服务 GMS 以及日志节点 CDC。由于笔者学习 PolarDB-X 的重点是计算节点 CN，因此先从 polardbx-sql 仓库下载对应的源码，下载完成后使用 IDEA 打开项目，并在项目根目录下，使用 JDK 8 执行 Maven 命令，安装 PolarDB-X 项目所需的依赖： # 确保 polardbx-rpc 子模块 (PolarDB-X Glue) 已初始化git submodule update --init# 编译打包mvn install -Denv=release -DskipTests DN GMS 初始化 编译打包完成后，我们通过 Docker 镜像拉起 CN 依赖的 DN 和 GMS 服务，执行以下脚本启动容器： # 运行 PolarDB-X 远程镜像并建立端口映射# 默认使用 4886 作为 MySQL 端口，32886 作为私有协议端口docker run -d --name some-dn-and-gms --env mode=dev -p 4886:4886 -p 32886:32886 polardbx/polardb-x 由于 PolarDB-X 的元数据存储在 MetaDB 中，我们需要使用 mysql -h127.0.0.1 -P4886 -uroot -padmin -D polardbx_meta_db_polardbx -e select passwd_enc from storage_info where inst_kind=2 查看 metaDbPasswd。执行出现如下异常，官方文档提供的 root/admin 的账号无法连接到 some-dn-and-gms 容器提供的 MySQL 服务。 mysql: [Warning] Using a password on the command line interface can be insecure.ERROR 1045 (28000): Access denied for user root@192.168.65.1 (using password: YES) 观察容器输出的日志，可以看到容器内启动了完整的 PolarDB-X 服务，包含了 CN、DN、GMS 等关键组件，输出的日志如下： 2024-08-30 09:18:49 JAVA_OPTS : -server -Xms2g -Xmx2g -Dtxc.vip.skip=true -Xss4m -XX:+AggressiveOpts -XX:-UseBiasedLocking -XX:-OmitStackTraceInFastThrow -XX:+UseG1GC -XX:MaxGCPauseMillis=250 -XX:+UseGCOverheadLimit -XX:+ExplicitGCInvokesConcurrent -Djava.awt.headless=true -Dcom.alibaba.java.net.VTOAEnabled=true -Djava.net.preferIPv4Stack=true -Dfile.encoding=UTF-8 -Ddruid.logType=slf4j -Xlog:gc*:/home/polarx/polardbx/build/run/polardbx-sql/bin/../logs/tddl/gc.log:time -XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=/home/polarx/polardbx/build/run/polardbx-sql/bin/../logs/tddl -XX:+CrashOnOutOfMemoryError -XX:ErrorFile=/home/polarx/polardbx/build/run/polardbx-sql/bin/../logs/tddl/hs_err_pid%p.log2024-08-30 09:18:49 TDDL_OPTS : -DinitializeGms=false -DforceCleanup=false -DappName=tddl -Dlogback.configurationFile=/home/polarx/polardbx/build/run/polardbx-sql/bin/../conf/logback.xml -Dtddl.conf=/home/polarx/polardbx/build/run/polardbx-sql/bin/../conf/server.properties2024-08-30 09:18:49 start polardb-x2024-08-30 09:18:49 cd to /home/polarx for continue2024-08-30 09:18:49 cn starts. 我们进入容器，并查看 /home/polarx/polardbx/build/run/polardbx-sql/conf/server.properties 配置的内容，里面包含了启动 CN 所需的 metaDbPasswd 配置。 serverPort =8527managerPort=3406charset=utf-8processors=4processorHandler=16processorKillExecutor=128syncExecutor=128managerExecutor=128serverExecutor=1024idleTimeout=trustedIps=127.0.0.1slowSqlTime=1000maxConnection=20000allowManagerLogin=1allowCrossDbQuery=trueenableLogicalDbWarmmingUp=truegalaxyXProtocol =2metaDbAddr =127.0.0.1:4886metaDbXprotoPort =34886metaDbUser=my_polarxmetaDbName=polardbx_meta_db_polardbxinstanceId=polardbx-polardbxmetaDbPasswd=DqQUThAumQ1QSUqiR+HhdaxMKwezoWmXAvyxBgJZxHz9s/ClvIFLoPCeh+zCYDO9 我们使用 PasswdUtil#decrypt 方法可以将密码解密为明文密码，dnPasswordKey 使用默认值 asdf1234ghjk5678，解密可以得到明文密码 qA1(F3$qK4)+R5+wZ5*^*eV0#eB2(iM7_wK9@，这样我们可以通过 my_polarx/qA1(F3$qK4)+R5+wZ5*^*eV0#eB2(iM7_wK9@ 访问 DN/GMS 服务，登录后我们可以查看 DN 和 GMS 服务中存储的数据。 mysql -h127.0.0.1 -P4886 -umy_polarx -p -D polardbx_meta_db_polardbx# 输入密码 qA1(F3$qK4)+R5+wZ5*^*eV0#eB2(iM7_wK9@mysql SHOW DATABASES;+---------------------------+| Database |+---------------------------+| __recycle_bin__ || information_schema || mysql || performance_schema || polardbx_meta_db_polardbx || sys |+---------------------------+6 rows in set (0.04 sec) IDEA 启动 TddlLauncher IDEA 配置调整 配置完成 DN 和 GMS 后，我们需要修改 resources/server.properties 配置，将 serverPort 改为 8527，metaDbAddr 改为 127.0.0.1:4886，metaDbXprotoPort 改为 32886，并增加 metaDbPasswd=DqQUThAumQ1QSUqiR+HhdaxMKwezoWmXAvyxBgJZxHz9s/ClvIFLoPCeh+zCYDO9，修改后的完整配置如下： # PolarDB-X 服务端口serverPort=8527managerPort=3406# PolarDB-X RPC 端口rpcPort=9090charset=utf-8processors=4processorHandler=16processorKillExecutor=128timerExecutor=8managerExecutor=256serverExecutor=1024idleTimeout=trustedIps=127.0.0.1slowSqlTime=1000maxConnection=20000allowManagerLogin=1allowCrossDbQuery=truegalaxyXProtocol=1# MetaDB 地址metaDbAddr=127.0.0.1:4886# MetaDB 私有协议端口metaDbXprotoPort=32886# MetaDB 用户metaDbUser=my_polarxmetaDbName=polardbx_meta_db_polardbx# PolarDB-X 实例名instanceId=polardbx-polardbx# metaDb 密码，通过查询获得metaDbPasswd=DqQUThAumQ1QSUqiR+HhdaxMKwezoWmXAvyxBgJZxHz9s/ClvIFLoPCeh+zCYDO9 然后使用 IDEA 先启动一次 TddlLauncher 入口类，然后选择 Edit Configurations 修改启动配置，并添加环境变量 dnPasswordKey=asdf1234ghjk5678。 然后选择 IDEA 中的 Preference - Build,Execution,Deployment - Build tools - maven - importing - VM options for importer，并将其设置为 -Xmx2048m -Xms2048m。再将 Preference - Build,Execution,Deployment - Compiler - Build Process - Shared heap size 设置为 4096。 然后我们修改 resources/logback.xml 文件，将日志配置最后的级别改为 DEBUG，并输出日志到 STDOUT，方便我们学习 PolarDB-X 时在控制台观察日志信息。 root level=DEBUG appender-ref ref=STDOUT//root ERR_X_PROTOCOL_CLIENT 异常排查 完成 IDEA 配置调整后，我们尝试启动 TddlLauncher，观察启动日志，可以发现如下的错误信息： 2024-09-02 08:12:44.168 [main] ERROR com.alibaba.polardbx.CobarConfig - [] Failed to init cobar server.com.alibaba.polardbx.common.exception.TddlRuntimeException: ERR-CODE: [PXC-10001][ERR_X_PROTOCOL_CLIENT] XDataSource to my_polarx#1c31783e@127.0.0.1:32886 Failed to init new TCP. XClientPool to my_polarx#1c31783e@127.0.0.1:32886 now 0 TCP(0 aging), 0 sessions(0 running, 0 idle), 0 waiting connection. at com.alibaba.polardbx.rpc.compatible.XDataSource.getConnection(XDataSource.java:210)\tat com.alibaba.polardbx.gms.metadb.MetaDbDataSource.initTsoServicesX(MetaDbDataSource.java:139)\tat com.alibaba.polardbx.gms.metadb.MetaDbDataSource.initXDataSourceByJdbcProps(MetaDbDataSource.java:202)\tat com.alibaba.polardbx.gms.metadb.MetaDbDataSource.initMetaDbDataSource(MetaDbDataSource.java:213)\tat com.alibaba.polardbx.gms.metadb.MetaDbDataSource.doInit(MetaDbDataSource.java:135)\tat com.alibaba.polardbx.common.model.lifecycle.AbstractLifecycle.init(AbstractLifecycle.java:43)\tat com.alibaba.polardbx.gms.metadb.MetaDbDataSource.getInstance(MetaDbDataSource.java:530)\tat com.alibaba.polardbx.gms.util.MetaDbUtil.getConnection(MetaDbUtil.java:53)\tat com.alibaba.polardbx.gms.metadb.schema.SchemaChangeAccessor.create(SchemaChangeAccessor.java:79)\tat com.alibaba.polardbx.gms.metadb.schema.SchemaChangeAccessor.doInit(SchemaChangeAccessor.java:75)\tat com.alibaba.polardbx.common.model.lifecycle.AbstractLifecycle.init(AbstractLifecycle.java:43)\tat com.alibaba.polardbx.gms.metadb.schema.SchemaChangeManager.doInit(SchemaChangeManager.java:62)\tat com.alibaba.polardbx.common.model.lifecycle.AbstractLifecycle.init(AbstractLifecycle.java:43)\tat com.alibaba.polardbx.gms.metadb.schema.SchemaChangeManager.getInstance(SchemaChangeManager.java:55)\tat com.alibaba.polardbx.config.loader.ServerLoader.initPolarDbXComponents(ServerLoader.java:222)\tat com.alibaba.polardbx.config.loader.ServerLoader.load(ServerLoader.java:191)\tat com.alibaba.polardbx.config.loader.ServerLoader.doInit(ServerLoader.java:96)\tat com.alibaba.polardbx.common.model.lifecycle.AbstractLifecycle.init(AbstractLifecycle.java:43)\tat com.alibaba.polardbx.CobarConfig.initCobarConfig(CobarConfig.java:80)\tat com.alibaba.polardbx.CobarConfig.init(CobarConfig.java:61)\tat com.alibaba.polardbx.CobarServer.init(CobarServer.java:154)\tat com.alibaba.polardbx.CobarServer.clinit(CobarServer.java:108)\tat com.alibaba.polardbx.server.TddlLauncher.main(TddlLauncher.java:126)Caused by: com.alibaba.polardbx.common.exception.TddlNestableRuntimeException: Failed to init new TCP. XClientPool to my_polarx#1c31783e@127.0.0.1:32886 now 0 TCP(0 aging), 0 sessions(0 running, 0 idle), 0 waiting connection.\tat com.alibaba.polardbx.rpc.pool.XConnectionManager.getConnection(XConnectionManager.java:514)\tat com.alibaba.polardbx.rpc.compatible.XDataSource.getConnection(XDataSource.java:193)\t... 22 common frames omittedCaused by: com.alibaba.polardbx.common.exception.TddlNestableRuntimeException: Failed to init new TCP.\tat com.alibaba.polardbx.rpc.pool.XClientPool.getConnection(XClientPool.java:491)\tat com.alibaba.polardbx.rpc.pool.XClientPool.getConnection(XClientPool.java:287)\tat com.alibaba.polardbx.rpc.pool.XConnectionManager.getConnection(XConnectionManager.java:502)\t... 23 common frames omittedCaused by: com.alibaba.polardbx.common.exception.TddlRuntimeException: ERR-CODE: [PXC-10001][ERR_X_PROTOCOL_CLIENT] XClientPool to my_polarx#1c31783e@127.0.0.1:32886 connect fail. at com.alibaba.polardbx.rpc.pool.XClientPool.getConnection(XClientPool.java:474)\t... 25 common frames omitted2024-09-02 08:12:44.171 [main] ERROR com.alibaba.polardbx.server.TddlLauncher - [] ## Something goes wrong when starting up the tddl server: java.lang.ExceptionInInitializerError\tat com.alibaba.polardbx.server.TddlLauncher.main(TddlLauncher.java:126)Caused by: ERR-CODE: [PXC-10001][ERR_X_PROTOCOL_CLIENT] XDataSource to my_polarx#1c31783e@127.0.0.1:32886 Failed to init new TCP. XClientPool to my_polarx#1c31783e@127.0.0.1:32886 now 0 TCP(0 aging), 0 sessions(0 running, 0 idle), 0 waiting connection. at com.alibaba.polardbx.rpc.compatible.XDataSource.getConnection(XDataSource.java:210)\tat com.alibaba.polardbx.gms.metadb.MetaDbDataSource.initTsoServicesX(MetaDbDataSource.java:139)\tat com.alibaba.polardbx.gms.metadb.MetaDbDataSource.initXDataSourceByJdbcProps(MetaDbDataSource.java:202)\tat com.alibaba.polardbx.gms.metadb.MetaDbDataSource.initMetaDbDataSource(MetaDbDataSource.java:213)\tat com.alibaba.polardbx.gms.metadb.MetaDbDataSource.doInit(MetaDbDataSource.java:135)\tat com.alibaba.polardbx.common.model.lifecycle.AbstractLifecycle.init(AbstractLifecycle.java:43)\tat com.alibaba.polardbx.gms.metadb.MetaDbDataSource.getInstance(MetaDbDataSource.java:530)\tat com.alibaba.polardbx.gms.util.MetaDbUtil.getConnection(MetaDbUtil.java:53)\tat com.alibaba.polardbx.gms.metadb.schema.SchemaChangeAccessor.create(SchemaChangeAccessor.java:79)\tat com.alibaba.polardbx.gms.metadb.schema.SchemaChangeAccessor.doInit(SchemaChangeAccessor.java:75)\tat com.alibaba.polardbx.common.model.lifecycle.AbstractLifecycle.init(AbstractLifecycle.java:43)\tat com.alibaba.polardbx.gms.metadb.schema.SchemaChangeManager.doInit(SchemaChangeManager.java:62)\tat com.alibaba.polardbx.common.model.lifecycle.AbstractLifecycle.init(AbstractLifecycle.java:43)\tat com.alibaba.polardbx.gms.metadb.schema.SchemaChangeManager.getInstance(SchemaChangeManager.java:55)\tat com.alibaba.polardbx.config.loader.ServerLoader.initPolarDbXComponents(ServerLoader.java:222)\tat com.alibaba.polardbx.config.loader.ServerLoader.load(ServerLoader.java:191)\tat com.alibaba.polardbx.config.loader.ServerLoader.doInit(ServerLoader.java:96)\tat com.alibaba.polardbx.common.model.lifecycle.AbstractLifecycle.init(AbstractLifecycle.java:43)\tat com.alibaba.polardbx.CobarConfig.initCobarConfig(CobarConfig.java:80)\tat com.alibaba.polardbx.CobarConfig.init(CobarConfig.java:61)\tat com.alibaba.polardbx.CobarServer.init(CobarServer.java:154)\tat com.alibaba.polardbx.CobarServer.clinit(CobarServer.java:108)\t... 1 moreCaused by: Failed to init new TCP. XClientPool to my_polarx#1c31783e@127.0.0.1:32886 now 0 TCP(0 aging), 0 sessions(0 running, 0 idle), 0 waiting connection.\tat com.alibaba.polardbx.rpc.pool.XConnectionManager.getConnection(XConnectionManager.java:514)\tat com.alibaba.polardbx.rpc.compatible.XDataSource.getConnection(XDataSource.java:193)\t... 22 moreCaused by: Failed to init new TCP.\tat com.alibaba.polardbx.rpc.pool.XClientPool.getConnection(XClientPool.java:491)\tat com.alibaba.polardbx.rpc.pool.XClientPool.getConnection(XClientPool.java:287)\tat com.alibaba.polardbx.rpc.pool.XConnectionManager.getConnection(XConnectionManager.java:502)\t... 23 moreCaused by: ERR-CODE: [PXC-10001][ERR_X_PROTOCOL_CLIENT] XClientPool to my_polarx#1c31783e@127.0.0.1:32886 connect fail. at com.alibaba.polardbx.rpc.pool.XClientPool.getConnection(XClientPool.java:474)\t... 25 more 从日志信息可以大致看出，是 CN 节点无法连接到 Docker 容器中的 DN 和 GMS 服务，搜索官方 Issue 列表，发现该问题反馈较多，官方同学之前回复的答案是将 server.properties 中的 galaxyXProtocol 设置为 1，具体参考 issues-34，笔者检查了该配置，目前使用的分支已经使用了相同的设置。 最近反馈的 issues-214 同样记录了该问题，目前还没有解决。笔者查看源码，发现 galaxyXProtocol 配置主要用于控制 CN 和 DN/GMS 节点通信的协议，可以配置为 1 和 2，分别对应了 GALAXY_X_PROTOCOL 和 OPEN_XRPC_PROTOCOL 协议。 String galaxyXProtocol = serverProps.getProperty(galaxyXProtocol);if (!StringUtil.isEmpty(galaxyXProtocol)) final int i = Integer.parseInt(galaxyXProtocol); if (1 == i) XConfig.GALAXY_X_PROTOCOL = true; XConfig.OPEN_XRPC_PROTOCOL = false; else if (2 == i) XConfig.GALAXY_X_PROTOCOL = false; XConfig.OPEN_XRPC_PROTOCOL = true; else XConfig.GALAXY_X_PROTOCOL = false; XConfig.OPEN_XRPC_PROTOCOL = false; else XConfig.GALAXY_X_PROTOCOL = false; XConfig.OPEN_XRPC_PROTOCOL = false; 进入容器查看 server.properties 配置，可以发现 galaxyXProtocol 配置为 2，并且私有协议的端口 metaDbXprotoPort 配置为 34886，正常情况下 CN 和 DN/GMS 通信需要保证协议类型以及端口一致，才能正常通信。 为了保证协议类型和协议端口一致，我们需要重新启动一个容器，并将 34886 端口映射到宿主机，执行以下脚本重新启动容器： docker stop some-dn-and-gmsdocker rm some-dn-and-gms# 映射 34886 端口docker run -d --name some-dn-and-gms --env mode=dev -p 4886:4886 -p 34886:34886 polardbx/polardb-x 然后修改项目 resources/server.properties 配置，将 galaxyXProtocol 设置为 2，metaDbXprotoPort 设置为 34886。 # PolarDB-X 服务端口serverPort=8527managerPort=3406# PolarDB-X RPC 端口rpcPort=9090charset=utf-8processors=4processorHandler=16processorKillExecutor=128timerExecutor=8managerExecutor=256serverExecutor=1024idleTimeout=trustedIps=127.0.0.1slowSqlTime=1000maxConnection=20000allowManagerLogin=1allowCrossDbQuery=truegalaxyXProtocol=2# MetaDB 地址metaDbAddr=127.0.0.1:4886# MetaDB 私有协议端口metaDbXprotoPort=34886# MetaDB 用户metaDbUser=my_polarxmetaDbName=polardbx_meta_db_polardbx# PolarDB-X 实例名instanceId=polardbx-polardbx# metaDb 密码，通过查询获得metaDbPasswd=DqQUThAumQ1QSUqiR+HhdaxMKwezoWmXAvyxBgJZxHz9s/ClvIFLoPCeh+zCYDO9 再次启动 TddlLauncher，并观察启动日志，可以发现 ERR_X_PROTOCOL_CLIENT 异常已经解决。 SigarException 异常排查 根据日志显示，TddlLauncher 启动过程中又出现了新的错误，具体异常信息如下： 0 [main] DEBUG Sigar - no libsigar-universal64-macosx.dylib in java.library.pathorg.hyperic.sigar.SigarException: no libsigar-universal64-macosx.dylib in java.library.path\tat org.hyperic.sigar.Sigar.loadLibrary(Sigar.java:172)\tat org.hyperic.sigar.Sigar.clinit(Sigar.java:100)\tat com.alibaba.polardbx.executor.handler.LogicalShowHtcHandler.clinit(LogicalShowHtcHandler.java:48)\tat com.alibaba.polardbx.repo.mysql.handler.CommandHandlerFactoryMyImp.init(CommandHandlerFactoryMyImp.java:397)\tat com.alibaba.polardbx.repo.mysql.spi.MyRepository.doInit(MyRepository.java:62)\tat com.alibaba.polardbx.common.model.lifecycle.AbstractLifecycle.init(AbstractLifecycle.java:43)\tat com.alibaba.polardbx.repo.mysql.spi.RepositoryFactoryMyImp.buildRepository(RepositoryFactoryMyImp.java:34)\tat com.alibaba.polardbx.executor.repo.RepositoryHolder.getOrCreateRepository(RepositoryHolder.java:54)\tat com.alibaba.polardbx.executor.common.TopologyHandler.createOne(TopologyHandler.java:279)\tat com.alibaba.polardbx.matrix.config.MatrixConfigHolder.initGroups(MatrixConfigHolder.java:844)\tat com.alibaba.polardbx.matrix.config.MatrixConfigHolder.doInit(MatrixConfigHolder.java:172)\tat com.alibaba.polardbx.common.model.lifecycle.AbstractLifecycle.init(AbstractLifecycle.java:43)\tat com.alibaba.polardbx.matrix.jdbc.TDataSource.doInit(TDataSource.java:201)\tat com.alibaba.polardbx.common.model.lifecycle.AbstractLifecycle.init(AbstractLifecycle.java:43)\tat com.alibaba.polardbx.matrix.jdbc.utils.TDataSourceInitUtils.initDataSource(TDataSourceInitUtils.java:33)\tat com.alibaba.polardbx.config.loader.AppLoader.loadSchema(AppLoader.java:121)\tat com.alibaba.polardbx.config.loader.GmsAppLoader.loadApp(GmsAppLoader.java:72)\tat com.alibaba.polardbx.config.loader.BaseAppLoader.loadApps(BaseAppLoader.java:90)\tat com.alibaba.polardbx.config.loader.GmsAppLoader.initDbUserPrivsInfo(GmsAppLoader.java:55)\tat com.alibaba.polardbx.config.loader.GmsClusterLoader.initClusterAppInfo(GmsClusterLoader.java:363)\tat com.alibaba.polardbx.config.loader.GmsClusterLoader.loadPolarDbXCluster(GmsClusterLoader.java:234)\tat com.alibaba.polardbx.config.loader.GmsClusterLoader.loadCluster(GmsClusterLoader.java:213)\tat com.alibaba.polardbx.config.loader.GmsClusterLoader.doInit(GmsClusterLoader.java:178)\tat com.alibaba.polardbx.common.model.lifecycle.AbstractLifecycle.init(AbstractLifecycle.java:43)\tat com.alibaba.polardbx.CobarConfig.doInit(CobarConfig.java:145)\tat com.alibaba.polardbx.common.model.lifecycle.AbstractLifecycle.init(AbstractLifecycle.java:43)\tat com.alibaba.polardbx.CobarServer.doInit(CobarServer.java:213)\tat com.alibaba.polardbx.common.model.lifecycle.AbstractLifecycle.init(AbstractLifecycle.java:43)\tat com.alibaba.polardbx.server.TddlLauncher.main(TddlLauncher.java:128) 参考 StackOverflow 上 Hyperic Sigar Mac Osx Error - No Library 讨论，需要下载 Mac 平台对应的动态链接库（hyperic-sigar-1.6.4.tar.gz 下载地址），然后将 libsigar-universal64-macosx.dylib 拷贝至 /Library/Java/Extensions/。由于笔者使用的是最新的 Mac M3 版本，目前官方并未提供 ARM 架构的 sigar 动态链接库，因此只好参考 CentOS 开发环境搭建笔记，在虚拟机中安装 CentOS 7，然后通过 IDEA 远程执行功能进行启动。 使用虚拟机启动时，同样会出现 SigarException，会提示在 java.library.path 中没有 libsigar-aarch64-linux.so，我们可以从 libsigar-aarch64-linux.so 下载 Linux 平台下的动态链接库，并将该文件复制到 java.library.path 对应的路径中。java.library.path 对应的具体路径，我们可以从 TddlLauncher 启动日志中获得，日志中输出的配置为 java.library.path=/usr/java/packages/lib/aarch64:/lib:/usr/lib，因此可以执行以下脚本，将动态链接库文件复制到对应目录中。 # java.library.path=/usr/java/packages/lib/aarch64:/lib:/usr/lib# 创建 java.library.path 目录mkdir -p /usr/java/packages/lib/aarch64/# 复制 libsigar-aarch64-linux.socp ~/libsigar-aarch64-linux.so /usr/java/packages/lib/aarch64/ 然后重启 TddlLauncher，可以发现 SigarException 已经解决，我们使用 mysql -h172.16.16.128 -P8527 -upolardbx_root -p123456 -A 访问虚拟机上的 PolarDB-X 服务，可以正常使用执行 SQL 语句。 AssertionError: bad type null 异常排查 执行 SELECT TABLE_SCHEMA, TABLE_NAME, TABLE_TYPE FROM information_schema.TABLES 语句查询系统表时，出现了如下的 AssertionError: bad type null 异常，从堆栈信息看是在优化器部分出现了报错。 Caused by: java.lang.AssertionError: bad type null\tat org.apache.calcite.plan.volcano.Dumpers.provenanceRecurse(Dumpers.java:116)\tat org.apache.calcite.plan.volcano.Dumpers.provenance(Dumpers.java:79)\tat org.apache.calcite.plan.volcano.VolcanoPlanner.findBestExp(VolcanoPlanner.java:537)\tat com.alibaba.polardbx.optimizer.core.planner.Planner.getCheapestFractionalPlan(Planner.java:1412)\tat com.alibaba.polardbx.optimizer.core.planner.Planner.optimizeByCBO(Planner.java:1324)\tat com.alibaba.polardbx.optimizer.core.planner.Planner.optimizeByPlanEnumerator(Planner.java:1239)\tat com.alibaba.polardbx.optimizer.core.planner.Planner.sqlRewriteAndPlanEnumerate(Planner.java:1186)\tat com.alibaba.polardbx.optimizer.core.planner.Planner.optimize(Planner.java:1160)\tat com.alibaba.polardbx.optimizer.core.planner.Planner.getPlan(Planner.java:945)\tat com.alibaba.polardbx.optimizer.core.planner.Planner.doBuildPlan(Planner.java:474)\tat com.alibaba.polardbx.optimizer.core.planner.Planner.doBuildPlan(Planner.java:510)\tat com.alibaba.polardbx.optimizer.core.planner.Planner.doPlan(Planner.java:2699)\tat com.alibaba.polardbx.optimizer.core.planner.Planner.plan(Planner.java:375)\tat com.alibaba.polardbx.optimizer.core.planner.Planner.plan(Planner.java:347)\tat com.alibaba.polardbx.optimizer.core.planner.Planner.planAfterProcessing(Planner.java:342)\tat com.alibaba.polardbx.optimizer.core.planner.Planner.plan(Planner.java:311)\tat com.alibaba.polardbx.optimizer.core.planner.Planner.plan(Planner.java:278)\tat com.alibaba.polardbx.repo.mysql.handler.LogicalShowTablesMyHandler.handle(LogicalShowTablesMyHandler.java:95)\tat com.alibaba.polardbx.executor.handler.HandlerCommon.handlePlan(HandlerCommon.java:153)\tat com.alibaba.polardbx.executor.AbstractGroupExecutor.executeInner(AbstractGroupExecutor.java:74)\tat com.alibaba.polardbx.executor.AbstractGroupExecutor.execByExecPlanNode(AbstractGroupExecutor.java:52)\tat com.alibaba.polardbx.executor.TopologyExecutor.execByExecPlanNode(TopologyExecutor.java:50)\tat com.alibaba.polardbx.transaction.TransactionExecutor.execByExecPlanNode(TransactionExecutor.java:141)\tat com.alibaba.polardbx.executor.ExecutorHelper.executeByCursor(ExecutorHelper.java:170)\tat com.alibaba.polardbx.executor.ExecutorHelper.execute(ExecutorHelper.java:92)\tat com.alibaba.polardbx.executor.ExecutorHelper.execute(ExecutorHelper.java:84)\tat com.alibaba.polardbx.executor.PlanExecutor.execByExecPlanNodeByOne(PlanExecutor.java:223)\tat com.alibaba.polardbx.executor.PlanExecutor.execute(PlanExecutor.java:91)\tat com.alibaba.polardbx.matrix.jdbc.TConnection.executeQuery(TConnection.java:737)\tat com.alibaba.polardbx.matrix.jdbc.TConnection.executeSQL(TConnection.java:510)\t... 19 common frames omitted 尝试咨询了下 PolarDB-X 社区大佬，反馈日志级别 Debug 会进入优化器更为严格的类型检查，会出现类型一致性的情况，可以通过调整日志级别来解决这个问题。 我们修改 polardbx-server 模块下的 logback.xml 文件，将 DEBUG 级别为 INFO 级别，然后重启 TddlLauncher 并再次执行系统表查询 SQL。 root level=INFO appender-ref ref=STDOUT//root 此时 SELECT TABLE_SCHEMA, TABLE_NAME, TABLE_TYPE FROM information_schema.TABLES LIMIT 5; 语句可以正常查询出结果。 mysql SELECT TABLE_SCHEMA, TABLE_NAME, TABLE_TYPE FROM information_schema.TABLES LIMIT 5;+--------------------+----------------------------+-------------+| TABLE_SCHEMA | TABLE_NAME | TABLE_TYPE |+--------------------+----------------------------+-------------+| __cdc__ | __cdc_ddl_record__ | BASE TABLE || __cdc__ | __cdc_instruction__ | BASE TABLE || information_schema | INFORMATION_SCHEMA_TABLES | SYSTEM VIEW || information_schema | INFORMATION_SCHEMA_COLUMNS | SYSTEM VIEW || information_schema | SCHEDULE_JOBS | SYSTEM VIEW |+--------------------+----------------------------+-------------+5 rows in set (0.35 sec) PolarDB-X 入门使用 Debug 创建分片表 前文我们成功地启动了 PolarDB-X 服务，并通过 mysql -h172.16.16.128 -P8527 -upolardbx_root -p123456 -A 成功连接上数据库。下面我们参考 CREATE DATABASE 和 CREATE TABLE 文档，创建测试数据库以及多张分片表，进行简单地功能测试： -- 指定分区模式为 ShardingCREATE DATABASE sharding_db PARTITION_MODE=sharding DEFAULT CHARACTER SET UTF8;USE sharding_db;-- 创建 3 张不同维度的分片表CREATE TABLE `sbtest_sharding_id` ( `id` int(11) NOT NULL AUTO_INCREMENT, `k` int(11) NOT NULL DEFAULT 0, `c` char(120) NOT NULL DEFAULT , `pad` char(60) NOT NULL DEFAULT , PRIMARY KEY (`id`)) DBPARTITION BY HASH(id); CREATE TABLE `sbtest_sharding_k` ( `id` int(11) NOT NULL AUTO_INCREMENT, `k` int(11) NOT NULL DEFAULT 0, `c` char(120) NOT NULL DEFAULT , `pad` char(60) NOT NULL DEFAULT , PRIMARY KEY (`id`)) DBPARTITION BY HASH(k);CREATE TABLE `sbtest_sharding_c` ( `id` int(11) NOT NULL AUTO_INCREMENT, `k` int(11) NOT NULL DEFAULT 0, `c` char(120) NOT NULL DEFAULT , `pad` char(60) NOT NULL DEFAULT , PRIMARY KEY (`id`)) DBPARTITION BY HASH(c); 通过 SHOW TOPOLOGY FROM sbtest_sharding_id; 语句可以查看分片表的拓扑结构，因为我们指定的是 DBPARTITION，即分库模式，默认会将分片表拆分到 8 个物理库中存储： mysql SHOW TOPOLOGY FROM sbtest_sharding_id;+------+--------------------------+-------------------------+----------------+-------------------+--------------------+---------------+| ID | GROUP_NAME | TABLE_NAME | PARTITION_NAME | SUBPARTITION_NAME | PHY_DB_NAME | DN_ID |+------+--------------------------+-------------------------+----------------+-------------------+--------------------+---------------+| 0 | SHARDING_DB_000000_GROUP | sbtest_sharding_id_85GE | | | sharding_db_000000 | polardbx_dn_0 || 1 | SHARDING_DB_000001_GROUP | sbtest_sharding_id_85GE | | | sharding_db_000001 | polardbx_dn_0 || 2 | SHARDING_DB_000002_GROUP | sbtest_sharding_id_85GE | | | sharding_db_000002 | polardbx_dn_0 || 3 | SHARDING_DB_000003_GROUP | sbtest_sharding_id_85GE | | | sharding_db_000003 | polardbx_dn_0 || 4 | SHARDING_DB_000004_GROUP | sbtest_sharding_id_85GE | | | sharding_db_000004 | polardbx_dn_0 || 5 | SHARDING_DB_000005_GROUP | sbtest_sharding_id_85GE | | | sharding_db_000005 | polardbx_dn_0 || 6 | SHARDING_DB_000006_GROUP | sbtest_sharding_id_85GE | | | sharding_db_000006 | polardbx_dn_0 || 7 | SHARDING_DB_000007_GROUP | sbtest_sharding_id_85GE | | | sharding_db_000007 | polardbx_dn_0 |+------+--------------------------+-------------------------+----------------+-------------------+--------------------+---------------+8 rows in set (0.06 sec)mysql SHOW TOPOLOGY FROM sbtest_sharding_k;+------+--------------------------+------------------------+----------------+-------------------+--------------------+---------------+| ID | GROUP_NAME | TABLE_NAME | PARTITION_NAME | SUBPARTITION_NAME | PHY_DB_NAME | DN_ID |+------+--------------------------+------------------------+----------------+-------------------+--------------------+---------------+| 0 | SHARDING_DB_000000_GROUP | sbtest_sharding_k_Irx4 | | | sharding_db_000000 | polardbx_dn_0 || 1 | SHARDING_DB_000001_GROUP | sbtest_sharding_k_Irx4 | | | sharding_db_000001 | polardbx_dn_0 || 2 | SHARDING_DB_000002_GROUP | sbtest_sharding_k_Irx4 | | | sharding_db_000002 | polardbx_dn_0 || 3 | SHARDING_DB_000003_GROUP | sbtest_sharding_k_Irx4 | | | sharding_db_000003 | polardbx_dn_0 || 4 | SHARDING_DB_000004_GROUP | sbtest_sharding_k_Irx4 | | | sharding_db_000004 | polardbx_dn_0 || 5 | SHARDING_DB_000005_GROUP | sbtest_sharding_k_Irx4 | | | sharding_db_000005 | polardbx_dn_0 || 6 | SHARDING_DB_000006_GROUP | sbtest_sharding_k_Irx4 | | | sharding_db_000006 | polardbx_dn_0 || 7 | SHARDING_DB_000007_GROUP | sbtest_sharding_k_Irx4 | | | sharding_db_000007 | polardbx_dn_0 |+------+--------------------------+------------------------+----------------+-------------------+--------------------+---------------+8 rows in set (0.01 sec)mysql SHOW TOPOLOGY FROM sbtest_sharding_c;+------+--------------------------+------------------------+----------------+-------------------+--------------------+---------------+| ID | GROUP_NAME | TABLE_NAME | PARTITION_NAME | SUBPARTITION_NAME | PHY_DB_NAME | DN_ID |+------+--------------------------+------------------------+----------------+-------------------+--------------------+---------------+| 0 | SHARDING_DB_000000_GROUP | sbtest_sharding_c_nfaT | | | sharding_db_000000 | polardbx_dn_0 || 1 | SHARDING_DB_000001_GROUP | sbtest_sharding_c_nfaT | | | sharding_db_000001 | polardbx_dn_0 || 2 | SHARDING_DB_000002_GROUP | sbtest_sharding_c_nfaT | | | sharding_db_000002 | polardbx_dn_0 || 3 | SHARDING_DB_000003_GROUP | sbtest_sharding_c_nfaT | | | sharding_db_000003 | polardbx_dn_0 || 4 | SHARDING_DB_000004_GROUP | sbtest_sharding_c_nfaT | | | sharding_db_000004 | polardbx_dn_0 || 5 | SHARDING_DB_000005_GROUP | sbtest_sharding_c_nfaT | | | sharding_db_000005 | polardbx_dn_0 || 6 | SHARDING_DB_000006_GROUP | sbtest_sharding_c_nfaT | | | sharding_db_000006 | polardbx_dn_0 || 7 | SHARDING_DB_000007_GROUP | sbtest_sharding_c_nfaT | | | sharding_db_000007 | polardbx_dn_0 |+------+--------------------------+------------------------+----------------+-------------------+--------------------+---------------+8 rows in set (0.02 sec) 初始化数据 创建好分片表后，我们再使用 sysbench 工具向 sbtest1 表插入 10w 条数据，执行如下脚本初始化数据： sysbench /opt/homebrew/Cellar/sysbench/1.0.20_6/share/sysbench/oltp_read_write.lua --tables=1 --table_size=100000 --mysql-user=polardbx_root --mysql-password=123456 --mysql-host=172.16.16.128 --mysql-port=8527 --mysql-db=sharding_db prepare 然后可以执行 INSERT ... SELECT ... 语句，将 sbtest1 中的数据复制到 sbtest_sharding_id、sbtest_sharding_k 和 sbtest_sharding_c 中。 INSERT INTO sbtest_sharding_id SELECT * FROM sbtest1;INSERT INTO sbtest_sharding_k SELECT * FROM sbtest1;INSERT INTO sbtest_sharding_c SELECT * FROM sbtest1; 使用 SELECT COUNT(1) 检查各个表的数据量，都是 10w 条记录，符合我们的预期。 mysql SELECT COUNT(1) FROM sbtest_sharding_id;+----------+| COUNT(1) |+----------+| 100000 |+----------+mysql SELECT COUNT(1) FROM sbtest_sharding_k;+----------+| COUNT(1) |+----------+| 100000 |+----------+mysql SELECT COUNT(1) FROM sbtest_sharding_c;+----------+| COUNT(1) |+----------+| 100000 |+----------+ 调用流程初探 初始化数据完成后，我们执行如下的关联查询语句，使用 k 字段关联 sbtest_sharding_id 和 sbtest_sharding_k 表，并通过 LIMIT 10 获取前 10 条记录，可以看到 PolarDB-X 查询的效率还是挺高的。 mysql SELECT sid.* FROM sbtest_sharding_id sid INNER JOIN sbtest_sharding_k sk ON sid.k = sk.k LIMIT 10;+----+-------+-------------------------------------------------------------------------------------------------------------------------+-------------------------------------------------------------+| id | k | c | pad |+----+-------+-------------------------------------------------------------------------------------------------------------------------+-------------------------------------------------------------+| 5 | 49982 | 44257470806-17967007152-32809666989-26174672567-29883439075-95767161284-94957565003-35708767253-53935174705-16168070783 | 34551750492-67990399350-81179284955-79299808058-21257255869 || 5 | 49982 | 44257470806-17967007152-32809666989-26174672567-29883439075-95767161284-94957565003-35708767253-53935174705-16168070783 | 34551750492-67990399350-81179284955-79299808058-21257255869 || 5 | 49982 | 44257470806-17967007152-32809666989-26174672567-29883439075-95767161284-94957565003-35708767253-53935174705-16168070783 | 34551750492-67990399350-81179284955-79299808058-21257255869 || 5 | 49982 | 44257470806-17967007152-32809666989-26174672567-29883439075-95767161284-94957565003-35708767253-53935174705-16168070783 | 34551750492-67990399350-81179284955-79299808058-21257255869 || 5 | 49982 | 44257470806-17967007152-32809666989-26174672567-29883439075-95767161284-94957565003-35708767253-53935174705-16168070783 | 34551750492-67990399350-81179284955-79299808058-21257255869 || 5 | 49982 | 44257470806-17967007152-32809666989-26174672567-29883439075-95767161284-94957565003-35708767253-53935174705-16168070783 | 34551750492-67990399350-81179284955-79299808058-21257255869 || 5 | 49982 | 44257470806-17967007152-32809666989-26174672567-29883439075-95767161284-94957565003-35708767253-53935174705-16168070783 | 34551750492-67990399350-81179284955-79299808058-21257255869 || 5 | 49982 | 44257470806-17967007152-32809666989-26174672567-29883439075-95767161284-94957565003-35708767253-53935174705-16168070783 | 34551750492-67990399350-81179284955-79299808058-21257255869 || 5 | 49982 | 44257470806-17967007152-32809666989-26174672567-29883439075-95767161284-94957565003-35708767253-53935174705-16168070783 | 34551750492-67990399350-81179284955-79299808058-21257255869 || 5 | 49982 | 44257470806-17967007152-32809666989-26174672567-29883439075-95767161284-94957565003-35708767253-53935174705-16168070783 | 34551750492-67990399350-81179284955-79299808058-21257255869 |+----+-------+-------------------------------------------------------------------------------------------------------------------------+-------------------------------------------------------------+10 rows in set (0.19 sec) 通过 EXPLAIN 语句可以查看该语句的执行计划，LogicalView 代表了对分片表的访问，括号内表示访问了分片表的哪些物理节点，并显示了对应的下推 SQL。Gather 表示对分片表底层的真实表进行了汇总处理，内部访问时采用了并发执行的方式。HashJoin 表示该语句物理执行时使用了 Hash Join 算法，关联的类型是 inner，关联字段是 k。Project 代表了投影列，我们逻辑 SQL 中指定的是 sid.*，PolarDB-X 执行时将 sid.* 展开为具体的列。Limit 则表示的是逻辑 SQL 中的 LIMIT 10，可以看出 PolarDB-X 采用了参数化 Plan 的方式，将字面量 10 转换为了参数形式。 mysql EXPLAIN SELECT sid.* FROM sbtest_sharding_id sid INNER JOIN sbtest_sharding_k sk ON sid.k = sk.k LIMIT 10;+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------+| LOGICAL EXECUTIONPLAN |+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------+| Limit(offset=0, fetch=?0) || Project(id=id, k=k, c=c, pad=pad) || HashJoin(condition=k = k, type=inner) || Gather(concurrent=true) || LogicalView(tables=[000000-000007].sbtest_sharding_id_85GE, shardCount=8, sql=SELECT `id`, `k`, `c`, `pad` FROM `sbtest_sharding_id` AS `sbtest_sharding_id`) || Gather(concurrent=true) || LogicalView(tables=[000000-000007].sbtest_sharding_k_Irx4, shardCount=8, sql=SELECT `k` FROM `sbtest_sharding_k` AS `sbtest_sharding_k`) || HitCache:true || Source:SPM_ACCEPT || BaselineInfo Id: 1669894141 || PlanInfo Id: 1700287147 || TemplateId: 63888ffd |+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------+12 rows in set (0.13 sec) 简单体验了下 PolarDB-X 查询功能后，我们来看看基本的调用流程，前面展示的查询语句，首先会经过 FrontendCommandHandler 处理，该类会根据 MySQL 命令类型，将 SQL 请求分发到不同的 FrontendConnection 实现类中，此处 source 变量对应的子类为 ServerConnection，source.query(data) 方法则会调用抽象父类 FrontendConnection#query 的方法。 /** * @param data requests on one connection must be processed sequentially */@Overridesynchronized public void handle(byte[] data) source.buildMDC(); source.setPacketId((byte) (0 0xff)); source.setLastActiveTime(System.nanoTime()); source.setSqlBeginTimestamp(System.currentTimeMillis()); switch (data[4]) case Commands.COM_INIT_DB: commandCount.doInitDB(); source.initDB(data); break; case Commands.COM_PING: commandCount.doPing(); source.ping(); break; case Commands.COM_QUIT: commandCount.doQuit(); source.close(); break; case Commands.COM_QUERY: commandCount.doQuery(); // 调用父类 FrontendConnection#query 方法 source.query(data); break; ... 而 FrontendConnection#query 方法则会继续调用 queryHandler.queryRaw 方法，queryRaw 方法会根据 SQL 语句的类型，调用不同的 Handler 进行处理，此处调用的是 SelectHandler.handle(sql, c, rs 8, hasMore) 处理。 // 执行查询if (queryHandler != null) queryHandler.queryRaw(mm.bytes(), mm.position(), mm.length() - mm.position(), cs); else writeErrMessage(ErrorCode.ER_YES, Empty QueryHandler); 内部逻辑会调用到 TConnection#executeQuery 方法，该方法是 SQL 执行的核心执行逻辑，包含了执行计划生成、执行计划执行两个重要步骤。逻辑中通过 Planner.getInstance().plan(sql, executionContext) 方法获取执行计划，内部会使用 Calcite 框架，基于 RBO 和 CBO 的优化规则，以及统计信息等计算代价，并生成最优的执行计划。由于执行计划生成相对耗时，因此 PolarDB-X 内部增加了执行计划缓存，用来提升查询的性能。 最优执行计划生成后，会交由执行器负责执行，执行器会根据执行计划中的不同算子，选择不同的执行器，例如：HashJoin 算子会选择 ParallelHashJoinExec 执行器，以并行的方式完成 Hash Join 的运算逻辑。最终，整个执行计划会由一个个不同的执行器进行计算，然后生成最终的结果集 ResultCursor。执行完成后，还会调用 updateTableStatistic 方法更新统计信息，从而保证后续执行计划能够生成最优 Plan。 /** * Separate execute(sql, ec) into two parts: plan and execute. If its * writing into broadcast table and has no transaction, a new transaction * will be open. */private ResultCursor executeQuery(ByteString sql, ExecutionContext executionContext, AtomicBoolean trxPolicyModified) ... // 获取元数据快照 final long[] metaVersions = MdlContext.snapshotMetaVersions(); final Parameters originParams = executionContext.getParams().clone(); // 获取执行计划 ExecutionPlan plan = Planner.getInstance().plan(sql, executionContext); ... // 如果元数据变更则重建执行计划 if (trxPolicyModified != null) trxPolicyModified.set(updateTransactionAndConcurrentPolicy(plan, executionContext)); if (PlanManagerUtil.canOptByForcePrimary(plan, executionContext) executionContext.isTsoTransaction()) // If this plan can be optimized, rebuild plan. plan = rebuildPlan(sql, executionContext, originParams, false); ... // 调用执行器执行 ResultCursor resultCursor = executor.execute(plan, executionContext); // 更新表统计信息 updateTableStatistic(plan, resultCursor, executionContext); return resultCursor; 由于执行计划生成、执行计划执行内部的逻辑非常复杂，本文就不过多介绍，感兴趣的朋友可以自行探索。后续的文章，我们会逐步探究 PolarDB-X 的实现细节，学习 PolarDB-X 实践 Calcite 框架的宝贵经验，并将这些优化经验用于实际的项目中。 结语 本文介绍了 PolarDB-X 开发环境搭建的基本步骤，以及在这个过程中遇到的问题及其解决方案，希望能够帮助大家快速上手 PolarDB-X 进行源码学习。搭建完成后简单介绍了 PolarDB-X 中分片表的创建和使用，借助 sysbench 工具，我们初始化了 3 张 10w 数据量的分片表，并展示了不同维度分片表之间的关联查询。 最后，以关联查询 SQL 为例，简单探索了下 PolarDB-X 的调用流程，流程中最核心的逻辑是：执行计划生成（优化器）和执行计划执行（执行器）。优化器和执行器的实现逻辑非常复杂，本文没有展开介绍，我们会在后续的文章中继续分享，欢迎感兴趣的朋友持续关注。 写在最后 笔者因为工作原因接触到 Calcite，前期学习过程中，深感 Calcite 学习资料之匮乏，因此创建了 Calcite 从入门到精通知识星球，希望能够将学习过程中的资料和经验沉淀下来，为更多想要学习 Calcite 的朋友提供一些帮助。 欢迎关注 欢迎关注「端小强的博客」微信公众号，会不定期分享日常学习和工作经验，欢迎大家关注交流。","tags":["PolarDB-X"],"categories":["PolarDB-X"]},{"title":"Java AOT 编译框架 GraalVM 快速入门","path":"/blog/java-aot-compiler-framework-graalvm-quick-start.html","content":"GraalVM 诞生的背景 过去 20 多年，Java 通过语言层虚拟化，实现了平台无关、架构中立等特性，彻底屏蔽了不同操作系统、不同指令集架构带来的差异，因而 Java 语言也取得了巨大的成功。但随着云原生时代的到来，面对相同的问题，云原生选择了操作系统层虚拟化方案——通过容器实现不可变基础设施，将程序连同它的运行环境一起封装到镜像里，这种方案越来越成为一种主流的应用程序分发方式。 云原生的兴起，同时也促进了微服务、Serverless 等技术发展，它们对镜像体积、内存消耗、启动速度，以及达到最高性能的时间等方面提出了新的要求，而这些却是 Java 语言所不擅长的领域。由于 Java 基于 JVM 虚拟机运行，哪怕再小的程序都需要带着完整的虚拟机和标准类库，使得镜像拉取和容器创建的效率降低。此外，Java 语言还存在基础内存开销和冷启动等问题，这些问题限制了 Java 语言在云原生环境下的应用。 为了紧跟云原生时代的发展，应对来自 Go、Rust 等原生语言的挑战，Java 社区提出了 GraalVM 方案，即通过 GraalVM 中的 AOT 提前编译技术，将 Java 生成的字节码文件，提前编译生成二进制的机器码，从而规避冷启动以及依赖 JVM 运行带来的诸多问题。 初识 GraalVM AOT 编译 GraalVM 简介 GraalVM 是一个高性能、支持多种编程语言的执行环境。它既可以在传统的 OpenJDK 上运行，也可以通过 AOT（Ahead-Of-Time）编译成可执行文件单独运行，甚至可以集成到数据库中运行。除此之外，它还移除了编程语言之间的边界，并且支持通过即时编译技术，将混杂了不同编程语言的代码编译到同一段二进制中，从而实现不同语言之间的无缝切换。 上图展示了 GraalVM 作为开放生态系统的体系结构，上层虚拟化层展示了 GraalVM 支持的编程语言，包括了：基于 JVM 的 Java、Kotlin、Scala，动态语言 JavaScript、Ruby、R、Python，以及基于 LLVM 的 C、C++ 和 Rust。这些语言经过 GraalVM 编译，可以运行在 OpenJDK、NodeJs 及 Oracle 数据库中，也可以作为二进制程序独立运行。 可以看到 GraalVM 对多语言及多运行环境提供了强大的支持，本文先重点关注 GraalVM AOT 技术在 Java 语言上的应用，其他相关的 GraalVM 主题我们在后续的文章中再做探讨。 AOT 编译简介 那么什么是 AOT 技术呢？熟悉 C、C++ 语言的同学对这个概念一定不陌生，所谓 AOT 编译，是指在程序运行之前，便将程序源码或字节码转换为机器码的过程，编译完成后的输出的是可执行文件或者动态链接库文件。和 AOT 编译相对的是 JIT 编译，JIT 编译是指在程序运行过程中，将字节码编译为可在硬件上直接执行的机器码，在 Java 语言中，HotSpot 虚拟机会将热点代码通过 JIT 编译器编译为机器码，从而提升程序的执行性能。 下图展示了 AOT 和 JIT 编译的运行流程，他们都需要通过 javac 编译器将 Java 源码编译为 Java 字节码。AOT 和 JIT 编译的差异主要体现在获得字节码后如何执行，传统的 Java 程序会运行在 JVM 之上，JVM 会通过解释执行的方式执行字节码逻辑，如果执行的代码是热点代码，则会通过 JIT 即时编译器，将字节码编译为二进制代码。AOT 编译则脱离了 JVM 运行时环境，直接通过 GraalVM 编译器将 Java 字节码编译为二进制文件，然后直接在操作系统上运行。 AOT 编译最突出的特点是脱离了 JVM 运行时环境，直接编译生成二进制可执行文件，无需在程序运行过程中耗费 CPU 资源来进行即时编译，程序能够在启动后立刻达到期望的最佳性能。下图展示了 Quarkus 框架使用 GraalVM AOT 性能对比，可以看到使用 AOT 编译后的程序消耗内存更少，并且启动后首次执行的耗时大幅度缩短。 AOT 编译的优势和劣势 混迹于技术圈的朋友，可能都听过这句话——没有银弹（西方传说中，使用银子弹可以杀死吸血鬼、狼人或怪兽，银弹引申为解决问题的有效方法）。对于 AOT 编译技术也是同样，它在带来低内存消耗、快速启动等优势的同时，同样也会带来一些问题，下面我们就来探讨下 AOT 编译的优势和劣势。 上图展示了 AOT 编译和传统的 JIT 编译的对比，相比 JIT 编译，AOT 编译具有如下的一些优势： 启动速度更快：AOT 编译直接生成了平台相关的机器码，无需再使用解释执行和 JIT 编译的方式，避免了解释执行低效，也避免了 JIT 编译带来的 CPU 开销。AOT 编译还解决了传统 Java 执行模型中无法充分预热，始终存在解释执行的问题，因此可以大幅度提升启动速度，并稳定保持较好的性能； 占用内存更少：由于 JVM 虚拟机运行需要占用内存，Java 语言实现动态特性也需要额外消耗内存，AOT 编译后程序无需运行在 JVM 虚拟机上，同时也去除了 Java 语言的动态特性，因此程序内存消耗会更少； 更小的应用包：AOT 编译后的程序无需依赖 JVM 虚拟机，因此打包的应用会更小，可以很方便地进行分发。 虽然 AOT 编译有如上众多的优点，但是鱼和熊掌不可兼得，由于 AOT 编译采用了静态执行的方式，不可避免地会带来如下的问题： 峰值吞吐量降低：由于 AOT 编译在编译器需要将全部的代码转换为机器码，因此 AOT 编译无法像 JVM 一样，在程序运行时动态地获取指标，来指导编译器编译出性能更优的机器码，因此峰值吞吐量相比于 JVM 会有所下降（最新的 Oracle GraalVM 版本提供了 Profile-Guided Optimizations 用来指导 AOT 编译，取得了不错的效果，更多信息可以参考 Optimizing Performance with GraalVM 及 A New GraalVM Release and New Free License）； 最大延迟增大：AOT 编译目前仅支持常规的 STOPCOPY 垃圾收集器，因此最大延迟相比 JVM 会增加（最新的 Oracle GraalVM 版本提供了 G1 垃圾回收器，最大延迟相比 GraalVM CE 会更小）； 封闭性：AOT 编译的基本原则是封闭性假设，即程序在编译期必须掌握运行时所需的所有信息，在运行时不能出现任何编译器未知的内容。这会导致 Java 程序中的很多动态特性无法继续使用，例如：反射、动态类加载、动态代理、JCA 加密机制（内部依赖了反射）、JNI、序列化等，如果程序中包含了这些动态特性，则需要通过额外的适配工作进行处理； 平台相关性：AOT 静态编译后程序由原来的平台无关性，变为平台相关性，用户需要考虑部署的平台架构，然后编译出不同的二进制程序； 生态变化：传统面向 Java 程序的调试、监控、Agent 等技术不再适用，因为运行的程序由 Java 程序变成了本地程序，用户需要使用 GDB 才能调试本地程序。可以说，AOT 编译除了源码仍然是 Java 外，其他的生态完全不同了，这些会成为 AOT 编译推广的阻力。 (adsbygoogle = window.adsbygoogle || []).push({}); 赞助商 GraalVM AOT 编译实战 安装 GraalVM SDK GraalVM 官方文档 Getting Started 页面提供了主流 Linux、Mac 和 Windows 平台安装 GraalVM 的教程，大家可以按照教程进行安装。由于笔者更喜欢 SDKMAN 这个工具，因此本小节会介绍下 SDKMAN 安装 GraalVM SDK 的步骤，大家可以按照自己熟悉的方式进行安装。 首先，我们需要安装 SDKMAN，参考官方文档 Installation，执行以下脚本即可完成安装，安装完成后使用 sdk version 查看版本信息。 curl -s https://get.sdkman.io | bashsource $HOME/.sdkman/bin/sdkman-init.shsdk version 然后我们执行 sdk list java 查看 Java SDK 可用版本： ================================================================================Available Java Versions for macOS ARM 64bit================================================================================ Vendor | Use | Version | Dist | Status | Identifier-------------------------------------------------------------------------------- GraalVM Oracle| | 24.ea.8 | graal | | 24.ea.8-graal | | 24.ea.7 | graal | installed | 24.ea.7-graal | | 23.ea.22 | graal | | 23.ea.22-graal | | 23.ea.21 | graal | | 23.ea.21-graal | | 22.0.2 | graal | installed | 22.0.2-graal | | 21.0.4 | graal | | 21.0.4-graal================================================================================Omit Identifier to install default version 21.0.4-tem: $ sdk install javaUse TAB completion to discover available versions $ sdk install java [TAB]Or install a specific version by Identifier: $ sdk install java 21.0.4-temHit Q to exit this list view================================================================================ 可以选择最新稳定版 22.0.2-graal 进行安装，执行 sdk install java 22.0.2-graal，安装完成后可以通过 sdk default java 22.0.2-graal 设置当前默认 JDK 版本，然后执行 java -version 查看 JDK 版本号。 java version 22.0.2 2024-07-16Java(TM) SE Runtime Environment Oracle GraalVM 22.0.2+9.1 (build 22.0.2+9-jvmci-b01)Java HotSpot(TM) 64-Bit Server VM Oracle GraalVM 22.0.2+9.1 (build 22.0.2+9-jvmci-b01, mixed mode, sharing) 安装完 GraalVM SDK 后，我们还需要安装 AOT 静态编译所需的本地工具链，例如：C 库的头文件、glibc-devel、zlib、gcc 和 libstdc++-static。本地工具链安装脚本如下： # MacOSxcode-select --install# CentOSsudo yum install gcc glibc-devel zlib-devel# Ubuntusudo apt-get install build-essential zlib1g-dev 更多关于本地工具链的安装步骤，请大家参考 GraalVM 官方文档先决条件。 GraalVM HelloWorld 学习一门新语言，通常都是从 HelloWorld 开始，我们也尝试使用 GraalVM 编译一个 Java 的 HelloWorld 程序，输出 Hello World! GraalVM!。 public final class HelloWorld public static void main(String[] args) System.out.println(Hello World! GraalVM!); 我们按照前文介绍的步骤，先使用 javac HelloWorld.java 命令将源码编译为字节码，再使用 GraalVM 编译器将字节码编译为本地代码，GraalVM 编译器的命令为 native-image，执行如下的命令可以编译出本地代码。native-image 命令参数较多，大家可以使用 native-image --help 查看完整列表进行探究。 # 编译生成 HelloWorldnative-image HelloWorld 上图展示了 GraalVM 编译过程，编译后会默认会生成 class 文件名对应的小写二进制文件，我们也可以使用 -o 参数指定二进制文件名，执行 ./helloworld 可以输出字符串。 # 执行 Native Image./helloworldHello World! GraalVM! 我们使用 time 命令对比 Java 执行方式和 Native 执行方式，可以明显看到不论是执行时间，还是 CPU 使用率，Native 执行方式都更有优势。 time java HelloWorldHello World! GraalVM!java HelloWorld 0.04s user 0.04s system 60% cpu 0.136 totaltime ./helloworldHello World! GraalVM!./helloworld 0.00s user 0.01s system 46% cpu 0.032 total 使用 Maven 构建 Native Image 在实际工作中，我们通常会使用 Maven 或 Gradle 工具，来构建 Native Image。本文将以 Maven 工具为例，为大家介绍下实际项目中如何构建一个 Native Image 可执行文件，Gradle 工具的使用大家可以参考官方文档 Native-Image#Gradle。 我们使用 IDEA 工具创建 Maven 项目，首先创建一个名为 graalvm-lecture 的 Empty Project，然后再其下创建子模块 hello-world， Archetype 使用 quickstart，它可以创建出一个包含 Hello World! Demo 的 Maven 项目。 然后在 pom.xml 文件中添加 maven-compiler-plugin 和 maven-jar-plugin 插件，用于编译 Java 源码，并将源码打包为可执行 jar 文件。 build plugins plugin groupIdorg.apache.maven.plugins/groupId artifactIdmaven-compiler-plugin/artifactId version3.12.1/version configuration forktrue/fork /configuration /plugin plugin groupIdorg.apache.maven.plugins/groupId artifactIdmaven-jar-plugin/artifactId version3.3.0/version configuration archive manifest mainClasscom.example.App/mainClass addClasspathtrue/addClasspath /manifest /archive /configuration /plugin /plugins /build 再添加一个新的 profile，id 可以设置为 native，并在改 profile 下添加 native-maven-plugin 插件，用于将字节码文件编译为二进制的 Native Image，configuration 下可以配置本地镜像名称 imageName，以及构建时的参数 buildArg，此处 --no-fallback 代表的是不进入 fallback 模式。当 GraalVM 发现使用了未被配置的动态特性时会默认回退到 JVM 模式。本选项会关闭回退行为，总是执行静态编译。 properties native.image.nameHelloWorld/native.image.name native.maven.plugin.version0.10.2/native.maven.plugin.version/propertiesprofiles profile idnative/id build plugins plugin groupIdorg.graalvm.buildtools/groupId artifactIdnative-maven-plugin/artifactId version$native.maven.plugin.version/version extensionstrue/extensions configuration imageName$native.image.name/imageName buildArgs buildArg--no-fallback/buildArg /buildArgs /configuration executions execution idbuild-native/id goals goalcompile-no-fork/goal /goals phasepackage/phase /execution execution idtest-native/id goals goaltest/goal /goals phasetest/phase /execution /executions /plugin /plugins /build /profile/profiles 配置完成后，我们再执行 mvn -Pnative package 编译二进制文件，如下图所示会在 target 目录下编译生成 HelloWorld 可执行文件。 执行 ./target/HelloWorld 会输出 Hello World!。 ./target/HelloWorldHello World! 使用 Tracing Agent 收集元数据 根据前文的介绍，我们知道 GraalVM AOT 基于封闭性假设，即程序在编译期必须掌握运行时所需的所有信息，在运行时不能出现任何编译器未知的内容。Java 程序中包含了很多动态特性，例如：反射、动态类加载、动态代理、JCA 加密机制（内部依赖了反射）、JNI、序列化等，这些都违反了封闭性假设。 GraalVM 允许通过配置将缺失的信息补充给编译器以满足封闭性，为此 GraalVM 设计了 jni-config.json、reflect-config.json、proxy-config.json、resource-config.json、predefined-classes-config.json 和 serialization-config.json 配置文件，分别用于JNI 回调目标信息、反射目标信息、动态代理目标接口信息、资源文件信息、提前定义动态类信息、序列化信息。虽然 JSON 格式便于阅读和编写，但是通过人工方式编写 JSON 配置工作量比较大，也容易出现遗漏，因此 GraalVM 提供了基于 JVMTI（JVM Tool Interface） 的 native-image-agent，用于挂载在应用程序上，在运行时监控并记录和动态特性相关的函数调用信息。 使用 GraalVM SDK，执行 java -agentlib:native-image-agent=config-output-dir=/path/to/config-dir/ ...（注意：-agentlib 参数声明在 -jar、类名或者参数命令之前指定） 命令启动代理，在程序运行时，Agent 工具会查找 native-image 需要的类、字段、方法和资源等信息，当程序运行结束时，会将可达性元数据 Reachability Metadata 写入到指定目录的 JSON 文件中。 此外，Agent 工具还支持使用 config-write-period-secs=n 指定写入元数据的间隔，以及使用 config-write-initial-delay-secs=n 指定首次写入元数据的延迟时间。 java -agentlib:native-image-agent=config-output-dir=/path/to/config-dir/,config-write-period-secs=300,config-write-initial-delay-secs=5 ... 通常，建议将元数据文件写入到类路径 META-INF/native-image/ 下，这样 native-image 工具就能自动地查找该目录下定义的 JSON 文件，满足 GraalVM AOT 封闭性。在 Java 程序中，最常见的动态特性是反射，下面我们以一个简单的反射程序为例，介绍下如何使用 Tracing Agent 收集元数据，以及如何使用元数据来执行包含反射的 Native 程序。 我们在上一个案例的 Maven 项目中创建一个 reflection 子模块，并添加如下的 Reflection 和 StringReverser 类，Reflection 类根据参数传递的类名、方法名和方法参数，进行反射调用。 public final class Reflection @SneakyThrows public static void main(final String[] args) if (3 != args.length) throw new IllegalArgumentException(You must provide class name, method name and arguments.); String className = args[0]; String methodName = args[1]; String arguments = args[2]; Class? clazz = Class.forName(className); Method method = clazz.getDeclaredMethod(methodName, String.class); Object result = method.invoke(null, arguments); System.out.println(result); StringReverser 类逻辑也比较简单，根据输入的字符串，实现反转并输出。 @SuppressWarnings(unused)public final class StringReverser public static String reverse(final String input) return new StringBuilder(input).reverse().toString(); 然后我们通过 IDEA 运行反射程序，并传入参数 com.strongduanmu.StringReverser reverse Hello World!，执行后输出 !dlroW olleH。 JVM 运行反射很容易，下面我们再来使用 GraalVM AOT 编译，尝试下运行反射程序，首先在 pom 中添加 native-maven-plugin 插件，然后执行 ./mvnw -Pnative clean package -f reflection 进行编译。 properties project.build.sourceEncodingUTF-8/project.build.sourceEncoding native.image.nameReflection/native.image.name native.maven.plugin.version0.10.2/native.maven.plugin.version/propertiesbuild plugins plugin groupIdorg.apache.maven.plugins/groupId artifactIdmaven-compiler-plugin/artifactId version3.12.1/version configuration forktrue/fork /configuration /plugin plugin groupIdorg.apache.maven.plugins/groupId artifactIdmaven-jar-plugin/artifactId version3.3.0/version configuration archive manifest mainClasscom.strongduanmu.Reflection/mainClass addClasspathtrue/addClasspath /manifest /archive /configuration /plugin /plugins/buildprofiles profile idnative/id build plugins plugin groupIdorg.graalvm.buildtools/groupId artifactIdnative-maven-plugin/artifactId version$native.maven.plugin.version/version extensionstrue/extensions configuration imageName$native.image.name/imageName buildArgs buildArg--no-fallback/buildArg /buildArgs /configuration executions execution idbuild-native/id goals goalcompile-no-fork/goal /goals phasepackage/phase /execution execution idtest-native/id goals goaltest/goal /goals phasetest/phase /execution /executions /plugin /plugins /build /profile/profiles 编译完成后，我们执行 ./reflection/target/Reflection com.strongduanmu.StringReverser reverse Hello World\\!，出现了如下报错： Exception in thread main java.lang.ClassNotFoundException: com.strongduanmu.StringReverser at org.graalvm.nativeimage.builder/com.oracle.svm.core.hub.ClassForNameSupport.forName(ClassForNameSupport.java:143) at org.graalvm.nativeimage.builder/com.oracle.svm.core.hub.ClassForNameSupport.forName(ClassForNameSupport.java:106) at java.base@22.0.2/java.lang.Class.forName(DynamicHub.java:1387) at java.base@22.0.2/java.lang.Class.forName(DynamicHub.java:1352) at java.base@22.0.2/java.lang.Class.forName(DynamicHub.java:1346) at com.strongduanmu.Reflection.main(Reflection.java:17) at java.base@22.0.2/java.lang.invoke.LambdaForm$DMH/sa346b79c.invokeStaticInit(LambdaForm$DMH) 提示无法找到 com.strongduanmu.StringReverser 类，我们的源码中不是已经定义了这个类吗？这个异常是因为 GraalVM AOT 编译时，会进行静态分析，由于 StringReverser 类程序中没有明确的调用，因此静态分析不会将其包含在本地执行文件中。为了支持反射功能，我们需要借助 GraalVM Tracing Agent 进行元数据收集，并将其以 JSON 格式配置在项目的 META-INF/native-image 目录中。 首先，我们在项目中创建 META-INF/native-image 目录： mkdir -p ./reflection/src/main/resources/META-INF/native-image 然后，在 JVM 反射程序中开启 Agent 代理，添加如下参数并执行字符串反转程序（注意，Tracing Agent 是 GraalVM 提供的功能，需要注意 JDK 的选择）： -agentlib:native-image-agent=config-output-dir=reflection/src/main/resources/META-INF/native-image 执行完成后会生成前面介绍的 6 个 JSON 文件，我们以 reflect-config.json 为例，文件里面声明了程序运行过程中使用到的类、方法和参数。 [ name:com.strongduanmu.StringReverser, methods:[name:reverse,parameterTypes:[java.lang.String] ]] 我们使用 ./mvnw -Pnative clean package -f reflection 再次进行编译，然后执行 ./reflection/target/Reflection com.strongduanmu.StringReverser reverse Hello World\\!，此时程序可以正常运行。 ./reflection/target/Reflection com.strongduanmu.StringReverser reverse Hello World\\!!dlroW olleH 结语 本文首先介绍了 GraalVM 诞生的背景，随着云原生时代的发展，应用体积、内存消耗以及启动速度越来越成为人们关注的问题，为了解决云原生环境下 Java 程序存在的问题，以及应对来自 Go、Rust 等原生语言的挑战，Java 社区提出了 GraalVM 方案，通过 GraalVM 中的 AOT 提前编译技术，将 Java 生成的字节码文件，提前编译生成二进制的机器码，从而规避冷启动以及依赖 JVM 运行带来的诸多问题。 然后我们对比了 GraalVM AOT 编译和传统的 JIT 编译之间的优势和劣势，GraalVM AOT 目前更适用于对启动速度、内存消耗有较高要求的场景，而对于吞吐量、延迟有较高要求的场景，则更推荐采用传统的 JIT 编译方式。此外，GraalVM AOT 编译带来的生态变化，会导致传统 Java 程序的监控、调试、Agent 技术不再适用，这些需要在选型 GraalVM AOT 技术时进行充分考虑。 最后一个部分，我们介绍了 GraalVM AOT 实战，从基础的 SDK 安装，到第一个 HelloWorld 程序，带大家一起体验了下 GraalVM 的基础使用。然后我们参考了项目实际情况，使用 Maven 工具构建了一个简单的二进制程序。此外，由于现实中的 Java 程序具有很多动态特性，我们展示了反射程序如何通过 Agent 收集元数据，并将元数据维护在 META-INF/native-image 目录下，最终成功执行了带有反射的本地程序。 GraalVM 为 Java 领域带来新的发展机遇，相信随着 GraalVM 的成熟，越来越多应用程序将以 Native 的方式运行在不同环境中。作为 Java 工程师，也需要紧跟时代发展，不断探索新的技术，完善自己的技能栈。本系列后续还会探究更多关于 GraalVM 的新技术，欢迎感兴趣的朋友持续关注，本文如有不足之处，也欢迎留言探讨。 欢迎关注 欢迎关注「端小强的博客」微信公众号，会不定期分享日常学习和工作经验，欢迎大家关注交流。","tags":["JVM","GraalVM"],"categories":["GraalVM"]},{"title":"无关性的基石之 Java 字节码技术初探","path":"/blog/cornerstone-of-irrelevance-preliminary-study-of-java-bytecode-technology.html","content":"前言 熟悉 Java 语言的朋友应该都听过 Write Once, Run Anywhere. 这样的口号，它主要阐述地是 Java 语言的跨平台特性。工程师只需要编写一次 Java 源码，再通过 Java 编译器将源码编译为字节码文件，就可以很方便地在不同操作系统的 JVM 上进行分发运行。Java 字节码技术是 Java 语言实现平台无关性的基石，也是学习 JVM 虚拟机实现的基础，了解 Java 字节码技术，可以帮助大家理解后续的类加载机制，以及 JVM 编译优化相关的内容。因此，本系列首先从 Java 字节码技术开始，和大家一起初步探究字节码的设计和执行逻辑。 什么是字节码 字节码即 Java ByteCode，它由单个字节（byte）的指令组成，理论上最多可以支持 256 个操作码（opcode），而实际上 Java 只使用了 200 左右的操作码，还有一些操作码则保留下来，用于调试等操作。操作码通常也称为指令，后面会跟随零至多个参数，即操作数（operand）。根据指令的特性，可以将字节码分为如下的 4 大类： 栈操作指令，包括与局部变量交互的指令； 流程控制指令； 方法调用指令； 算术运算及类型转换指令。 除此之外，还有一些用于执行专门任务的指令，例如同步指令、异常指令等，完整的 JVM 指令可以参考 Java 虚拟机指令操作码和助记符映射关系。 (adsbygoogle = window.adsbygoogle || []).push({}); 赞助商 如何查看字节码 通过 javap 命令查看 JDK 工具自带了 javap 命令，可以用于查看 class 文件中的字节码，执行 javap -h 可以查看该命令详细的使用说明。用户使用 javap 命令时，需要在后面指定参数以及 class 字节码文件名，常用的参数有 -c 和 -v ，-c 参数用于对代码进行反编译，可以查看 class 文件中的字节码信息，-v 参数则用于打印附加信息，例如：constant pool 常量池信息。 ❯ javap -hUsage: javap options classeswhere possible options include: -? -h --help -help Print this help message # 打印帮助信息 -version Version information # 版本信息 -v -verbose Print additional information # 打印附加信息，例如：constant pool 常量池信息 -l Print line number and local variable tables # 打印行号和本地变量表 -public Show only public classes and members # 仅显示 public 类和成员 -protected Show protected/public classes and members # 显示 protected/public 类和成员 -package Show package/protected/public classes and members (default) # 显示 package/protected/public 类和成员（默认） -p -private Show all classes and members # 显示所有类和成员 -c Disassemble the code # 对代码进行反编译 -s Print internal type signatures # 打印内部类型签名 -sysinfo Show system info (path, size, date, MD5 hash) of class being processed # 显示系统信息（路径、大小、日期、MD5 哈希值） -constants Show final constants # 显示 final 常量 --module module, -m module Specify module containing classes to be disassembled --module-path path Specify where to find application modules --system jdk Specify where to find system modules --class-path path Specify where to find user class files -classpath path Specify where to find user class files -cp path Specify where to find user class files -bootclasspath path Override location of bootstrap class files 我们编写一个如下的简单 HelloByteCode 程序作为示例，程序 main 方法创建了一个 HelloByteCode 对象（源码请参考 HelloByteCode），并调用了 sayHello 方法，输出 Hello, ByteCode! 字符串。 public final class HelloByteCode public static void main(String[] args) HelloByteCode helloByteCode = new HelloByteCode(); helloByteCode.sayHello(); private void sayHello() System.out.println(Hello, ByteCode!); 然后我们使用 javac 命令将源码编译为字节码，-g 参数用于生成所有 debug 信息，javac 命令默认开启了优化功能，会去除字节码中的本地变量表 LocalVariableTable。 javac -g HelloByteCode.java 获取到字节码文件后，我们再通过 javap 命令查看字节码信息，-c 参数用于对代码进行反编译，-v 参数则用于打印附加信息。如下展示了完整的字节码信息，大家可以先尝试理解下字节码的含义，在下个小节我们将对字节码进行深入探究。 ❯ javap -c -v HelloByteCodeWarning: File ./HelloByteCode.class does not contain class HelloByteCodeClassfile /Users/duanzhengqiang/IdeaProjects/jvm-lecture/src/main/java/com/strongduanmu/jvm/bytecode/HelloByteCode.class Last modified 2024年7月5日; size 736 bytes MD5 checksum 591e8e496f42a858607d95d6db85bdd8 Compiled from HelloByteCode.javapublic final class com.strongduanmu.jvm.bytecode.HelloByteCode minor version: 0 major version: 55 flags: (0x0031) ACC_PUBLIC, ACC_FINAL, ACC_SUPER this_class: #2 // com/strongduanmu/jvm/bytecode/HelloByteCode super_class: #8 // java/lang/Object interfaces: 0, fields: 0, methods: 3, attributes: 1Constant pool: #1 = Methodref #8.#24 // java/lang/Object.init:()V #2 = Class #25 // com/strongduanmu/jvm/bytecode/HelloByteCode #3 = Methodref #2.#24 // com/strongduanmu/jvm/bytecode/HelloByteCode.init:()V #4 = Methodref #2.#26 // com/strongduanmu/jvm/bytecode/HelloByteCode.sayHello:()V #5 = Fieldref #27.#28 // java/lang/System.out:Ljava/io/PrintStream; #6 = String #29 // Hello, ByteCode! #7 = Methodref #30.#31 // java/io/PrintStream.println:(Ljava/lang/String;)V #8 = Class #32 // java/lang/Object #9 = Utf8 init #10 = Utf8 ()V #11 = Utf8 Code #12 = Utf8 LineNumberTable #13 = Utf8 LocalVariableTable #14 = Utf8 this #15 = Utf8 Lcom/strongduanmu/jvm/bytecode/HelloByteCode; #16 = Utf8 main #17 = Utf8 ([Ljava/lang/String;)V #18 = Utf8 args #19 = Utf8 [Ljava/lang/String; #20 = Utf8 helloByteCode #21 = Utf8 sayHello #22 = Utf8 SourceFile #23 = Utf8 HelloByteCode.java #24 = NameAndType #9:#10 // init:()V #25 = Utf8 com/strongduanmu/jvm/bytecode/HelloByteCode #26 = NameAndType #21:#10 // sayHello:()V #27 = Class #33 // java/lang/System #28 = NameAndType #34:#35 // out:Ljava/io/PrintStream; #29 = Utf8 Hello, ByteCode! #30 = Class #36 // java/io/PrintStream #31 = NameAndType #37:#38 // println:(Ljava/lang/String;)V #32 = Utf8 java/lang/Object #33 = Utf8 java/lang/System #34 = Utf8 out #35 = Utf8 Ljava/io/PrintStream; #36 = Utf8 java/io/PrintStream #37 = Utf8 println #38 = Utf8 (Ljava/lang/String;)V public com.strongduanmu.jvm.bytecode.HelloByteCode(); descriptor: ()V flags: (0x0001) ACC_PUBLIC Code: stack=1, locals=1, args_size=1 0: aload_0 1: invokespecial #1 // Method java/lang/Object.init:()V 4: return LineNumberTable: line 3: 0 LocalVariableTable: Start Length Slot Name Signature 0 5 0 this Lcom/strongduanmu/jvm/bytecode/HelloByteCode; public static void main(java.lang.String[]); descriptor: ([Ljava/lang/String;)V flags: (0x0009) ACC_PUBLIC, ACC_STATIC Code: stack=2, locals=2, args_size=1 0: new #2 // class com/strongduanmu/jvm/bytecode/HelloByteCode 3: dup 4: invokespecial #3 // Method init:()V 7: astore_1 8: aload_1 9: invokevirtual #4 // Method sayHello:()V 12: return LineNumberTable: line 6: 0 line 7: 8 line 8: 12 LocalVariableTable: Start Length Slot Name Signature 0 13 0 args [Ljava/lang/String; 8 5 1 helloByteCode Lcom/strongduanmu/jvm/bytecode/HelloByteCode;SourceFile: HelloByteCode.java 通过 jclasslib 查看 除了通过 javap 命令查看之外，我们还可以通过 jclasslib 可视化查看字节码。jclasslib 不仅提供了 Idea 插件，还提供了独立的软件包，大家可以按需选择使用。由于使用方式类似，本文以 Idea 插件的方式展示如何通过 jclasslib 查看字节码，首先选中 HelloByteCode 源码文件，然后选择 View - Show Bytecode With Jclasslib。 选择完成后，可以在右侧的 Tab 中查看字节码。此外，jclasslib 插件在查看字节码时，可以点击 Show JVM Spec 查看 JVM 虚拟机规范，查看相关字节码指令的作用。 深入理解字节码 Classfile 前文介绍了两种查看字节码的方法，想必大家对于字节码中的内容还有诸多疑问。本节我将带领大家逐行分析，一起深入探究字节码的内容，尝试理解字节码的含义与作用。首先，我们来看下字节码中的 Classfile，具体内容如下： Classfile /Users/duanzhengqiang/IdeaProjects/jvm-lecture/src/main/java/com/strongduanmu/jvm/bytecode/HelloByteCode.class Last modified 2024年7月5日; size 736 bytes MD5 checksum 591e8e496f42a858607d95d6db85bdd8 Compiled from HelloByteCode.java Classfile 声明了当前字节码来源的 class 文件路径，并在 Classfile 下方显示了 class 文件的最近修改时间，MD5 校验值以及编译的来源文件。 Class 基础信息 public final class com.strongduanmu.jvm.bytecode.HelloByteCode minor version: 0 major version: 55 flags: (0x0031) ACC_PUBLIC, ACC_FINAL, ACC_SUPER this_class: #2 // com/strongduanmu/jvm/bytecode/HelloByteCode super_class: #8 // java/lang/Object interfaces: 0, fields: 0, methods: 3, attributes: 1 第二部分 public final class com.strongduanmu.jvm.bytecode.HelloByteCode 则展示了 Class 类的版本号区间 [minor version: 0, major version: 55]，major version: 55 对应了 JDK 11，表示当前 Class 类支持 JDK 11 及以下版本。 flags 代表了访问标识符，0x0031 是访问标识符值的累加，ACC_PUBLIC 表示当前是一个 public 类，ACC_FINAL 表示当前是一个 final 类，ACC_SUPER 则是 JDK 早期用于标记当前类显式声明的父类，从 JDK 1.1 开始，所有类都必须显式声明它们的父类（即使是 Object），因此 ACC_SUPER 访问标志实际上总是被设置。更多访问标识符的说明请参考下表： 标记名 值 含义 ACC_PUBLIC 0x0001 可以被包的类外访问。 ACC_FINAL 0x0010 不允许有子类。 ACC_SUPER 0x0020 当用到 invokespecial 指令时，需要特殊处理的父类方法。 ACC_INTERFACE 0x0200 标识定义的是接口而不是类。 ACC_ABSTRACT 0x0400 不能被实例化。 ACC_SYNTHETIC 0x1000 标识并非 Java 源码生成的代码。 ACC_ANNOTATION 0x2000 标识注解类型。 ACC_ENUM 0x4000 标识枚举类型。 this_class 表示当前 class 类，后面跟随的 #2 表示引用常量池中的第二个常量，即注释中显示的 com/strongduanmu/jvm/bytecode/HelloByteCode。super_class 表示当前 class 类的超类，#8 表示常量池中的第八个常量，即 java/lang/Object。interfaces: 0, fields: 0, methods: 3, attributes: 1 表示当前 class 类中接口、字段、方法和属性的数量。 常量池 Constant pool: #1 = Methodref #8.#24 // java/lang/Object.init:()V #2 = Class #25 // com/strongduanmu/jvm/bytecode/HelloByteCode #3 = Methodref #2.#24 // com/strongduanmu/jvm/bytecode/HelloByteCode.init:()V #4 = Methodref #2.#26 // com/strongduanmu/jvm/bytecode/HelloByteCode.sayHello:()V #5 = Fieldref #27.#28 // java/lang/System.out:Ljava/io/PrintStream; #6 = String #29 // Hello, ByteCode! #7 = Methodref #30.#31 // java/io/PrintStream.println:(Ljava/lang/String;)V #8 = Class #32 // java/lang/Object #9 = Utf8 init #10 = Utf8 ()V #11 = Utf8 Code #12 = Utf8 LineNumberTable #13 = Utf8 LocalVariableTable #14 = Utf8 this #15 = Utf8 Lcom/strongduanmu/jvm/bytecode/HelloByteCode; #16 = Utf8 main #17 = Utf8 ([Ljava/lang/String;)V #18 = Utf8 args #19 = Utf8 [Ljava/lang/String; #20 = Utf8 helloByteCode #21 = Utf8 sayHello #22 = Utf8 SourceFile #23 = Utf8 HelloByteCode.java #24 = NameAndType #9:#10 // init:()V #25 = Utf8 com/strongduanmu/jvm/bytecode/HelloByteCode #26 = NameAndType #21:#10 // sayHello:()V #27 = Class #33 // java/lang/System #28 = NameAndType #34:#35 // out:Ljava/io/PrintStream; #29 = Utf8 Hello, ByteCode! #30 = Class #36 // java/io/PrintStream #31 = NameAndType #37:#38 // println:(Ljava/lang/String;)V #32 = Utf8 java/lang/Object #33 = Utf8 java/lang/System #34 = Utf8 out #35 = Utf8 Ljava/io/PrintStream; #36 = Utf8 java/io/PrintStream #37 = Utf8 println #38 = Utf8 (Ljava/lang/String;)V Constant pool 表示常量池，其中声明了字节码中需要使用的常量，#1、#2 等表示常量的编号，字节码中使用常量时，只需要引用相关的编号即可。Methodref、Class、Fieldref 是常量的类型，分别表示方法引用，Class 类以及字段引用，更多常量类型可参考如下常量类型表格。常量中可以通过编号引用其他常量，例如：#8.#24，代表了对 Object 对象 init 方法的引用，字节码注释 java/lang/Object.init:()V 已经很好地向我们展示了方法引用。 常量类型 值 含义 CONSTANT_Class 7 类或接口 CONSTANT_Fieldref 9 字段引用 CONSTANT_Methodref 10 类方法引用 CONSTANT_InterfaceMethodref 11 接口方法引用 CONSTANT_String 8 java.lang.String 类型的常量 CONSTANT_Integer 3 4 字节整型常量 CONSTANT_Float 4 4 字节浮点型常量 CONSTANT_Long 5 8 字节长整型常量 CONSTANT_Double 6 8 字节双精度浮点型常量 CONSTANT_NameAndType 12 字段或方法的名称和类型，类型通过字段描述符（例如：[Ljava/lang/String;）或方法描述符（例如：(Ljava/lang/String;)V）进行表示 CONSTANT_Utf8 1 UTF-8 编码表示的字符串常量值 CONSTANT_MethodHandle 15 方法句柄 CONSTANT_MethodType 16 方法类型 CONSTANT_InvokeDynamic 18 invohecynamic 动态方法调用 字节码 介绍完常量池后，我们再来关注下最核心的字节码指令，由于本文示例程序中包含了私有方法，因此需要使用 javap -c -v -p HelloByteCode 查看包含私有方法在内的所有成员变量和方法。如下展示了字节码信息，可以看到总共包含了 3 个方法——HelloByteCode 构造方法、main 方法、sayHello 方法，下面我们将分别进行探究学习。 public com.strongduanmu.jvm.bytecode.HelloByteCode(); descriptor: ()V flags: (0x0001) ACC_PUBLIC Code: stack=1, locals=1, args_size=1 0: aload_0 1: invokespecial #1 // Method java/lang/Object.init:()V 4: return LineNumberTable: line 3: 0 LocalVariableTable: Start Length Slot Name Signature 0 5 0 this Lcom/strongduanmu/jvm/bytecode/HelloByteCode; public static void main(java.lang.String[]); descriptor: ([Ljava/lang/String;)V flags: (0x0009) ACC_PUBLIC, ACC_STATIC Code: stack=2, locals=2, args_size=1 0: new #7 // class com/strongduanmu/jvm/bytecode/HelloByteCode 3: dup 4: invokespecial #9 // Method init:()V 7: astore_1 8: aload_1 9: invokevirtual #10 // Method sayHello:()V 12: return LineNumberTable: line 6: 0 line 7: 8 line 8: 12 LocalVariableTable: Start Length Slot Name Signature 0 13 0 args [Ljava/lang/String; 8 5 1 helloByteCode Lcom/strongduanmu/jvm/bytecode/HelloByteCode; private void sayHello(); descriptor: ()V flags: (0x0002) ACC_PRIVATE Code: stack=2, locals=1, args_size=1 0: getstatic #13 // Field java/lang/System.out:Ljava/io/PrintStream; 3: ldc #19 // String Hello, ByteCode! 5: invokevirtual #21 // Method java/io/PrintStream.println:(Ljava/lang/String;)V 8: return LineNumberTable: line 11: 0 line 12: 8 LocalVariableTable: Start Length Slot Name Signature 0 9 0 this Lcom/strongduanmu/jvm/bytecode/HelloByteCode; HelloByteCode 构造方法 public com.strongduanmu.jvm.bytecode.HelloByteCode(); descriptor: ()V flags: (0x0001) ACC_PUBLIC Code: stack=1, locals=1, args_size=1 0: aload_0 1: invokespecial #1 // Method java/lang/Object.init:()V 4: return LineNumberTable: line 3: 0 LocalVariableTable: Start Length Slot Name Signature 0 5 0 this Lcom/strongduanmu/jvm/bytecode/HelloByteCode; HelloByteCode 构造方法是 Java 编译器默认生成的，了解 Java 的朋友都知道，当我们在程序中没有定义任何构造方法时，Java 编译器会默认生成无参的构造方法。public com.strongduanmu.jvm.bytecode.HelloByteCode(); 是构造方法的方法声明，HelloByteCode 前面会带上完整的包路径。 descriptor 则是方法描述符，()V 中 () 表示入参，默认构造方法的入参为空，() 之后是返回值，由于构造方法没有任何返回值，因此返回值为 void，缩写为 V。 flags 表示访问标识符，ACC_PUBLIC 表示该构造方法为 public 构造方法，更多访问标识符类型可参考 Class 基础信息。 Code 则对应了具体的代码逻辑，stack=1, locals=1, args_size=1 中的 stack 表示当前方法执行时最大的栈使用深度，HelloByteCode 构造方法栈深度为 1，locals 表示本地变量表中槽位的个数，args_size 表示方法的参数个数。 好奇的同学可能会问：默认无参构造方法的参数个数为什么是 1？因为在 Java 中，非静态方法（包括构造方法）会有一个 this 引用，并且 this 会作为方法的隐藏参数，this 会存储在本地变量表的第 1 个槽位，所以字节码里 args_size 为 1。 0: aload_0 中的 0 表示当前指令位于该方法指令的 0 位置，aload_0 表示将第一个引用类型本地变量压入栈顶。下图展示了 Local Variable 和 Stack 关系，由于 JVM 是一个基于栈的计算机器，因此在计算的过程中会执行很多压入和弹出操作，即 Load 和 Store 指令。 1: invokespecial #1 中的 1 表示当前指令位于该方法指令的 1 位置，invokespecial 表示调用超类构造方法、实例初始化方法或私有方法，#1 引用了常量池中的第一个常量，即 #1 = Methodref #8.#24 // java/lang/Object.init:()V。从整个指令来看，调用的是超类构造方法：Object 类的 init 方法。 4: return 中的 4 表示当前指令位于该方法指令的 4 位置，return 表示从当前方法返回 void。 LineNumberTable 行号表记录了源码中行号和方法指令的位置，line 3: 0 中的 3 表示源码中的第 3 行，0 表示方法指令的第 0 个位置，即 0: aload_0。 LocalVariableTable 本地变量表记录了当前方法中的本地变量，Start 表示起始位置，标识了该变量在字节码中的哪行开始起作用，Length 表示指令作用范围，对应的是字节码中的位置。可以看到，默认构造方法有一个 this 变量，Slot 表示槽位，Signature 表示变量的签名信息，其中 LClassName 是引用类型的字段描述符。下表展示了常用类型的字段描述符，在字节码中我们会经常看到这些字段描述符信息，大家可以随时查阅表格加强理解。 字符 类型 含义 B byte 有符号字节型数 C char Unicode 字符，UTF-16 编码 D double 双精度浮点数 F float 单精度浮点数 I int 整型数 J long 长整数 S short 有符号短整数 Z boolean 布尔值 true/false L Classname; reference 一个名为 Classname 的实例 [ reference 一个一维数组 字段描述符示例： int 实例变量的描述符是 I； java.lang.Object 描述符是 Ljava/lang/Object;； double 的三维数组 double d[][][] 描述符是 [[[D。 main 方法 public static void main(java.lang.String[]); descriptor: ([Ljava/lang/String;)V flags: (0x0009) ACC_PUBLIC, ACC_STATIC Code: stack=2, locals=2, args_size=1 0: new #7 // class com/strongduanmu/jvm/bytecode/HelloByteCode 3: dup 4: invokespecial #9 // Method init:()V 7: astore_1 8: aload_1 9: invokevirtual #10 // Method sayHello:()V 12: return LineNumberTable: line 6: 0 line 7: 8 line 8: 12 LocalVariableTable: Start Length Slot Name Signature 0 13 0 args [Ljava/lang/String; 8 5 1 helloByteCode Lcom/strongduanmu/jvm/bytecode/HelloByteCode; 有了 HelloByteCode 构造方法的基础，我们阅读 main 方法的字节码会很轻松，descriptor 表示了方法描述符，([Ljava/lang/String;)V 表示入参为 String 数组，返回值为 void，这和我们记忆中的 main 方法声明是完全一致的。flags 声明了 main 方法为 public 公有方法，并且是 static 静态方法。 Code 部分 stack=2, locals=2, args_size=1 中的 stack=2表示当前方法执行时使用的最大栈深度为 2，我们将在后文字节码执行过程小节中，为大家详细介绍栈在执行过程中的作用。locals=2 表示该方法的本地变量数为 2，即：args 和 helloByteCode，那么为什么 main 方法的本地变量没有 this？我们前文介绍过，只有非静态方法会有当前对象的 this 引用，而 main 方法是一个静态方法。args_size=1 表示了 main 方法有一个参数 args。 字节码部分，为了方便大家理解，可以参考下面的字节码指令图。0: new #7 表示方法字节码的第 0 个位置，new 是一个操作码，表示创建一个对象，并将其引用值压入栈顶，后面紧跟着的 #7 是操作数，引用的是常量池中的第 7 个常量，由于 JVM 操作数栈是基于槽（Slot），每个槽默认为 32 位宽，即 4 位 16 进制数，因此 07 之前需要用 00 补位。参考 Java 虚拟机指令操作码和助记符映射关系，我们可以快速查阅到 new 操作码对应的 16 进制为 bb，因此 0: new #7 使用 16 进制的表现形式为 bb0007。 3: dup 表示字节码的第 3 个位置，dup（全称 duplicate） 表示复制栈顶数值，并将复制值压入栈顶，后面没有操作数，因此它的 16 进制表示为 59。 4: invokespecial #9 前文已经介绍，该操作码表示调用超类构造方法，实例初始化方法或私有方法，此处表示对 HelloByteCode 初始化时 init 方法的调用，invokespecial 操作码对应的 16 进制为 b7，同样操作数需要用 00 补位，最终 4: invokespecial #9 16 进制表示为 b70009。 astore_1 表示将栈顶引用型数值存入第二个本地变量，aload_1 表示将第二个引用类型本地变量推至栈顶，这两个操作码后面都没有操作数，它们 16 进制的表示分别为 4c 和 2b。 9: invokevirtual #10 表示调用实例方法，此处为调用 sayHello 实例方法，invokevirtual 操作码对应的 16 进制为 b6，经过补位后的 16 进制表示为 b6000a。最后 return 操作码表示了方法返回 void，对应的 16 进制表示为 b1。 那么，根据字节码助记符转换出来的 16 进制存储在哪里呢？答案是 class 文件。我们可以使用 Sublime Text 文本编辑器查看 class 文件中的内容，具体内容如下，通过比对能够找到 main 方法对应的 16 进制内容——bb00 0759 b700 094c 2bb6 000a b1。 main 方法中其他的 LineNumberTable 和 LocalVariableTable，和前文介绍的内容基本一致，感兴趣的读者可以自行尝试理解，本小节就不再一一介绍。 sayHello 方法 private void sayHello(); descriptor: ()V flags: (0x0002) ACC_PRIVATE Code: stack=2, locals=1, args_size=1 0: getstatic #13 // Field java/lang/System.out:Ljava/io/PrintStream; 3: ldc #19 // String Hello, ByteCode! 5: invokevirtual #21 // Method java/io/PrintStream.println:(Ljava/lang/String;)V 8: return LineNumberTable: line 11: 0 line 12: 8 LocalVariableTable: Start Length Slot Name Signature 0 9 0 this Lcom/strongduanmu/jvm/bytecode/HelloByteCode; 最后，我们再来看下 sayHello 方法，和构造方法及 main 方法不同的是，sayHello 方法是一个私有方法，因此 flags 对应的是 ACC_PRIVATE。 Code 部分，sayHello 方法使用到了 getstatic 和 ldc 两个操作码，getstatic 操作码表示获取指定类的静态域，并将其压入栈顶，此处表示对 System 类中的静态成员变量 PrintStream out 的调用。ldc 操作码表示将 int，float 或 String 型常量值从常量池中推至栈顶，此处指将字符串常量 Hello, ByteCode! 推至栈顶。invokevirtual 操作码前文已经介绍，表示调用实例方法，此处表示调用 PrintStream 对象的 println 方法。 SourceFile SourceFile 内容比较简单，用于声明当前 class 文件的源文件，此处为 HelloByteCode.java。 字节码执行过程 抽象执行流程 前文我们详细介绍了 javap 命令展示字节码信息的具体含义，本节我们再来了解下字节码的执行过程。如果不考虑 JVM 中的异常处理逻辑，我们可以用下面的伪代码来表示字节码的执行过程。首先，JVM 会自动计算 PC 程序计数器的值进行加 1，然后根据 PC 程序计数器指示的位置，读取字节码流中的操作码，由于字节码中一些操作码后面存在操作数，因此会判断当前操作码是否存在操作数，存在则继续读取操作数，不存在或者读取完则执行指令（操作码 + 操作数），执行完成后继续读取剩余的字节码内容。 do 自动计算 PC 程序计数器的值加 1; 根据 PC 程序计数器指示的位置，从字节码流中取出操作码; if (字节码存在操作数) 从字节码流中取出操作数; 执行操作码所定义的操作; while (字节码流长度 0); 执行流程说明 了解了字节码抽象执行流程，我们再结合下面的 BasicCalculator 示例，来观察下具体每个字节码执行时，局部变量表和操作栈之间的交互变化。 public final class BasicCalculator public static void main(String[] args) System.out.println(new BasicCalculator().calculate()); private int calculate() int a = 100; int b = 200; int c = 300; return (a + b) * c; 我们分别使用 javac -g BasicCalculator.java 和 javap -c -v -p BasicCalculator 编译和查看字节码信息，可以得到如下的核心逻辑： private int calculate(); descriptor: ()I flags: (0x0002) ACC_PRIVATE Code: stack=2, locals=4, args_size=1 0: bipush 100 2: istore_1 3: sipush 200 6: istore_2 7: sipush 300 10: istore_3 11: iload_1 12: iload_2 13: iadd 14: iload_3 15: imul 16: ireturn LineNumberTable: line 10: 0 line 11: 3 line 12: 7 line 13: 11 LocalVariableTable: Start Length Slot Name Signature 0 17 0 this Lcom/strongduanmu/jvm/bytecode/BasicCalculator; 3 14 1 a I 7 10 2 b I 11 6 3 c I 从字节码中可以看到，这段计算逻辑总共包含了 4 个本地变量，分别是：this、a、b 和 c，执行计算逻辑时，多次调用了 push 和 store 指令，下面我们将针对字节码中的每个指令进行逐个分析。 执行 bipush 100 指令： 首先，我们来看下第 1 个指令 bipush，它表示将单字节的常量值（-128 ~ 127）推至栈顶，此案例是将 100 推至栈顶。上图展示了程序计数器、局部变量表和操作栈的状态，程序计数器指向了当前需要执行字节码的位置 0，JVM 首先会读取 bipush 操作码，由于该操作码包含了 1 个字节的操作数，因此会继续读取对应的值 100，读取完成后执行指令，会将 100 推至操作栈的栈顶。 执行 istore_1 指令： 然后 JVM 会继续执行 istore_1 指令，该指令表示将栈顶 int 型数值存入第二个本地变量，即上图所示的 a 变量中。 执行 sipush 200 指令： 根据程序计数器指示的位置 3，JVM 会继续执行对应的 sipush 指令，该指令会将短整型常量（-32768 ~ 32767）推至栈顶，此处会将 200 推至操作栈的栈顶。执行完成后程序计数器会变更为 6，因为操作码 sipush 占用 1 个字节，操作数 200 占用 2 个字节。 执行 istore_2 指令： JVM 继续执行 istore_2 指令，该指令会将栈顶 int 型数值存入第三个本地变量，即上图所示的 b 变量。 执行 sipush 300 指令： sipush 指令前文已经介绍其作用，此处只有操作数不同，该指令会将 300 压入到栈顶。 执行 istore_3 指令： 根据程序计数器指示的位置 10，JVM 会执行 istore_3 指令，该指令会将栈顶 int 型数值存入第四个本地变量，即存入变量 c 的位置。 执行 iload_1 指令： 程序计数器 11 位置指向了 iload_1 指令，该指令表示将第二个 int 型本地变量推至栈顶，上图展示了将变量 a 的值 100 推至栈顶的过程。 执行 iload_2 指令： 程序计数器 12 位置指向了 iload_2 指令，该指令表示将第三个 int 型本地变量推至栈顶，上图展示了将变量 b 的值 200 推至栈顶的过程。 执行 iadd 指令： 程序计数器 13 位置指向了 iadd 指令，该指令表示将栈顶两 int 型数值相加并将结果压入栈顶，上图展示了将栈顶的 100 和 200 相加，并将结果 300 压入栈顶的过程。 执行 iload_3 指令： 程序计数器 14 位置指向了 iload_3 指令，该指令表示将第四个 int 型本地变量推至栈顶，上图展示了将变量 c 的值 300 推至栈顶的过程。 执行 imul 指令： 程序计数器 15 位置指向了 imul 指令，该指令表示将栈顶两 int 型数值相乘并将结果压入栈顶，上图展示了将栈顶的 300 和 300 相乘，并将结果 90000 压入栈顶的过程。 执行 ireturn 指令： 程序计数器 16 位置指向了 ireturn 指令，该指令表示从当前方法返回 int，上图展示了将栈顶的 90000 返回给调用逻辑的过程。 到这里，我们就详细介绍了示例计算逻辑执行时，程序计数器、局部变量表以及操作栈之间的变化，虽然这个示例较为简单，但是它已经很好地展示了字节码执行的过程。如果大家感兴趣，可以尝试采用上面的方式来理解更复杂的业务逻辑。 常用字节码指令 最后一个部分，我们再来介绍下常用的字节码指令，帮助大家能够在实际项目中看懂更加复杂的字节码指令含义。文章一开始，我们就介绍了字节码指令包括了：栈操作指令、流程控制指令、流程控制指令 和 算术运算及类型转换指令 4 大类，除了这 4 大类，对象初始化指令 也同样重要，下面我们将按照这几大类，为大家介绍常用的字节码指令。 对象初始化指令 通过 new HelloByteCode() 进行对象初始化，是大家日常开发的常用操作，通过前文的示例我们可以发现，对象初始化操作通常可以对应如下的 3 个 JVM 指令，分别是 new、dup 和 invokespecial： 0: new #7 // class com/strongduanmu/jvm/bytecode/HelloByteCode3: dup4: invokespecial #9 // Method init:()V 那为什么需要 3 条指令才能完成对象初始化呢？是因为： 字节码中的 new 指令只负责创建对象，并将对象引用压入栈顶，new 指令不会调用对象的构造方法； dup 指令用于复制栈顶的值，此时操作栈上会出现连续相同的两个对象引用，dup 复制的对象引用会用于下一步 invokespecial 指令执行，而 new 指令创建的对象引用则会用于正常代码逻辑调用； 调用构造方法是由 invokespecial 指令来完成，此处会调用 HelloByteCode#init 实例方法，因此需要从操作栈上弹出 this 对象。 在完成对象初始化后，通常会将对象实例赋给局部变量或全局变量，因此，紧随其后的指令可能有如下几种： astore_n 或 astore n，n 表示本地变量表中的位置，该命令会将对象存储到本地变量表的指定位置； putfield 将对象赋值给实例变量； putstatic 将对象赋值给静态变量。 栈操作指令 前文我们介绍对象初始化时提到了 dup 指令，它就是一个栈操作指令，除了 dup 指令外，栈操作指令还包括：pop、pop2、dup_x1、dup_x2、dup2、dup2_x1、dup2_x2 和 swap 指令。 dup、dup2、pop 和 pop2 是最基础的栈操作指令，它们的作用如下： dup 用于复制栈顶数值，并将复制值压入栈顶，dup 指令只能适用于下表分类 1 的类型，这些类型只占用一个槽位 Slot； dup2 用于复制栈顶一个 long、double 类型，或者两个非 long、非 double 的其他类型数值，并将复制值压入栈顶（long、double 类型占用两个槽位 Slot）； pop 将栈顶数值弹出，pop 指令只能适用于下表分类 1 的类型，这些类型只占用一个槽位 Slot； pop2 将栈顶的一个 long、double 类型，或者两个非 long、非 double 的其他类型数值弹出（long、double 类型占用两个槽位 Slot）。 实际类型 JVM 计算类型 分类 boolean int 1 byte int 1 char int 1 short int 1 int int 1 float float 1 reference reference 1 returnAddress returnAddress 1 long long 2 double double 2 dup_x1、dup_x2、dup2_x1、dup2_x2 这几个指令会稍微复杂一些，前缀部分的 dup 和 dup2 和前面介绍的指令含义相同，表示复制 1 个和 2 个槽位 Slot 数值，后缀 _xn 代表将栈顶复制的值插入到栈的指定位置，我们只需将指令的 dup 和 _xn 中的系数相加，结果即为需要插入的位置，例如： dup_x1 插入位置为 1 + 1 = 2，即栈顶 2 个 Slot 下的位置； dup_x2 插入位置为 1 + 2 = 3，即栈顶 3 个 Slot 下的位置； dup2_x1 插入位置为 2 + 1 = 3，即栈顶 3 个 Slot下的位置； dup2_x2 插入位置为 2 + 2 = 4，即栈顶 4 个 Slot 下的位置。 swap 指令表示将栈顶最顶端的两个数值互换，这个数值不能是占用 2 个槽位的 long 或 double 类型，那么如果需要互换两个 long 或 double 类型，JVM 虚拟机该如何操作呢？答案就是使用 dup2_x2 指令，复制栈顶的 2 个 Slot 对应的 long 或 double 类型，并插入到栈顶 4 个 Slot 下的位置，再使用 pop2 指令弹出栈顶的 long 或 double 类型，这样就完成了 long 或 double 类型的交换。 流程控制指令 流程控制指令主要包括分支和循环等操作，这些指令会通过检查条件来控制程序的执行流程，一般包括了：if... then... else...、for...、try ... catch (...) 等常用语句。为了方便介绍流程控制指令，我们编写了如下的简单示例： public final class FlowControl public static void main(String[] args) CollectionInteger numbers = Arrays.asList(1, 2, 3, 4, 5); for (int each : numbers) if (each % 2 == 0) continue; System.out.println(each); 编译后查看字节码，可以得到的字节码信息，0: iconst_5 表示初始化一个 int 型常量 5，它用于表示数组的长度。紧接着 1: anewarray 就创建了一个 Integer 类型的数组。然后依次初始化常量 1 ~ 5，并使用 Integer.valueOf 将基础类型转换为包装类型，然后存储到数组的 0 ~ 4 位置中。数组初始化完成后，会调用 Arrays.asList 方法转换为 List 对象，再使用 astore_1 将集合对象存储到本地变量表的第二个位置中。 Code: stack=4, locals=4, args_size=1 0: iconst_5 1: anewarray #2 // class java/lang/Integer 4: dup 5: iconst_0 6: iconst_1 7: invokestatic #3 // Method java/lang/Integer.valueOf:(I)Ljava/lang/Integer; 10: aastore 11: dup 12: iconst_1 13: iconst_2 14: invokestatic #3 // Method java/lang/Integer.valueOf:(I)Ljava/lang/Integer; 17: aastore 18: dup 19: iconst_2 20: iconst_3 21: invokestatic #3 // Method java/lang/Integer.valueOf:(I)Ljava/lang/Integer; 24: aastore 25: dup 26: iconst_3 27: iconst_4 28: invokestatic #3 // Method java/lang/Integer.valueOf:(I)Ljava/lang/Integer; 31: aastore 32: dup 33: iconst_4 34: iconst_5 35: invokestatic #3 // Method java/lang/Integer.valueOf:(I)Ljava/lang/Integer; 38: aastore 39: invokestatic #4 // Method java/util/Arrays.asList:([Ljava/lang/Object;)Ljava/util/List; 42: astore_1 ... 下面这部分字节码才是流程控制的关键，可以看到 for ... each 语句编译之后使用了迭代器（似乎并不总是使用 for index++ 执行），循环过程中先调用 hasNext 方法判断迭代器是否需要继续执行，该返回会返回 true，false，JVM 虚拟机会将其转换为 1 和 0 压入栈顶。56: ifeq 91 则会判断栈顶 int 数值是否等于 0，相等则跳转到程序计数器 91 的位置执行。 如果不等于 0 则代表当前集合仍然有值，会通过 60: invokeinterface #7, 1 调用迭代器的 next 方法，然后通过 irem 指令从栈顶取出 iconst_2 定义的常量 2，并进行取模运算。75: ifne 81 会判断栈顶 int 类型值是否不等于 0，不等于则跳转到程序计数器 81 的位置，执行 println 操作，否则执行 78: goto 50 指令跳转到程序计数器 50 位置，继续执行下一次循环操作。 Code: stack=4, locals=4, args_size=1 ... 43: aload_1 44: invokeinterface #5, 1 // InterfaceMethod java/util/Collection.iterator:()Ljava/util/Iterator; 49: astore_2 50: aload_2 51: invokeinterface #6, 1 // InterfaceMethod java/util/Iterator.hasNext:()Z 56: ifeq 91 59: aload_2 60: invokeinterface #7, 1 // InterfaceMethod java/util/Iterator.next:()Ljava/lang/Object; 65: checkcast #2 // class java/lang/Integer 68: invokevirtual #8 // Method java/lang/Integer.intValue:()I 71: istore_3 72: iload_3 73: iconst_2 74: irem 75: ifne 81 78: goto 50 81: getstatic #9 // Field java/lang/System.out:Ljava/io/PrintStream; 84: iload_3 85: invokevirtual #10 // Method java/io/PrintStream.println:(I)V 88: goto 50 91: return 关于更多的流程控制指令，大家可以自行尝试编写代码分析，字节码指令含义可参考比较 Comparisons 和控制 Control。 方法调用指令 前文我们已经介绍了部分方法调用指令，例如：在构造方法初始化时，会调用 invokespecial 进行初始化。下面列举了常用方法调用指令的含义： invokevirtual：调用实例方法； invokespecial：调用超类构造方法，实例初始化方法，私有方法； invokestatic：调用静态方法； invokeinterface：调用接口方法； invokedynamic：调用动态方法。 细心的朋友可能会发现，invokevirtual 和 invokeinterface 有些类似，一个是调用实例方法，一个是调用接口方法。那么他们之间有什么差异呢？我们通过一个小例子来说明下，下面的示例中有 A 和 B 两个类，A 类中定义了 method1 和 method2，B 类继承了 A 类，并重写了 method2，然后增加了 method3。此外，B 类还实现了接口 X，重写了 methodX，C 类也同样实现了 X 接口，并重写了 methodX。 在 JVM 中执行方法时，需要先解析该方法。在类的定义中有一个方法表，所有方法都对应了一个编号，JVM 解析过程中会从方法表中查找方法对应的编号位置。这个案例中，如果我们调用 B 类的 method2 方法，由于该方法在 B 类中进行了重写，因此可以直接在 B 类的方法表中快速查找到，因此使用 invokevirtual B#method2 指令即可。如果我们调用 B 类的 method1 方法，由于该方法 B 类没有重写，继承的是父类的方法，因此使用 invokevirtual B#method1 指令时，会通过指针找到 A#method1 方法。 此外，我们还可以使用 X 接口调用 methodX 方法，此时会生成 invokeinterface 指令进行调用，由于不同对象 methodX 方法的编号位置不同，在执行 invokeinterface 指令时，就需要在运行时动态地查找不同类中的位置，效率会有些影响。当然，尽管接口调用会有一些额外开销，但也无需为了这点小的优化而不去使用接口，因为 JVM 中的 JIT 编译器会帮我们优化这部分的性能损耗。 class A\tmethod1\tmethod2class B extends A implement X\t@Override\tmethod2\tmethod3\t@Override\tmethodXclass C implements X\tmethodC\t@Override\tmethodX 除了 invokevirtual 和 invokeinterface 外，invokedynamic 指令也值得拿出来说一说。从 JDK 7 开始，JVM 新增了 invokedynamic 指令，这条指令是为了实现动态类型语言（Dynamically Typed Language） 而进行地改进之一，也是 JDK 8 支持 Lambda 表达式的基础。 我们知道在不改变字节码的情况下，想要实现调用 Demo 类的 test 方法，只有两种办法： 使用 new Demo().test();，先创建 Demo 类，然后通过对象调用 test 方法； 使用反射 API，通过 Demo.class.getMethod 获取到 Method 对象，然后调用 Method.invoke 反射调用 test 方法。 这两个方法都需要显式地把方法 test 和类型 Demo 关联起来，假设我们还有一个类型 Demo2，它也有一个一模一样的方法 test，这时候我们又要如何调用 test 方法呢？在 JDK 8 之前，我们通常会定义一个公共接口，然后将 test 方法声明在接口中，并让 Demo 和 Demo2 实现这个接口，这样我们就可以面向接口进行调用。 虽然面向接口可以实现，但是总体上体验很差，增加了很多额外的工作，为了解决这个问题，JDK 8 实现了全新的 Lambda 表达式。Lambda 表达式的本质是对象引用，它基于 invokedynamic 指令，配合新增的方法句柄 Method Hanlders（它可以用来描述一个跟类型 Demo 无关的 test 方法签名，甚至不包括方法名称，这样就可以做到不同的对象调用同一个签名的方法），可以在运行时再决定由哪个类来接收被调用的方法。关于 invokedynamic 指令的更多细节，可以参考 JVM 之动态方法调用：invokedynamic 进行学习了解。 算术运算及类型转换指令 JVM 指令中还包含了很多算术运算指令，对于所有的数值类型 int（boolean、byte、char 和 short 都会被当做 int 运算）、long、float 和 double 都有加、减、乘、除、取模和取反指令，下表展示了不同数值类型对应的运算指令。 类型/运算 add(+) subtract(-) multiply(*) divide(/) remainder(%) negate(-) int iadd isub imul idiv irem ineg long ladd lsub lmul ldiv lrem lneg float fadd fsub fmul fdiv frem fneg double dadd dsub dmul ddiv drem dneg 当我们将某一种类型的值赋值给另外一种类型，或者不同的类型混合运算时，这时候就会发生类型转换，通常隐式类型转换都是由低精度向高精度转换，例如：int 转换为 long，float 转换为 double，而高精度向低精度转换则需要显式类型转换，例如：(int) 1000L。 From/To int long float double byte char short int - i2l i2f i2d i2b i2c i2s long l2i - l2f l2d - - - float f2i f2l - f2d - - - double d2i d2l d2f - - - - 上表展示了常见的数值类型转换指令，可以发现 int、long、float 和 double 类型之间都有相互转换的指令，而 long、float 和 double 却没有转换的指令。我们编写如下简单的转换逻辑，将 b 变量调整为 float 类型，由于隐式类型转换会有低精度向高精度转换，因此 ((a + b) * c) 的结果为 float 类型，然后我们再用 (byte) 强制进行类型转换。 private int calculate() int a = 100; float b = 200; int c = 300; return (byte) ((a + b) * c); 我们编译这段代码并使用 javap 查看字节码，发现 JVM 会先使用 f2i 将 float 类型的结果转换为 int 类型，然后再使用 i2b 将 int 类型转换为 byte 类型，实际上用了两个指令实现了 float 到 byte 的转换。 Code: stack=2, locals=4, args_size=1 0: bipush 100 2: istore_1 3: ldc #7 // float 200.0f 5: fstore_2 6: sipush 300 9: istore_3 10: iload_1 11: i2f 12: fload_2 13: fadd 14: iload_3 15: i2f 16: fmul 17: f2i 18: i2b 19: ireturn 结语 本文从字节码的基础概念开始，为大家介绍了 javap 和 jclasslib 两种查看字节码的方式，通过工具我们能跟观察到字节码中的关键信息，包括：Classfile、Class 基础信息、常量池、字节码和 SourceFile，其中最重要的常量池和字节码本文也进行了深入探究。 然后我们又结合一个示例计算逻辑，逐个字节码为大家介绍字节码的执行过程，相信大家一定对程序计数器、局部变量表以及操作栈之间的关系有了深刻的理解。 在最后一个部分，我们对常用的字节码指令进行了介绍，期望能够帮助大家了解常用字节码的作用，并能跟在实际的项目中理解这些字节码，进而帮助我们排查问题。 由于 JVM 虚拟机规范中总共定义了 200 多个字节码，考虑到文章的篇幅，我们无法一一进行介绍，感兴趣的读者可以参考 Java 虚拟机指令操作码和助记符映射关系，探究学习其他字节码指令。 欢迎关注 欢迎关注「端小强的博客」微信公众号，会不定期分享日常学习和工作经验，欢迎大家关注交流。","tags":["JVM"],"categories":["JVM"]},{"title":"ANTLR 解析性能优化指南","path":"/blog/improving-the-performance-of-an-antlr-parser.html","content":"本文翻译自 Improving the performance of an ANTLR parser，主要介绍了 Antlr 解析性能优化相关的经验，文中也增加了笔者个人的理解，期望对广大 Antlr 用户有所帮助。 前言 很多人问我们如何提高解析器的性能，有时解析器是用一些旧库完成的，有时解析器是用 ANTLR 创建的。在本文中，我们提供了一些关于提高 ANTLR 解析器性能的建议。 首先，我们需要声明一下：如果想要绝对最佳的性能，你可能需要选择自定义解析器。你将会花费十倍的维护成本，并且效率会降低，但你将获得最佳的性能。这就是 C# 或 Java 等语言的官方解析器的构建方式。在这些情况下，自定义解析器是有意义的，但在绝大多数情况下，它没有意义。使用 ANTLR 或其他解析器生成器比手动编写自定义解析器要高效得多。此外，除非你有构建自定义解析器的经验，否则你的性能可能也会更差。 你还应该检查问题是否确实出在解析器上，而不是解析后执行的操作。在大多数应用程序中，解析只是程序所做工作的一小部分。毕竟，你不想只解析一些代码，而是希望利用解析获得的信息做一些事情。 现在这个问题已经解决了，让我们看看如何改进你的 ANTLR 解析器。 ANTLR 运行时有所不同 只需要一个 ANTLR 工具就可以为所有支持的目标语言生成解析器。但是，每种支持的语言都需要不同的运行时。 每个运行时都会有不同的性能和潜在问题。运行时通常会遵循其各自语言的性能特征。例如，Python 运行时通常比 C# 运行时慢。根据运行时的成熟度，也有一些例外。例如，新的运行时的性能可能低于其可能的性能，因为它仍未针对性能进行优化。 你可以在官方文档中看到受支持目标的更新列表：运行时库和代码生成目标。 这意味着如果你遇到性能问题，更改运行时的目标语言可能会很有用。例如，你可以用 C++ 而不是 Python 生成解析器。这并不意味着你必须用另一种语言重写整个程序。解析器主要用于将代码转换为其他内容的管道中。解析器本身将生成解析树。然后可以将其转换为抽象语法树并由你的应用程序使用。我们为客户使用的常见策略是： 我们用 Java 创建解析器，将解析树转换为 AST，然后输出 JSON 或 XML 文件； 然后客户端应用程序将以他们喜欢的任何语言使用此文件； 这使我们能够确保获得的一定级别的性能，这些性能在 Python 中可能无法达到，并且它还允许客户端在自己的代码中继续使用 Python，这些代码会为了某些目标而消费 AST 结果。 一些示例数字 TODO 欢迎关注 欢迎关注「端小强的博客」微信公众号，会不定期分享日常学习和工作经验，欢迎大家关注交流。","tags":["Antlr"],"categories":["Antlr"]},{"title":"Java 虚拟机指令操作码和助记符映射关系","path":"/blog/opcode-mnemonics-by-opcode.html","content":"前言 本文整理了 Java 虚拟机指令操作码和助记符之间的映射关系，可以用于日常学习 Java 字节码时快速查阅。需要注意的是，操作码 186 对应的 invokedynamic 指令是 JDK 7 版本新增的指令，在 JDK 7 之前的版本没有该指令。 常量 Constants 字节码（十进制） 字节码（十六进制） 助记符 指令含义 00 0x00 nop 无操作 01 0x01 aconst_null 将 null 推至栈顶 02 0x02 iconst_m1 将 int 型 -1 推至栈顶 03 0x03 iconst_0 将 int 型 0 推至栈顶 04 0x04 iconst_1 将 int 型 1 推至栈顶 05 0x05 iconst_2 将 int 型 2 推至栈顶 06 0x06 iconst_3 将 int 型 3 推至栈顶 07 0x07 iconst_4 将 int 型 4 推至栈顶 08 0x08 iconst_5 将 int 型 5 推至栈顶 09 0x09 lconst_0 将 long 型 0 推至栈顶 10 0x0a lconst_1 将 long 型 1 推至栈顶 11 0x0b fconst_0 将 float 型 0 推至栈顶 12 0x0c fconst_1 将 float 型 1 推至栈顶 13 0x0d fconst_2 将 float 型 2 推至栈顶 14 0x0e dconst_0 将 double 型 0 推至栈顶 15 0x0f dconst_1 将 double 型 1 推至栈顶 16 0x10 bipush 将单字节的常量值（-128 ~ 127）推至栈顶 17 0x11 sipush 将一个短整型常量（-32768 ~ 32767）推至栈顶 18 0x12 ldc 将 int，float 或 String 型常量值从常量池中推至栈顶 19 0x13 ldc_w 将 int，float 或 String 型常量值从常量池中推至栈顶（宽索引） 20 0x14 ldc2_w 将 long 或 double 型常量值从常量池中推至栈顶（宽索引） (adsbygoogle = window.adsbygoogle || []).push({}); 赞助商 加载 Loads 字节码（十进制） 字节码（十六进制） 助记符 指令含义 21 0x15 iload 将指定的 int 型本地变量推至栈顶 22 0x16 lload 将指定的 long 型本地变量推至栈顶 23 0x17 fload 将指定的 float 型本地变量推至栈顶 24 0x18 dload 将指定的 double 型本地变量推至栈顶 25 0x19 aload 将指定的引用类型本地变量推至栈顶 26 0x1a iload_0 将第一个 int 型本地变量推至栈顶 27 0x1b iload_1 将第二个 int 型本地变量推至栈顶 28 0x1c iload_2 将第三个 int 型本地变量推至栈顶 29 0x1d iload_3 将第四个 int 型本地变量推至栈顶 30 0x1e lload_0 将第一个 long 型本地变量推至栈顶 31 0x1f lload_1 将第二个 long 型本地变量推至栈顶 32 0x20 lload_2 将第三个 long 型本地变量推至栈顶 33 0x21 lload_3 将第四个 long 型本地变量推至栈顶 34 0x22 fload_0 将第一个 float 型本地变量推至栈顶 35 0x23 fload_1 将第二个 float 型本地变量推至栈顶 36 0x24 fload_2 将第三个 float 型本地变量推至栈顶 37 0x25 fload_3 将第四个 float 型本地变量推至栈顶 38 0x26 dload_0 将第一个 double 型本地变量推至栈顶 39 0x27 dload_1 将第二个 double 型本地变量推至栈顶 40 0x28 dload_2 将第三个 double 型本地变量推至栈顶 41 0x29 dload_3 将第四个 double 型本地变量推至栈顶 42 0x2a aload_0 将第一个引用类型本地变量推至栈顶 43 0x2b aload_1 将第二个引用类型本地变量推至栈顶 44 0x2c aload_2 将第三个引用类型本地变量推至栈顶 45 0x2d aload_3 将第四个引用类型本地变量推至栈顶 46 0x2e iaload 将 int 型数组指定索引的值推至栈顶 47 0x2f laload 将 long 型数组指定索引的值推至栈顶 48 0x30 faload 将 float 型数组指定索引的值推至栈顶 49 0x31 daload 将 double 型数组指定索引的值推至栈顶 50 0x32 aaload 将引用类型数组指定索引的值推至栈顶 51 0x33 baload 将 boolean 或 byte 型数组指定索引的值推至栈顶 52 0x34 caload 将 char 型数组指定索引的值推至栈顶 53 0x35 saload 将 short 型数组指定索引的值推至栈顶 存储 Stores 字节码（十进制） 字节码（十六进制） 助记符 指令含义 54 0x36 istore 将栈顶 int 型数值存入指定本地变量 55 0x37 lstore 将栈顶 long 型数值存入指定本地变量 56 0x38 fstore 将栈顶 float 型数值存入指定本地变量 57 0x39 dstore 将栈顶 double 型数值存入指定本地变量 58 0x3a astore 将栈顶引用类型数值存入指定本地变量 59 0x3b istore_0 将栈顶 int 型数值存入第一个本地变量 60 0x3c istore_1 将栈顶 int 型数值存入第二个本地变量 61 0x3d istore_2 将栈顶 int 型数值存入第三个本地变量 62 0x3e istore_3 将栈顶 int 型数值存入第四个本地变量 63 0x3f lstore_0 将栈顶 long 型数值存入第一个本地变量 64 0x40 lstore_1 将栈顶 long 型数值存入第二个本地变量 65 0x41 lstore_2 将栈顶 long 型数值存入第三个本地变量 66 0x42 lstore_3 将栈顶 long 型数值存入第四个本地变量 67 0x43 fstore_0 将栈顶 float 型数值存入第一个本地变量 68 0x44 fstore_1 将栈顶 float 型数值存入第二个本地变量 69 0x45 fstore_2 将栈顶 float 型数值存入第三个本地变量 70 0x46 fstore_3 将栈顶 float 型数值存入第四个本地变量 71 0x47 dstore_0 将栈顶 double 型数值存入第一个本地变量 72 0x48 dstore_1 将栈顶 double 型数值存入第二个本地变量 73 0x49 dstore_2 将栈顶 double 型数值存入第三个本地变量 74 0x4a dstore_3 将栈顶 double 型数值存入第四个本地变量 75 0x4b astore_0 将栈顶引用型数值存入第一个本地变量 76 0x4c astore_1 将栈顶引用型数值存入第二个本地变量 77 0x4d astore_2 将栈顶引用型数值存入第三个本地变量 78 0x4e astore_3 将栈顶引用型数值存入第四个本地变量 79 0x4f iastore 将栈顶 int 型数值存入指定数组的指定索引位置 80 0x50 lastore 将栈顶 long 型数值存入指定数组的指定索引位置 81 0x51 fastore 将栈顶 float 型数值存入指定数组的指定索引位置 82 0x52 dastore 将栈顶 double 型数值存入指定数组的指定索引位置 83 0x53 aastore 将栈顶引用型数值存入指定数组的指定索引位置 84 0x54 bastore 将栈顶 boolean 或 byte 型数值存入指定数组的指定索引位置 85 0x55 castore 将栈顶 char 型数值存入指定数组的指定索引位置 86 0x56 sastore 将栈顶 short 型数值存入指定数组的指定索引位置 栈操作 Stack 字节码（十进制） 字节码（十六进制） 助记符 指令含义 87 0x57 pop 将栈顶数值弹出（数值不能是 long 或 double 类型的） 88 0x58 pop2 将栈顶的一个（对于 long 或 double 类型）或两个数值（对于非 long 或 double 的其他类型）弹出 89 0x59 dup 复制栈顶数值并将复制值压入栈顶 90 0x5a dup_x1 复制栈顶数值并将两个复制值压入栈顶 91 0x5b dup_x2 复制栈顶数值并将三个（或两个）复制值压入栈顶 92 0x5c dup2 复制栈顶一个（对于 long 或 double 类型）或两个（对于非 long 或 double 的其他类型）数值并将复制值压入栈顶 93 0x5d dup2_x1 dup_x1 指令的双倍版本 94 0x5e dup2_x2 dup_x2 指令的双倍版本 95 0x5f swap 将栈顶最顶端的两个数值互换（数值不能是 long 或 double 类型） 数学运算 Math 字节码（十进制） 字节码（十六进制） 助记符 指令含义 96 0x60 iadd 将栈顶两 int 型数值相加并将结果压入栈顶 97 0x61 ladd 将栈顶两 long 型数值相加并将结果压入栈顶 98 0x62 fadd 将栈顶两 float 型数值相加并将结果压入栈顶 99 0x63 dadd 将栈顶两 double 型数值相加并将结果压入栈顶 100 0x64 isub 将栈顶两 int 型数值相减并将结果压入栈顶 101 0x65 lsub 将栈顶两 long 型数值相减并将结果压入栈顶 102 0x66 fsub 将栈顶两 float 型数值相减并将结果压入栈顶 103 0x67 dsub 将栈顶两 double 型数值相减并将结果压入栈顶 104 0x68 imul 将栈顶两 int 型数值相乘并将结果压入栈顶 105 0x69 lmul 将栈顶两 long 型数值相乘并将结果压入栈顶 106 0x6a fmul 将栈顶两 float 型数值相乘并将结果压入栈顶 107 0x6b dmul 将栈顶两 double 型数值相乘并将结果压入栈顶 108 0x6c idiv 将栈顶两 int 型数值相除并将结果压入栈顶 109 0x6d ldiv 将栈顶两 long 型数值相除并将结果压入栈顶 110 0x6e fdiv 将栈顶两 float 型数值相除并将结果压入栈顶 111 0x6f ddiv 将栈顶两 double 型数值相除并将结果压入栈顶 112 0x70 irem 将栈顶两 int 型数值作取模运算并将结果压入栈顶 113 0x71 lrem 将栈顶两 long 型数值作取模运算并将结果压入栈顶 114 0x72 frem 将栈顶两 float 型数值作取模运算并将结果压入栈顶 115 0x73 drem 将栈顶两 double 型数值作取模运算并将结果压入栈顶 116 0x74 ineg 将栈顶 int 型数值取负并将结果压入栈顶 117 0x75 lneg 将栈顶 long 型数值取负并将结果压入栈顶 118 0x76 fneg 将栈顶 float 型数值取负并将结果压入栈顶 119 0x77 dneg 将栈顶 double 型数值取负并将结果压入栈顶 120 0x78 ishl 将 int 型数值左移指定位数并将结果压入栈顶 121 0x79 lshl 将 long 型数值左移指定位数并将结果压入栈顶 122 0x7a ishr 将 int 型数值右（带符号）移指定位数并将结果压入栈顶 123 0x7b lshr 将 long 型数值右（带符号）移指定位数并将结果压入栈顶 124 0x7c iushr 将 int 型数值右（无符号）移指定位数并将结果压入栈顶 125 0x7d lushr 将 long 型数值右（无符号）移指定位数并将结果压入栈顶 126 0x7e iand 将栈顶两 int 型数值按位与并将结果压入栈顶 127 0x7f land 将栈顶两 long 型数值按位与并将结果压入栈顶 128 0x80 ior 将栈顶两 int 型数值按位或并将结果压入栈顶 129 0x81 lor 将栈顶两 long 型数值按位或并将结果压入栈顶 130 0x82 ixor 将栈顶两 int 型数值按位异或并将结果压入栈顶 131 0x83 lxor 将栈顶两 long 型数值按位异或并将结果压入栈顶 132 0x84 iinc 将指定 int 型变量增加指定值（如 i++，i–，i+=2 等） 转换 Conversions 字节码（十进制） 字节码（十六进制） 助记符 指令含义 133 0x85 i2l 将栈顶 int 型数值强制转换为 long 型数值并将结果压入栈顶 134 0x86 i2f 将栈顶 int 型数值强制转换为 float 型数值并将结果压入栈顶 135 0x87 i2d 将栈顶 int 型数值强制转换为 double 型数值并将结果压入栈顶 136 0x88 l2i 将栈顶 long 型数值强制转换为 int 型数值并将结果压入栈顶 137 0x89 l2f 将栈顶 long 型数值强制转换为 float 型数值并将结果压入栈顶 138 0x8a l2d 将栈顶 long 型数值强制转换为 double 型数值并将结果压入栈顶 139 0x8b f2i 将栈顶 float 型数值强制转换为 int 型数值并将结果压入栈顶 140 0x8c f2l 将栈顶 float 型数值强制转换为 long 型数值并将结果压入栈顶 141 0x8d f2d 将栈顶 float 型数值强制转换为 double 型数值并将结果压入栈顶 142 0x8e d2i 将栈顶 double 型数值强制转换为 int 型数值并将结果压入栈顶 143 0x8f d2l 将栈顶 double 型数值强制转换为 long 型数值并将结果压入栈顶 144 0x90 d2f 将栈顶 double 型数值强制转换为 float 型数值并将结果压入栈顶 145 0x91 i2b 将栈顶 int 型数值强制转换为 byte 型数值并将结果压入栈顶 146 0x92 i2c 将栈顶 int 型数值强制转换为 char 型数值并将结果压入栈顶 147 0x93 i2s 将栈顶 int 型数值强制转换为 short 型数值并将结果压入栈顶 比较 Comparisons 字节码（十进制） 字节码（十六进制） 助记符 指令含义 148 0x94 lcmp 比较栈顶两 long 型数值大小，并将结果（1，0 或 -1）压入栈顶 149 0x95 fcmpl 比较栈顶两 float 型数值大小，并将结果（1，0 或 -1）压入栈顶; 当其中一个数值为 NaN 时，将 -1 压入栈顶 150 0x96 fcmpg 比较栈顶两 float 型数值大小，并将结果（1，0 或 -1）压入栈顶; 当其中一个数值为 NaN 时，将 1 压入栈顶 151 0x97 dcmpl 比较栈顶两 double 型数值大小，并将结果（1，0 或 -1）压入栈顶; 当其中一个数值为 NaN 时，将 -1 压入栈顶 152 0x98 dcmpg 比较栈顶两 double 型数值大小，并将结果（1，0 或 -1）压入栈顶; 当其中一个数值为 NaN 时，将 1 压入栈顶 153 0x99 ifeq 当栈顶 int 型数值等于 0 时跳转 154 0x9a ifne 当栈顶 int 型数值不等于 0 时跳转 155 0x9b iflt 当栈顶 int 型数值小于 0 时跳转 156 0x9c ifge 当栈顶 int 型数值大于等于 0 时跳转 157 0x9d ifgt 当栈顶 int 型数值大于 0 时跳转 158 0x9e ifle 当栈顶 int 型数值小于等于 0 时跳转 159 0x9f if_icmpeq 比较栈顶两 int 型数值大小，当结果等于 0 时跳转 160 0xa0 if_icmpne 比较栈顶两 int 型数值大小，当结果不等于 0 时跳转 161 0xa1 if_icmplt 比较栈顶两 int 型数值大小，当结果小于 0 时跳转 162 0xa2 if_icmpge 比较栈顶两 int 型数值大小，当结果大于等于 0 时跳转 163 0xa3 if_icmpgt 比较栈顶两 int 型数值大小，当结果大于 0 时跳转 164 0xa4 if_icmple 比较栈顶两 int 型数值大小，当结果小于等于 0 时跳转 165 0xa5 if_acmpeq 比较栈顶两引用型数值，当结果相等时跳转 166 0xa6 if_acmpne 比较栈顶两引用型数值，当结果不相等时跳转 控制 Control 字节码（十进制） 字节码（十六进制） 助记符 指令含义 167 0xa7 goto 无条件跳转 168 0xa8 jsr 跳转至指定的 16 位 offset 位置，并将 jsr 的下一条指令地址压入栈顶 169 0xa9 ret 返回至本地变量指定的 index 的指令位置（一般与 jsr 或 jsr_w 联合使用） 170 0xaa tableswitch 用于 switch 条件跳转，case 值连续（可变长度指令） 171 0xab lookupswitch 用于 switch 条件跳转，case 值不连续（可变长度指令） 172 0xac ireturn 从当前方法返回 int 173 0xad lreturn 从当前方法返回 long 174 0xae freturn 从当前方法返回 float 175 0xaf dreturn 从当前方法返回 double 176 0xb0 areturn 从当前方法返回对象引用 177 0xb1 return 从当前方法返回 void 引用 References 字节码（十进制） 字节码（十六进制） 助记符 指令含义 178 0xb2 getstatic 获取指定类的静态域，并将其压入栈顶 179 0xb3 putstatic 为指定类的静态域赋值 180 0xb4 getfield 获取指定类的实例域，并将其压入栈顶 181 0xb5 putfield 为指定类的实例域赋值 182 0xb6 invokevirtual 调用实例方法 183 0xb7 invokespecial 调用超类构造方法，实例初始化方法，私有方法 184 0xb8 invokestatic 调用静态方法 185 0xb9 invokeinterface 调用接口方法 186 0xba invokedynamic 调用动态方法 187 0xbb new 创建一个对象，并将其引用值压入栈顶 188 0xbc newarray 创建一个指定的原始类型（如 int，float，char 等）的数组，并将其引用值压入栈顶 189 0xbd anewarray 创建一个引用型（如类，接口，数组）的数组，并将其引用值压入栈顶 190 0xbe arraylength 获取数组的长度值并压入栈顶 191 0xbf athrow 将栈顶的异常抛出 192 0xc0 checkcast 检验类型转换，检验未通过将抛出 ClassCastException 193 0xc1 instanceof 检验对象是否是指定类的实际，如果是将 1 压入栈顶，否则将 0 压入栈顶 194 0xc2 monitorenter 获得对象的锁，用于同步方法或同步块 195 0xc3 monitorexit 释放对象的锁，用于同步方法或同步块 扩展 Extended 字节码（十进制） 字节码（十六进制） 助记符 指令含义 196 0xc4 wide 扩展本地变量的宽度 197 0xc5 multianewarray 创建指定类型和指定维度的多维数组（执行该指令时，操作栈中必须包含各维度的长度值），并将其引用压入栈顶 198 0xc6 ifnull 为 null 时跳转 199 0xc7 ifnonnull 不为 null 时跳转 200 0xc8 goto_w 无条件跳转（宽索引） 201 0xc9 jsr_w 跳转至指定的 32 位 offset 位置，并将 jsr_w 的下一条指令地址压入栈顶 保留 Reserved 字节码（十进制） 字节码（十六进制） 助记符 指令含义 202 0xca breakpoint 调试断点 254 0xfe impdep1 用于在特定硬件中使用的语言后门 255 0xff impdep2 用于在特定硬件中使用的语言后门 欢迎关注 欢迎关注「端小强的博客」微信公众号，会不定期分享日常学习和工作经验，欢迎大家关注交流。","tags":["JVM"],"categories":["JVM"]},{"title":"深度探究 Apache Calcite SQL 校验器实现原理","path":"/blog/in-depth-exploration-of-implementation-principle-of-apache-calcite-sql-validator.html","content":"注意：本文基于 Calcite 1.35.0 版本源码进行学习研究，其他版本可能会存在实现逻辑差异，对源码感兴趣的读者请注意版本选择。 前言 在上一篇 Apache Calcite System Catalog 实现探究中，我们介绍了经典的数据库处理流程，包括：SQL 解析、SQL 绑定、SQL 优化以及计划执行。SQL 绑定主要的作用是将 SQL 解析生成的 AST 和数据库的元数据进行绑定，从而生成具有语义的 AST。SQL 绑定会通过自底向上的方式遍历 AST，对抽象语法树中的节点进行绑定分析，绑定的过程中会将表、列等元数据附在语法树上，最后生成具有语义的语法树 Bounded AST。 Calcite 通过 SQL 校验器实现 SQL 绑定，SQL 校验器所需的 System Catalog 信息，我们在上篇文章已经做了详细的介绍，感兴趣的读者可以阅读回顾相关内容。本文将重点介绍 Calcite SQL 校验器的整体设计，梳理校验器中不同类的用途，然后通过具体案例来展示 SQL 校验器的整体流程，并对流程中的关键方法进行代码级别的分析，力求让大家能够深刻理解 Calcite 的 SQL 校验器。 SQL 校验器整体设计 SQL 校验器的核心类为 SqlValidator，它负责使用 Calcite 元数据信息对 AST 进行验证，最终生成具有语义信息的 AST。在 Calcite 中，可以通过 SqlValidatorUtil.newValidator 方法快速创建一个 SqlValidator。 除了 SqlValidator 校验器类之外，Calcite 为了将 SQL 中的名称解析为对象，还在校验器内部构建了两个对象：SqlValidatorScope 和 SqlValidatorNamespace，SqlValidatorScope 表示名称解析的范围，代表了在查询中的某一个位置，当前可见的字段名和表名。SqlValidatorNamespace 则表示了校验过程中查询语句的数据来源，不同的查询位置都有不同类型的 namespace 类，例如：表名对应的 IdentifierNamespace，Select 语句对应的 SelectNamespace，以及 UNION、EXCEPT、INTERSECT 对应的 SetopNamespace。下面我们针对核心的 SqlValidator、SqlValidatorScope 和 SqlValidatorNamespace 分别进行探究，了解其设计细节以及适用场景。 SqlValidator SqlValidator 校验器根据元数据对 SQL 解析的 AST 进行校验，得到具有语义信息的绑定 AST。SqlValidator 通过访问者模式对 AST 进行校验，调用 SqlNode#validate 方法时，校验器内部会调用 validateXxx 方法，例如：调用 SqlLiteral.validate(SqlValidator, SqlValidatorScope) 会调用 validateLiteral(SqlLiteral); ，调用 SqlCall.validate(SqlValidator, SqlValidatorScope) 则会调用 validCall(SqlCall, SqlValidatorScope);。 SqlValidator 接口定义了 Calcite 校验器的主要方法，它提供了基础的 getCatalogReader 和 getOperatorTable 方法，分别用于获取元数据信息和运算符、函数。校验 SqlNode 则是通过 validate 方法，会按照 AST 结构进行遍历校验，最终返回已校验 SqlNode。 public interface SqlValidator // 获取校验器使用的 CatalogReader，用于获取元数据信息 SqlValidatorCatalogReader getCatalogReader(); // 获取校验器使用的 SqlOperatorTable，用于获取运算符和函数 SqlOperatorTable getOperatorTable(); // 校验 SqlNode 对应的表达式树，返回已校验的树 SqlNode validate(SqlNode topNode); // 获取已验证节点的类型 RelDataType getValidatedNodeType(SqlNode node); // 获取 SqlNode 所属的 Namespace SqlValidatorNamespace getNamespace(SqlNode node); // 展开 * 号对应的列 SqlNodeList expandStar(SqlNodeList selectList, SqlSelect query, boolean includeSystemVars); // 展开 order by 子句中的序号和别名列 SqlNode expandOrderExpr(SqlSelect select, SqlNode orderExpr); // 返回 SqlNode 结果集列的原始类型，该类型中包含 catalog, schema, table, column List@Nullable ListString getFieldOrigins(SqlNode sqlQuery); 此外，为了对 SqlValidator 校验过程中的一些行为进行控制，Calcite 提供了 SqlValidator#Config 配置类，通过 withXxx 方法可以方便地设置校验器的属性，常见的属性设置方法如下，withDefaultNullCollation 可以设置 NULL 值排序规则，withColumnReferenceExpansion 则可以用于指定 Order By 语句中的列引用是否展开，withConformance 方法用于设置 SQL 兼容模式。 interface Config // 默认 SqlValidator 配置类 SqlValidator.Config DEFAULT = ImmutableSqlValidator.Config.builder().withTypeCoercionFactory(TypeCoercions::createTypeCoercion).build(); // 配置默认 NULL 值排序规则 Config withDefaultNullCollation(NullCollation nullCollation); // 配置列引用是否展开 Config withColumnReferenceExpansion(boolean expand); // 配置 SQL 兼容模式 Config withConformance(SqlConformance conformance); SqlValidatorScope SqlValidatorScope 主要用于声明校验过程中名称解析的范围，Calcite 对 SqlValidatorScope 的具体描述为 A SqlValidatorScope describes the tables and columns accessible at a particular point in the query，即：SqlValidatorScope 描述了查询中的某个具体位置可以访问的表和列。 Calcite 根据不同的 SQL 类型实现了众多 SqlValidatorScope 子类，以满足不同场景下的 SQL 校验需求，SqlValidatorScope 继承体系如下： SelectScope 表示查询语句的名称解析范围，该范围中可见的对象包含了 FROM 子句的对象以及从父节点继承的对象。如下展示了一个常见的查询语句，该语句中包含了关联查询、子查询以及排序。 SELECT expr1 FROM t1, t2, (SELECT expr2 FROM t3) AS q3 WHERE c1 IN (SELECT expr3 FROM t4) ORDER BY expr4 Calcite 会将该语句拆分为 4 个 SelectScope 分别表示不同表达式对象的可见范围。 expr1 可以访问 t1, t2, q3 中的对象； expr2 可以访问 t3 中的对象； expr3 可以访问 t4, t1, t2 中的对象（实际测试 MySQL，expr3 同样可以访问 q3 临时表）； expr4 可以访问 t1, t2, q3, 以及在 SELECT 子句中定义的任何列别名（取决于方言）。 SqlValidatorNamespace SqlValidatorNamespace 描述了由 SQL 查询某个部分返回的关系（Relation，关系是一组无序的元素或记录，这些元素或记录的属性用来表示实体），例如：在查询 SELECT emp.deptno, age FROM emp, dept 时，FROM 子句形成了一个包含 emp 和 dept 两张表，以及这些表中列组成的行类型在内的命名空间。不同的 RelNode 类型有与之对应的 Namespace 对象，下图展示了 Calcite 中定义的常见 SqlValidatorNamespace 实现类。 SelectNamespace 表示了查询语句对应的命名空间，我们同样以如下的查询语句为例： SELECT expr1 FROM t1, t2, (SELECT expr2 FROM t3) AS q3 WHERE c1 IN (SELECT expr3 FROM t4) ORDER BY expr4 Calcite 会从查询语句中提取出 4 个命名空间，分别如下所示，命名空间代表的关系，简单理解可以认为是查询过程中的数据来源。 t1：t1 表所代表的关系； t2：t2 表所代表的关系； (SELECT expr2 FROM t3) AS q3：子查询所代表的关系； (SELECT expr3 FROM t4)：子查询所代表的关系。 (adsbygoogle = window.adsbygoogle || []).push({}); 赞助商 SQL 校验器执行流程 前文我们对 Caclite 校验器中核心的 SqlValidator、SqlValidatorScope 和 SqlValidatorNamespace 类进行了介绍，想必大家对校验器有了一些基础的认识。本节我们通过如下所示的 CsvTest#testPushDownProjectAggregateNested 单测，来跟踪下 SQL 校验器的执行流程，该示例 SQL 中包含了常用的子查询、聚合查询以及 MAX 和 COUNT 聚合函数，可以帮助大家了解这些核心类在校验流程中是如何使用的。 final String sql = explain plan + extra + for + select gender, max(qty) + from ( + select name, gender, count(*) qty + from EMPS + group by name, gender) t + group by gender; SqlValidator 初始化 首先，会初始化 SqlValidator 对象，初始化时会将校验器所需的 SqlOperatorTable 和 SqlValidatorCatalogReader 等对象传入进来，SqlOperatorTable 用于查找 SQL 运算符和函数，SqlValidatorCatalogReader 则用于校验时查找元数据信息。此外，初始化时还会创建不同的 AggFinder 对象，用于后续从 AST 中提取不同的聚合函数，以及创建 TypeCoercion 类型转换类，它主要用于 SQL 中可能存在的隐式类型转换。 protected SqlValidatorImpl(SqlOperatorTable opTab, SqlValidatorCatalogReader catalogReader, RelDataTypeFactory typeFactory, Config config) // 初始化 SqlOperatorTable，用于查找 SQL 运算符和函数 this.opTab = requireNonNull(opTab, opTab); // 用于查找元数据信息 this.catalogReader = requireNonNull(catalogReader, catalogReader); this.typeFactory = requireNonNull(typeFactory, typeFactory); final RelDataTypeSystem typeSystem = typeFactory.getTypeSystem(); // 获取类型系统中的时间框架集合 this.timeFrameSet = requireNonNull(typeSystem.deriveTimeFrameSet(TimeFrames.CORE), timeFrameSet); this.config = requireNonNull(config, config); // It is assumed that unknown type is nullable by default unknownType = typeFactory.createTypeWithNullability(typeFactory.createUnknownType(), true); booleanType = typeFactory.createSqlType(SqlTypeName.BOOLEAN); final SqlNameMatcher nameMatcher = catalogReader.nameMatcher(); // 初始化 AggFinder，用于从 AST 中遍历获取不同的聚合函数 aggFinder = new AggFinder(opTab, false, true, false, null, nameMatcher); aggOrOverFinder = new AggFinder(opTab, true, true, false, null, nameMatcher); overFinder = new AggFinder(opTab, true, false, false, aggOrOverFinder, nameMatcher); groupFinder = new AggFinder(opTab, false, false, true, null, nameMatcher); aggOrOverOrGroupFinder = new AggFinder(opTab, true, true, true, null, nameMatcher); // 初始化类型转换类，用于隐式类型转换 TypeCoercion typeCoercion = config.typeCoercionFactory().create(typeFactory, this); this.typeCoercion = typeCoercion; ... validate 流程 介绍完 SqlValidator 初始化逻辑，我们再来深入探究下校验器的核心逻辑 validate 方法，其实现逻辑如下。首先会创建用于 root 节点校验的 EmptyScope，并将 EmptyScope 作为 CatalogScope 的父类，CatalogScope 可以查看所有 Schema 中的元数据，在校验过程中能够帮助对 schema.table.column 进行列解析。 @Overridepublic SqlNode validate(SqlNode topNode) // 创建 EmptyScope 用于 root 节点校验 SqlValidatorScope scope = new EmptyScope(this); // CatalogScope 可以查看所有 Schema 中的元数据，它的父类是 EmptyScope scope = new CatalogScope(scope, ImmutableList.of(CATALOG)); // 校验名称解析范围内的表达式 final SqlNode topNode2 = validateScopedExpression(topNode, scope); // 获取校验后的节点类型 final RelDataType type = getValidatedNodeType(topNode2); Util.discard(type); return topNode2; 然后会调用 validateScopedExpression 进行校验，这部分是 SQL 校验器的核心逻辑。下面展示了该方法的代码实现，内部依次调用了 performUnconditionalRewrites、registerQuery 和 validate 方法，我们将对这些方法内部实现细节进行深入探究。 private SqlNode validateScopedExpression(SqlNode topNode, SqlValidatorScope scope) // 重写 SqlNode 进行标准化，以方便后续的逻辑计划优化 SqlNode outermostNode = performUnconditionalRewrites(topNode, false); cursorSet.add(outermostNode); top = outermostNode; if (outermostNode.isA(SqlKind.TOP_LEVEL)) // 注册 Scope 和 Namespace registerQuery(scope, null, outermostNode, outermostNode, null, false); // 校验 SqlNode outermostNode.validate(this, scope); if (!outermostNode.isA(SqlKind.TOP_LEVEL)) // 推断类型 deriveType(scope, outermostNode); return outermostNode; performUnconditionalRewrites 首先，我们来探究下 performUnconditionalRewrites 的内部实现逻辑，它主要用于 SqlNode 重写标准化，从而方便后续的逻辑计划优化。方法内部会先判断当前 SqlNode 的类型，根据 SqlCall 和 SqlNodeList 分别进行处理。Calcite SqlNode 体系我们之前在 Apache Calcite SQL Parser 原理剖析中已经进行了详细介绍，不熟悉的朋友可以阅读下这篇文章。 如果当前 SqlNode 是 SqlCall（SqlCall 代表了对 SqlOperator 的调用，Calcite 中每个操作都可以对应一个 SqlCall，例如查询操作是 SqlSelectOperator，对应的 SqlNode 是 SqlSelect），则会获取 SqlCall 对应的 SqlKind 和 OperandList。SqlKind 是一个枚举类，表示了 SqlNode 对应的类型，常用的类型有：SELECT、INSERT、ORDER_BY、WITH 等，更多类型可以查看 SqlKind 源码。 // 判断当前 SqlNode 是否为 SqlCallif (node instanceof SqlCall) ... SqlCall call = (SqlCall) node; // 获取 SqlKind 类型 final SqlKind kind = call.getKind(); // 获取 SqlNode 中包含的运算符 final ListSqlNode operands = call.getOperandList(); for (int i = 0; i operands.size(); i++) SqlNode operand = operands.get(i); ... // 每一个运算法调用 performUnconditionalRewrites 并设置到 SqlCall 中 SqlNode newOperand = performUnconditionalRewrites(operand, childUnderFrom); if (newOperand != null newOperand != operand) call.setOperand(i, newOperand); // 当前运算符为未解析函数 SqlUnresolvedFunction if (call.getOperator() instanceof SqlUnresolvedFunction) final SqlUnresolvedFunction function = (SqlUnresolvedFunction) call.getOperator(); final ListSqlOperator overloads = new ArrayList(); // 从 SqlOperatorTable 中查找函数，查找的范围包括内置函数以及元数据中注册的函数 opTab.lookupOperatorOverloads(function.getNameAsId(), function.getFunctionType(), SqlSyntax.FUNCTION, overloads, catalogReader.nameMatcher()); if (overloads.size() == 1) // 查找到函数则设置新的运算符 ((SqlBasicCall) call).setOperator(overloads.get(0)); 然后对 OperandList 进行遍历，此处 SqlCall 为 SqlSelect，而 SqlSelect 中的 OperandList 会按照 keywordList, selectList, from, where, groupBy, having, windowDecls, qualify, orderBy, offset, fetch, hints 的顺序进行返回，后续操作都会基于这个从 0 开始的顺序进行处理。下图展示了单测中 SQL 对应的初始 SqlNode，OperandList 包含了下图所示的 keywordList、selectList 等运算符，会逐个调用 performUnconditionalRewrites 进行处理。 如果当前 SqlCall 运算符为未解析函数 SqlUnresolvedFunction，则会调用 SqlOperatorTable#lookupOperatorOverloads 方法，从内置函数以及元数据中查找函数，并重新设置运算符。 如果 SqlNode 为 SqlNodeList，则会遍历其中的 SqlNode。本案例中 selectList 就是 SqlNodeList 类型，此时会遍历 SqlNode 并调用 performUnconditionalRewrites，然后将新的 SqlNode 设置到 SqlNodeList 中。 // 判断当前 SqlNode 是否为 SqlNodeList else if (node instanceof SqlNodeList) // SqlNodeList 会遍历其中的 SqlNode 并调用 performUnconditionalRewrites，并将新的 SqlNode 设置到 SqlNodeList 中 final SqlNodeList list = (SqlNodeList) node; for (int i = 0; i list.size(); i++) SqlNode operand = list.get(i); SqlNode newOperand = performUnconditionalRewrites(operand, false); if (newOperand != null) list.set(i, newOperand); 除了前面介绍的会将 SqlUnresolvedFunction 转换为对应解析函数外，performUnconditionalRewrites 主要会将如下的非标准 SqlNode 转换为标准的 SqlNode，具体包括：VALUES、ORDER_BY、EXPLICIT_TABLE、DELETE、UPDATE 和 MERGE。VALUES 改写由于在类似 FROM (VALUES(...)) [ AS alias ] 场景中存在问题，目前已经不进行改写。ORDER_BY 语句则会被改写为 SqlSelect 和 SqlWith，并将排序相关的子句下沉到 SqlSelect 中。EXPLICIT_TABLE 则会将 将 Table t 子句改写为 SELECT * FROM t 语句。DELETE 和 UPDATE 会根据条件改写生成 SqlSelect，代表了删除和更新语句所需要删除和更新的数据。MERGE 改写会将 SqlMerge 改写为 SqlUpdate 或 SqlInsert，以复用 SqlUpdate 和 SqlInsert 的处理逻辑。 final SqlKind kind = node.getKind();switch (kind) case VALUES: // VALUES 改写由于在类似 FROM (VALUES(...)) [ AS alias ] 场景中存在问题，目前已经不进行改写 return node; case ORDER_BY: SqlOrderBy orderBy = (SqlOrderBy) node; handleOffsetFetch(orderBy.offset, orderBy.fetch); if (orderBy.query instanceof SqlSelect) SqlSelect select = (SqlSelect) orderBy.query; // 将 SqlOrderBy 转换为 SqlSelect，排序子句下沉到 SqlSelect 中 if (select.getOrderList() == null) // push ORDER BY into existing select select.setOrderBy(orderBy.orderList); select.setOffset(orderBy.offset); select.setFetch(orderBy.fetch); return select; // 将 SqlOrderBy 转换为 SqlWith，排序子句下沉到 SqlWith 的 SqlSelect 子句中 if (orderBy.query instanceof SqlWith ((SqlWith) orderBy.query).body instanceof SqlSelect) SqlWith with = (SqlWith) orderBy.query; SqlSelect select = (SqlSelect) with.body; if (select.getOrderList() == null) // push ORDER BY into existing select select.setOrderBy(orderBy.orderList); select.setOffset(orderBy.offset); select.setFetch(orderBy.fetch); return with; final SqlNodeList selectList = new SqlNodeList(SqlParserPos.ZERO); selectList.add(SqlIdentifier.star(SqlParserPos.ZERO)); final SqlNodeList orderList; SqlSelect innerSelect = getInnerSelect(node); ... return new SqlSelect(SqlParserPos.ZERO, null, selectList, orderBy.query, null, null, null, null, null, orderList, orderBy.offset, orderBy.fetch, null); case EXPLICIT_TABLE: // 将 Table t 子句改写为 SELECT * FROM t // (TABLE t) is equivalent to (SELECT * FROM t) SqlCall call = (SqlCall) node; final SqlNodeList selectList = new SqlNodeList(SqlParserPos.ZERO); selectList.add(SqlIdentifier.star(SqlParserPos.ZERO)); return new SqlSelect(SqlParserPos.ZERO, null, selectList, call.operand(0), null, null, null, null, null, null, null, null, null); case DELETE: SqlDelete call = (SqlDelete) node; SqlSelect select = createSourceSelectForDelete(call); // 为 SqlDelete 增加待删除数据的 SqlSelect 子句 call.setSourceSelect(select); break; case UPDATE: SqlUpdate call = (SqlUpdate) node; SqlSelect select = createSourceSelectForUpdate(call); // 为 SqlUpdate 增加待删除数据的 SqlSelect 子句 call.setSourceSelect(select); ... break; case MERGE: // 将 SqlMerge 改写为 SqlUpdate 或 SqlInsert SqlMerge call = (SqlMerge) node; rewriteMerge(call); break; default: break;return node; 执行完 performUnconditionalRewrites 方法，最终生成的 SqlNode 如下图所示，可以看到 SqlUnresolvedFunction 函数已经被改写为对应的内置函数，由于本示例中没有包含 OrderBy、Update 等语句，感兴趣的读者可以自行调整单测中的 SQL 以覆盖对应的逻辑。 registerQuery 对 SqlNode 重写标准化之后，Calcite 会调用 registerQuery 方法生成 SqlValidatorScope 和 SqlValidatorNamespace 对象，这两个对象在前文中已经进行了介绍，他们分别用于声明校验过程中名称解析的范围，以及描述由 SQL 查询某个部分返回的关系（简单理解可以认为是查询过程中的数据来源）。 在调用 registerQuery 方法前，Calcite 会判断当前的 SqlNode 是否为顶层节点，SqlKind 类中定义的顶层节点包含了 QUERY、DML 和 DDL 语句，而 QUERY 中又包含了 SELECT、UNION、INTERSECT、EXCEPT、VALUES、WITH、ORDER_BY 和 EXPLICIT_TABLE 等具体类型，更多类型可以参考 SqlKind 源码。 private SqlNode validateScopedExpression(SqlNode topNode, SqlValidatorScope scope) ... // 判断 SqlNode 是否为顶层节点 if (outermostNode.isA(SqlKind.TOP_LEVEL)) registerQuery(scope, null, outermostNode, outermostNode, null, false); ... return outermostNode;// SqlKind 中定义的 TOP_LEVELpublic static final EnumSetSqlKind TOP_LEVEL = concat(QUERY, DML, DDL);public static final EnumSetSqlKind QUERY = EnumSet.of(SELECT, UNION, INTERSECT, EXCEPT, VALUES, WITH, ORDER_BY, EXPLICIT_TABLE); 判断 SqlNode 为顶层节点后， 会继续调用 registerQuery 方法并传入参数，下面展示了 registerQuery 方法的实现，parentScope 表示当前 SqlNode 的父名称解析范围，顶层节点的 parentScope 为 CatalogScope（CatalogScope 可以查看所有 Schema 中的元数据，它的父类是 EmptyScope）。usingScope 则用于添加当前 Scope 需要使用的子 Scope，node 表示当前的 SqlNode，enclosingNode 和 node 通常是相同的，enclosingNode 通常表示 FROM 子句最顶层的节点 ，可以从 enclosingNode 中获取别名等信息，alias 表当前查询在父查询中的名称，如果 usingScope 不为空则必须指定 alias。 private void registerQuery(SqlValidatorScope parentScope, @Nullable SqlValidatorScope usingScope, SqlNode node, SqlNode enclosingNode, @Nullable String alias, boolean forceNullable) Preconditions.checkArgument(usingScope == null || alias != null); registerQuery(parentScope, usingScope, node, enclosingNode, alias, forceNullable, true); registerQuery 方法内部会根据 SqlKind 进行判断，对不同类型的 SQL 进行处理，本文示例为 SELECT 语句，因此我们先专注于 SELECT 相关的逻辑。下面展示了 registerQuery 方法的实现逻辑，首先会创建 SelectNamespace 对象，该对象会记录 SqlSelect 及当前的校验器。然后调用 registerNamespace 方法，将 SelectNamespace 记录在 namespaces 对象中，namespaces 的结构为 SqlNode, SqlValidatorNamespace，如果 usingScope 不为 null，则将当前 SelectNamespace 注册为 usingScope 的子节点。再创建 SelectScope，并将其记录到全局的 scopes 中，scopes 的结构为 SqlNode, SqlValidatorScope。 SqlCall call;ListSqlNode operands;switch (node.getKind()) case SELECT: final SqlSelect select = (SqlSelect) node; // 创建 SelectNamespace，记录 SqlSelect 和当前的校验器 final SelectNamespace selectNs = createSelectNamespace(select, enclosingNode); // 将 SelectNamespace 记录在 namespaces 对象中，namespaces 的结构为 `SqlNode, SqlValidatorNamespace` // 如果 usingScope 不为 null，则将当前 SelectNamespace 注册为 usingScope 的子节点 registerNamespace(usingScope, alias, selectNs, forceNullable); // 选择第一个不为 null 的 scope 作为 windowParentScope，此处为 CatalogScope final SqlValidatorScope windowParentScope = first(usingScope, parentScope); // 创建 SelectScope，会记录 parentScope，windowParentScope 以及 SqlSelect SelectScope selectScope = new SelectScope(parentScope, windowParentScope, select); // 将 selectScope 记录到全局的 scopes 中, scopes 的结构为 SqlNode, SqlValidatorScope scopes.put(select, selectScope); ... 然后 Calcite 会对各个不同的子句进行注册，首先会从 WHERE 语句开始，并将 WHERE 对应的 Scope 存储到 clauseScopes 中，再调用 registerOperandSubQueries 注册 WHERE 运算符中的子查询，最终整个方法会将 SELECT 语句中的 Projection、From、Where、Order By、Group By、Having 等子句都进行注册，生成对应的 Scope 和 Namespace 对象。 SqlCall call;ListSqlNode operands;switch (node.getKind()) case SELECT: ... // Start by registering the WHERE clause // clauseScopes 用于记录 Select 中的子句，用于后续子句的注册 clauseScopes.put(IdPair.of(select, Clause.WHERE), selectScope); // 注册 WHERE 运算符中的子查询 registerOperandSubQueries(selectScope, select, SqlSelect.WHERE_OPERAND); // 注册 QUALIFY 子句中的子查询，QUALIFY 子句通常用于 WINDOW 函数结果的过滤 registerOperandSubQueries( selectScope, select, SqlSelect.QUALIFY_OPERAND); // Register FROM with the inherited scope parentScope, not // selectScope, otherwise tables in the FROM clause would be // able to see each other. final SqlNode from = select.getFrom(); if (from != null) // 注册 FROM，本示例中包含了 FROM 子查询，因此子查询会再次调用 registerQuery final SqlNode newFrom = registerFrom(parentScope, selectScope, true, from, from, null, null, false, false); if (newFrom != from) select.setFrom(newFrom); // If this is an aggregating query, the SELECT list and HAVING // clause use a different scope, where you can only reference // columns which are in the GROUP BY clause. SqlValidatorScope aggScope = selectScope; if (isAggregate(select)) aggScope = new AggregatingSelectScope(selectScope, select, false); clauseScopes.put(IdPair.of(select, Clause.SELECT), aggScope); else clauseScopes.put(IdPair.of(select, Clause.SELECT), selectScope); if (select.getGroup() != null) GroupByScope groupByScope = new GroupByScope(selectScope, select.getGroup(), select); clauseScopes.put(IdPair.of(select, Clause.GROUP_BY), groupByScope); // 注册 GROUP BY 中的子句 registerSubQueries(groupByScope, select.getGroup()); // 注册 HAVING 运算符中的子查询 registerOperandSubQueries(aggScope, select, SqlSelect.HAVING_OPERAND); // 注册投影列中的子查询，并会将子查询转换为标量子查询 registerSubQueries(aggScope, SqlNonNullableAccessors.getSelectList(select)); final SqlNodeList orderList = select.getOrderList(); if (orderList != null) // If the query is SELECT DISTINCT, restrict the columns // available to the ORDER BY clause. if (select.isDistinct()) aggScope = new AggregatingSelectScope(selectScope, select, true); OrderByScope orderScope = new OrderByScope(aggScope, orderList, select); clauseScopes.put(IdPair.of(select, Clause.ORDER), orderScope); // 注册 ORDER BY 中的子句 registerSubQueries(orderScope, orderList); // ... break; registerQuery 执行完成后，注册的信息会存储在 SqlValidatorImpl 对象中，下图展示了对象中存储的内容，分别包括了 scopes、clauseScopes 和 namespaces。scopes 对象用于存储查询节点和它们对应的 SqlValidatorScope 之间的映射，此案例中查询节点为两个 SELECT 语句，分别对应了不同的 SelectScope 对象。clauseScopes 对象则用于存储子句和 SqlValidatorScope 之间的映射，用于表示当前子句可见的范围。namespaces 代表了数据来源，此案例中共有 3 个 namespace，分别是：EMPS 代表的表数据源，以及另外两个查询数据源。 validate 完成 registerQuery 后，校验器中已经包含了校验所需的 Scope 和 Namespace 对象，下面我们再来探究下核心的校验逻辑。校验器首先会调用 SqlNode#validate 方法，该方法在不同的 SqlNode 中具有不同的实现。由于本文案例为查询语句，因此先关注 SqlSelect 的实现逻辑，其它类型的 SqlNode 读者可以自行探究。 // 校验 SqlNodeoutermostNode.validate(this, scope);// SqlSelect#validate 实现@Overridepublic void validate(SqlValidator validator, SqlValidatorScope scope) validator.validateQuery(this, scope, validator.getUnknownType()); 从上面的源码可以看到，SqlSelect#validate 方法会调用校验器的 validateQuery 方法，validateQuery 中核心的处理逻辑为 validateNamespace 和 validateAccess，分别校验 Namespace 以及对表的访问。 @Overridepublic void validateQuery(SqlNode node, SqlValidatorScope scope, RelDataType targetRowType) ... // 校验 Namespace validateNamespace(ns, targetRowType); switch (node.getKind()) case EXTEND: // Until we have a dedicated namespace for EXTEND deriveType(requireNonNull(scope, scope), node); break; default: break; if (node == top) validateModality(node); // 校验对表的访问 validateAccess(node, ns.getTable(), SqlAccessEnum.SELECT); validateSnapshot(node, scope, ns); validateNamespace validateNamespace 方法实现逻辑如下，内部会调用 SqlValidatorNamespace#validate 方法，然后设置校验后的节点类型。 /** * Validates a namespace. * * @param namespace Namespace * @param targetRowType Desired row type, must not be null, may be the data * type unknown. */protected void validateNamespace(final SqlValidatorNamespace namespace, RelDataType targetRowType) // 调用 SqlValidatorNamespace 实现类的 validate namespace.validate(targetRowType); SqlNode node = namespace.getNode(); if (node != null) // 设置已校验节点类型 setValidatedNodeType(node, namespace.getType()); 本案例中 SqlValidatorNamespace 实现类为 SelectNamespace，SelectNamespace 则继承了抽象类 AbstractNamespace，在调用 SelectNamespace#validate 方法时，会调用父类 AbstractNamespace#validate 方法，该方法会根据 AbstractNamespace 中记录的 status 状态决定如何处理。如果 status 为 UNVALIDATED 状态，则会调用 validateImpl 方法，validateImpl 方法是一个抽象方法，具体的实现逻辑在 AbstractNamespace 子类中。 @Overridepublic final void validate(RelDataType targetRowType) switch (status) case UNVALIDATED: try status = SqlValidatorImpl.Status.IN_PROGRESS; Preconditions.checkArgument(rowType == null, Namespace.rowType must be null before validate has been called); RelDataType type = validateImpl(targetRowType); Preconditions.checkArgument(type != null, validateImpl() returned null); setType(type); finally status = SqlValidatorImpl.Status.VALID; break; case IN_PROGRESS: throw new AssertionError(Cycle detected during type-checking); case VALID: break; default: throw Util.unexpected(status); protected abstract RelDataType validateImpl(RelDataType targetRowType); SelectNamespace#validateImpl 内部则会调用 validator.validateSelect 方法，SqlValidatorImpl#validateSelect 具体实现逻辑如下，方法内会依次调用 validateFrom、validateWhereClause、validateGroupClause、validateHavingClause、validateWindowClause、validateQualifyClause、validateSelectList 和 validateOrderList 子句。 protected void validateSelect(SqlSelect select, RelDataType targetRowType) // Namespace is either a select namespace or a wrapper around one. final SelectNamespace ns = getNamespaceOrThrow(select).unwrap(SelectNamespace.class); // 获取 Select 语句中的投影列表 final SqlNodeList selectItems = SqlNonNullableAccessors.getSelectList(select); RelDataType fromType = unknownType; ... // Make sure that items in FROM clause have distinct aliases. final SelectScope fromScope = (SelectScope) getFromScope(select); ... if (select.getFrom() == null) if (this.config.conformance().isFromRequired()) throw newValidationError(select, RESOURCE.selectMissingFrom()); else // 校验 From 子句 validateFrom(select.getFrom(), fromType, fromScope); // 校验 Where 子句 validateWhereClause(select); // 校验 Group 子句 validateGroupClause(select); // 校验 Having 子句 validateHavingClause(select); // 校验 Window 子句 validateWindowClause(select); // 校验 Qualify 子句 validateQualifyClause(select); handleOffsetFetch(select.getOffset(), select.getFetch()); // Validate the SELECT clause late, because a select item might // depend on the GROUP BY list, or the window function might reference // window name in the WINDOW clause etc. // 校验 Select 投影列 final RelDataType rowType = validateSelectList(selectItems, select, targetRowType); ns.setType(rowType); // Validate ORDER BY after we have set ns.rowType because in some // dialects you can refer to columns of the select list, e.g. // SELECT empno AS x FROM emp ORDER BY x // 校验 Order 子句 validateOrderList(select); ... 从前文 registerQuery 结果可以看出，From 子句对应的 Namespace 为 IdentifierNamespace，执行 validateFrom 时内部会调用 IdentifierNamespace#validateImpl 方法，该方法内部实现逻辑如下。 @Overridepublic RelDataType validateImpl(RelDataType targetRowType) // 从元数据中解析表名，并组装为 TableNamespace resolvedNamespace = resolveImpl(id); if (resolvedNamespace instanceof TableNamespace) // 从 TableNamespace 中获取 table SqlValidatorTable table = ((TableNamespace) resolvedNamespace).getTable(); // 判断是否需要将 identifier 展开为全限定名称 if (validator.config().identifierExpansion()) // TODO: expand qualifiers for column references also ListString qualifiedNames = table.getQualifiedName(); if (qualifiedNames != null) // Assign positions to the components of the fully-qualified // identifier, as best we can. We assume that qualification // adds names to the front, e.g. FOO.BAR becomes BAZ.FOO.BAR. ListSqlParserPos poses = new ArrayList(Collections.nCopies(qualifiedNames.size(), id.getParserPosition())); int offset = qualifiedNames.size() - id.names.size(); // Test offset in case catalog supports fewer qualifiers than catalog reader. if (offset = 0) for (int i = 0; i id.names.size(); i++) poses.set(i + offset, id.getComponentParserPosition(i)); id.setNames(qualifiedNames, poses); RelDataType rowType = resolvedNamespace.getRowType(); ... return rowType; resolveImpl 方法会根据 id 从元数据中解析出表名，并封装为 TableNamespace 对象，下图展示了解析之后的对象，对象中包含了从元数据中获取的 table 对象。然后会判断是否需要将 identifier 展开为全限定名称，此案例中会将 EMPS 展开为 SALES.EMPS 并设置到 names 属性中。 看完 From 子句校验，我们再来看下 Select 投影列校验逻辑，首先会遍历 SELECT 投影列，如果投影列仍然是一个子查询，则会调用 handleScalarSubQuery 处理标量子查询，判断是否符合标量子查询的规范。否则会调用 expandSelectItem 方法展开投影列，将 NAME 展开为 EMPS.NAME（expandSelectItem 方法也可以将星号展开为全部列，本示例中没有涉及，感兴趣读者可以自行探究）。展开后的投影列 newSelectList 会分别设置到 select 节点和 selectScope 中，方便后续校验继续使用。 protected RelDataType validateSelectList(final SqlNodeList selectItems, SqlSelect select, RelDataType targetRowType) // Validate SELECT list. Expand terms of the form * or TABLE.*. final SqlValidatorScope selectScope = getSelectScope(select); final ListSqlNode expandedSelectItems = new ArrayList(); final SetString aliases = new HashSet(); final PairListString, RelDataType fieldList = PairList.of(); // 遍历 selectItems 投影列，如果是 SqlSelect 则处理标量子查询 for (SqlNode selectItem : selectItems) if (selectItem instanceof SqlSelect) handleScalarSubQuery(select, (SqlSelect) selectItem, expandedSelectItems, aliases, fieldList); else // Use the field list size to record the field index // because the select item may be a STAR(*), which could have been expanded. final int fieldIdx = fieldList.size(); final RelDataType fieldType = targetRowType.isStruct() targetRowType.getFieldCount() fieldIdx ? targetRowType.getFieldList().get(fieldIdx).getType() : unknownType; // 展开投影列，将 NAME 展开为 EMPS.NAME expandSelectItem(selectItem, select, fieldType, expandedSelectItems, aliases, fieldList, false); // Create the new select list with expanded items. Pass through // the original parser position so that any overall failures can // still reference the original input text. SqlNodeList newSelectList = new SqlNodeList(expandedSelectItems, selectItems.getParserPosition()); if (config.identifierExpansion()) // 将展开后的 newSelectList 设置到 select 中 select.setSelectList(newSelectList); // 将展开后的 newSelectList 设置到 selectScope 中 getRawSelectScopeNonNull(select).setExpandedSelectList(expandedSelectItems); // TODO: when SELECT appears as a value sub-query, should be using // something other than unknownType for targetRowType inferUnknownTypes(targetRowType, selectScope, newSelectList); for (SqlNode selectItem : expandedSelectItems) validateNoAggs(groupFinder, selectItem, SELECT); validateExpr(selectItem, selectScope); return typeFactory.createStructType(fieldList); 其他子句的校验作用类似，会将 Identifier 展开为全限定名，并对语句的合法性进行校验，感兴趣的读者可以使用前文的案例进行 Debug 探究，如有疑问可以留言交流。 validateAccess validateAccess 方法用于校验当前节点对表是否具有访问权限，node 参数表示当前节点，table 表示要访问的表，requiredAccess 则表示需要访问的类型。table.getAllowedAccess() 会从 RelOptTableImpl#getAllowedAccess 获取可访问类型，会返回全部类型，包括：SELECT、UPDATE、INSERT 和 DELETE，如果不具有访问权限，则抛出异常。 /** * Validates access to a table. * * @param table Table * @param requiredAccess Access requested on table */private void validateAccess(SqlNode node, @Nullable SqlValidatorTable table, SqlAccessEnum requiredAccess) if (table != null) // RelOptTableImpl#getAllowedAccess 返回全局访问类型，包括了 SELECT、UPDATE、INSERT 和 DELETE SqlAccessType access = table.getAllowedAccess(); // 如果不具有访问权限，则抛出异常 if (!access.allowsAccess(requiredAccess)) throw newValidationError(node, RESOURCE.accessNotAllowed(requiredAccess.name(), table.getQualifiedName().toString())); 完成校验后，返回的 outermostNode 结构如下，可以看到 SqlNode 的 Identifier 都进行了全限定名展开，根据 names 属性可以很快速地获取到列对象所属的表，以及表对象所属的 Schema。使用 SqlValidator#getFieldOrigins 方法可以获取列的原始类型，该类型中包含 catalog, schema, table, column，可以用来实现 SQL 血缘分析等需求。函数对象则会查找到 Calcite 内置的函数，或者用户在元数据中定义的函数，这些校验后的对象将在后续 SqlNode 转换 RelNode 过程中发挥重要作用。 整体流程总结 前面我们对 Calcite SQL 校验器的执行流程进行了详细的分析，为了避免过于陷入代码细节，最后我们来总结下 SQL 校验的整体流程，帮助大家进行理解。Calcite SQL 校验总体上可以分为 4 个步骤： 第一步：SqlValidatorImpl 初始化。在这个步骤中，会将校验器所需的 SqlOperatorTable 和 SqlValidatorCatalogReader 等对象传入进来，SqlOperatorTable 用于查找 SQL 运算符和函数，SqlValidatorCatalogReader 则用于校验时查找元数据信息； 第二步：重写 SqlNode 进行标准化。Calcite 会将非标准的 SqlNode 转换为标准的 SqlNode，具体包括：VALUES、ORDER_BY、EXPLICIT_TABLE、DELETE、UPDATE 和 MERGE； 第三步：注册 Scope 和 Namespace。通过 registerQuery 方法生成 SqlValidatorScope 和 SqlValidatorNamespace 对象，分别用于声明校验过程中名称解析的范围，以及描述由 SQL 查询某个部分返回的关系（简单理解可以认为是查询过程中的数据来源）； 第四步：校验 Select 语句。校验器依次调用 validateFrom、validateWhereClause、validateGroupClause、validateHavingClause、validateWindowClause、validateQualifyClause、validateSelectList 和 validateOrderList 方法校验不同的子句，不存在的表或列会抛出异常，存在则进行全限定名展开，例如将 EMPS 展开为 SALES.EMPS。 结语 本文介绍了 Caclite 校验器的整体设计，带大家一起了解了校验器中核心的 SqlValidator、SqlValidatorScope、SqlValidatorNamespace 类，并通过一个简单的单测，和大家一起跟踪了 SQL 校验器的执行流程，经过校验器处理，Caclite SqlNode 最终包含了展开的全限定名，以及经过解析的函数对象。 SQL 校验的目的是为了将 SqlNode 进行标准化，并分析出 SQL 上下文的语义关系，以方便后续将 SqlNode 对象转换为 RelNode 关系代数对象。下一篇，我们将继续探究 Calcite SqlNode 转换 RelNode 的实现原理，看看 Calcite 如何表示关系代数，以及 SqlNode AST 如何转换为 SqlNode 关系代数？在转换过程中，Calcite 又包含了哪些隐藏的优化方式？欢迎大家持续关注后续文章，如果有感兴趣的问题，也欢迎大家留言交流。 写在最后 笔者因为工作原因接触到 Calcite，前期学习过程中，深感 Calcite 学习资料之匮乏，因此创建了 Calcite 从入门到精通知识星球，希望能够将学习过程中的资料和经验沉淀下来，为更多想要学习 Calcite 的朋友提供一些帮助。 欢迎关注 欢迎关注「端小强的博客」微信公众号，会不定期分享日常学习和工作经验，欢迎大家关注交流。","tags":["Calcite"],"categories":["Calcite"]},{"title":"JVM 学习资料整理","path":"/blog/jvm-learning-materials.html","content":"前言 JVM 虚拟机原理及其相关的调优技术是每一位 Java 工程师必备的技能，随着 JDK 不断地发展，新的底层实现和调优方法需要我们不断跟进学习。正所谓「常学常新、常悟常进、常践常得」，笔者将重新学习和梳理 JVM 虚拟机相关知识，并会整理成博文以加深自己对 JVM 的理解，也希望能够帮助其他学习 JVM 的同学。 个人整理 JVM 知识图谱：思维导图（密码: y7km） 重学 JVM 第 1 弹：无关性的基石之 Java 字节码技术初探 重学 JVM 第 2 弹：TODO 重学 JVM 第 3 弹：TODO 重学 JVM 第 4 弹：TODO 重学 JVM 第 5 弹：TODO 重学 JVM 第 6 弹：TODO 重学 JVM 第 7 弹：TODO 重学 JVM 第 8 弹：TODO 重学 JVM 第 9 弹：TODO 重学 JVM 第 10 弹：Java AOT 编译框架 GraalVM 快速入门 重学 JVM 第 11 弹：GraalVM 编译动态链接库之 MySQL UDF 实现 网络资料 经典书籍 JVM 虚拟机规范（SE7）中文版 深入理解Java虚拟机：JVM高级特性与最佳实践（第3版） 深入理解 JAVA 内存模型 自己动手写 Java 虚拟机 综合介绍 RednaxelaFX 大神资料合集 字节码 大咖微讲堂之《JVM 字节码的探索与实践应用》 GraalVM GraalVM 与 Java 静态编译：原理与应用 Java Developer’s Introduction to GraalVM Optimizing Performance with GraalVM Run Code in Any Language Anywhere with GraalVM 欢迎关注 欢迎关注「端小强的博客」微信公众号，会不定期分享日常学习和工作经验，欢迎大家关注交流。","tags":["JVM"],"categories":["JVM"]},{"title":"初探分布式事务常见方案及实现原理","path":"/blog/basic-introduction-to-common-distributed-transaction-solutions.html","content":"前言 TODO 什么是分布式事务 基本定义 使用场景 随着互联网、金融等行业的快速发展，业务越来越复杂，一个完整的业务往往需要调用多个子业务或服务，随着业务的不断增多，涉及的服务及数据也越来越多越来越复杂。传统的系统难以支撑，出现了应用和数据库等的分布式系统。分布式系统又带来了数据一致性的问题，从而产生了分布式事务。 如何实现分布式事务 基于第一个强一致的思路，就有了基于数据库本身支持的协议，XA 分布式事务。XA 整体设计思路可以概括为，如何在现有事务模型上微调扩展，实现分布式事务。 X/Open，即现在的 open group，是一个独立的组织，主要负责制定各种行业技术标准。 X/Open 组织主要由各大知名公司或者厂商进行支持，这些组织不光遵循 X/Open 组织定义的行业技术标准，也参与到标准的制定。 XA 分布式协议 应用程序(Application Program ，简称AP)：用于定义事务边界(即定义事务的开始和结束)，并且在事务边界内对资源进行操作。 资源管理器(Resource Manager，简称 RM)：如数据库、文件系统等，并提供访问资源的方式 事务管理器(Transaction Manager ，简称 TM)：负责分配事务唯一标识，监控事务的执行进度，并负责事务的提交、回滚等。 XA 接口，如下的命令会在单机的 MySQL 数据库上执行。 xa_start ：负责开启或者恢复一个事务分支 xa_end： 负责取消当前线程与事务分支的关联（表示 SQL 执行完成） xa_prepare：询问 RM 是否准备好提交事务分支（判断当前数据库是否锁定资源，是否能够提交） xa_commit：通知 RM 提交事务分支 xa_rollback： 通知 RM 回滚事务分支 xa_recover : 需要恢复的 XA 事务（查看当前的事务列表，即完成了 prepare 操作，但未提交或回滚） 思考：为什么 XA 事务又叫两阶段事务？ MySQL 数据库中，使用 SHOW ENGINES; 语句可以查看存储引擎是否支持 XA 事务，可以看到最常用的 InnoDB 存储引擎是支持 XA 事务的。 mysql SHOW ENGINES;+--------------------+---------+----------------------------------------------------------------+--------------+------+------------+| Engine | Support | Comment | Transactions | XA | Savepoints |+--------------------+---------+----------------------------------------------------------------+--------------+------+------------+| ndbcluster | NO | Clustered, fault-tolerant tables | NULL | NULL | NULL || FEDERATED | NO | Federated MySQL storage engine | NULL | NULL | NULL || MEMORY | YES | Hash based, stored in memory, useful for temporary tables | NO | NO | NO || InnoDB | DEFAULT | Supports transactions, row-level locking, and foreign keys | YES | YES | YES || PERFORMANCE_SCHEMA | YES | Performance Schema | NO | NO | NO || MyISAM | YES | MyISAM storage engine | NO | NO | NO || ndbinfo | NO | MySQL Cluster system information storage engine | NULL | NULL | NULL || MRG_MYISAM | YES | Collection of identical MyISAM tables | NO | NO | NO || BLACKHOLE | YES | /dev/null storage engine (anything you write to it disappears) | NO | NO | NO || CSV | YES | CSV storage engine | NO | NO | NO || ARCHIVE | YES | Archive storage engine | NO | NO | NO |+--------------------+---------+----------------------------------------------------------------+--------------+------+------------+11 rows in set (0.01 sec) 如下演示了在 MySQL 数据库上执行 XA 事务的过程，分布式事务通过相同的 xid 来关联，表示他们属于同一个分布式事务。 mysql XA START test;Query OK, 0 rows affected (0.01 sec)mysql INSERT INTO t_order_0 VALUES(1000, 1, OK, 1, TEST, NOW());Query OK, 1 row affected, 1 warning (0.01 sec)mysql SELECT * FROM t_order_0;+----------+---------+--------+-------------+--------+---------------+| order_id | user_id | status | merchant_id | remark | creation_date |+----------+---------+--------+-------------+--------+---------------+| 1000 | 1 | OK | 1 | TEST | 2024-04-29 |+----------+---------+--------+-------------+--------+---------------+1 row in set (0.01 sec)mysql XA END test;Query OK, 0 rows affected (0.00 sec)mysql XA PREPARE test;Query OK, 0 rows affected (0.00 sec)mysql XA RECOVER;+----------+--------------+--------------+------+| formatID | gtrid_length | bqual_length | data |+----------+--------------+--------------+------+| 1 | 4 | 0 | test |+----------+--------------+--------------+------+1 row in set (0.00 sec)mysql XA ROLLBACK test;Query OK, 0 rows affected (0.01 sec)mysql SELECT * FROM t_order_0;Empty set (0.00 sec) 分布式事务常见方案 XA 分布式事务 欢迎关注 欢迎关注「端小强的博客」微信公众号，会不定期分享日常学习和工作经验，欢迎大家关注交流。","tags":["Transaction","Distributed Transaction"],"categories":["Transaction"]},{"title":"BenchmarkSQL & ShardingSphere-JDBC 性能测试实战","path":"/blog/use-benchmark-sql-test-shardingsphere-jdbc-performance.html","content":"TPC-C 模型简介 TPC 是一系列事务处理和数据库基准测试的规范。其中TPC-C（Transaction Processing Performance Council）是针对 OLTP 的基准测试模型。TPC-C 测试模型给基准测试提供了一种统一的测试标准，可以大体观察出数据库服务稳定性、性能以及系统性能等一系列问题。对数据库展开 TPC-C 基准性能测试，一方面可以衡量数据库的性能，另一方面可以衡量采用不同硬件软件系统的性价比，也是被业内广泛应用并关注的一种测试模型。 TODO BenchmarkSQL 使用入门 TODO ShardingSphere-JDBC 性能测试实战 # 1. 创建 jdbc 依赖 lib 目录cd ~/Downloadsmkdir shardingsphere-jdbc-libcd shardingsphere-jdbc-lib# 2. 增加软链到 BenchmarkSQL lib/ext 目录ln -s /Users/duanzhengqiang/Downloads/shardingsphere-jdbc-lib /Users/duanzhengqiang/IdeaProjects/benchmarksql-og/lib/extunlink /Users/duanzhengqiang/IdeaProjects/benchmarksql-og/lib/ext# 3. 下载 JDBC 和 Cluster ZK Jar 包-- 下载 JDBC Jar 包：https://maven.apache.org/plugins/maven-dependency-plugin/examples/copying-artifacts.htmlmvn dependency:copy -Dartifact=org.apache.shardingsphere:shardingsphere-jdbc:5.4.2-SNAPSHOT -Dpackaging=jar -DoutputDirectory=/Users/duanzhengqiang/Downloads/shardingsphere-jdbc-libmvn dependency:copy -Dartifact=org.apache.shardingsphere:shardingsphere-cluster-mode-repository-zookeeper:5.4.2-SNAPSHOT -Dpackaging=jar -DoutputDirectory=/Users/duanzhengqiang/Downloads/shardingsphere-jdbc-lib# 4. 进去 SS 项目路径，下载 JDBC 和 Cluster ZK 依赖 Jar 包cd /Users/duanzhengqiang/IdeaProjects/shardingsphere/jdbc-- 下载 JDBC Jar 包依赖，copy-dependencies 依赖 pom 文件mvn dependency:copy-dependencies -DoutputDirectory=/Users/duanzhengqiang/Downloads/shardingsphere-jdbc-libcd /Users/duanzhengqiang/IdeaProjects/shardingsphere/mode/type/cluster/repository/provider/zookeepermvn dependency:copy-dependencies -DoutputDirectory=/Users/duanzhengqiang/Downloads/shardingsphere-jdbc-lib TODO 欢迎关注 欢迎关注「端小强的博客」微信公众号，会不定期分享日常学习和工作经验，欢迎大家关注交流。","tags":["ShardingSphere","In Action"],"categories":["ShardingSphere"]},{"title":"CBO 优化的基石——Apache Calcite 统计信息和代价模型详解","path":"/blog/cornerstone-of-cbo-optimization-apache-calcite-statistics-and-cost-model.html","content":"注意：本文基于 Calcite 1.35.0 版本源码进行学习研究，其他版本可能会存在实现逻辑差异，对源码感兴趣的读者请注意版本选择。 前言 在上一篇深入理解 Apache Calcite ValcanoPlanner 优化器一文中，我们介绍了 Calcite VolcanoPlanner 的理论基础、核心概念和整体流程，VolcanoPlanner 在优化时会计算不同执行计划的代价 Cost，然后通过代价的比较，最终寻找出最小代价的执行计划。代价 Cost 的计算依赖于统计信息和代价模型，统计信息是否准确，代价模型是否合理，直接影响了 VolcanoPlanner 优化的效果。上一篇文章中，我们对 Calcite 统计信息和代价模型进行了简单的介绍，今天我们将结合一个多表关联查询的案例，和大家一起探究下 Calcite 是如何使用统计信息和代价模型，以及在优化过程中，Calcite 会使用哪些优化方式得到最优执行计划。 统计信息和代价模型 在探究 Calcite 如何使用统计信息和代价模型之前，让我们先来了解下相关的基础知识，笔者参考了一些优秀的网络资料，进行了总结整理，原文链接见文末参考资料，感兴趣的读者可阅读研究。 统计信息 统计信息 Statistic 为优化器的 Cost 计算提供了必要的数据支撑，通常我们可以将统计信息划分为基础统计信息和高级统计信息。 基础统计信息负责大部分通用场景下的 Cost 计算，具体包括表级别的统计信息 Row Count，单个字段的统计信息：每个字段的 NDV 值（The Number of Distinct Values），Max 值，Min 值，NULL 值，Histogram 值（分布信息，用于区间查询）, Count-Min Sketch 值（用于等值查询），DataSize 值 等。由于基础统计信息对 Cost 计算至关重要，需要做到自动收集，自动更新，否则很可能因为基础统计信息的缺失，导致优化器产生灾难性的执行计划。 高级统计信息主要用于提升复杂场景下的决策质量，通常包括多字段间的关联度（Column Group）、Functional Deplendency、数据倾斜 等，高级统计信息需要手工触发，只有在必要的时候才会收集。 基数估计 有了统计信息后，我们就可以对执行计划中的各个算子进行基数估计（Cardinality Estimation），估算这些算子产生结果的行数（或基数）。如下图所示，通过基数估计我们可以选择更优的 JOIN 顺序以减少中间结果。Scan 算子的行数可以直接从表的统计信息 Row Count 获取，而对于 Filter 算子，可以使用输入的 Scan 算子的行数乘以谓词的选择率。 下面展示了常用算子基数估计的例子： Scan 算子基数估计：统计信息中收集的表 Row Count； Filter 算子基数估计：对于唯一属性的谓词，相等条件下结果不超过一行，对于非唯一属性的谓词，相等条件下结果使用 NDV 估算选择率，然后计算结果。对于范围查询，我们可以使用直方图估算选择率，然后计算结果； Join 算子基数估计：LeftRowCount * RightRowCount * Join Condition Selectivity； Union 算子基数估计：LeftRowCount + RightRowCount； Agg 算子基数估计：Group By 列的 Distinct 值数量（NDV）。 代价模型 代价模型（Cost Model）是用于估算物理执行计划的代价，代价通常使用 CPU、Memory、IO、Net 四元组来描述，每一算子都会通过上述四元组来描述其代价。执行计划的代价即是其全部算子的代价之和，最终优化器会根据求和后的 CPU、Memory、IO、Net 加权计算出执行计划的最终代价。 CPU：代表 CPU 的消耗数值； Memory：代表 Memory 的占用量； IO：代表磁盘的逻辑 IO 次数； Net：代表网络的逻辑 IO 次数（交互次数及传输量）； 最终 Cost = (CPU, Memory, IO, Net) · (w1, w2, w3, w4)，w 为权重向量。 (adsbygoogle = window.adsbygoogle || []).push({}); 赞助商 Calcite 统计信息实现 Calcite 将统计信息存储在元数据对象中进行管理，通过 RelMetadataQuery 类提供了所有元数据的访问入口，该类包含每个元数据的访问方法，访问方法中需要传递对应的关系代数类 RelNode 及其他参数。例如，获取基数 Cardinality 只需要 RelNode，而获取选择率 Selectivity 还需要传入谓词 predicate： class RelMetadataQuery // Cardinality public Double getRowCount(RelNode rel) ... // Selectivity public Double getSelectivity(RelNode rel, RexNode predicate) ... 下面展示了 JdbcAdapterTest#testJoin3TablesPlan 单测，测试 SQL 中包含了 scott.emp、scott.dept 和 scott.salgrade 3 张表，分别使用了等值和非等值关联条件。 @Testvoid testJoin3TablesPlan() CalciteAssert.model(JdbcTest.SCOTT_MODEL) .query(select empno, ename, dname, grade + from scott.emp e inner join scott.dept d + on e.deptno = d.deptno + inner join scott.salgrade s + on e.sal s.losal and e.sal s.hisal) .explainContains(PLAN=JdbcToEnumerableConverter + JdbcProject(EMPNO=[$0], ENAME=[$1], DNAME=[$5], GRADE=[$6]) + JdbcJoin(condition=[AND(($2, $7), ($2, $8))], joinType=[inner]) + JdbcJoin(condition=[=($3, $4)], joinType=[inner]) + JdbcProject(EMPNO=[$0], ENAME=[$1], SAL=[$5], DEPTNO=[$7]) + JdbcTableScan(table=[[SCOTT, EMP]]) + JdbcProject(DEPTNO=[$0], DNAME=[$1]) + JdbcTableScan(table=[[SCOTT, DEPT]]) + JdbcTableScan(table=[[SCOTT, SALGRADE]]) ) .runs() .enable(CalciteAssert.DB == CalciteAssert.DatabaseInstance.HSQLDB) .planHasSql(SELECT \\t\\.\\EMPNO\\, \\t\\.\\ENAME\\, + \\t0\\.\\DNAME\\, \\SALGRADE\\.\\GRADE\\ + FROM (SELECT \\EMPNO\\, \\ENAME\\, \\SAL\\, \\DEPTNO\\ + FROM \\SCOTT\\.\\EMP\\) AS \\t\\ + INNER JOIN (SELECT \\DEPTNO\\, \\DNAME\\ + FROM \\SCOTT\\.\\DEPT\\) AS \\t0\\ ON \\t\\.\\DEPTNO\\ = \\t0\\.\\DEPTNO\\ + INNER JOIN \\SCOTT\\.\\SALGRADE\\ + ON \\t\\.\\SAL\\ \\SALGRADE\\.\\LOSAL\\ + AND \\t\\.\\SAL\\ \\SALGRADE\\.\\HISAL\\); 我们将结合此案例，来探究下 Calcite 统计信息入口类 RelMetadataQuery 如何进行初始化，它的内部又将调用哪些元数据对象以获取统计信息，此外，Caclite 又是如何基于统计信息进行基数估计。搞清楚这些问题后，相信大家对 Calcite 统计信息的实现会有更深刻的理解。 RelMetadataQuery 初始化 执行 JdbcAdapterTest#testJoin3TablesPlan 单测，首先会调用 RelOptCluster.create(planner, rexBuilder); 方法初始化 RelOptCluster 对象，初始化 RelOptCluster 时内部会调用 setMetadataProvider 和 setMetadataQuerySupplier 方法，下面的小节我们将分别探究下这两个初始化方法的内部实现细节。 /** * Creates a cluster. * * pFor use only from @link #create and @link RelOptQuery. */RelOptCluster(RelOptPlanner planner, RelDataTypeFactory typeFactory, RexBuilder rexBuilder, AtomicInteger nextCorrel, MapString, RelNode mapCorrelToRel) this.nextCorrel = nextCorrel; this.mapCorrelToRel = mapCorrelToRel; this.planner = Objects.requireNonNull(planner, planner); this.typeFactory = Objects.requireNonNull(typeFactory, typeFactory); this.rexBuilder = rexBuilder; this.originalExpression = rexBuilder.makeLiteral(?); // set up a default rel metadata provider, // giving the planner first crack at everything setMetadataProvider(DefaultRelMetadataProvider.INSTANCE); setMetadataQuerySupplier(RelMetadataQuery::instance); this.emptyTraitSet = planner.emptyTraitSet(); assert emptyTraitSet.size() == planner.getRelTraitDefs().size(); setMetadataProvider 初始化 我们先来看下 setMetadataProvider 方法，该方法会传入 DefaultRelMetadataProvider.INSTANCE 实例，方法内部会将 metadataProvider 记录在 RelOptCluster 中，并将 metadataProvider 封装为 JaninoRelMetadataProvider，然后设置在 ThreadLocal 中方便上下文进行访问。 @EnsuresNonNull(this.metadataProvider, this.metadataFactory)public void setMetadataProvider(@UnknownInitialization RelOptCluster this, RelMetadataProvider metadataProvider) this.metadataProvider = metadataProvider; // 将 metadataProvider 封装在 metadataFactory 中，内部会缓存 UnboundMetadata this.metadataFactory = new org.apache.calcite.rel.metadata.MetadataFactoryImpl(metadataProvider); // Wrap the metadata provider as a JaninoRelMetadataProvider // and set it to the ThreadLocal, // JaninoRelMetadataProvider is required by the RelMetadataQuery. RelMetadataQueryBase.THREAD_PROVIDERS.set(JaninoRelMetadataProvider.of(metadataProvider)); DefaultRelMetadataProvider.INSTANCE 实例初始化逻辑如下，作为默认的元数据提供器，DefaultRelMetadataProvider 定义了所有常用的关系代数 RelNode 的处理器，如果使用调用链方式时（使用 ChainedRelMetadataProvider），需要将 DefaultRelMetadataProvider 放在最后一个作为兜底方案。 /** * Creates a new default provider. This provider defines catch-all * handlers for generic RelNodes, so it should always be given lowest * priority when chaining. * * pUse this constructor only from a sub-class. Otherwise use the singleton * instance, @link #INSTANCE. */protected DefaultRelMetadataProvider() super( ImmutableList.of( RelMdPercentageOriginalRows.SOURCE, RelMdColumnOrigins.SOURCE, RelMdExpressionLineage.SOURCE, RelMdTableReferences.SOURCE, RelMdNodeTypes.SOURCE, RelMdRowCount.SOURCE, RelMdMaxRowCount.SOURCE, RelMdMinRowCount.SOURCE, RelMdUniqueKeys.SOURCE, RelMdColumnUniqueness.SOURCE, RelMdPopulationSize.SOURCE, RelMdSize.SOURCE, RelMdParallelism.SOURCE, RelMdDistribution.SOURCE, RelMdLowerBoundCost.SOURCE, RelMdMemory.SOURCE, RelMdDistinctRowCount.SOURCE, RelMdSelectivity.SOURCE, RelMdExplainVisibility.SOURCE, RelMdPredicates.SOURCE, RelMdAllPredicates.SOURCE, RelMdCollation.SOURCE)); super 方法会将集合中初始化的 RelMetadataProvider 集合传递给父类 ChainedRelMetadataProvider，并维护在 providers 变量中，后续调用 ChainedRelMetadataProvider#apply 和 ChainedRelMetadataProvider#handlers 方法时会从 providers 中获取。 上图展示了 RelMetadataProvider 继承体系，RelMetadataProvider 接口定义了获取关系表达式元数据的方法，通常不建议直接调用，而应当使用 RelMetadataQuery 入口类获取元数据。关于设计 RelMetadataProvider 的背景和动机，可以参考 RelationalExpressionMetadata Wiki。 RelMetadataProvider 接口定义如下： public interface RelMetadataProvider // 为具体的关系代数类或子类，获取特定类型的统计信息，该方法返回的是 UnboundMetadata 函数式接口 // 使用时可以调用 bind 方法返回绑定的元数据对象 @Deprecated @Nullable M extends @Nullable Metadata @Nullable UnboundMetadataM apply( Class? extends RelNode relClass, Class? extends M metadataClass); @Deprecated M extends Metadata MultimapMethod, MetadataHandlerM handlers(MetadataDefM def); // 获取实现特定 MetadataHandler 接口的 MetadataHandler 集合 // MetadataHandler 是元数据处理器的标记接口，MetadataHandler#getDef 方法用于获取元数据处理器的定义，包含了元数据类，处理器类和处理方法 ListMetadataHandler? handlers(Class? extends MetadataHandler? handlerClass); RelMetadataProvider#apply 方法用于为具体的关系代数类或子类，获取特定类型的统计信息，该方法返回的是 UnboundMetadata 函数式接口，使用时可以调用 bind 方法返回绑定的元数据对象。该方法的使用示例如下，执行完 bind 方法会返回元数据对象 Selectivity，然后可以调用 getSelectivity 获取选择率统计信息。 RelMetadataProvider provider;LogicalFilter filter;RexNode predicate;UnboundMetadataSelectivity unboundMetadata = provider.apply(LogicalFilter.class, Selectivity.class);Selectivity selectivity = unboundMetadata.bind(filter, mq);Double d = selectivity.getSelectivity(predicate); RelMetadataProvider#handlers 方法用于获取实现特定 MetadataHandler 接口的 MetadataHandler 集合。MetadataHandler 是元数据处理器的标记接口，MetadataHandler#getDef 方法用于获取元数据处理器的定义，包含了元数据类，处理器类和处理方法。 /** * Marker interface for a handler of metadata. * * @param M Kind of metadata */public interface MetadataHandlerM extends Metadata MetadataDefM getDef(); static SortedMapString, Method handlerMethods(Class? extends MetadataHandler? handlerClass) final ImmutableSortedMap.BuilderString, Method map = ImmutableSortedMap.naturalOrder(); Arrays.stream(handlerClass.getDeclaredMethods()).filter(m - !m.getName().equals(getDef)) .filter(m - !m.isSynthetic()).filter(m - !Modifier.isStatic(m.getModifiers())).forEach(m - map.put(m.getName(), m)); return map.build(); MetadataHandler 接口还提供了静态方法 handlerMethods，该方法负责查找 MetadataHandler 实现类中定义的处理方法，结果会按照 MethodName, Method 结构返回，并且会过滤掉静态、synthetic（Java 编译器生成的方法）和 getDef 方法。 介绍完 RelMetadataProvider 接口的相关方法后，我们再回过头看下 DefaultRelMetadataProvider 初始化时，传递给父类的 22 个 providers 是如何创建的，他们分别又有哪些实际的用途。下面我们以 RelMdPercentageOriginalRows.SOURCE 为例，来看下具体的元数据提供器的内部实现。RelMdPercentageOriginalRows.SOURCE 实现逻辑如下，可以看到此处通过反射的方式初始化了 3 个元数据提供器，并使用 ChainedRelMetadataProvider 进行了包装，ChainedRelMetadataProvider 内部使用了责任链模式，会按照责任链进行元数据处理。 public static final RelMetadataProvider SOURCE = ChainedRelMetadataProvider.of(ImmutableList.of( ReflectiveRelMetadataProvider.reflectiveSource(new RelMdPercentageOriginalRowsHandler(), BuiltInMetadata.PercentageOriginalRows.Handler.class), ReflectiveRelMetadataProvider.reflectiveSource(new RelMdCumulativeCost(), BuiltInMetadata.CumulativeCost.Handler.class), ReflectiveRelMetadataProvider.reflectiveSource(new RelMdNonCumulativeCost(), BuiltInMetadata.NonCumulativeCost.Handler.class))); 我们重点关注下 ReflectiveRelMetadataProvider.reflectiveSource 的逻辑，该方法的第一个参数是 MetadataHandler 对象，此案例中分别为： RelMdPercentageOriginalRowsHandler； RelMdCumulativeCost； RelMdNonCumulativeCost。 他们都继承了 RelMdPercentageOriginalRows，这些类负责具体关系代数的元数据获取，提供了 getPercentageOriginalRows、getCumulativeCost 和 getNonCumulativeCost 方法。 第二个参数则是 MetadataHandler 的子接口，该接口声明了特定 RelNode 获取元数据的方法，此案例中子接口分别为： BuiltInMetadata.PercentageOriginalRows.Handler.class； BuiltInMetadata.CumulativeCost.Handler.class； BuiltInMetadata.NonCumulativeCost.Handler.class。 这些 Handler 接口分别声明了 getPercentageOriginalRows、getCumulativeCost 和 getNonCumulativeCost 方法，他们接收的参数为 RelNode。那么 Handler 接口里面的方法是如何与 MetadataHandler 对象关联上的呢？我们来具体看下 ReflectiveRelMetadataProvider 类的实现逻辑： @SuppressWarnings(deprecation)public static M extends Metadata RelMetadataProvider reflectiveSource(MetadataHandler? extends M handler, Class? extends MetadataHandlerM handlerClass) // When deprecated code is removed, handler.getDef().methods will no longer be required return reflectiveSource(handler, handler.getDef().methods, handlerClass);@Deprecated // to be removed before 2.0private static RelMetadataProvider reflectiveSource(final MetadataHandler target, final ImmutableListMethod methods, final Class? extends MetadataHandler? handlerClass) // 计算哪些方法可以作为给定元数据方法的处理程序，最终的结果记录在 Space2 对象中 final Space2 space = Space2.create(target, methods); final ConcurrentMapClassRelNode, UnboundMetadata methodsMap = new ConcurrentHashMap(); // 遍历元数据处理方法支持的 RelNode 类 for (ClassRelNode key : space.classes) ImmutableNullableList.BuilderMethod builder = ImmutableNullableList.builder(); for (final Method method : methods) // 根据 RelNode 类和 getPercentageOriginalRows 方法查找处理方法 Method builder.add(space.find(key, method)); final ListMethod handlerMethods = builder.build(); // 动态创建 UnboundMetadata 实现类 final UnboundMetadata function = (rel, mq) - (Metadata) Proxy.newProxyInstance( space.metadataClass0.getClassLoader(), new Class[]space.metadataClass0, (proxy, method, args) - // Suppose we are an implementation of Selectivity // that wraps filter, a LogicalFilter. Then we // implement // Selectivity.selectivity(rex) // by calling method // new SelectivityImpl().selectivity(filter, rex) ... // 通过反射调用目标对象上的方法，根据上面获取的 handlerMethods 调用对应的 MetadataHandler 方法 return handlerMethod.invoke(target, args1); ... ); methodsMap.put(key, function); return new ReflectiveRelMetadataProvider(methodsMap, space.metadataClass0, space.providerMap, handlerClass); ReflectiveRelMetadataProvider 类通过反射将元数据方法转发给目标对象（MetadataHandler 对象）上的方法。目标对象上的方法必须是公共且非静态的，并且除了首个参数为 RelNode 类型或其子类，其他参数都需要与元数据方法的签名保持相同。 在 reflectiveSource 方法中，首先会计算哪些方法可以作为给定元数据方法的处理程序，最终的结果会记录在 Space2 对象中，下图展示了 Space2 对象中记录的 classes 和 handlerMap 信息。然后遍历 Space2 对象中记录的 classes 集合，并根据 RelNode 类和 getPercentageOriginalRows 方法查找处理方法 Method，再通过 Proxy.newProxyInstance 动态代理生成 UnboundMetadata 对象，内部核心逻辑是调用目标对象 MetadataHandler 的对应元数据方法。 其他类型的元数据提供器初始化流程和 RelMdPercentageOriginalRows.SOURCE 基本一致，感兴趣的读者可以自行阅读源码，下表整理了 Calcite 中提供的 22 个元数据提供器以及他们的主要作用，希望能够帮助大家理解 Calcite 元数据信息。 元数据提供器类型 元数据提供器作用 RelMdPercentageOriginalRows.SOURCE 用于估计此表达式生成的行数，与去除所有单表筛选条件时生成的行数之间的百分比。 RelMdColumnOrigins.SOURCE 列源信息，即表达式输出的结果列，由哪些基础表的基础列所构成，由于 Union 和 LogicalProject 等表达式，列源信息会返回一个集合。 RelMdExpressionLineage.SOURCE 表达式血缘信息，记录了表达式的来源以及如何被处理，返回结果是表达式集合。 RelMdTableReferences.SOURCE 表引用信息，用于返回给定表达式中使用的所有表，这些表使用 RexTableInputRef.RelTableRef 唯一标识。 RelMdNodeTypes.SOURCE RelNode 类型信息，返回结果是 Multimap，key 是 RelNode Class，values 是 RelNode 集合。 RelMdRowCount.SOURCE 用于估计关系表达式返回的行数，对于 TableScan 会调用 estimateRowCount 获取统计信息中表的行数，其他关系表达式会通过基数估计的方式获取行数。 RelMdMaxRowCount.SOURCE 用于估计关系表达式返回的最大行数。 RelMdMinRowCount.SOURCE 用于估计关系表达式返回的最小行数。 RelMdUniqueKeys.SOURCE 获取表达式中的唯一键集合，每个唯一键使用 ImmutableBitSet 表示，每个位置表示基于 0 的列序号。 RelMdColumnUniqueness.SOURCE 判断特定关系表达式中的列集合是否唯一，例如：关系表达式 TableScan 包含 T(A, B, C, D) 四列，唯一键为 (A, B)，则 areColumnsUnique([0, 1]) 返回 true，areColumnsUnique([0] 返回 false。 RelMdPopulationSize.SOURCE 用于估计指定的 groupKey 原始数据源（通常指基表）中的不同行数，估计时会忽略表达式中的任何过滤条件。 RelMdSize.SOURCE 获取特定关系表达式中每行或者每列的平均大小（以 bytes 为单位）。 RelMdParallelism.SOURCE 获取关系表达式并行度相关的元数据，isPhaseTransition 方法返回当前关系表达式是否支持在其他进程执行，splitCount 则返回数据集的分割数，以广播为例 splitCount 为 1。 RelMdDistribution.SOURCE 获取数据行的分布信息，返回 RelDistribution 对象。 RelMdLowerBoundCost.SOURCE 获取 RelNode 的最小代价。 RelMdMemory.SOURCE 获取算子使用内存的元数据信息。 RelMdDistinctRowCount.SOURCE 用于估计 groupKey 分组后产生的行数，进行 groupKey 分组的数据行会使用 predicate 进行过滤。 RelMdSelectivity.SOURCE 用于估计表达式输出行中满足给定谓词的百分比，也叫选择率。 RelMdExplainVisibility.SOURCE 确定关系表达式是否应该在特定级别的 EXPLAIN PLAN 输出中可见。 RelMdPredicates.SOURCE 获取可以上拉的谓词集合，谓词上拉主要是将内层子查询中的谓词上拉到外层查询中，参与外层的谓词推导过程，帮助生成更多有意义的谓词。 RelMdAllPredicates.SOURCE 获取所有的谓词集合。 RelMdCollation.SOURCE 获取哪些列被排序的元数据信息，返回 RelCollation 集合。 setMetadataQuerySupplier 初始化 介绍完 setMetadataProvider，我们再来看下 setMetadataQuerySupplier 初始化时会处理哪些逻辑。setMetadataQuerySupplier 方法的逻辑很简单，就是将传入的 Supplier 记录在 RelOptCluster 中，使用时通过 getMetadataQuery 获取 RelMetadataQuery 对象。 /** * Sets up the customized @link RelMetadataQuery instance supplier that to * use during rule planning. * * pNote that the @code mqSupplier should return * a fresh new @link RelMetadataQuery instance because the instance would be * cached in this cluster, and we may invalidate and re-generate it * for each @link RelOptRuleCall cycle. */@EnsuresNonNull(this.mqSupplier)public void setMetadataQuerySupplier(@UnknownInitialization RelOptCluster this, SupplierRelMetadataQuery mqSupplier) this.mqSupplier = mqSupplier;/** * Returns the current RelMetadataQuery. * * pThis method might be changed or moved in future. * If you have a @link RelOptRuleCall available, * for example if you are in a @link RelOptRule#onMatch(RelOptRuleCall) * method, then use @link RelOptRuleCall#getMetadataQuery() instead. */public RelMetadataQuery getMetadataQuery() if (mq == null) mq = castNonNull(mqSupplier).get(); return mq; setMetadataQuerySupplier 方法的参数为 RelMetadataQuery::instance，RelMetadataQuery 构造方法逻辑如下，会从前文介绍的 RelMetadataQueryBase.THREAD_PROVIDERS 中获取 JaninoRelMetadataProvider，并同时传入 RelMetadataQuery 中的 EMPTY 进行初始化。 /** * Creates the instance with @link JaninoRelMetadataProvider instance * from @link #THREAD_PROVIDERS and @link #EMPTY as a prototype. */protected RelMetadataQuery() this(castNonNull(THREAD_PROVIDERS.get()), EMPTY.get()); 到这里我们就完成了 setMetadataQuerySupplier 初始化，RelOptCluster 中记录了 RelMetadataQuery 对象，而 RelMetadataQuery 对象中则初始化了各种通过动态代理创建的 Handler 对象，通过 Handler 对象又可以将元数据请求转发到不同元数据对象的内部方法。 RelMetadataQuery 获取统计信息 前文介绍了 RelMetadataQuery 初始化流程，下面我们再来探究下 RelMetadataQuery 获取统计信息的流程。RelMetadataQuery 是获取统计信息的门面类，内部提供了不同 RelNode 类型对应的元数据获取方法，我们以 getRowCount 方法为例，介绍下获取统计信息的内部逻辑。 /** * Returns the * @link BuiltInMetadata.RowCount#getRowCount() * statistic. * * @param rel the relational expression * @return estimated row count, or null if no reliable estimate can be * determined */public /* @Nullable: CALCITE-4263 */ Double getRowCount(RelNode rel) for (; ; ) try Double result = rowCountHandler.getRowCount(rel, this); return RelMdUtil.validateResult(castNonNull(result)); catch (MetadataHandlerProvider.NoHandler e) rowCountHandler = revise(BuiltInMetadata.RowCount.Handler.class); getRowCount 方法的逻辑很简单，会调用 RelMetadataQuery 内部维护的 rowCountHandler.getRowCount 方法获取行数统计信息，然后使用 RelMdUtil 对结果进行校验并返回。从下图可以看到，首次调用 getRowCount 方法时会抛出 NoHandler 异常，此时会调用 revise 方法对 rowCountHandler 进行再次初始化。 revise 方法实现逻辑如下，会调用 MetadataHandlerProvider 的 revise 方法，此处为 JaninoRelMetadataProvider。revise 方法内部会调用 HANDLERS.get 方法从缓存中获取 MetadataHandler。 /** * Re-generates the handler for a given kind of metadata, adding support for * @code class_ if it is not already present. */protected H extends MetadataHandler? H revise(ClassH def) return getMetadataHandlerProvider().revise(def);// JaninoRelMetadataProvider#revise 方法public synchronized H extends MetadataHandler? H revise(ClassH handlerClass) try final Key key = new Key(handlerClass, provider); //noinspection unchecked return handlerClass.cast(HANDLERS.get(key)); catch (UncheckedExecutionException | ExecutionException e) throw Util.throwAsRuntime(Util.causeOrSelf(e)); 如果缓存未命中，则调用 generateCompileAndInstantiate 方法编译生成 MetadataHandler 对象。generateCompileAndInstantiate 方法第一个参数为 Class? extends MetadataHandler? extends Metadata，此处为 interface org.apache.calcite.rel.metadata.BuiltInMetadata$RowCount$Handler。第二个参数为 MetadataHandler 集合，通过 handlers 方法过滤出 key.handlerClass 的子类对象。 /** * Cache of pre-generated handlers by provider and kind of metadata. * For the cache to be effective, providers should implement identity * correctly. */private static final LoadingCacheKey, MetadataHandler? HANDLERS = maxSize(CacheBuilder.newBuilder(), CalciteSystemProperty.METADATA_HANDLER_CACHE_MAXIMUM_SIZE.value()) .build(CacheLoader.from(key - generateCompileAndInstantiate(key.handlerClass, key.provider.handlers(key.handlerClass)))); 根据下图可知，key.provider 主要为 ChainedRelMetadataProvider 和 ReflectiveRelMetadataProvider，ChainedRelMetadataProvider 会按照责任链方式遍历 providers，而 ReflectiveRelMetadataProvider 则会根据 key.handlerClass 过滤出子类对象。 ReflectiveRelMetadataProvider#handlers 方法实现逻辑如下，经过过滤我们得到了一个 RelMdRowCount 处理器。 // ReflectiveRelMetadataProvider#handlers 方法@Overridepublic ListMetadataHandler? handlers(Class? extends MetadataHandler? handlerClass) if (this.handlerClass.isAssignableFrom(handlerClass)) return handlers; else return ImmutableList.of(); 下面会调用 generateCompileAndInstantiate 方法生成最终的 MetadataHandler 对象。generateHandler 方法会在 org.apache.calcite.rel.metadata.janino 包下生成 GeneratedMetadata_RowCountHandler 类，该类包含一个 getRowCount 方法，会根据 RelNode 类型将请求转发到 RelMdRowCount 处理器中。 private static MH extends MetadataHandler? MH generateCompileAndInstantiate(ClassMH handlerClass, List? extends MetadataHandler? extends Metadata handlers) final List? extends MetadataHandler? extends Metadata uniqueHandlers = handlers.stream().distinct().collect(Collectors.toList()); // 生成代码 RelMetadataHandlerGeneratorUtil.HandlerNameAndGeneratedCode handlerNameAndGeneratedCode = RelMetadataHandlerGeneratorUtil.generateHandler(handlerClass, uniqueHandlers); try return compile(handlerNameAndGeneratedCode.getHandlerName(), handlerNameAndGeneratedCode.getGeneratedCode(), handlerClass, uniqueHandlers); catch (CompileException | IOException e) throw new RuntimeException(Error compiling: + handlerNameAndGeneratedCode.getGeneratedCode(), e); generateHandler 方法生成的类逻辑如下： public final class GeneratedMetadata_RowCountHandler implements org.apache.calcite.rel.metadata.BuiltInMetadata.RowCount.Handler private final Object methodKey0 = new org.apache.calcite.rel.metadata.janino.DescriptiveCacheKey(Double Handler.getRowCount()); public final org.apache.calcite.rel.metadata.RelMdRowCount provider0; public GeneratedMetadata_RowCountHandler(org.apache.calcite.rel.metadata.RelMdRowCount provider0) // 初始化处理器类 this.provider0 = provider0; public org.apache.calcite.rel.metadata.MetadataDef getDef() return provider0.getDef(); public java.lang.Double getRowCount(org.apache.calcite.rel.RelNode r, org.apache.calcite.rel.metadata.RelMetadataQuery mq) while (r instanceof org.apache.calcite.rel.metadata.DelegatingMetadataRel) r = ((org.apache.calcite.rel.metadata.DelegatingMetadataRel) r).getMetadataDelegateRel(); final Object key; key = methodKey0; // 先从缓存中获取统计信息 final Object v = mq.map.get(r, key); if (v != null) if (v == org.apache.calcite.rel.metadata.NullSentinel.ACTIVE) throw new org.apache.calcite.rel.metadata.CyclicMetadataException(); if (v == org.apache.calcite.rel.metadata.NullSentinel.INSTANCE) return null; // 命中缓存直接返回 return (java.lang.Double) v; mq.map.put(r, key, org.apache.calcite.rel.metadata.NullSentinel.ACTIVE); try // 未命中则查询统计信息 final java.lang.Double x = getRowCount_(r, mq); mq.map.put(r, key, org.apache.calcite.rel.metadata.NullSentinel.mask(x)); return x; catch (java.lang.Exception e) mq.map.row(r).clear(); throw e; private java.lang.Double getRowCount_(org.apache.calcite.rel.RelNode r, org.apache.calcite.rel.metadata.RelMetadataQuery mq) // 根据不同的 RelNode 类型，从处理器中获取统计信息 if (r instanceof org.apache.calcite.adapter.enumerable.EnumerableLimit) return provider0.getRowCount((org.apache.calcite.adapter.enumerable.EnumerableLimit) r, mq); else if (r instanceof org.apache.calcite.plan.volcano.RelSubset) return provider0.getRowCount((org.apache.calcite.plan.volcano.RelSubset) r, mq); else if (r instanceof org.apache.calcite.rel.core.Aggregate) return provider0.getRowCount((org.apache.calcite.rel.core.Aggregate) r, mq); else if (r instanceof org.apache.calcite.rel.core.Calc) return provider0.getRowCount((org.apache.calcite.rel.core.Calc) r, mq); else if (r instanceof org.apache.calcite.rel.core.Exchange) return provider0.getRowCount((org.apache.calcite.rel.core.Exchange) r, mq); else if (r instanceof org.apache.calcite.rel.core.Filter) return provider0.getRowCount((org.apache.calcite.rel.core.Filter) r, mq); else if (r instanceof org.apache.calcite.rel.core.Intersect) return provider0.getRowCount((org.apache.calcite.rel.core.Intersect) r, mq); else if (r instanceof org.apache.calcite.rel.core.Join) return provider0.getRowCount((org.apache.calcite.rel.core.Join) r, mq); else if (r instanceof org.apache.calcite.rel.core.Minus) return provider0.getRowCount((org.apache.calcite.rel.core.Minus) r, mq); else if (r instanceof org.apache.calcite.rel.core.Project) return provider0.getRowCount((org.apache.calcite.rel.core.Project) r, mq); else if (r instanceof org.apache.calcite.rel.core.Sort) return provider0.getRowCount((org.apache.calcite.rel.core.Sort) r, mq); else if (r instanceof org.apache.calcite.rel.core.TableModify) return provider0.getRowCount((org.apache.calcite.rel.core.TableModify) r, mq); else if (r instanceof org.apache.calcite.rel.core.TableScan) return provider0.getRowCount((org.apache.calcite.rel.core.TableScan) r, mq); else if (r instanceof org.apache.calcite.rel.core.Union) return provider0.getRowCount((org.apache.calcite.rel.core.Union) r, mq); else if (r instanceof org.apache.calcite.rel.core.Values) return provider0.getRowCount((org.apache.calcite.rel.core.Values) r, mq); else if (r instanceof org.apache.calcite.rel.SingleRel) return provider0.getRowCount((org.apache.calcite.rel.SingleRel) r, mq); else if (r instanceof org.apache.calcite.rel.RelNode) return provider0.getRowCount((org.apache.calcite.rel.RelNode) r, mq); else throw new java.lang.IllegalArgumentException(No handler for method [public abstract java.lang.Double org.apache.calcite.rel.metadata.BuiltInMetadata$RowCount$Handler.getRowCount(org.apache.calcite.rel.RelNode,org.apache.calcite.rel.metadata.RelMetadataQuery)] applied to argument of type [ + r.getClass() + ]; we recommend you create a catch-all (RelNode) handler); 可以看到生成的 GeneratedMetadata_RowCountHandler 内部还增加了统计信息的缓存以提升性能。根据经验来看，关系代数的统计信息在较长时间内会保持不变，Calcite 会监测子节点统计信息，当子节点发生变化时会主动失效缓存数据。 最终，getRowCount 方法会调用到 RelMdRowCount 类中的 getRowCount 方法，我们来看下常用的关系代数行数是如何计算的。下面展示了 TableScan 获取行数的方法，首先会判断 RelOptTable 是否实现了 BuiltInMetadata.RowCount.Handler 接口，如果实现了接口则可以直接调用 handler.getRowCount 获取，否则调用 estimateRowCount 方法进行估算。TableScan#estimateRowCount 方法会调用 table.getStatistic().getRowCount() 从统计信息中获取行数。 // RelMdRowCount#getRowCountpublic @Nullable Double getRowCount(TableScan rel, RelMetadataQuery mq) final BuiltInMetadata.RowCount.Handler handler = rel.getTable().unwrap(BuiltInMetadata.RowCount.Handler.class); if (handler != null) return handler.getRowCount(rel, mq); return rel.estimateRowCount(mq);// TableScan#estimateRowCountpublic double estimateRowCount(RelMetadataQuery mq) return table.getRowCount();// RelOptTableImpl#getRowCountpublic double getRowCount() if (rowCount != null) return rowCount; if (table != null) final Double rowCount = table.getStatistic().getRowCount(); if (rowCount != null) return rowCount; return 100d; 对于 Join 等复杂的关系代数表达式，统计信息的获取会更加复杂，此处调用了 RelMdUtil.getJoinRowCount 方法。getJoinRowCount 方法中会根据 Join 类型计算统计信息，对于最常用的 INNER JOIN，计算 rowCount 时会采用 left * right * selectivity 方式，即左侧节点 rowCount 乘以右侧节点 rownCount，再乘以 Join 关联条件的选择率。LEFT JOIN 和 RIGHT JOIN 则是在 INNER JOIN 的基础上，增加了左侧节点或右侧节点的剩余行，FULL JOIN 则是计算了左右两侧的剩余行。 // RelMdRowCount#getRowCountpublic @Nullable Double getRowCount(Join rel, RelMetadataQuery mq) return RelMdUtil.getJoinRowCount(mq, rel, rel.getCondition());// RelMdUtil#getJoinRowCountDouble selectivity = mq.getSelectivity(join, condition);if (selectivity == null) return null;double innerRowCount = left * right * selectivity;switch (join.getJoinType()) case INNER: return innerRowCount; case LEFT: return left * (1D - selectivity) + innerRowCount; case RIGHT: return right * (1D - selectivity) + innerRowCount; case FULL: return (left + right) * (1D - selectivity) + innerRowCount; default: throw Util.unexpected(join.getJoinType()); 其他统计信息以及不同关系代数对应的统计信息计算逻辑，大家可以自行阅读统计信息处理器源码进行探究，有问题欢迎留言探讨。 Calcite 代价模型实现 在上一篇深入理解 Apache Calcite ValcanoPlanner 优化器一文中，我们跟随 ValcanoPlanner 内部执行逻辑，已经大致了解过了 Calcite 代价模型的相关逻辑，本节我们再来详细了解下代价模型具体由哪些类组成，使用代价模型计算代价时，它的内部逻辑又是如何实现的。 Calcite 代价模型组成 如下图所示，Calcite 代价模型主要包含了 RelOptCostFactory 代价工厂类和 RelOptCost 代价接口。RelOptCost 代价接口主要提供了 Rows、Cpu 和 Io 3 个指标，并提供了代价大小比较以及加减乘除运算能力，通过扩展 RelOptCost 代价接口，我们可以增加更多指标，例如在分布式数据库中比较重要的网络开销。 在 Caclite 中，RelOptCost 接口有两个实现类：RelOptCostImpl 和 VolcanoCost。RelOptCostImpl 是 RelOptCost 接口的默认实现，它只关注 Rows 指标，Cpu 和 Io 默认都返回 0，Caclite 中主要在 RBO 优化时使用 RelOptCostImpl 代价。VolcanoCost 与 RelOptCostImpl 不同，它同时考虑了 Rows、Cpu 和 Io 指标，Calcite 中主要在 CBO 优化时使用 VolcanoCost 代价。 RelOptCostFactory 工厂类用于创建具体的代价对象，接口中提供了 makeCost（创建代价对象）、makeHugeCost（创建巨大但非无穷的代价对象）、makeInfiniteCost（创建无穷大的代价对象）、makeTinyCost（创建很小的正成本代价对象） 和 makeZeroCost（创建零成本代价对象） 方法，这些方法会在计算算子代价时使用。 /** * Cost model for query planning. */public interface RelOptCostFactory /** * Creates a cost object. */ RelOptCost makeCost(double rowCount, double cpu, double io); /** * Creates a cost object representing an enormous non-infinite cost. */ RelOptCost makeHugeCost(); /** * Creates a cost object representing infinite cost. */ RelOptCost makeInfiniteCost(); /** * Creates a cost object representing a small positive cost. */ RelOptCost makeTinyCost(); /** * Creates a cost object representing zero cost. */ RelOptCost makeZeroCost(); Calcite 中每一个 RelNode 都具体实现了 computeSelfCost 接口，RelNode 类可以重写该方法以实现自定义的代价计算，也可以复用父类 AbstractRelNode 中实现的代价计算逻辑，可以看到 AbstractRelNode#computeSelfCost 调用了 planner.getCostFactory() 方法，通过优化器对象获取代价工厂类，然后再调用代价创建的方法。 /** * Returns the cost of this plan (not including children). The base * implementation throws an error; derived classes should override. * * pNOTE jvs 29-Mar-2006: Dont call this method directly. Instead, use * @link RelMetadataQuery#getNonCumulativeCost, which gives plugins a * chance to override the rels default ideas about cost. * * @param planner Planner for cost calculation * @param mq Metadata query * @return Cost of this plan (not including children) */// RelNode#computeSelfCost@NullableRelOptCost computeSelfCost(RelOptPlanner planner, RelMetadataQuery mq);// AbstractRelNode#computeSelfCostpublic @Nullable RelOptCost computeSelfCost(RelOptPlanner planner, RelMetadataQuery mq) // by default, assume cost is proportional to number of rows double rowCount = mq.getRowCount(this); return planner.getCostFactory().makeCost(rowCount, rowCount, 0); 根据 RelNode#computeSelfCost 方法注释，我们可以看到不允许直接调用 computeSelfCost 方法，用户获取代价时需要通过 RelMetadataQuery#getNonCumulativeCost 方法来获取当前节点的代价。下面的部分，我们再来探究下如何通过 RelMetadataQuery 门面类获取代价信息，它的内部又是如何与代价模型交互的。 Calcite 代价计算逻辑 Calcite 代价计算都是通过调用 RelMetadataQuery 进行获取，而 RelMetadataQuery 对象可以从优化器全局对象 RelOptCluster 中获取，用户可以从 RelOptNode#getCluster 中快速获取 RelOptCluster， 因此在 Calcite 中访问元数据非常容易。下面展示了前一小节介绍的 RelMetadataQuery#getNonCumulativeCost 方法获取当前 RelNode 的代价逻辑： /** * Returns the * @link BuiltInMetadata.NonCumulativeCost#getNonCumulativeCost() * statistic. * * @param rel the relational expression * @return estimated cost, or null if no reliable estimate can be determined */public @Nullable RelOptCost getNonCumulativeCost(RelNode rel) for (; ; ) try return nonCumulativeCostHandler.getNonCumulativeCost(rel, this); catch (MetadataHandlerProvider.NoHandler e) nonCumulativeCostHandler = revise(BuiltInMetadata.NonCumulativeCost.Handler.class); 该方法最终会调用到 RelMdPercentageOriginalRows#getNonCumulativeCost，可以看到该方法内部也是调用 RelNode 对应的 computeSelfCost 方法。 // RelMdPercentageOriginalRows#getNonCumulativeCostpublic @Nullable RelOptCost getNonCumulativeCost(RelNode rel, RelMetadataQuery mq) return rel.computeSelfCost(rel.getCluster().getPlanner(), mq); 此案例中 RelNode 为 JdbcTableScan 实现类，它继承了 TableScan，而 TableScan 的 computeSelfCost 实现逻辑如下，它调用了 VolcanoCost#Factory 创建了 VolcanoCost 对象。 public @Nullable RelOptCost computeSelfCost(RelOptPlanner planner, RelMetadataQuery mq) double dRows = table.getRowCount(); double dCpu = dRows + 1; // ensure non-zero cost double dIo = 0; return planner.getCostFactory().makeCost(dRows, dCpu, dIo); 至此，我们就有了一个代表 RelNode 代价的 VolcanoCost 对象，VolcanoCost 对象属性及值如下，其中包含了 RelNode 对应的行数、CPU 消耗以及 IO 消耗。 有了 VolcanoCost 对象后，Planner 会在优化过程中调用 VolcanoCost 的内部方法，例如：进行代价的大小比较，以选择代价最小的执行计划。以及计算当前 RelNode 的累加代价，会将当前 RelNode 的代价和子节点的代价进行累加。VolcanoCost 大小比较和累加的方法实现如下。 public boolean isLt(RelOptCost other) // 为什么为 true? Calcite 源码未进行说明 if (true) // 判断 rowCount 是否小于另一个 RelOptCost VolcanoCost that = (VolcanoCost) other; return this.rowCount that.rowCount; return isLe(other) !equals(other);public RelOptCost plus(RelOptCost other) VolcanoCost that = (VolcanoCost) other; if ((this == INFINITY) || (that == INFINITY)) return INFINITY; // 将所有的指标相加 return new VolcanoCost(this.rowCount + that.rowCount, this.cpu + that.cpu, this.io + that.io); 总体来看 Calcite VolcanoCost 实现逻辑相对简单，在进行代价比较时只考虑了单一的 rowCount 指标，其他指标并未进行比较。在实际使用中，我们需要对代价计算逻辑进行更精细的设计，例如：考虑分布式数据库中关心的网络传输开销，以及为不同的指标设计权重，参照权重进行综合的代价计算，这些需要大家不断去打磨提升。 结语 本文首先介绍了数据库领域的基础知识——统计信息和代价模型，让大家对相关的知识有一些了解。然后我们结合 Calcite 源码，一起学习了 Calcite 统计信息相关的实现逻辑，从 RelMetadataQuery 初始化，一直聊到 RelMetadataQuery 获取统计信息。这部分为大家介绍了初始化过程中涉及到的细节，Calcite 内部提供了 22 种元数据处理器，我们以 RelMdPercentageOriginalRows.SOURCE 为例进行了详细介绍，最后以表格的形式总结了所有元数据处理器的作用。此外，我们还介绍了 RelMetadataQuery 获取统计信息时，如何通过 Janio 生成代码，对元数据处理逻辑进行转发，在这个过程中，我们发现 Calcite 对统计信息进行了缓存以提升性能。 最后，本文介绍了 Calcite 代价模型相关的实现，了解了代价模型相关的接口和实现类，我们可以通过扩展接口增加代价模型中的指标。代价模型计算部分，我们以 getNonCumulativeCost 方法为例，简单地进行了探索，目前 Calcite 中的计算逻辑较为简单，如果我们想要进行更精细地计算，则需要自己去进行扩展实现，根据权重对不同指标进行计算从而获得一个综合代价。 阅读完本文，想必大家一定还有很多疑惑？Calcite 提供了 22 种元数据处理器，文章中只看到了如何获取统计信息，如何基于统计信息进行基数估计和代价计算，那么基础的统计信息我们又如何获取呢？另外，Calcite 代价计算逻辑目前看起来非常简单，那么生产级别的代价要如何计算，不同指标的权重应当如何设置？ 不仅大家会有这样的疑惑，笔者同样也有，本着打破砂锅问到底的原则，下一篇文章我们将探索 PolarDB-X 中统计信息和代价模型的相关实现，PolarDB-X 内核采用 Calcite 实现 SQL 优化，内部实现了丰富的统计信息指标收集，并对代价模型进行了扩展。学习 PolarDB-X 中统计信息和代价模型的实现，能够帮助大家快速了解生产级别的查询优化器是如何实现的，帮助大家在工作中更好地落地实践，欢迎感兴趣的朋友持续关注。 写在最后 笔者因为工作原因接触到 Calcite，前期学习过程中，深感 Calcite 学习资料之匮乏，因此创建了 Calcite 从入门到精通知识星球，希望能够将学习过程中的资料和经验沉淀下来，为更多想要学习 Calcite 的朋友提供一些帮助。 欢迎关注 欢迎关注「端小强的博客」微信公众号，会不定期分享日常学习和工作经验，欢迎大家关注交流。","tags":["Calcite"],"categories":["Calcite"]},{"title":"深入理解 Apache Calcite ValcanoPlanner 优化器","path":"/blog/deep-understand-of-apache-calcite-volcano-planner.html","content":"注意：本文基于 Calcite 1.35.0 版本源码进行学习研究，其他版本可能会存在实现逻辑差异，对源码感兴趣的读者请注意版本选择。 前言 在上一篇深入理解 Apache Calcite HepPlanner 优化器一文中，我们介绍了查询优化器的基本概念和用途，并结合 Calcite HepPlanner 深入分析了启发式优化器的实现原理。启发式优化器使用相对简单，它直接对逻辑执行计划进行等价变换从而实现 SQL 优化，常见的启发式优化包含了：列裁剪、谓词下推等。启发式优化器实现简单，自然也存在一些缺陷，例如：它对执行的顺序有要求，不同的执行顺序可能会导致优化规则的失效，使得优化达不到预期的效果。 正是由于启发式优化器存在这些问题，使得它无法适应所有的 SQL 场景，因此当前主流的数据库系统更多是使用基于代价的优化器，或者将两者结合使用。基于代价的优化器能够为多个等价的执行计划生成代价 Cost 信息，然后选择代价最小的选项作为最终的执行计划，从而达到提升 SQL 执行效率的目的。 本文将重点为大家介绍 Calcite 中基于代价的优化器 VolcanoPlanner，首先我们会了解 VolcanoPlanner 背后的理论基础——Volcano/Cascades Optimizer，然后会介绍 VolcanoPlanner 的核心概念以及执行流程，最后再深入探究 Calcite VolcanoPlanner 的源码细节，结合一些实际的 SQL 优化案例，期望能够让大家彻底搞懂 VolcanoPlanner 优化器。 Volcano/Cascades 优化器 Calcite VolcanoPlanner 优化器是基于 Goetz Graefe 的两篇经典优化器论文 The Volcano Optimizer Generator: Extensibility and Efficient Search 和 The Cascades Framework for Query Optimization 实现的，因此在探究 VolcanoPlanner 优化器实现细节之前，让我们先来回顾下这两篇论文的核心思想，方便后续的学习和理解。 Volcano 优化器生成器 Volcano Optimizer Generator 的定位是一个优化器的生成器，其核心贡献是提供了一个搜索引擎。论文中提出了数据库查询优化器的基本框架，数据库实现者只需要为自己的 Data Model 实现相应的接口，便可以实现一个查询优化器。本文暂时忽略生成器相关的概念，只介绍论文在优化器方面提出的一些思路： Volcano Optimizer 使用两阶段优化的方式，它使用 Logical Algebra 来表示各种关系代数算子，而使用 Physical Algebra 来表示各种关系代数算子的实现算法。Logical Algebra 之间使用 Transformation 来完成变换，而 Logical Algebra 到 Physical Algebra 之间的转换则基于代价（Cost-Based）进行选择； Volcano Optimizer 中的变化都使用 Rule 来描述。例如 Logical Algebra 之间的变化使用 Transformation Rule，而 Logical Algebra 到 Physical Algebra 之间的转换使用 Implementation Rule； Volcano Optimizer 中各个算子、表达式的结果使用 Property 来表示。Logical Propery 可以从 Logical Algebra 中提取，主要包括算子的 Schema、统计信息等。Physical Property 可以从 Physical Algebra 中提取，表示算子所产生的数据具有的物理属性，比如按照某个 Key 排序、按照某个 Key 分布在集群中等； Volcano Optimizer 的搜索采用自顶向下的动态规划算法（记忆化搜索）。 Cascades 优化器 Cascades Optimizer 是对 Volcano Optimizer 的进一步优化，Cascades Optimizer 提出了 Memo、Rule、Pattern 和 Search Algorithm 等基本概念，下面我们将围绕这些概念一一进行介绍。 Memo 数据结构 Cascades Optimizer 在搜索的过程中，它的搜索空间是一个关系代数算子树所组成的森林，而保存这个森林的数据结构就是 Memo。Memo 包含了两个最基本的概念：Expression Group（下文简称 Group） 和 Group Expression（对应关系代数算子）。每个 Group 中保存的是逻辑等价的 Group Expression，而 Group Expression 的子节点是由 Group 组成。下图是由五个 Group 组成的 Memo： 通过上面的 Memo 结构，我们可以提取出以下两棵等价的算子树，使用 Memo 结构存储下面两棵树，可以避免存储冗余的算子（如 Scan A 以及 Scan B）。 Rule 的改进 在 Volcano Optimizer 中，Rule 被分为了 Transformation Rule 和 Implementation Rule 两种。其中 Transformation Rule 用来在 Memo 中添加等价的关系代数算子。Transformation Rule 具有原子性，只作用于算子树的一个局部小片段，每个 Transformation Rule 都有自己的匹配条件，通过不停的应用匹配上的 Transformation Rule 来扩展搜索空间，寻找可能的最优解。Implementation Rule 则是为 Group Expression 选择物理算子。在 Cascades Optimizer 中，不再区分这两类 Rule。 Pattern 匹配规则 Pattern 用于描述 Group Expression 的局部特征。每个 Rule 都有自己的 Pattern，只有满足了相应 Pattern 的 Group Expression 才能够应用该 Rule。下图中左侧定义了一个 Selection - Projection 的 Pattern，并在右侧 Memo 中红色虚线内匹配上了 Group Expression。 Searching Algorithm Cascades Optimizer 为 Rule 的应用顺序做了细致的设计，例如每个 Rule 都有 promise 和 condition 两个方法，其中 promise 用来表示 Rule 在当前搜索过程中的重要性，promise 值越高，则该规则越可能有用，当 promise 值小于等于 0 时，这个 Rule 就不会被执行。而 condition 直接通过返回一个布尔值决定一个 Rule 是否可以在当前过程中被应用。当一个 Rule 被成功应用之后，会计算下一步有可能会被应用的 Rule 的集合。 Cascades Optimizer 的搜索算法与 Volcano Optimizer 有所不同，Volcano Optimizer 将搜索分为两个阶段，在第一个阶段枚举所有逻辑等价的 Logical Algebra，而在第二阶段运用动态规划的方法自顶向下地搜索代价最小的 Physical Algebra。Cascades Optimizer 则将这两个阶段融合在一起，通过提供一个 Guidance 来指导 Rule 的执行顺序，在枚举逻辑等价算子的同时也进行物理算子的生成，这样做可以避免枚举所有的逻辑执行计划，但是其弊端就是错误的 Guidance 会导致搜索在局部收敛，因而搜索不到最优的执行计划。 Volcano/Cascades Optimzier 都使用了 Branch-And-Bound 方法对搜索空间进行剪枝。由于两者都采用了自顶向下的搜索，在搜索的过程中可以为算子设置其 Cost Upper Bound，如果在向下搜索的过程中还没有搜索到叶子节点就超过了预设的 Cost Upper Bound，就可以对这个搜索分支预先进行剪枝。 (adsbygoogle = window.adsbygoogle || []).push({}); 赞助商 VolcanoPlanner 基础介绍 前面部分我们介绍了 Volcano/Cascades 优化器的理论基础，想必大家已经对优化器的原理有了一些基础的认识。为了避免陷入代码细节，我们学习 VolcanoPlanner 之前，先来了解下 VolcanoPlanner 中涉及到的核心概念，理解这些概念会让我们阅读源码更加轻松。然后我们会从整体角度，再来学习下 VolcanoPlanner 的处理流程，看看 Calcite 逻辑计划是如何优化并转换为物理执行计划的。 核心概念 RelNode Caclite 源码中对 RelNode 的定义为 A RelNode is a relational expression，即关系代数表达式，RelNode 继承 RelOptNode 接口，表示可以被优化器优化。关系代数表达式用于处理数据，所以他们通常使用动词命名，例如：Sort、Join、Project、Filter、Scan 等。在 Caclite 中，不建议直接实现 RelNode 接口，而是推荐继承 AbstractRelNode 抽象类。 AbstractRelNode 抽象类的核心属性和方法如下： public abstract class AbstractRelNode implements RelNode /** * RelTraitSet that describes the traits of this RelNode. */ protected RelTraitSet traitSet; @Pure @Override public final @Nullable Convention getConvention(@UnknownInitialization AbstractRelNode this) return traitSet == null ? null : traitSet.getTrait(ConventionTraitDef.INSTANCE); @Override public final RelDataType getRowType() if (rowType == null) rowType = deriveRowType(); assert rowType != null : this; return rowType; @Override public void register(RelOptPlanner planner) Util.discard(planner); @Override public ListRelNode getInputs() return Collections.emptyList(); @Override public double estimateRowCount(RelMetadataQuery mq) return 1.0; @Override public @Nullable RelOptCost computeSelfCost(RelOptPlanner planner, RelMetadataQuery mq) // by default, assume cost is proportional to number of rows double rowCount = mq.getRowCount(this); return planner.getCostFactory().makeCost(rowCount, rowCount, 0); traitSet 用于记录当前 RelNode 的物理特征 RelTrait，Calcite 中提供了 Convention 、RelCollation 和 RelDistribution 3 种物理特征，分别表示调用约定（代表某一种数据源，不同数据源上的算子需要使用 Converter 进行转换）、排序和分布特征； getConvention 方法是用于获取当前 RelNode 中记录的 Convention 特征； getRowType 用于获取当前数据行的类型信息，RelNode 根节点的 RelDataType 可以代表最终查询结果的行记录类型信息； getInputs 用于获取当前 RelNode 的子节点，RelNode 通过 inputs 组织成一个树形结构； estimateRowCount 方法用于估计当前 RelNode 返回的行数，行数信息可以用来计算 RelNode 的代价 Cost； computeSelfCost 方法用于计算当前 RelNode 的代价 Cost； register 方法用于注册当前 RelNode 特有的优化规则，例如：InnodbTableScan 实现了 register 方法，注册了和 InnodbTableScan 这类 RelNode 相关的优化规则。 RelSet Calcite 对 RelSet 的定义为 A RelSet is an equivalence-set of expressions，即一组等价的关系代数集合，同一个 RelSet 中的关系代数具有相同的调用规约（Calling Convention）。RelSet 类中的核心属性如下： class RelSet // 等价的关系代数集合 final ListRelNode rels = new ArrayList(); // 物理属性相同的等价关系代数集合 final ListRelSubset subsets = new ArrayList(); // 等价的 RelSet @MonotonicNonNull RelSet equivalentSet; RelSet 类是等价关系代数的集合类，不是 RelNode； 等价的关系代数集合存储在 rels 中，他们具有相同的调用规约，但是其他物理属性可能不相同，例如：RelCollation 和 RelDistribution； 物理属性相同的等价关系代数集合会存储在 subsets 中，RelSubset 对象会根据物理属性对关系代数进行归类，相同物理属性的关系代数会存储在同一个 RelSubset 中。 RelSubset Caclite 对 RelSubset 的定义为 Subset of an equivalence class where all relational expressions have the same physical properties.，即 RelSet 等价类的子集，它会按照物理属性将关系代数 RelNode 进行分类，物理属性相同的 RelNode 会在同一个 RelSubSet 中。RelSubset 类中的核心属性如下： public class RelSubset extends AbstractRelNode /** * Cost of best known plan (it may have improved since). */ RelOptCost bestCost; /** * The set this subset belongs to. */ final RelSet set; /** * Best known plan. */ @Nullable RelNode best; /** * Returns the rel nodes in this rel subset. All rels must have the same * traits and are logically equivalent. * * @return all the rels in the subset */ public IterableRelNode getRels() return () - Linq4j.asEnumerable(set.rels).where(v1 - v1.getTraitSet().satisfies(traitSet)).iterator(); RelSubset 实现了 AbstractRelNode，是一个特殊的关系代数 RelNode； RelSubSet 中记录了物理属性相同的关系代数 RelNode，并且这些关系代数不是直接存储在 RelSubSet 中，而是通过引用 RelSet 对象并通过 traitSet 过滤得到； RelSubSet 会计算内部关系代数的最优代价 bestCost，并记录当前最优的执行计划 best，bestCost 和 best 会随着优化的执行而不断更新。 处理流程 介绍完 VolcanoPlanner 中的核心概念，让我们再来了解下 Calcite 优化器的处理流程，Julain 在 2016 年举办的 Hadoop Summit 大会上分享了 Cost-based Query Optimization in Apache Phoenix using Apache Calcite，其中介绍了 Caclite 优化器的处理流程，虽然已经过去了很久，但是仍然可以作为 VolcanoPlanner 的参考资料。 上图展示了 VolcanoPlanner 的处理流程，可以看到 SQL 语句被解析为 AST 后，通过 SqlToRelConverter 将 AST 转换为 RelNode 和 RexNode。RelNode Tree 就是我们常说的逻辑执行计划。方框内是 VolcanoPlanner 的核心流程，主要包含了如下几个关键步骤： 将匹配的规则 Rule 添加到 RuleQueue 中，Calcite 提供了 IterativeRuleQueue 和 TopDownRuleQueue； 应用匹配的规则 Rule，对 RelNode Tree 进行转换； 进行相应的迭代，直到 RuleQueue 中的 Rule 全部迭代完成或者代价 Cost 不再变化； 基于 RelNode 的代价和深度匹配 Importance，Importance 描述了 RuleMatch 的重要程度，Importance 大的优先处理，每一轮迭代都会实时调整。 除了以上的几个关键步骤外，图中还描述了 VolcanoPlanner 中的重要组成部分：计划树（Plan Tree）、优化规则（Rules）、代价模型（Cost Model） 和 元数据提供器（Metadata Providers）。计划树通过前文介绍的 RelSet 和 RelSubset 维护了优化过程中所需的数据结构，优化规则用于对 RelNode 进行优化，以生成等价且更优的关系代数，代价模型用于计算 RelNode 的代价和累积代价，元数据提供器则提供了代价计算所需的一些统计信息，例如：Filter 选择性、Join 选择性等。这些组成部分在 VolcanoPlanner 中相互配合，共同完成了优化过程，在下面的源码探秘部分，我们将一一进行研究学习。 VolcanoPlanner 源码探秘 介绍完 VolcanoPlanner 中的核心概念和基础流程，想必大家对 VolcanoPlanner 已经有了初步地认识，但是想要彻底理解 VolcanoPlanner，还需要结合一些案例，对源码进行深入学习理解，才能知其然知其所以然。本小节将以 CsvTest#testSelectSingleProjectGz 测试 Case 为例，和大家一起探秘 VolcanoPlanner 源码。如下展示了测试 Case，使用了 smart 模型，表示使用 TranslatableTable 进行优化处理。 @Testvoid testSelectSingleProjectGz() throws SQLException sql(smart, select * from EMPS where name = Alice).ok(); VolcanoPlanner 初始化 首先，我们来跟踪下 VolcanoPlanner 初始化流程，看下在初始化阶段，优化器都做了哪些准备工作。执行示例程序，在 CalcitePrepareImpl#createPlanner 方法中，我们可以看到如下初始化逻辑： /** * Creates a query planner and initializes it with a default set of rules. */protected RelOptPlanner createPlanner(final CalcitePrepare.Context prepareContext, @Nullable Context externalContext, @Nullable RelOptCostFactory costFactory) if (externalContext == null) externalContext = Contexts.of(prepareContext.config()); // 初始化 VolcanoPlanner，允许用户传入代价工厂 costFactory，默认使用 VolcanoCost.FACTORY final VolcanoPlanner planner = new VolcanoPlanner(costFactory, externalContext); // 设置标量表达式 scalar expressions 的执行器 planner.setExecutor(new RexExecutorImpl(DataContexts.EMPTY)); planner.addRelTraitDef(ConventionTraitDef.INSTANCE); if (CalciteSystemProperty.ENABLE_COLLATION_TRAIT.value()) planner.addRelTraitDef(RelCollationTraitDef.INSTANCE); // 是否开启自顶向下优化，会根据该参数是否开启，初始化不同类型的 RuleDriver 和 RuleQueue planner.setTopDownOpt(prepareContext.config().topDownOpt()); // 注册默认优化规则 RelOptUtil.registerDefaultRules(planner, prepareContext.config().materializationsEnabled(), enableBindable); return planner; 创建 VolcanoPlanner 对象时，允许用户传入 costFactory 代价工厂，默认会使用 VolcanoCost.FACTORY 工厂类。初始化优化器时，同时会设置标量表达式（scalar expressions）执行器，负责计算表达式的结果。setTopDownOpt 方法会根据配置判断是否开启自顶向下优化，该配置默认为 false，同时会根据该参数初始化 RuleDriver 和 RuleQueue，本文先关注 Calcite 默认的 IterativeRuleDriver 和 IterativeRuleQueue，后续文章会再探讨 Volcano Cascades 论文中提出的 TopDownRuleDriver 和 TopDownRuleQueue。 RelOptUtil.registerDefaultRules 方法会注册默认的优化规则，内部调用 planner.addRule 方法，将规则记录在优化器父类 AbstractRelOptPlanner 的 mapDescToRule 属性中。 @Experimentalpublic static void registerDefaultRules(RelOptPlanner planner, boolean enableMaterializations, boolean enableBindable) if (CalciteSystemProperty.ENABLE_COLLATION_TRAIT.value()) registerAbstractRelationalRules(planner); registerAbstractRules(planner); registerBaseRules(planner); if (enableMaterializations) registerMaterializationRules(planner); if (enableBindable) for (RelOptRule rule : Bindables.RULES) planner.addRule(rule); planner.addRule(Bindables.BINDABLE_TABLE_SCAN_RULE); planner.addRule(CoreRules.PROJECT_TABLE_SCAN); planner.addRule(CoreRules.PROJECT_INTERPRETER_TABLE_SCAN); if (CalciteSystemProperty.ENABLE_ENUMERABLE.value()) registerEnumerableRules(planner); planner.addRule(EnumerableRules.TO_INTERPRETER); if (enableBindable CalciteSystemProperty.ENABLE_ENUMERABLE.value()) planner.addRule(EnumerableRules.TO_BINDABLE); if (CalciteSystemProperty.ENABLE_STREAM.value()) for (RelOptRule rule : StreamRules.RULES) planner.addRule(rule); planner.addRule(CoreRules.FILTER_REDUCE_EXPRESSIONS); Calcite JDBC 默认注册了 101 个优化规则，这些优化规则的作用，我们后续文章会进行分类学习，在实际使用中可以选择自己需要的优化规则去使用。到这里，Calicte 就完成了 VolcanoPlanner 的优化，并默认注册了 101 个优化规则。 setRoot 流程 VolcanoPlanner 初始化完成后，又会调用 SqlParser 进行 SQL 解析，并使用 SqlToRelConverter 将 AST 转换为 RelNode 逻辑执行计划，可以得到如下的 Logical Plan： LogicalProject(EMPNO=[$0], NAME=[$1], DEPTNO=[$2], GENDER=[$3], CITY=[$4], EMPID=[$5], AGE=[$6], SLACKER=[$7], MANAGER=[$8], JOINEDAT=[$9]) LogicalFilter(condition=[=($1, Alice)]) CsvTableScan(table=[[SALES, EMPS]], fields=[[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]]) Calcite JDBC 流程中将优化器的调用封装在了 Program 中，如下示例展示了调用逻辑，最核心的方式是 setRoot 和 findBestExp，本小节先关注 setRoot 方法的实现逻辑，看看示例中的两次 setRoot 都进行了哪些处理。 /** * Returns the standard program with user metadata provider. */public static Program standard(RelMetadataProvider metadataProvider) final Program program1 = (planner, rel, requiredOutputTraits, materializations, lattices) - for (RelOptMaterialization materialization : materializations) planner.addMaterialization(materialization); for (RelOptLattice lattice : lattices) planner.addLattice(lattice); // setRoot 设置 RelSubset 根节点 planner.setRoot(rel); // 变换 trait 属性，将 Convention NONE 变换为 ENUMERABLE final RelNode rootRel2 = rel.getTraitSet().equals(requiredOutputTraits) ? rel : planner.changeTraits(rel, requiredOutputTraits); assert rootRel2 != null; // setRoot 设置 RelSubset 根节点 planner.setRoot(rootRel2); final RelOptPlanner planner2 = planner.chooseDelegate(); // 查找最佳执行计划 final RelNode rootRel3 = planner2.findBestExp(); assert rootRel3 != null : could not implement exp; return rootRel3; ; return sequence(subQuery(metadataProvider), new Programs.DecorrelateProgram(), new Programs.TrimFieldsProgram(), program1, calc(metadataProvider)); 第一轮 setRoot 第一次调用 setRoot 方法，直接传递了原始的 RelNode，未进行 Trait 变换，setRoot 方法负责将 RelNode Tree 转换为 RelSubset Tree，并设置到 VolcanoPlanner 中的 root 属性中。如下是 setRoot 的代码实现，registerImpl 是其核心逻辑。 // RelSubset 根节点protected @MonotonicNonNull RelSubset root;@Overridepublic void setRoot(RelNode rel) // 核心逻辑 this.root = registerImpl(rel, null); if (this.originalRoot == null) this.originalRoot = rel; rootConvention = this.root.getConvention(); ensureRootConverters(); registerImpl 方法用于注册新的关系代数表达式，并将匹配的规则加入到队列中。如果 set（等价集合） 参数不为 null，则将当前表达式加入到等价集合中，如果已经注册了相同的表达式，则无需将其加入到等价集合以及队列中。 private RelSubset registerImpl(RelNode rel, @Nullable RelSet set) // 如果 rel 已经是 RelSubset，则直接调用 registerSubset 注册 if (rel instanceof RelSubset) return registerSubset(set, (RelSubset) rel); ... // Ensure that its sub-expressions are registered. // 监听该表达式将要注册的通知 rel = rel.onRegister(this); ... onRegister 第一次调用 setRoot 方法时，rel 参数为 LogicalProject，因此会调用后续逻辑进行处理，onRegister 方法会确保 RelNode 的子节点也注册生成 RelSubset。AbstractRelNode#onRegister 方法实现逻辑如下，getInputs 方法会获取当前 RelNode 的子节点（返回 LogicalFilter 子节点），并调用 VolcanoPlanner#ensureRegistered 方法，确保所有的子节点都会进行注册，然后重新 copy 生成新的 RelNode。 @Overridepublic RelNode onRegister(RelOptPlanner planner) // 获取子节点 ListRelNode oldInputs = getInputs(); ListRelNode inputs = new ArrayList(oldInputs.size()); for (final RelNode input : oldInputs) // 调用 VolcanoPlanner#ensureRegistered 注册子节点 RelNode e = planner.ensureRegistered(input, null); assert e == input || RelOptUtil.equal(rowtype of rel before registration, input.getRowType(), rowtype of rel after registration, e.getRowType(), Litmus.THROW); inputs.add(e); RelNode r = this; if (!Util.equalShallow(oldInputs, inputs)) // 复制生成新的 RelNode r = copy(getTraitSet(), inputs); // 重新计算 Digest 摘要信息，是 RelNode 的唯一标识 r.recomputeDigest(); return r; VolcanoPlanner#ensureRegistered 方法会对当前子节点 LogicalFilter 进行注册，先调用 getSubset 从 mapRel2Subset 获取当前 RelNode 对应的 RelSubset，如果不存在则调用 VolcanoPlanner#register 方法进行注册。 // 维护已注册的 RelNode 和 RelSubset 映射关系private final IdentityHashMapRelNode, RelSubset mapRel2Subset = new IdentityHashMap();@Overridepublic RelSubset ensureRegistered(RelNode rel, @Nullable RelNode equivRel) RelSubset result; // 从 mapRel2Subset 中获取 RelSubset final RelSubset subset = getSubset(rel); if (subset != null) if (equivRel != null) final RelSubset equivSubset = getSubsetNonNull(equivRel); // 如果当前节点的等价集合和已知的等价集合不同，则进行合并 if (subset.set != equivSubset.set) merge(equivSubset.set, subset.set); result = canonize(subset); else // 如果 RelSubset 为空则进行注册 result = register(rel, equivRel); ... return result; VolcanoPlanner#register 方法会调用 VolcanoPlanner#registerImpl 对 LogicalFilter 节点进行注册，然后逻辑又重新回到了 VolcanoPlanner#registerImpl 方法。onRegister 方法会对 LogicalFilter 节点的子节点 CsvTableScan 进行注册，由于 CsvTableScan 节点没有子节点，因此在 onRegister 方法处理时会中断递归，此外，由于没有子节点，CsvTableScan 会返回当前 RelNode。 @Overridepublic RelNode onRegister(RelOptPlanner planner) // 获取子节点 ListRelNode oldInputs = getInputs(); ListRelNode inputs = new ArrayList(oldInputs.size()); for (final RelNode input : oldInputs) ... RelNode r = this; if (!Util.equalShallow(oldInputs, inputs)) // 复制生成新的 RelNode r = copy(getTraitSet(), inputs); // 重新计算 Digest 摘要信息，是 RelNode 的唯一标识 r.recomputeDigest(); return r; 然后优化器会从双端队列 ruleCallStack 中获取首个元素，并记录到 provenanceMap 中，provenanceMap 用于记录 RelNode 的来源，包括 UnknownProvenance、DirectProvenance 和 RuleProvenance。如果当前参数的 RelSet 为 null，则会创建一个 RelSet 并添加到 allSets 中。registerClass 方法允许 RelNode 注册自己特有的优化规则，本案例中 CsvTableScan 注册了 CsvRules.PROJECT_SCAN 规则。完成规则注册后，优化器会调用 addRelToSet 和 fireRules 方法，这部分是 VolcanoPlanner 的核心逻辑，下面我们来一起深入分析下。 private RelSubset registerImpl(RelNode rel, @Nullable RelSet set) if (rel instanceof RelSubset) return registerSubset(set, (RelSubset) rel); // Ensure that its sub-expressions are registered. rel = rel.onRegister(this); // Record its provenance. (Rule call may be null.) // 从双端队列中获取 VolcanoRuleCall，并记录到 provenanceMap 中 final VolcanoRuleCall ruleCall = ruleCallStack.peek(); if (ruleCall == null) provenanceMap.put(rel, Provenance.EMPTY); else provenanceMap.put(rel, new RuleProvenance(ruleCall.rule, ImmutableList.copyOf(ruleCall.rels), ruleCall.id)); ... // Place the expression in the appropriate equivalence set. // 如果当前 RelSet 为空，则创建一个 RelSet 并添加到 allSets 中 if (set == null) set = new RelSet(nextSetId++, Util.minus(RelOptUtil.getVariablesSet(rel), rel.getVariablesSet()), RelOptUtil.getVariablesUsed(rel)); this.allSets.add(set); ... // Allow each rel to register its own rules. // 触发当前 RelNode#register 方法，允许注册自己的优化规则 // CsvTableScan#register 方法注册了 CsvRules.PROJECT_SCAN 规则 registerClass(rel); // 当前节点注册完成后，调用 addRelToSet 添加到等价集合中 RelSubset subset = addRelToSet(rel, set); ... // Queue up all rules triggered by this relexps creation. // 对注册的规则进行匹配筛选 fireRules(rel); ... return subset; addRelToSet 每个 RelNode 注册完成后会调用 addRelToSet 添加到等价集 RelSet 中，set.add(rel) 内部会调用 RelSet#getOrCreateSubset 方法，该方法会根据特征 Trait 判断 RelSubset 是否已经存在，不存在则创建 RelSubset 实例，此时 bestCost 为 VolcanoCost.INFINITY。然后会将返回的 RelSubset 维护到 mapRel2Subset 中，方便后续复用。propagateCostImprovements 会重新计算节点的代价，如果它的代价小于 RelSubset 的代价，则更新 RelSubset 中的 bestCost 和 best。 private RelSubset addRelToSet(RelNode rel, RelSet set) // 添加到等价集合中 RelSet 和 RelSubset 中，并返回 RelSubset RelSubset subset = set.add(rel); // 维护 Rel 和 Subset 映射关系 mapRel2Subset.put(rel, subset); // While a tree of RelNodes is being registered, sometimes nodes costs // improve and the subset doesnt hear about it. You can end up with // a subset with a single rel of cost 99 which thinks its best cost is // 100. We think this happens because the back-links to parents are // not established. So, give the subset another chance to figure out // its cost. try // 重新计算是否存在更小的 cost，存在则更新 RelSubset 中的 bestCost 和 best propagateCostImprovements(rel); catch (CyclicMetadataException e) // ignore if (ruleDriver != null) ruleDriver.onProduce(rel, subset); return subset; propagateCostImprovements 方法的实现逻辑如下，方法内部定义了一个优先级队列，队列会根据 RelNode 的代价 RelOptCost 进行排序，从而方便获取最小代价的 RelNode。然后从队列中弹出 RelNode，并遍历 RelNode 对应 RelSet 中的 RelSubset，判断当前计算的代价是否小于已知的最小代价，如果代价更小则更新最小代价 bestCost 和最优计划 best。 void propagateCostImprovements(RelNode rel) RelMetadataQuery mq = rel.getCluster().getMetadataQuery(); // RelNode 和 RelOptCost 映射，方便后续获取 RelOptCost MapRelNode, RelOptCost propagateRels = new HashMap(); // 优先级队列，按照 RelOptCost 排序 PriorityQueueRelNode propagateHeap = new PriorityQueue((o1, o2) - ...); // 获取 RelNode 对应的代价 propagateRels.put(rel, getCostOrInfinite(rel, mq)); // 添加到队列中 propagateHeap.offer(rel); RelNode relNode; // 从队列中弹出 RelNode while ((relNode = propagateHeap.poll()) != null) RelOptCost cost = requireNonNull(propagateRels.get(relNode), propagateRels.get(relNode)); // 遍历当前 RelNode 对应 RelSet 中的 RelSubset 集合（Trait 不同存储在不同 RelSubset 中） for (RelSubset subset : getSubsetNonNull(relNode).set.subsets) // 判断 Trait 是否相同 if (!relNode.getTraitSet().satisfies(subset.getTraitSet())) continue; // 判断代价是否小于已知最小代价 if (!cost.isLt(subset.bestCost)) continue; // Update subset best cost when we find a cheaper rel or the current // bests cost is changed subset.timestamp++; LOGGER.trace(Subset cost changed: subset [] cost was now , subset, subset.bestCost, cost); // 更新最小代价和最优计划 subset.bestCost = cost; subset.best = relNode; // since best was changed, cached metadata for this subset should be removed mq.clearCache(subset); // 遍历 RelSubset 的父节点（CsvTableScan 对应 RelSet 的父节点为空） for (RelNode parent : subset.getParents()) mq.clearCache(parent); // 计算父节点代价 RelOptCost newCost = getCostOrInfinite(parent, mq); RelOptCost existingCost = propagateRels.get(parent); if (existingCost == null || newCost.isLt(existingCost)) // 如果父节点代价更小，则加入 propagateHeap 重新计算 propagateRels.put(parent, newCost); if (existingCost != null) // Cost reduced, force the heap to adjust its ordering propagateHeap.remove(parent); propagateHeap.offer(parent); 计算 RelNode 代价是通过 VolcanoPlanner#getCostOrInfinite 方法，如果 getCost 返回的代价为 null，则会返回无穷大代价 infCost。getCost 方法会先判断当前 RelNode 是否已经是 RelSubset，如果是则直接返回 bestCost。然后根据 noneConventionHasInfiniteCost 标记以及当前 RelNode 的 Trait 判断是否针对 None Convention 直接返回无穷大代价，noneConventionHasInfiniteCost 参数可以通过 VolcanoPlanner#setNoneConventionHasInfiniteCost 方法设置。当前节点的代价计算是调用 RelMetadataQuery#getNonCumulativeCost 方法获取，然后获取子节点的代价进行累加，即 RelNode 总代价 = RelNode 自身代价 + 所有子节点代价。 private RelOptCost getCostOrInfinite(RelNode rel, RelMetadataQuery mq) RelOptCost cost = getCost(rel, mq); return cost == null ? infCost : cost;@Overridepublic @Nullable RelOptCost getCost(RelNode rel, RelMetadataQuery mq) // 如果已经是 RelSubset，直接返回 bestCost if (rel instanceof RelSubset) return ((RelSubset) rel).bestCost; // 根据 noneConventionHasInfiniteCost 标记以及当前 RelNode 的 Trait 判断是否针对 None Convention 直接返回无穷大代价 // noneConventionHasInfiniteCost 参数可以通过 VolcanoPlanner#setNoneConventionHasInfiniteCost 设置 if (noneConventionHasInfiniteCost rel.getTraitSet().getTrait(ConventionTraitDef.INSTANCE) == Convention.NONE) return costFactory.makeInfiniteCost(); // 获取当前 RelNode 的代价 RelOptCost cost = mq.getNonCumulativeCost(rel); if (cost == null) return null; // 判断代价是否为正数代价，不满足则返回最小代价 if (!zeroCost.isLt(cost)) // cost must be positive, so nudge it cost = costFactory.makeTinyCost(); // 获取子节点的代价进行累加，即 RelNode 总代价 = RelNode 自身代价 + 所有子节点代价 for (RelNode input : rel.getInputs()) RelOptCost inputCost = getCost(input, mq); if (inputCost == null) return null; cost = cost.plus(inputCost); return cost; RelMetadataQuery#getNonCumulativeCost 方法如下，Calcite 会通过 Janino 动态生成 nonCumulativeCostHandler 对象，然后调用 RelMdPercentageOriginalRows#getNonCumulativeCost 方法，该方法会调用 RelNode#computeSelfCost 方法，此处为 CsvTableScan 实现的方法。CsvTableScan 会调用父类 TableScan 中的方法，此时会获取统计信息中获取行数信息 rowCount，然后使用优化器中的 CostFactory 计算代价。 // RelMetadataQuery#getNonCumulativeCost 方法public @Nullable RelOptCost getNonCumulativeCost(RelNode rel) for (; ; ) try return nonCumulativeCostHandler.getNonCumulativeCost(rel, this); catch (MetadataHandlerProvider.NoHandler e) nonCumulativeCostHandler = revise(BuiltInMetadata.NonCumulativeCost.Handler.class); // RelMdPercentageOriginalRows#getNonCumulativeCost 方法public @Nullable RelOptCost getNonCumulativeCost(RelNode rel, RelMetadataQuery mq) return rel.computeSelfCost(rel.getCluster().getPlanner(), mq);// CsvTableScan#computeSelfCost 方法public @Nullable RelOptCost computeSelfCost(RelOptPlanner planner, RelMetadataQuery mq) // Multiply the cost by a factor that makes a scan more attractive if it // has significantly fewer fields than the original scan. // // The + 2D on top and bottom keeps the function fairly smooth. // // For example, if table has 3 fields, project has 1 field, // then factor = (1 + 2) / (3 + 2) = 0.6 return super.computeSelfCost(planner, mq).multiplyBy(((double) fields.length + 2D) / ((double) table.getRowType().getFieldCount() + 2D));// TableScan#computeSelfCost 方法@Overridepublic @Nullable RelOptCost computeSelfCost(RelOptPlanner planner, RelMetadataQuery mq) double dRows = table.getRowCount(); double dCpu = dRows + 1; // ensure non-zero cost double dIo = 0; return planner.getCostFactory().makeCost(dRows, dCpu, dIo);// RelOptTableImpl#getRowCount 方法@Overridepublic double getRowCount() if (rowCount != null) return rowCount; if (table != null) // CSV 示例中未注册统计信息，默认为 Statistics.UNKNOWN，rowCount 为 null final Double rowCount = table.getStatistic().getRowCount(); if (rowCount != null) return rowCount; // 默认返回 100d return 100d; 最终返回的 CsvTableScan VolcanoCost 对象如下图所示，记录了 cpu、io 和 rowCount 信息。 propagateCostImprovements 方法会按照前文所述，将 RelSubset 中的代价和新计算的代价进行比较，如果发现更小代价，则会更新 bestCost 和 best 属性，RelSubset 更新后的对象如下图所示。 fireRules 生成完 RelSubset 并计算 RelNode 的代价后，VolcanoPlanner 会调用 fireRules 方法，对队列中的规则进行匹配筛选，fireRules 实现逻辑如下。 /** * Fires all rules matched by a relational expression. * * @param rel Relational expression which has just been created (or maybe * from the queue) */void fireRules(RelNode rel) for (RelOptRuleOperand operand : classOperands.get(rel.getClass())) // 判断当前 Rel 是否匹配规则 if (operand.matches(rel)) final VolcanoRuleCall ruleCall; ruleCall = new DeferringRuleCall(this, operand); ruleCall.match(rel); classOperands 中记录了 RelNode 和 RelOptRuleOperand 的对应关系，RelOptRuleOperand 用于判断 RelOptRule 是否可以用于某个关系代数。下图展示了 CsvTableScan 对应的 RelOptRuleOperand 集合，这些 RelOptRuleOperand 都是和 TableScan 相关的规则。 对于每一个 RelOptRuleOperand，都会调用其 matches 方法，方法内会判断 RelNode 是否是 RelOptRuleOperand 中记录的 clazz 实例，以及 RelNode 是否包含定义的 trait 特征，最后会使用 predicate 方法对 RelNode 进行匹配。 public boolean matches(RelNode rel) if (!clazz.isInstance(rel)) return false; if ((trait != null) !rel.getTraitSet().contains(trait)) return false; return predicate.test(rel); 好奇的读者可能会问，RelOptRuleOperand 记录的这些信息是在什么时候初始化的？以 CsvProjectTableScanRule 为例，在该优化规则初始化时，会调用 super(config) 方法，使用 OperandBuilderImpl.operand(config.operandSupplier()) 初始化 RelOptRuleOperand 类，感兴趣的朋友可以自行探究下。 RelOptRuleOperand 匹配成功后，会创建一个 DeferringRuleCall，该类表示对 Rule 的调用，并且可以延迟执行。然后调用 DeferringRuleCall#match 方法应用当前匹配的规则，match 方法会调用 VolcanoRuleCall#matchRecurse 方法，如果规则匹配则会调用 onMatch 方法。DeferringRuleCall#onMatch 方法会匹配规则以及 RelNode 封装成 VolcanoRuleMatch，然后添加到 RuleQueue 中。 // VolcanoRuleCall#matchRecurse 方法private void matchRecurse(int solve) final ListRelOptRuleOperand operands = getRule().operands; // 当求解顺序参数等于操作数时，判断当前 Rule 是否 if (solve == operands.size()) // We have matched all operands. Now ask the rule whether it // matches; this gives the rule chance to apply side-conditions. // If the side-conditions are satisfied, we have a match. if (getRule().matches(this)) onMatch(); else ... // DeferringRuleCall#onMatch 方法/** * Rather than invoking the rule (as the base method does), creates a * @link VolcanoRuleMatch which can be invoked later. */protected void onMatch() final VolcanoRuleMatch match = new VolcanoRuleMatch(volcanoPlanner, getOperand0(), rels, nodeInputs); volcanoPlanner.ruleDriver.getRuleQueue().addMatch(match); 至此，setRoot 就完成了对 CsvTableScan 节点的处理，为 CsvTableScan 生成了 RelSet 和 RelSubset，并筛选了 CsvTableScan 匹配的规则。CsvTableScan 对应的 RelSubset 会以 inputs 的形式返回，提供给 LogicalFilter 作为子节点，LogicalFilter 仍然会按照前文介绍的 onRegister - addRelToSet - fireRules 的流程进行处理，并同样返回 RelSubset 作为 LogicalProject 的子节点。LogicalFilter 和 LogicalProject 由于 Convention 为 None，因此计算代价时，他们的代价为正无穷，执行完第一轮 setRoot 方法，最终会得到如下的 RelSubset 树。 LogicalProject(subset=[rel#14:RelSubset#2.NONE.[]], EMPNO=[$0], NAME=[$1], DEPTNO=[$2], GENDER=[$3], CITY=[$4], EMPID=[$5], AGE=[$6], SLACKER=[$7], MANAGER=[$8], JOINEDAT=[$9]) LogicalFilter(subset=[rel#12:RelSubset#1.NONE.[]], condition=[=($1, Alice)]) CsvTableScan(subset=[rel#10:RelSubset#0.ENUMERABLE.[]], table=[[SALES, EMPS]], fields=[[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]]) RelSubset 树是通过成员变量 final RelSet set 变量实现，RelSet 中维护了当前 RelNode，通过 RelNode 的 input 维护了 RelSubset 子节点，以此类推，形成了一颗 RelSubset 树，整体结构如下图所示。 第二轮 setRoot 在调用第二轮 setRoot 前，会优先判断当前 RelNode 的 Trait 是否和目标 Trait 相同，不相同则调用优化器的 changeTraits 方法变换特征。由于 RelNode 中的 Convention Trait 是 NONE，目标 Convention Trait 是 ENUMERABLE，因此会先调用 changeTraits 方法。 final RelNode rootRel2 = rel.getTraitSet().equals(requiredOutputTraits) ? rel : planner.changeTraits(rel, requiredOutputTraits); changeTraits changeTraits 实现逻辑如下，会传入 RelNode 和期望的 RelTraitSet，然后先调用 ensureRegistered 确保所有的 RelNode 都注册成 RelSubset，然后调用 getOrCreateSubset 方法生成 RelTraitSet 对应的 RelSubset。 public RelNode changeTraits(final RelNode rel, RelTraitSet toTraits) RelSubset rel2 = ensureRegistered(rel, null); if (rel2.getTraitSet().equals(toTraits)) return rel2; return rel2.set.getOrCreateSubset(rel.getCluster(), toTraits, true); 此时，根节点 RelSubSet 的 Convention 已经变换为 ENUMERABLE，子节点 RelSubSet 的 Convention 仍然是 NONE，后续需要关注子节点 Convention 的变换时机。 registerSubset 由于经过了第一轮 setRoot 以及 changeTraits 处理，rootRel2 变成了一颗 RelSubset 树，在第二轮 setRoot 调用 registerImpl 时，由于 RelNode 已经是 RelSubset，因此会调用 registerSubset 方法。 // VolcanoPlanner#registerImpl 方法private RelSubset registerImpl(RelNode rel, @Nullable RelSet set) if (rel instanceof RelSubset) return registerSubset(set, (RelSubset) rel); ... registerSubset 方法实现逻辑如下，首先会尝试对 RelSet 进行合并，由于当前案例中 RelSet set 为 null，未覆盖 merge 逻辑，后续我们会探索其他复杂案例的 RelSet 合并操作。canonize 方法用于处理当前 RelSubset 存在多个等价的 RelSubset 时，获取原始的 RelSubSet。 private RelSubset registerSubset(@Nullable RelSet set, RelSubset subset) if ((set != subset.set) (set != null) (set.equivalentSet == null)) LOGGER.trace(Register # , and merge sets, subset.getId(), subset); merge(set, subset.set); return canonize(subset);/** * If a subset has one or more equivalent subsets (owing to a set having * merged with another), returns the subset which is the leader of the * equivalence class. * * @param subset Subset * @return Leader of subsets equivalence class */private static RelSubset canonize(final RelSubset subset) RelSet set = subset.set; if (set.equivalentSet == null) return subset; // 循环获取原始的 RelSet，然后创建对应 Trait 的 RelSubset do set = set.equivalentSet; while (set.equivalentSet != null); return set.getOrCreateSubset(subset.getCluster(), subset.getTraitSet(), subset.isRequired()); ensureRootConverters 最后会执行 ensureRootConverters 方法，确保根节点的等价集合都包含了 AbstractConverter，以便于发现代价更小的实现时，能够将 RelSubset 转换为根节点。ensureRootConverters 方法实现逻辑如下，如果根节点中记录的等价关系代数 RelNode 已经是 AbstractConverter，则直接添加到 subsets 集合中。然后判断根节点的所有 RelSubset，如果发现 root trait 和 subset trait 不同时，将会注册一个 AbstractConverter（AbstractConverter 是一个 RelNode，用于将一个关系代数转换为指定 Convention 的关系代数）。 /** * Ensures that the subset that is the root relational expression contains * converters to all other subsets in its equivalence set. * * pThus the planner tries to find cheap implementations of those other * subsets, which can then be converted to the root. This is the only place * in the plan where explicit converters are required; elsewhere, a consumer * will be asking for the result in a particular convention, but the root has * no consumers. */void ensureRootConverters() final SetRelSubset subsets = new HashSet(); for (RelNode rel : root.getRels()) if (rel instanceof AbstractConverter) subsets.add((RelSubset) ((AbstractConverter) rel).getInput()); for (RelSubset subset : root.set.subsets) final ImmutableListRelTrait difference = root.getTraitSet().difference(subset.getTraitSet()); // 当 root trait 和 subset trait 不同时，注册一个 AbstractConverter（AbstractConverter 是一个 RelNode） if (difference.size() == 1 subsets.add(subset)) register(new AbstractConverter(subset.getCluster(), subset, difference.get(0).getTraitDef(), root.getTraitSet()), root); 然后调用 register 方法，分别将 AbstractConverter 和 root 节点作为参数传入，然后调用 ensureRegistered 方法将 RelNode 注册为 RelSubset，此处 root 节点已经为 RelSubset，所以会直接返回，并获取到 RelSubset 对应的 RelSet。 public RelSubset register(RelNode rel, @Nullable RelNode equivRel) final RelSet set; if (equivRel == null) set = null; else ... equivRel = ensureRegistered(equivRel, null); set = getSet(equivRel); return registerImpl(rel, set); 然后逻辑会再次调用到 registerImpl 方法，当发现当前节点是 Converter 时，会尝试将 Converter merge 到 Converter 子节点的 RelSet 中。 private RelSubset registerImpl(RelNode rel, @Nullable RelSet set) ... // Converters are in the same set as their children. if (rel instanceof Converter) final RelNode input = ((Converter) rel).getInput(); final RelSet childSet = castNonNull(getSet(input)); // if ((set != null) (set != childSet) (set.equivalentSet == null)) merge(set, childSet); ... else set = childSet; ... 第二轮 setRoot 结束后，RelSubset 的树形结构如下图所示，根节点的 Convention 变成了 ENUMERABLE，根节点 RelSet 中记录的 rels 增加了 AbstractConverter，subsets 增加了 Convention 为 ENUMERABLE 的 RelSubset，其他子节点的信息和第一轮 setRoot 一致。 findBestExp 流程 完成了 setRoot 流程后，最后一步就是调用 findBestExp 方法，根据 setRoot 阶段生成的 RelSubset 树以及其中记录的代价信息，寻找最优的执行计划。下面是 findBestExp 方法的实现，核心的处理逻辑主要是 ruleDriver.drive() 和 buildCheapestPlan 方法，ruleDriver.drive() 负责从 ruleQueue 中取出匹配的规则并进行关系代数变换，并和之前的代价进行比较以寻找每一个节点的最小代价实现。buildCheapestPlan 方法则遍历整个 RelSubset 树，寻找出全局最优的执行计划。 /** * Finds the most efficient expression to implement the query given via * @link org.apache.calcite.plan.RelOptPlanner#setRoot(org.apache.calcite.rel.RelNode). * * @return the most efficient RelNode tree found for implementing the given * query */@Overridepublic RelNode findBestExp() assert root != null : root must not be null; // 确保所有等价集都包含 AbstractConverter，以便于发现代价更小的实现时，能够将 RelSubset 转换为根节点 ensureRootConverters(); // 注册物化视图相关的关系代数，本文暂时不涉及，后续文章会单独解读物化视图和 Lattice 格 registerMaterializations(); // 寻找最优 plan，即 cost 最小的 plan，先找到每个节点的最优 plan，然后构建全局最优 plan // ruleDriver 包括 IterativeRuleDriver 和 TopDownRuleDriver 两种，本文案例使用的是 IterativeRuleDriver ruleDriver.drive(); // 构建全局最优 plan RelNode cheapest = root.buildCheapestPlan(this); return cheapest; drive 本文案例中 driver 的实现类为 IterativeRuleDriver，该方法负责应用规则，按照优化规则对关系代数进行变换。IterativeRuleDriver#drive 方法实现逻辑如下，该方法使用了一个 while(true) 死循环，会不断地从 ruleQueue 中弹出规则，并调用 VolcanoRuleMatch#onMatch 方法对关系代数进行变换。当 ruleQueue 中没有匹配的规则，或者优化器抛出了 VolcanoTimeoutException 时，此时会中断循环。 public void drive() while (true) // 从 ruleQueue 中弹出匹配规则 VolcanoRuleMatch match = ruleQueue.popMatch(); if (match == null) break; // 判断规则是否匹配 assert match.getRule().matches(match); try // 调用 onMatch 方法对关系代数进行变换 match.onMatch(); catch (VolcanoTimeoutException e) LOGGER.warn(Volcano planning times out, cancels the subsequent optimization.); planner.canonize(); break; // The root may have been merged with another subset. Find the new root subset. planner.canonize(); 当前的案例中，preQueue 中记录了 2 个需要预先处理的匹配规则：ExpandConversionRule 和 ProjectRemoveRule，ruleQueue 包含了 4 个匹配规则，分别是 EnumerableFilterRule、ProjectFilterTransposeRule、EnumerableProjectRule 和 ExpandConversionRule。 ExpandConversionRule 则用于将 AbstractConverter 转换为 converters 链，converters 链会将原始的关系代数转换到目标特征。ProjectRemoveRule 负责将仅返回其输入的 Project 节点转换为其子节点，例如：Project(ArrayReader(a), $input0) becomes ArrayReader(a)。 EnumerableFilterRule 和 EnumerableProjectRule 在 Calcite 中属于 ConverterRule，负责将 LogicalFilter、LogicalProject 转换为 EnumerableFilter 和 EnumerableProject。ProjectFilterTransposeRule 会将 Project 和 Filter 进行转置变换，属于 TransformationRule。 从队列中弹出 VolcanoRuleMatch 后会调用 VolcanoRuleMatch#onMatch 方法进行关系代数变换，方法实现逻辑如下。VolcanoRuleMatch 继承了 RelOptRuleCall，RelOptRuleCall 代表了对 RelOptRule 的调用，并传递了一组关系表达式作为参数。开始 onMatch 前，会将当前的 VolcanoRuleCall 添加到 deque 头部，然后调用不同 rule 的 onMatch 方法，完成后 finally 代码块会从 deque 头部弹出。 // VolcanoRuleMatch 继承了 RelOptRuleCall，RelOptRuleCall 代表了对 RelOptRule 的调用，并传递了一组关系表达式作为参数protected void onMatch() try ... // 遍历 VolcanoRuleMatch 中记录的 rels for (int i = 0; i rels.length; i++) RelNode rel = rels[i]; // 获取对应的 RelSubset RelSubset subset = volcanoPlanner.getSubset(rel); // 检查 subset 不能为空，并输出 debug 日志 ... // 将当前的 VolcanoRuleCall 添加到 deque 头部，push 内部调用 addFirst volcanoPlanner.ruleCallStack.push(this); try // 调用 VolcanoRuleCall 中缓存的 rule#onMatch 方法 getRule().onMatch(this); finally // 从 ruleCallStack 中弹出首个对象，调用 deque removeFirst 方法 volcanoPlanner.ruleCallStack.pop(); catch (Exception e) throw new RuntimeException(Error while applying rule + getRule() + , args + Arrays.toString(rels), e); 以 EnumerableFilterRule 为例，onMatch 方法会先调用 convert 方法，将 LogicalFilter 转换为 EnumerableFilter，然后调用 transformTo 方法对 RelNode 树进行变换。 public void onMatch(RelOptRuleCall call) RelNode rel = call.rel(0); if (rel.getTraitSet().contains(inTrait)) // 将 LogicalFilter 转换为 EnumerableFilter final RelNode converted = convert(rel); if (converted != null) // 调用 transformTo 方法对 RelNode 树进行变换 call.transformTo(converted); VolcanoRuleCall#transformTo 实现逻辑如下，由于 EnumerableFilter 是转换的节点，会调用 ensureRegistered 方法对该节点进行重新注册，此时会计算 EnumerableFilter 的代价，并更新 RelSubset 中记录的最小代价。 public void transformTo(RelNode rel, MapRelNode, RelNode equiv, RelHintsPropagator handler) // 对 Hint 进行处理，将原始 RelNode 的 Hint 复制到新的 RelNode 中 rel = handler.propagate(rels[0], rel); try ... // Registering the root relational expression implicitly registers // its descendants. Register any explicit equivalences first, so we // dont register twice and cause churn. // 遍历等价集，并进行注册，本案例中 EnumerableFilter 等价集合为空 for (Map.EntryRelNode, RelNode entry : equiv.entrySet()) volcanoPlanner.ensureRegistered(entry.getKey(), entry.getValue()); // 注册 EnumerableFilter 并重新计算最小代价 RelSubset subset = volcanoPlanner.ensureRegistered(rel, rels[0]); ... catch (Exception e) throw new RuntimeException(Error occurred while applying rule + getRule(), e); 变换完成后 RelSubset 树更新了 bestCost，并且 rels 中同时记录了 LogicalFilter 和 EnumerableFilter。 buildCheapestPlan 变换完成后，会调用 RelSubset#buildCheapestPlan 方法构建代价最小的执行计划，buildCheapestPlan 方法实现逻辑如下，首先会初始化 CheapestPlanReplacer 类，它负责遍历 RelSubset 树并将每个节点替换为代价最小的 RelNode，遍历完成后返回全局最小代价的执行计划。 /** * Recursively builds a tree consisting of the cheapest plan at each node. */RelNode buildCheapestPlan(VolcanoPlanner planner) // 初始化树遍历器，会遍历 RelSubset 树并进行节点替换 CheapestPlanReplacer replacer = new CheapestPlanReplacer(planner); // Replacer 内部维护了 final MapInteger, RelNode visited = new HashMap(); 记录当前节点是否遍历过 final RelNode cheapest = replacer.visit(this, -1, null); ... return cheapest; CheapestPlanReplacer#visit 是处理的核心逻辑，其实现细节如下，首先会根据 RelNode 的 Id 标识从 visited 中获取最优节点，如果当前节点已经遍历过则会直接返回。如果 visited 中未包含，则会判断节点是否为 RelSubset，案例中的节点已经都变换为 RelSubset，因此这一步会找出 RelSubset 中的最小代价 cheapest 进行替换。然后会继续遍历子节点寻找 cheapest 进行替换，替换后的子节点会和原子节点进行比对，不同则会将新的子节点复制到当前节点中。 public RelNode visit(RelNode p, int ordinal, @Nullable RelNode parent) // 每一个 RelNode 都有个唯一 Id final int pId = p.getId(); // 从 visited 中获取当前节点是否已经遍历过，如果遍历过则直接返回 RelNode prevVisit = visited.get(pId); if (prevVisit != null) // return memoized result of previous visit if available return prevVisit; // 判断当前节点为 RelSubset，则进行进一步处理 if (p instanceof RelSubset) RelSubset subset = (RelSubset) p; // 获取 RelSubset 中记录的最优 plan RelNode cheapest = subset.best; if (cheapest == null) // 如果获取不到最优 plan，则抛出异常 ... LOGGER.trace(Caught exception in class=, method=visit, getClass().getName(), e); throw e; p = cheapest; ... // 获取当前节点的子节点，进行遍历处理，获取最优 plan ListRelNode oldInputs = p.getInputs(); ListRelNode inputs = new ArrayList(); for (int i = 0; i oldInputs.size(); i++) RelNode oldInput = oldInputs.get(i); // 遍历子节点 RelNode input = visit(oldInput, i, p); inputs.add(input); // 新的子节点和老的子节点不同，则将新的子节点复制到当前节点中 if (!inputs.equals(oldInputs)) final RelNode pOld = p; p = p.copy(p.getTraitSet(), inputs); planner.provenanceMap.put(p, new VolcanoPlanner.DirectProvenance(pOld)); // 记录到 visited visited.put(pId, p); // memoize result for pId return p; 最终，我们得到了如下的最优执行计划，Calcite 执行器会生成执行代码，执行并返回查询结果。 EnumerableFilter(condition=[=($1, Alice)]) CsvTableScan(table=[[SALES, EMPS]], fields=[[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]]) 整体流程总结 前文我们以简单的查询语句为例，一起探究了 VolcanoPlanner 优化器实现细节，想必大家阅读完一定有所收获。为了加深大家对优化器的理解，最后我们再进行一些梳理总结，上图展示了 VolcanoPlanner 优化器的整体流程，总体上可以分为三步： 第一步：注册优化器规则。通过调用 addRule 方法，我们可以快速将优化器规则注册进来，这些规则会维护在 VolcanoPlanner 的 classOperands 对象中，后续筛选规则时会从该对象中获取规则； 第二步：初始化 RelSubset。这步会遍历逻辑计划树，将每个节点注册成为 RelSubset 并维护节点的代价信息，然后将逻辑计划树转换为 RelSubset 树，RelSubset 对象关联了所属的 RelSet 对象，该对象维护了当前节点的等价集合，RelSubset 中记录的是当前已知代价最小的关系代数。fireRules 方法负责筛选规则，会将匹配的规则添加到队列中； 第三步：查找最优计划。根据前文初始化的 RelSubset 树以及队列中记录的匹配规则，该步骤会调用 drive 方法应用规则，然后通过 onMatch 方法对关系代数进行变换，完成变换后会重新计算代价信息，并更新 RelSubset 和 RelSet 对象。最后会调用 buildCheapestPlan 方法，从 RelSubset 树中获取整体代价最小的执行计划。 结语 本文首先介绍了 Volcano/Cascades 优化器的理论基础，Volcano 优化器生成器论文中介绍的 Logical Algebra、Physical Algebra、Transformation Rule，以及 Cascades 优化器论文中介绍的 Memo 数据结构，Pattern 匹配规则等概念在 Calcite VolcanoPlanner 中都有体现，大家在阅读代码时可以参考论文中的概念进行理解。 然后介绍了 VolcanoPlanner 中的一些基础概念——RelNode、RelSet 和 RelSubset，理解了这些概念对学习 VolcanoPlanner 原理非常有帮助。同时，我们参考了 Julain 分享的 Cost-based Query Optimization in Apache Phoenix using Apache Calcite，提前了解了 VolcanoPlanner 的处理流程，整体上对优化流程有了一些了解。最后，本文结合一个简单的案例，深入 Calcite 源码细节，带领大家一起探究了整个流程。 限于文章的篇幅以及案例的选择，VolcanoPlanner 优化器的一些细节本文无法全面覆盖，还请各位读者多多包涵。下一篇文章，我们将关注 VolcanoPlanner 中的统计信息和代价模型，并会通过一个多表关联查询的案例，一起探究下 VolcanoPlanner 优化器是如何使用统计信息和代价模型进行代价计算，在多表关联查询 SQL 中，VolcanoPlanner 又会使用哪些优化方式得到最优执行计划。欢迎大家持续关注后续文章，如果有感兴趣的问题，也欢迎大家留言交流。 写在最后 笔者因为工作原因接触到 Calcite，前期学习过程中，深感 Calcite 学习资料之匮乏，因此创建了 Calcite 从入门到精通知识星球，希望能够将学习过程中的资料和经验沉淀下来，为更多想要学习 Calcite 的朋友提供一些帮助。 欢迎关注 欢迎关注「端小强的博客」微信公众号，会不定期分享日常学习和工作经验，欢迎大家关注交流。","tags":["Calcite"],"categories":["Calcite"]},{"title":"使用 Wireshark 解决 BenchmarkSQL 压测 Proxy 异常","path":"/blog/use-wireshark-to-solve-benchmarksql-exception-with-shardingsphere-proxy.html","content":"问题背景 最近 ShardingSphere 社区用户反馈，他使用 BenchmarkSQL 工具对 ShardingSphere Proxy 进行性能测试，在执行 ./runDatabaseBuild.sh props.mysql 初始化数据阶段，出现了 ArrayIndexOutOfBoundsException 异常，而且异常能够稳定复现。考虑到 ShardingSphere 团队内部也覆盖了 BenchmarkSQL 测试，每天定时会拉取最新代码进行性能压测，保证 ShardingSphere 性能持续稳定，笔者一开始怀疑用户配置方面存在问题，但是查看之后并没有发现配置问题。为了搞清问题的原因，笔者开展了一番测试和调查，本文记录了调查的过程和问题的分析，最终通过 Wireshark 抓包确认异常是 ShardingSphere Proxy 协议实现不完善导致。 问题分析 异常初步分析 根据用户反馈的异常堆栈，程序是在 LoadDataWorker#364 行执行 executeBatch 时抛出了异常，可以看出 BenchmarkSQL 初始化数据使用的 JDBC addBatch 和 executeBatch 批量接口。 java.lang.ArrayIndexOutOfBoundsException: Index 38928 out of bounds for length 38928 at com.mysql.cj.NativeQueryBindings.getBinding(NativeQueryBindings.java:191) at com.mysql.cj.NativeQueryBindings.setFromBindValue(NativeQueryBindings.java:198) at com.mysql.cj.jdbc.ClientPreparedStatement.setOneBatchedParameterSet(ClientPreparedStatement.java:591) at com.mysql.cj.jdbc.ClientPreparedStatement.executeBatchWithMultiValuesClause(ClientPreparedStatement.java:675) at com.mysql.cj.jdbc.ClientPreparedStatement.executeBatchInternal(ClientPreparedStatement.java:409) at com.mysql.cj.jdbc.StatementImpl.executeBatch(StatementImpl.java:795) at LoadDataWorker.loadWarehouse(LoadDataWorker.java:364) at LoadDataWorker.run(LoadDataWorker.java:187) at java.base/java.lang.Thread.run(Thread.java:1583) 最终 ArrayIndexOutOfBoundsException 异常出现在 MySQL 驱动中，NativeQueryBindings#191 逻辑根据参数偏移位 parameterIndex 获取对应的参数值。从异常信息 Index 38928 out of bounds for length 38928 可以看出，bindValues 数组当前只有 38928 个值，需要通过 0-38927 来获取，初步判断应该是 Proxy 对于预编译 SQL 中的参数处理存在问题。 /** * Returns the structure representing the value that (can be)/(is) * bound at the given parameter index. * * @param parameterIndex * 0-based * @param forLongData * is this for a stream? * @return BindValue */public BindValue getBinding(int parameterIndex, boolean forLongData) if (this.bindValues[parameterIndex] != null this.bindValues[parameterIndex].isStream() !forLongData) this.longParameterSwitchDetected = true; return this.bindValues[parameterIndex]; BenchmarkSQL 分析 大致了解问题发生的原因后，打算使用 BenchmarkSQL 复现该问题。首先从 benchmarksql 仓库下载 BenchmarkSQL 程序，该程序目前支持了 ShardingSphere JDBC 和 Proxy 性能压测，本文暂时只关注 Proxy 压测出现的异常问题，JDBC 后续文章可以介绍使用方法。 下载完成后，修改 props.mysql 中的数据源配置，将 MySQL 数据源指向 ShardingSphere Proxy，再执行 ./runDatabaseBuild.sh props.mysql 命令初始化数据。 db=mysqldriver=com.mysql.cj.jdbc.Driverconn=jdbc:mysql://127.0.0.1:3307/encrypt_db?useSSL=falseuseServerPrepStmts=truerewriteBatchedStatements=truecachePrepStmts=trueprepStmtCacheSize=8192prepStmtCacheSqlLimit=8000user=rootpassword=rootwarehouses=1loadWorkers=4terminals=1runTxnsPerTerminal=10runMins=0limitTxnsPerMin=300terminalWarehouseFixed=false//The following five values must add up to 100//The default percentages of 45, 43, 4, 4 4 match the TPC-C specnewOrderWeight=45paymentWeight=43orderStatusWeight=4deliveryWeight=4stockLevelWeight=4// Directory name to create for collecting detailed result data.// Comment this out to suppress.resultDirectory=result/mysql/mysql8.0.direct.wh.81_%tY-%tm-%td_%tH%tM%tSosCollectorScript=./misc/os_collector_linux.pyosCollectorInterval=1osCollectorSSHAddr=chexiaopeng01@quick07v.mm.bjat.qianxin-inc.cnosCollectorDevices=net_eth0 blk_sda 执行一段时间后，本地使用 BenchmarkSQL 程序复现异常，异常信息如下图所示： 为了方便问题分析，尝试使用 IDEA 配置 BenchmarkSQL 初始化程序进行 Debug 定位，首先看下 runDatabaseBuild.sh 脚本的逻辑，主要包含了：1. 初始化表结构；2. 加载数据；3. 初始化索引、外键等。根据前文的异常堆栈，是在第二步加载数据中出现的异常，因此先注释第二步的脚本，单独 Debug 执行。 #!/bin/sh# ... 省略BEFORE_LOAD=tableCreatesAFTER_LOAD=indexCreates foreignKeys extraHistID buildFinish# 初始化表结构for step in $BEFORE_LOAD ; do ./runSQL.sh $PROPS $stepdone# 加载数据#./runLoader.sh $PROPS $*# 初始化索引、外键等for step in $AFTER_LOAD ; do ./runSQL.sh $PROPS $stepdone 调整完脚本后，尝试执行 ./runDatabaseBuild.sh props.mysql 初始化表结构，如果多次执行则需要先执行清理脚本 ./runDatabaseDestroy.sh props.mysql。然后参考 ./runLoader.sh $PROPS $\\* 逻辑配置 IDEA 启动类，脚本实际调用了 LoadData 类。 #!/usr/bin/env bash# ... 省略myOPTQuickCSP=-Dquickcsp.loglevel.default=warn -Dquickcsp.loglevel.tde=warn -Dlogback.configurationFile=quickcsp.logback.xml java -cp $myCP -Dprop=$PROPS $myOPTQuickCSP LoadData $* 可以通过 IDEA 直接打开 src/LoadData 工程，为了读取数据源配置，我们需要配置系统变量 prop 指定配置文件路径，然后执行 LoadData#main 方法进行初始化数据。根据异常堆栈的位置，设置条件断点，本地复现了异常。 bindValues 只有 38928 个，按照 SQL 中的参数个数 17 计算，只设置了 2289 组参数，与逻辑中期望的 10000 组参数相差甚远。 排查 MySQL 驱动逻辑发现 bindValues 是由 parameterCount 控制，可以看到 parameterCount 传递的值为 38928，和 10000 * 17 结果不一致，继续排查发现 parameterCount 是从服务端即 Proxy 获取。 Proxy 中处理预编译 SQL Prepare 是通过 MySQLComStmtPrepareExecutor 类进行的，通过 debug 可以看出 Proxy 处理的 parameterCount 是符合预期的，并且也成功写入到 MySQLComStmtPrepareOKPacket 包中返回。问题看起来似乎有点复杂，Proxy 返回了正确的 parameterCount，而 MySQL 驱动接收到的却是一个错误的值，这到底是为什么？ Wireshark 抓包分析 由于 BenchmarkSQL#LoadData 初始化逻辑较为复杂，其中还包含了其他业务表的处理，不利于问题的排查，本地尝试编写一个最小化 Demo 复现问题，逻辑如下： @Testvoid assertBenchmarkSQL() throws SQLException try (Connection connection = DriverManager.getConnection(jdbc:mysql://127.0.0.1:3307/encrypt_db?useSSL=falseallowPublicKeyRetrieval=trueuseServerPrepStmts=truerewriteBatchedStatements=truecachePrepStmts=trueprepStmtCacheSize=1000prepStmtCacheSqlLimit=2048, root, root); PreparedStatement stmtStock = connection.prepareStatement(INSERT INTO bmsql_stock (s_i_id, s_w_id, s_quantity, s_dist_01, s_dist_02, s_dist_03, s_dist_04, s_dist_05, s_dist_06, s_dist_07, s_dist_08, s_dist_09, s_dist_10, s_ytd, s_order_cnt, s_remote_cnt, s_data) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?))) for (int s_i_id = 1; s_i_id = 100000; s_i_id++) if (s_i_id != 1 (s_i_id - 1) % 10000 == 0) stmtStock.executeBatch(); stmtStock.clearBatch(); stmtStock.setInt(1, s_i_id); stmtStock.setInt(2, s_i_id + 1); stmtStock.setInt(3, s_i_id + 2); stmtStock.setString(4, ); stmtStock.setString(5, ); stmtStock.setString(6, ); stmtStock.setString(7, ); stmtStock.setString(8, ); stmtStock.setString(9, ); stmtStock.setString(10, ); stmtStock.setString(11, ); stmtStock.setString(12, ); stmtStock.setString(13, ); stmtStock.setInt(14, 0); stmtStock.setInt(15, 0); stmtStock.setInt(16, 0); stmtStock.setString(17, sData); stmtStock.addBatch(); 执行单测程序，复现了前文的异常，Prxoy debug 仍然是同样的行为，Proxy 返回正确的 parameterCount，而 MySQL 驱动获取的结果却是错误的。 Wireshark 工具简介 为了弄清楚 Proxy 返回给 MySQL 驱动过程中发生了什么问题，我们需要通过抓包方式进行问题排查，由于该问题在本机已经复现，可以直接使用 Wireshark 进行抓包。本地打开 Wireshark 如下图所示，列表中展示了本机网卡，需要根据使用情况进行选择。 当前 Demo 连接的是本地运行 Proxy 实例，客户端通过 127.0.0.1 端口 3307 进行连接，流量都经过 Loopback 网卡，因此选择 Loopback 作为抓包对象。选择网卡后，Wireshark 即开始抓包。由于网卡中可能会有很多其他进程的流量，需要过滤出指定端口的流量： tcp.port == 3307 其他抓包注意事项： Proxy 部署在服务上时，此时无法使用 Wireshark 进行抓包，可以使用 Linux 系统自带的 tcpdump 命令，tcpdump 的抓包结果文件可以通过 Wireshark 打开； # 对网卡 eth0 抓包，过滤 TCP 端口 3307，并将抓包结果写入到 /path/to/dump.captcpdump -i eth0 -w /path/to/dump.cap tcp port 3307 客户端连接 MySQL，可能会自动启用 SSL 加密，抓包结果无法直接解析出协议内容。对于 MySQL 命令行以及 JDBC 驱动，可以分别使用如下方式关闭 SSL。 # 使用 MySQL 命令行客户端可以指定参数禁用 SSLmysql --ssl-mode=disable# 使用 JDBC 可以在 URL 中增加参数jdbc:mysql://127.0.0.1:3306/db?useSSL=false Wireshark 读取抓包内容 Wireshark 支持读取多种抓包文件格式，包括 tcpdump 的抓包格式。Wireshark 默认会把 3306 端口解码为 MySQL 协议、5432 端口解码为 PostgreSQL 协议。对于 Proxy 可能使用不同端口的情况，可以使用 Decode As... 指定端口的解码协议。例如，Proxy 使用了 3307 端口，可以按照以下步骤把 3307 端口解码为 MySQL 协议： 当 Wirekshark 能够解析出 MySQL 协议后，我们可以增加过滤条件，只显示 MySQL 协议数据： tcp.port == 3307 and mysql 使用 Wireshark 抓包定位 了解了 Wireshark 基本使用方式后，我们执行前文编写的最小化 Demo，抓包获取到了如下报文信息，响应报文中返回的 parameterCount 为 38928，转换为 16 进制为 9810（2 字节），而以类似的方式计算 170000 对应的 16 进制是 029810（3 字节）。 可以看出报文返回的信息丢失了一个字节，那么 MySQL 协议里面 parameterCount 最多可以传输几个字节呢？参考 MySQL 协议文档及 MySQLComStmtPrepareOKPacket 实现，parameterCount 参数最大只能存储 2 字节的数值，即 ffff=65535。 /** * COM_STMT_PREPARE_OK packet for MySQL. * * @see a href=https://dev.mysql.com/doc/dev/mysql-server/latest/page_protocol_com_stmt_prepare.html#sect_protocol_com_stmt_prepare_response_okCOM_STMT_PREPARE_OK/a */@RequiredArgsConstructorpublic final class MySQLComStmtPrepareOKPacket extends MySQLPacket private static final int STATUS = 0x00; private final int statementId; private final int columnCount; private final int parameterCount; private final int warningCount; @Override protected void write(final MySQLPacketPayload payload) payload.writeInt1(STATUS); payload.writeInt4(statementId); // TODO Column Definition Block should be added in future when the meta data of the columns is cached. payload.writeInt2(columnCount); payload.writeInt2(parameterCount); payload.writeReserved(1); payload.writeInt2(warningCount); 既然 MySQL 协议中定义的 parameterCount 最大为 65535，那么 BenchmarkSQL 测试原生 MySQL 也应当报错，而实际反馈原生 MySQL 不会出现异常。为了一探究竟，我们打算再测试下原生 MySQL，看下协议上是如何处理的。调整 JDBC URL 直接指向 MySQL 数据库，并执行单测程序。 测试并抓包后发现，原生 MySQL 同样不支持 2 字节以上的 parameterCount，MySQL 会直接抛出异常。此时 MySQL 驱动捕获到 1390 异常码后，会将预编译 SQL 转换为非预编译 SQL，直接将参数拼接在 VALUES 中，然后再次发起请求。 到这里问题终于明确了，Proxy 对于预编译参数超过 65535 的情况，未进行异常校验，导致通过 Netty 返回报文时丢失了一个字节，进而出现 MySQL 驱动中报出的参数 Index 越界异常。 (adsbygoogle = window.adsbygoogle || []).push({}); 赞助商 问题解决 使用 Wireshark 我们搞清楚了 Proxy 执行 BenchmarkSQL 出现参数 Index 越界的原因，当预编译参数超过 65535 时，需要参考 MySQL 的行为抛出异常，此时 MySQL 驱动会再次发起非预编译的请求，将参数拼接在 VALUES 中。在 Proxy MySQLComStmtPrepareExecutor 类中，我们增加对参数个数的校验，超过 65535 则抛出异常。 private CollectionDatabasePacket createPackets(final SQLStatementContext sqlStatementContext, final int statementId, final MySQLServerPreparedStatement serverPreparedStatement) CollectionDatabasePacket result = new LinkedList(); CollectionProjection projections = getProjections(sqlStatementContext); int parameterCount = sqlStatementContext.getSqlStatement().getParameterCount(); ShardingSpherePreconditions.checkState(parameterCount = MAX_PARAMETER_COUNT, TooManyPlaceholdersException::new); result.add(new MySQLComStmtPrepareOKPacket(statementId, projections.size(), parameterCount, 0)); int characterSet = connectionSession.getAttributeMap().attr(MySQLConstants.MYSQL_CHARACTER_SET_ATTRIBUTE_KEY).get().getId(); int statusFlags = ServerStatusFlagCalculator.calculateFor(connectionSession); if (parameterCount 0) result.addAll(createParameterColumnDefinition41Packets(sqlStatementContext, characterSet, serverPreparedStatement)); result.add(new MySQLEofPacket(statusFlags)); if (!projections.isEmpty() sqlStatementContext instanceof SelectStatementContext) result.addAll(createProjectionColumnDefinition41Packets((SelectStatementContext) sqlStatementContext, characterSet)); result.add(new MySQLEofPacket(statusFlags)); return result; 异常码和异常信息都参考 MySQL 进行定义，完整的修复代码请参考 PR#29296。 ER_PS_MANY_PARAM(XOpenSQLState.GENERAL_ERROR, 1390, Prepared statement contains too many placeholders), 修改完成后，再次使用 BenchmarkSQL 进行测试，此时异常问题已经得到了解决。 结语 本文介绍了 BenchmarkSQL 测试 Proxy 出现参数 Index 越界异常后，使用 Wireshark 排查问题的过程。通过强大的 Wireshark 工具，我们很清晰地观测到请求过程中出现的问题，进而找到解决问题的方案，这也印证了那句老话「工欲善其事，必先利其器」。本案例的排查思路也适合其他 Proxy 接入端的问题，希望对大家有用，由于本人对 Wireshark 使用经验有限，如果问题也欢迎指正。 欢迎关注 欢迎关注「端小强的博客」微信公众号，会不定期分享日常学习和工作经验，欢迎大家关注交流。","tags":["ShardingSphere","In Action","Wireshark"],"categories":["In Action"]},{"title":"深入理解 Apache Calcite HepPlanner 优化器","path":"/blog/deep-understand-of-apache-calcite-hep-planner.html","content":"注意：本文基于 Calcite 1.35.0 版本源码进行学习研究，其他版本可能会存在实现逻辑差异，对源码感兴趣的读者请注意版本选择。 什么是 RBO Calcite 中的 RBO 规则 (adsbygoogle = window.adsbygoogle || []).push({}); 赞助商 HepPlanner 中的基础概念 HepPlanner：基于规则的启发式优化器，实现了 RelOptPlanner 优化器接口； HepProgram：提供了维护各种类型 HepInstruction 的容器，并支持指定 HepInstruction 被 HepPlanner 优化时处理的顺序； HepProgramBuilder：用于创建 HepProgram； HepInstruction：代表了 HepProgram 中的一个指令，目前包含了许多实现类，具体实现类的用途如下表所示： BeginGroup, EndGroup: 开始和结束一组指令组CommonRelSubExprRels: 寻找公共子关系表达式的指令ConverterRules: MatchLimit: 修改当前 Program 匹配 limit 的指令MatchOrder: 修改当前 Program 匹配次序的指令Placeholder: RuleClass: 执行 rules 中符合指定规则类型 allRules 的规则的指令(对 allRules 的过滤)RuleCollection: 执行多个 rules 的指令(不必在 allRules 中)RuleInstance: 执行一个 rule 的指令(不必在 allRules 中)RuleLookup: SubProgram: 用来执行子 HepProgram 的指令 HepPlanner 优化器原理 代码版本：HEAD is now at 413eded69 [CALCITE-5275] Release Calcite 1.32.0 https://github.com/apache/calcite/commit/413eded693a9087402cc1a6eefeca7a29445d337 以如下 SQL 为例： SELECT MAX(order_id), MIN(order_id), SUM(order_id), AVG(order_id), COUNT(1) FROM t_order_federate GROUP BY user_id 逻辑计划如下： LogicalProject(EXPR$0=[$1], EXPR$1=[$2], EXPR$2=[$3], EXPR$3=[$4], EXPR$4=[$5]) LogicalAggregate(group=[0], EXPR$0=[MAX($1)], EXPR$1=[MIN($1)], EXPR$2=[SUM($2)], EXPR$3=[AVG($2)], EXPR$4=[COUNT()]) LogicalProject(user_id=[$1], order_id=[$0], $f2=[CAST($0):DECIMAL(19, 9)]) LogicalScan(table=[[federate_jdbc, t_order_federate]]) HepPlanner 构建逻辑如下，内部会维护 HepProgram。 /** * Create new instance of hep planner. * * @return hep planner instance */public static RelOptPlanner createHepPlanner() HepProgramBuilder builder = new HepProgramBuilder(); builder.addGroupBegin().addRuleCollection(getFilterRules()).addGroupEnd().addMatchOrder(HepMatchOrder.BOTTOM_UP); builder.addGroupBegin().addRuleCollection(getProjectRules()).addGroupEnd().addMatchOrder(HepMatchOrder.BOTTOM_UP); builder.addGroupBegin().addRuleCollection(getAggregationRules()).addGroupEnd().addMatchOrder(HepMatchOrder.BOTTOM_UP); builder.addGroupBegin().addRuleCollection(getCalcRules()).addGroupEnd().addMatchOrder(HepMatchOrder.BOTTOM_UP); builder.addGroupBegin().addRuleCollection(getSubQueryRules()).addGroupEnd().addMatchOrder(HepMatchOrder.BOTTOM_UP); builder.addMatchLimit(DEFAULT_MATCH_LIMIT); return new HepPlanner(builder.build()); HepProgram 结构如下： setRoot: // 从 LogicalProject 开始将 rel 添加到 graph 中- addRelToGraph- graph.vertexSet() 是否包含 rel，包含则返回- inputs = rel.getInputs() 获取子节点，此处是 LogicalAggregate- 循环 inputs 递归调用 addRelToGraph- 递归 Tree 叶子节点，以 LogicalScan 为例- Util.equalShallow 浅判断 inputs 和 new inputs 是否相同，newInputs 中存储的是 HepRelVertex 对象，也是一种 AbstractRelNode- 不相同则调用 rel = rel.copy(rel.getTraitSet(), newInputs); 替换原有 rel，并将原有 rel 赋给 oldRel- 调用 rel.recomputeDigest(); 清除摘要信息- 判断 noDag 标识，默认为 false，表示使用 dag 构建 hep planner，true 则使用 tree 构建 hep planner - false 则通过 mapDigestToVertex.get(rel.getRelDigest()); 获取等价的 equivVertex，getRelDigest 获取 rel 摘要信息 - 获取到 equivVertex 则返回- noDag 为 true 或者未获取到 equivVertex 继续执行- new HepRelVertex(rel); 创建一个新的 vertex 顶点对象，并添加到 graph 中- 调用 updateVertex 方法将 vertex 添加到 mapDigestToVertex 中，key 为 digest- 有了 vertex 顶点后，再循环 rel inputs，graph.addEdge 添加边，此处 LogicalScan 没有 input 因此跳过// LogicalScan 创建完 vertex 后，会返回到递归上一层的 for 循环中，此时对象为 LogicalProject，此时会将 LogicalScan 创建的 vertex 添加到 newInputs 中- Util.equalShallow 浅判断 inputs 和 newInputs 是否相等 - LogicalProject 这层循环不相同，input 为 LogicalScan，newinput 为 HepRelVertex(LogicalScan) - 调用 LogicalProject copy 方法，用原有的 trait 和 newInputs 创建新的 LogicalProject- noDag 为 false，从 mapDigestToVertex 中获取 equivVertex- 获取不到则 new HepRelVertex(rel) 创建 LogicalProject 对应的边 Vertex，然后添加到 graph 中- 调用 updateVertex 更新 mapDigestToVertex，将 LogicalProject Vertex 添加进去，key 为 digest- 循环 LogicalProject input 构建 edge 边（从 project 到 input），此处 input 为 LogicalScan 的 vertex（前面通过 copy 方法生成） - 调用 getVertex(vertex) 方法获取当前 projection 的 VertexInfo（前面 addVertex 时添加，默认构建空的 VertexInfo）， - 调用 getVertex(targetVertex) 获取 input 的 VertexInfo（前面 - 通过 edgeFactory.createEdge(vertex, targetVertex) 创建当前节点到子节点的 edge - 然后调用 edges.add(edge) 添加 edge，并判断是否是新添加的 edge - 如果 edge 是新添加的，则 info.outEdges.add(edge); 为当前节点的 vert info 添加外边，为 targetInfo.inEdges.add(edge) 子节点添加内边 - 否则返回 null- nTransformations++ 记录变换的次数// LogicalProject 创建 Vertex后，返回递归上一层，此时 rel 为 LogicalAggregate，会将 LogicalProject 创建的 Vertex 添加到 newInputs- 同上一步的逻辑，使用 newInputs 复制新的 LogicalAggregate- 创建 Vertex，然后添加到 graph 中- 循环 LogicalProject input 构建 edge（从 LogicalAggregate 指向 LogicalProject）// LogicalAggregate 创建 Vertex后，返回递归上一层，此时 rel 为 LogicalProject- 同上一步的逻辑，使用 newInputs 复制新的 LogicalProject- 创建 Vertex，然后添加到 graph 中- 循环 LogicalProject input 构建 edge（从 LogicalProject 指向 LogicalAggregate）// 最终生成了四个顶点，以及三条边，由父节点指向子节点// 生成的 HepRelVertex 会赋值给 root findBestExp: - 判断 root 是否为空// mainProgram 是构建 HepPlanner 时传入的- executeProgram(mainProgram)\t// 初始化的 PrepareContext 中只有 planner，programState 和 endGroupState 暂时为空\t- 调用 HepInstruction.PrepareContext.create 创建 PrepareContext 对象，用于为 Instruction 初始化 state - 调用 program.prepare(px) 初始化 HepState - new State(px, instructions) 构建 State 对象 - matchLimit 默认值为 Integer.MAX_VALUE // HepMatchOrder 指定了遍历图的顺序，包含：ARBITRARY 任意，BOTTOM_UP 自底向上，从叶子节点开始，TOP_DOWN 自顶向下，DEPTH_FIRST 深度优先 - matchOrder 默认值为 HepMatchOrder.DEPTH_FIRST - PrepareContext 对象初始化 ProgramState - 遍历所有的指令 instructions - 判断 instruction 是否为 BeginGroup - 构建 actions，key 为 BeginGroup 对应的 EndGroup，value 为 Consumer 函数式接口，定义了重新构建 State 的动作，动作中将新状态 set 到 states 中 - 初始化 HepState state 为 null，并添加到 states 中 - 非 BeginGroup 指令 - 调用 instruction.prepare(px2) 初始化 state，并添加到 states 中 // 在 EndGroup 时，执行前面 BeginGroup 定义的动作，设置 BeginGroup 对应的 state - 如果 actions.containsKey(instruction) 则执行 action // 将 states 复制到 instructionStates 中 - this.instructionStates = ImmutableList.copyOf(states); - 调用 state.execute() 执行 - 调用 planner.executeProgram(HepProgram.this, this); - state.init(); 初始化 HepProgram.State matchLimit = Integer.MAX_VALUE，matchOrder = HepMatchOrder.DEPTH_FIRST - 遍历 instructionStates // BeginGroup - 调用 instructionState.execute() 方法，内部调用 planner.executeBeginGroup - 设置 BeginGroup 中的 state.programState.group = state.endGroup - 执行 collectGarbage（暂时忽略） // RuleCollection - 调用 instructionState.execute() 方法, planner.executeRuleCollection(RuleCollection.this, this); - applyRules - 将 rules 添加到 group.ruleSet 中，group 为 EndGroup#State // EndGroup - 调用 instructionState.execute() 方法, 内部调用 planner.executeEndGroup(EndGroup.this, this); - applyRules - 根据 fixedPoint 标记循环执行 - 调用 getGraphIterator 获取图遍历器 - 判断 programState.matchOrder 决定遍历顺序，此处为深度优先 - 遍历迭代器 getGraphIterator - 循环当前 group 中的 rules - 调用 applyRule，使用当前 rule 进行优化 - 判断 rule 是否为 ConverterRule // 暂时忽略 - 判断 rule 是否为 CommonRelSubExprRule // 暂时忽略 - 调用 matchOperands 判断是否匹配操作符 PushProjectIntoScanRule 匹配之后，会调用 transformTo 修改当前的 rel，然后会重新调用 addToGraph 方法，新节点看起来 digest 未变更，导致获取到仍然是之前未下推的 HepRelVertex。 private void applyRules(HepProgram.State programState, CollectionRelOptRule rules, boolean forceConversions) final HepInstruction.EndGroup.State group = programState.group; if (group != null) checkArgument(group.collecting); SetRelOptRule ruleSet = requireNonNull(group.ruleSet, group.ruleSet); ruleSet.addAll(rules); return; LOGGER.trace(Applying rule set , rules); final boolean fullRestartAfterTransformation = programState.matchOrder != HepMatchOrder.ARBITRARY programState.matchOrder != HepMatchOrder.DEPTH_FIRST; int nMatches = 0; boolean fixedPoint; do IteratorHepRelVertex iter = getGraphIterator(programState, requireNonNull(root, root)); fixedPoint = true; while (iter.hasNext()) HepRelVertex vertex = iter.next(); for (RelOptRule rule : rules) HepRelVertex newVertex = applyRule(rule, vertex, forceConversions); if (newVertex == null || newVertex == vertex) continue; ++nMatches; if (nMatches = programState.matchLimit) return; if (fullRestartAfterTransformation) iter = getGraphIterator(programState, requireNonNull(root, root)); else // To the extent possible, pick up where we left // off; have to create a new iterator because old // one was invalidated by transformation. iter = getGraphIterator(programState, newVertex); if (programState.matchOrder == HepMatchOrder.DEPTH_FIRST) nMatches = depthFirstApply(programState, iter, rules, forceConversions, nMatches); if (nMatches = programState.matchLimit) return; // Remember to go around again since were // skipping some stuff. fixedPoint = false; break; while (!fixedPoint); 1.1. 先执行 executeProgram(mainProgram) 逻辑，并将 mainProgram 赋予 currentProgram，然后循环 currentProgram.instructions。 for (HepInstruction instruction : currentProgram.instructions) instruction.execute(this); int delta = nTransformations - nTransformationsLastGC; if (delta graphSizeLastGC) // The number of transformations performed since the last // garbage collection is greater than the number of vertices in // the graph at that time. That means there should be a // reasonable amount of garbage to collect now. We do it this // way to amortize garbage collection cost over multiple // instructions, while keeping the highwater memory usage // proportional to the graph size. collectGarbage(); ​ instruction.execute(this); execute 方法内部调用 applyRules 方法，按照规则进行优化。 applyRules(instruction.rules, true); 可以看到有三个优化规则。 HepMatchOrder 代表了基于规则优化的顺序，包含了 ARBITRARY（任意顺序）、BOTTOM_UP（自底向上）、TOP_DOWN（自顶向下） 和 DEPTH_FIRST（深度优先）。然后循环进行匹配： boolean fixedPoint;do IteratorHepRelVertex iter = getGraphIterator(requireNonNull(root, root)); fixedPoint = true; while (iter.hasNext()) HepRelVertex vertex = iter.next(); for (RelOptRule rule : rules) HepRelVertex newVertex = applyRule(rule, vertex, forceConversions); if (newVertex == null || newVertex == vertex) continue; ++nMatches; if (nMatches = requireNonNull(currentProgram, currentProgram).matchLimit) return; if (fullRestartAfterTransformation) iter = getGraphIterator(requireNonNull(root, root)); else // To the extent possible, pick up where we left // off; have to create a new iterator because old // one was invalidated by transformation. iter = getGraphIterator(newVertex); if (requireNonNull(currentProgram, currentProgram).matchOrder == HepMatchOrder.DEPTH_FIRST) nMatches = depthFirstApply(iter, rules, forceConversions, nMatches); if (nMatches = requireNonNull(currentProgram, currentProgram).matchLimit) return; // Remember to go around again since were // skipping some stuff. fixedPoint = false; break; while (!fixedPoint); 写在最后 笔者因为工作原因接触到 Calcite，前期学习过程中，深感 Calcite 学习资料之匮乏，因此创建了 Calcite 从入门到精通知识星球，希望能够将学习过程中的资料和经验沉淀下来，为更多想要学习 Calcite 的朋友提供一些帮助。 欢迎关注 欢迎关注「端小强的博客」微信公众号，会不定期分享日常学习和工作经验，欢迎大家关注交流。","tags":["Calcite"],"categories":["Calcite"]},{"title":"Apache Calcite System Catalog 实现探究","path":"/blog/explore-apache-calcite-system-catalog-implementation.html","content":"注意：本文基于 Calcite 1.35.0 版本源码进行学习研究，其他版本可能会存在实现逻辑差异，对源码感兴趣的读者请注意版本选择。 前言 在上一篇 Apache Calcite SQL Parser 原理剖析一文中，我们详细介绍了 Apache Calcite SQL 解析引擎的实现原理，从基础的 JavaCC 使用，再到 Caclite SQL 解析引擎的内部实现，最后介绍了 Calcite SqlNode 体系和 SQL 生成。Calcite 在完成基础的 SQL 解析后，第二个关键的步骤就是 SQL 校验，而 SQL 校验则依赖用户向 Calcite 注册的系统目录（System Catalog），本文会先重点关注 Calcite 系统目录的实现，下一篇再深入探究 Calcite 校验器的内部机制。 什么是 System Catalog 在 CMU 15-445 Query Planning Optimization I 课程中介绍了数据库系统的整体架构，系统目录（System Catalog）主要负责存储数据库的元数据信息，具体包括：表、列、索引、视图、用户、权限以及内部统计信息等。从上图可以看出，系统目录在数据库绑定校验、逻辑计划树改写和执行计划优化等阶段发挥了重要作用。 不同数据库系统都有自己的元数据信息获取方法，ANSI 标准规定通过 INFORMATION_SCHEMA 只读视图查询元数据信息，目前大部分数据库都遵循了这个规范，同时也都提供了一些快捷命令，例如：MySQL SHOW TABLES 命令，PostgreSQL \\d 命令等。 Calcite 作为流行的查询引擎，也提供了系统目录的支持，但是 Calcite 不直接存储系统目录中的元数据信息，用户需要通过 API 将元数据注册到 Calcite 中，才可以使用系统目录提供的能力。下面让我们一起来深入了解下 Calcite System Catalog 体系及其内部实现。 (adsbygoogle = window.adsbygoogle || []).push({}); 赞助商 Calcite System Catalog 体系 在 Caclite 中，Catalog 主要用来定义 SQL 查询过程中所需要的元数据和命名空间，具体实现是抽象类 CalciteSchema（如下所示），CalciteSchema 有 CachingCalciteSchema 和 SimpleCalciteSchema 两个子类，他们的区别主要是是否缓存表、函数和子模式。CalciteSchema 类中包含了 Schema、Table、RelDataType、Function 等核心对象，下面我们将针对这些对象进行逐一的介绍，了解他们在 Calcite System Catalog 体系中的具体作用。 public abstract class CalciteSchema private final @Nullable CalciteSchema parent; public final Schema schema; public final String name; /** * Tables explicitly defined in this schema. Does not include tables in * @link #schema. */ protected final NameMapTableEntry tableMap; protected final NameMultimapFunctionEntry functionMap; protected final NameMapTypeEntry typeMap; protected final NameMapLatticeEntry latticeMap; protected final NameSet functionNames; protected final NameMapFunctionEntry nullaryFunctionMap; protected final NameMapCalciteSchema subSchemaMap; private @Nullable List? extends ListString path; Schema 根据 SQL 标准定义，Schema 是一个描述符的持久命名集合（a persistent, named collection of descriptors），Schema 中通常包含了表、列、数据类型、视图、存储过程、关系、主键和外键等对象。而 Schema 在 Calcite 中，则是针对数据库 Database 或 Catalog 的抽象，Schema 中可以包含子 Schema，也可以包含若干个表。 如下图所示，Calcite Schema 支持任意层级的嵌套，可以很方便地适配不同的数据库，借助 Schema 的嵌套结构，Calcite 衍生出了 NameSpace 概念，通过 NameSpace 可以对不同 Schema 下的对象进行有效地隔离。例如在最底层 SubSchema 中定义的表、函数等对象，只能在当前的 Schema 中使用，如果想要在多个 Schema 中共享对象，则可以考虑在共同的父 Schema 中进行定义。 Schema 接口定义如下，可以看到它提供了 getTable、getType、getFunctions 和 getSubSchema 等访问方法，常见的 Schema 接口实现类有 AbstractSchema、CsvSchema、JdbcCatalogSchema 等。AbstractSchema 对 Schema 接口的方法进行了实现，并提供了可重写的 getTableMap、getFunctionMultimap 和 getSubSchemaMap 方法，用于向 Schema 中注册表、函数和子 Schema。CsvSchema 和 JdbcCatalogSchema 都是继承了 AbstractSchema 完成 Schema 注册，大家也可以参考该方式简化注册 Schema 的流程。 public interface Schema @Nullable Table getTable(String name); SetString getTableNames(); @Nullable RelProtoDataType getType(String name); SetString getTypeNames(); // 根据名称查询具有相同名称，但参数列表不同的重载函数，Calcite 将调用 Schemas.resolve 选择合适的函数 CollectionFunction getFunctions(String name); SetString getFunctionNames(); @Nullable Schema getSubSchema(String name); SetString getSubSchemaNames(); ... 通过上面介绍的方式，我们可以实现 Schema 的初始注册及查询，但如果我们需要在运行过程中对 Schema 进行修改，那又该如何操作呢？Calcite 提供了 Schema 的子接口 SchemaPlus，它对 Schema 接口进行了扩展，能够支持表、函数及 Schema 的添加和删除操作。用户通常无需直接实例化 SchemaPlus 的子类，Calcite 内部提供了 SchemaPlus 的生成方法，例如：CalciteSchema#plus() 方法。 public interface SchemaPlus extends Schema SchemaPlus add(String name, Schema schema); void add(String name, Table table); /** * Removes a table from this schema, used e.g. to clean-up temporary tables. */ default boolean removeTable(String name) // Default implementation provided for backwards compatibility, to be removed before 2.0 return false; void add(String name, Function function); void add(String name, RelProtoDataType type); void add(String name, Lattice lattice); ... 此外，在 Schema 接口中还包含了 Table、RelDataType、Function 等对象，这些和全局共享的 Table、RelProtoDataType、Function 对象作用一样，只是生效的范围不同，我们将在下面的全局对象中分别介绍。 Table Table 表示 Calcite 中的一张表，Table 可以定义在某个 Schema 中，也可以定义在全局范围，全局表会在所有 Schema 中生效。Table 接口包含的主要方法如下，getRowType 方法用于获取表的行类型，行类型包含了所有列及其类型。getStatistic 方法用于获取 Statistic 统计信息对象，用于在查询优化阶段计算代价。getJdbcTableType 方法则用于获取表类型。 public interface Table // 获取表的行类型 RelDataType getRowType(RelDataTypeFactory typeFactory); // 获取表的统计信息 Statistic getStatistic(); // 获取表类型 Schema.TableType getJdbcTableType(); Calcite Table 接口有很多实现类，分别适用于不同的场景，它的继承体系如下图所示： AbstractTable 抽象类是对 Table 接口的基础实现，Caclite 中的大多数表都是继承 AbstractTable。TranslatableTable 、ProjectableFilterableTable 和 FilterableTable 我们在 Apache Calcite 快速入门指南 - Calcite 元数据定义 小节中介绍过他们的区别，具体如下： TranslatableTable：TranslatableTable 则通过 toRel 方法将 RelOptTable 对象转换为 RelNode，例如之前文章介绍的 CsvTableScan，后续可以使用优化规则对 CsvTableScan 进行变换从而实现下推等优化； ScannableTable：ScannableTable 接口，用于扫描全部数据记录，Calcite 会调用 scan 方法获取全部数据； FilterableTable：FilterableTable 接口，可以在扫描数据过程中，根据 scan 方法传入的 ListRexNode filters 参数进行数据过滤。 ModifiableTable 类用于处理对表进行更新的场景，例如执行 INSERT、UPDATE 和 DELETE 语句，通过调用 toModificationRel 方法将 RelOptTable 转换为 TableModify 算子。 ViewTable 类则用于视图处理，通过将视图定义语句 viewSql 转化为 AST 及关系代数，并在 toRel 处理过程中将原有的视图查询语句展开，变换为对原始表的查询，从而实现视图语义。 RelDataType RelDataType 代表了关系表达式返回的数据行类型或者标量表达式的类型，Calcite 支持了所有的 SQL 数据类型，也包括结构和数组类型。RelDataType 接口中的主要方法如下： public interface RelDataType // 获取结构类型中的字段，Calcite 中关系表达式返回的数据行类型使用 RelDataType 表示，每一列的类型通过 RelDataTypeField 表示 // RelDataTypeField 内部仍然封装了 RelDataType 表示字段类型 ListRelDataTypeField getFieldList(); // 当前类型是否支持为空 boolean isNullable(); RelDataType getComponentType(); RelDataType getKeyType(); RelDataType getValueType(); // 当前类型的字符集编码 Charset getCharset(); // 当前类型的排序规则 SqlCollation getCollation(); // 获取该类型的 JDBC 精度（字段长度，例如：-4.75，precision 为 3） int getPrecision(); // 获取该类型的范围（小数位数，例如：-4.75，scale 为 2） int getScale(); // 获取 SQL 类型 SqlTypeName getSqlTypeName(); getFieldList 方法用于获取结构类型中的字段，Calcite 中关系表达式返回的数据行类型使用 RelDataType 表示，每一列的类型通过 RelDataTypeField 表示，RelDataTypeField 内部仍然封装了 RelDataType 表示字段类型。isNullable 方法表示当前类型是否支持为空，getCharset 用于获取当前类型的字符集编码，getCollation 用于获取当前类型的排序规则。getPrecision 和 getScale 方法分别用于获取该类型的精度和范围，精度表示字段的长度，范围则表示小数的位数。 Function Calcite 对函数的定义是：接受参数并返回结果的命名表达式。函数通过 Schema 进行注册，可以通过 Schema#getFunctions 获取函数，然后根据参数类型获取对应的函数。下面是 Function 接口声明： public interface Function // 获取函数参数 ListFunctionParameter getParameters(); Function 接口提供了 getParameters 获取函数参数的方法，Function 接口有 ScalarFunction、AggregateFunction、TableFunction 和 TableMarco 等几个主要的子接口。ScalarFunction 对应标量函数，也就是函数返回的结果为一个标量，AggregateFunction 对应聚合函数，会将多个值聚合计算为一个标量返回。 TableFunction 和 TableMacro 都对应了表函数，会返回一个表，他们的区别是 TableMacro 会在编译期间进行调用，编译期展开表达式允许 Calcite 实现更加强大的查询优化，例如我们可以对视图在编译期进行展开。相比于 TableMacro，TableFunction 则需要在执行阶段才能知道表的结果。 以上我们介绍了 Calcite System Catalog 体系中涉及到的类及其主要作用，下面小节我们将结合源码一起看看 Calcite System Catalog 逻辑是如何实现的。 Calcite System Catalog 实现 为了更好地结合源码进行介绍，我们使用 CsvTest#testPushDownProjectAggregateNested 单测作为示例，测试 Case 中包含了表、列、函数等基本对象，并且涉及了分组查询和子查询等复杂语句，可以很好地覆盖到前文介绍的 Catalog 对象。 final String sql = explain plan + extra + for + select gender, max(qty) + from ( + select name, gender, count(*) qty + from EMPS + group by name, gender) t + group by gender;sql(smart, sql).returns(expected).ok(); System Catalog 初始化 首先我们来探究下 System Catalog 初始化流程，单测程序使用了 Calcite JDBC，在执行 DriverManager.getConnection(jdbc:calcite:, info) 方法获取连接时会进行 Catalog 初始化，该方法内部会创建 CalciteConnectionImpl 对象，而 CalciteConnectionImpl 内部又会创建 rootSchema，具体逻辑如下： // CalciteConnectionImpl 初始化this.rootSchema = requireNonNull(rootSchema !=null ? rootSchema : CalciteSchema.createRootSchema(true)); CalciteSchema.createRootSchema 方法会根据参数传递的标识决定是否创建 metadata Schema，metadata Schema 会注册 COLUMNS、TABLES 等系统表以提供相关的查询。 MetadataSchema 类继承了 AbstractSchema，并提供了 getTableMap 方法来获取表的元数据信息，具体的实现逻辑如下。在定义表的元数据时，案例中使用了 CalciteMetaImpl.MetadataTable 类，它继承了 AbstractQueryableTable 和 AbstractTable，enumerator 方法负责返回系统表对应的结果集，getJdbcTableType 方法则返回了 Schema.TableType.SYSTEM_TABLE，表明当前注册的表是系统表。 class MetadataSchema extends AbstractSchema // 定义表的元数据信息 private static final MapString, Table TABLE_MAP = ImmutableMap.of(COLUMNS, new CalciteMetaImpl.MetadataTableMetaColumn(MetaColumn.class) @Override public EnumeratorMetaColumn enumerator(final CalciteMetaImpl meta) final String catalog; try catalog = meta.getConnection().getCatalog(); catch (SQLException e) throw new RuntimeException(e); // 查询元数据中注册表的列信息 return meta.tables(catalog).selectMany(meta::columns).enumerator(); , TABLES, new CalciteMetaImpl.MetadataTableMetaTable(MetaTable.class) @Override public EnumeratorMetaTable enumerator(CalciteMetaImpl meta) final String catalog; try catalog = meta.getConnection().getCatalog(); catch (SQLException e) throw new RuntimeException(e); // 查询元数据中注册表信息 return meta.tables(catalog).enumerator(); ); // 重写 AbstractSchema#getTableMap 以提供表的元数据信息 @Override protected MapString, Table getTableMap() return TABLE_MAP; 业务相关的 Schema 和表信息注册，则是通过单测中配置的 model 进行声明，此案例中 model 为 smart.json。ModelHandler 类负责读取 model 文件，并解析 JSON 中的内容，再通过 SchemaPlus 注册新的 Schema，注册完成后会使用 connection.setSchema(jsonRoot.defaultSchema); 为当前连接设置默认 Schema。 // ModelHandler#visit 方法public void visit(JsonCustomSchema jsonSchema) try // 获取当前 Schema 对象 final SchemaPlus parentSchema = currentMutableSchema(sub-schema); // 初始化 SchemaFactory final SchemaFactory schemaFactory = AvaticaUtils.instantiatePlugin(SchemaFactory.class, jsonSchema.factory); // 调用 SchemaFactory 创建 Schema 对象 final Schema schema = schemaFactory.create(parentSchema, jsonSchema.name, operandMap(jsonSchema, jsonSchema.operand)); // 通过 SchemaPlus 注册新的 Schema final SchemaPlus schemaPlus = parentSchema.add(jsonSchema.name, schema); populateSchema(jsonSchema, schemaPlus); catch (Exception e) throw new RuntimeException(Error instantiating + jsonSchema, e); 系统表和业务表 Schema 注册完成后，我们得到了如下的元数据信息，可以看到 SALES Schema 中的 tableMap 暂时为空，因为它使用了懒加载的方式，在具体需要使用到这些元数据时，才会去读取 CSV 文件注册表的元数据信息。CSV 元数据注册我们在 Apache Calcite 快速入门指南 - Calcite 元数据定义 中已经详细进行了介绍，感兴趣的朋友可以阅读学习。 System Catalog 使用场景 完成 System Catalog 初始化后，我们再来探究下 Catalog 具体的使用场景。让我们回到 CsvTest#testPushDownProjectAggregateNested 单测，statement.executeQuery(sql); 方法负责执行 SQL，内部会调用 CalciteMetaImpl#prepareAndExecute 方法进行准备和执行。准备阶段会调用 CalcitePrepareImpl#prepareSql 方法，prepareSql 核心逻辑如下，首先会使用 Catalog 信息初始化 CalciteCatalogReader，然后通过 Planner Factory 创建优化器 RelOptPlanner，最终使用 prepare2_ 方法对 SQL 进行解析、校验、优化以及代码生成。 T CalciteSignatureT prepare_(Context context, QueryT query, Type elementType, long maxRowCount) if (SIMPLE_SQLS.contains(query.sql)) return simplePrepare(context, castNonNull(query.sql)); final JavaTypeFactory typeFactory = context.getTypeFactory(); // 初始化 CalciteCatalogReader CalciteCatalogReader catalogReader = new CalciteCatalogReader(context.getRootSchema(), context.getDefaultSchemaPath(), typeFactory, context.config()); // 创建 Planner Factory final ListFunction1Context, RelOptPlanner plannerFactories = createPlannerFactories(); RuntimeException exception = Util.FoundOne.NULL; for (Function1Context, RelOptPlanner plannerFactory : plannerFactories) // 获取优化器 RelOptPlanner final RelOptPlanner planner = plannerFactory.apply(context); try CalcitePreparingStmt preparingStmt = getPreparingStmt(context, elementType, catalogReader, planner); // 解析、校验、优化并生成执行代码 return prepare2_(context, query, elementType, maxRowCount, catalogReader, preparingStmt); catch (RelOptPlanner.CannotPlanException e) exception = e; throw exception; 由于本文主要探讨 Calcite System Catalog 相关的实现，因此主要聚焦在 CalciteCatalogReader 类，后续流程中的 SQL 校验、SQL AST 转关系代数 RelNode 都会使用到 CalciteCatalogReader。 上图展示了 CatalogReder 相关接口和类，CatalogReder 是校验器和优化器读取表元数据的接口，它继承了 RelOptSchema、SqlValidatorCatalogReader 和 SqlOperatorTable 接口。RelOptSchema 接口是 RelOptTable 对象的集合，接口提供了 getTableForMember 方法用于获取指定名称的 RelOptTable 对象。SqlValidatorCatalogReader 接口用于为 SqlValidator 提供元数据信息，SqlValidatorCatalogReader 接口的主要方法如下： public interface SqlValidatorCatalogReader extends Wrapper // 根据名称查找表 SqlValidatorTable getTable(ListString names); // 根据名称查找自定义类型 RelDataType getNamedType(SqlIdentifier typeName); // 根据名称查找模式中的对象 ListSqlMoniker getAllSchemaObjectNames(ListString names); // 返回用于查找表的所有 schema 路径 ListListString getSchemaPaths(); // 返回 SqlNameMatcher 对象，用于大小写匹配 SqlNameMatcher nameMatcher(); // 根据 projection 创建对应的 RelDataType RelDataType createTypeFromProjection(RelDataType type, ListString columnNameList); // 获取 root schema CalciteSchema getRootSchema(); CalciteConnectionConfig getConfig(); SqlOperatorTable 接口用于查找 SQL 运算符和函数，内部提供了 lookupOperatorOverloads 方法，会根据运算符名称以及语法类型进行查找，并根据 SqlNameMatcher 决定是否区分大小写，查找到的结果会追加到 operatorList 中返回。 public interface SqlOperatorTable /** * Retrieves a list of operators with a given name and syntax. For example, * by passing SqlSyntax.Function, the returned list is narrowed to only * matching SqlFunction objects. * * @param opName name of operator * @param category function category to look up, or null for any matching operator * @param syntax syntax type of operator * @param operatorList mutable list to which to append matches * @param nameMatcher Name matcher */ void lookupOperatorOverloads(SqlIdentifier opName, @Nullable SqlFunctionCategory category, SqlSyntax syntax, ListSqlOperator operatorList, SqlNameMatcher nameMatcher); /** * Retrieves a list of all functions and operators in this table. Used for * automated testing. Depending on the table type, may or may not be mutable. * * @return list of SqlOperator objects */ ListSqlOperator getOperatorList(); 了解了这三个接口主要提供的方法后，我们来看下 CalciteCatalogReader 中具体的实现逻辑。CalciteCatalogReader 在初始化时，会传入 rootSchema、defaultSchema、typeFactory 和 config，然后根据 config 中的 caseSensitive 属性初始化 SqlNameMatcher，用于名称的大小写匹配。 public CalciteCatalogReader(CalciteSchema rootSchema, ListString defaultSchema, RelDataTypeFactory typeFactory, CalciteConnectionConfig config) this(rootSchema, SqlNameMatchers.withCaseSensitive(config != null config.caseSensitive()), ImmutableList.of(Objects.requireNonNull(defaultSchema, defaultSchema), ImmutableList.of()), typeFactory, config); CalciteCatalogReader 类主要提供了 getTable 和 lookupOperatorOverloads 方法，getTable 用于获取表的元数据信息，主要实现逻辑如下，names 参数会传入表的完整路径，此案例中为 SALES - EMPS 表示 SALES schema 下的 EMPS 表。然后调用 SqlValidatorUtil.getTableEntry 方法先获取 schema 信息，再从 schema 中获取 table，如果 table 实现了 Wrapper 接口则转换为 Prepare.PreparingTable，否则使用 RelOptTableImpl.create 直接创建一个 RelOptTableImpl 对象。 @Overridepublic Prepare.@Nullable PreparingTable getTable(final ListString names) // 通过 getTableEntry 查找 TableEntry，先获取 schema，再获取 table CalciteSchema.TableEntry entry = SqlValidatorUtil.getTableEntry(this, names); if (entry != null) final Table table = entry.getTable(); // 如果实现了 Wrapper 接口则转换为 Prepare.PreparingTable if (table instanceof Wrapper) final Prepare.PreparingTable relOptTable = ((Wrapper) table).unwrap(Prepare.PreparingTable.class); if (relOptTable != null) return relOptTable; return RelOptTableImpl.create(this, table.getRowType(typeFactory), entry, null); return null; lookupOperatorOverloads 方法则用于查找指定的运算符，单测程序中使用了 MAX 和 COUNT 函数，我们结合函数来看下 lookupOperatorOverloads 方法的内部实现。首先，会判断当前 syntax 是否是 FUNCTION，然后根据 category 决定 predicate 过滤逻辑，如果是表函数则过滤 TableMacro 和 TableFunction，否则过滤表函数之外的函数类型。 @Overridepublic void lookupOperatorOverloads(final SqlIdentifier opName, @Nullable SqlFunctionCategory category, SqlSyntax syntax, ListSqlOperator operatorList, SqlNameMatcher nameMatcher) if (syntax != SqlSyntax.FUNCTION) return; final Predicateorg.apache.calcite.schema.Function predicate; if (category == null) predicate = function - true; else if (category.isTableFunction()) predicate = function - function instanceof TableMacro || function instanceof TableFunction; else predicate = function - !(function instanceof TableMacro || function instanceof TableFunction); getFunctionsFrom(opName.names).stream().filter(predicate).map(function - toOp(opName, function)).forEachOrdered(operatorList::add); 然后调用 getFunctionsFrom 方法，根据函数名称获取匹配的函数集合，getFunctionsFrom 方法具体实现如下，会遍历 schemaNameList 并调用 CalciteSchema#getFunctions 获取函数。 private Collectionorg.apache.calcite.schema.Function getFunctionsFrom(ListString names) final Listorg.apache.calcite.schema.Function functions2 = new ArrayList(); final ListListString schemaNameList = new ArrayList(); // 获取 schemaNameList ... // 遍历 schemaNameList，并调用 CalciteSchema#getFunctions 获取函数 for (ListString schemaNames : schemaNameList) CalciteSchema schema = SqlValidatorUtil.getSchema(rootSchema, Iterables.concat(schemaNames, Util.skipLast(names)), nameMatcher); if (schema != null) final String name = Util.last(names); boolean caseSensitive = nameMatcher.isCaseSensitive(); functions2.addAll(schema.getFunctions(name, caseSensitive)); return functions2; CalciteSchema#getFunctions 方法会先从全局的 functionMap 函数中获取 MAX 函数，然后再从 schema 内部定义的函数中获取，由于我们没有在 schema 中定义函数，因此 getFunctions 返回结果为空。 /** * Returns a collection of all functions, explicit and implicit, with a given * name. Never null. */public final CollectionFunction getFunctions(String name, boolean caseSensitive) final ImmutableList.BuilderFunction builder = ImmutableList.builder(); // Add explicit functions. for (FunctionEntry functionEntry : Pair.right(functionMap.range(name, caseSensitive))) builder.add(functionEntry.getFunction()); // Add implicit functions. addImplicitFunctionsToBuilder(builder, name, caseSensitive); return builder.build(); 但是跟踪代码可以发现，此时 operatorList 中已经查找到了 MAX 函数，那这个函数是从哪里获取的呢？ 顺着调用链路可以发现，ChainedSqlOperatorTable#lookupOperatorOverloads 方法会循环 SqlOperatorTable，依次执行 lookupOperatorOverloads 方法，并且优先从 SqlStdOperatorTable 中查找 MAX 函数，如果标准函数表中未查找到，再从 schema 中用户定义的函数进行查找。 ChainedSqlOperatorTable 则是在 Calcite 初始化 SqlValidator 时创建的，首先会根据上下文信息中定义的数据库获取数据库特有函数和标准函数，然后依次注册数据库内置函数和标准函数，再将这些 SqlOperatorTable 封装为 ChainedSqlOperatorTable 进行使用。 private static SqlValidator createSqlValidator(Context context, CalciteCatalogReader catalogReader) // 获取当前数据库特有函数和标准函数 final SqlOperatorTable opTab0 = context.config().fun(SqlOperatorTable.class, SqlStdOperatorTable.instance()); final ListSqlOperatorTable list = new ArrayList(); // 注册数据库内置函数 list.add(opTab0); // 注册用户自定义函数 list.add(catalogReader); // 封装为 ChainedSqlOperatorTable final SqlOperatorTable opTab = SqlOperatorTables.chain(list); final JavaTypeFactory typeFactory = context.getTypeFactory(); final CalciteConnectionConfig connectionConfig = context.config(); final SqlValidator.Config config = SqlValidator.Config.DEFAULT.withLenientOperatorLookup(connectionConfig.lenientOperatorLookup()).withConformance(connectionConfig.conformance()).withDefaultNullCollation(connectionConfig.defaultNullCollation()).withIdentifierExpansion(true); return new CalciteSqlValidator(opTab, catalogReader, typeFactory, config); 到这里，我们就大致了解了 Caclite System Catalog 的主要使用场景，在下一篇探究 Calcite 校验器的文章中，我们还会遇到更多调用 CalciteCatalogReader 获取元数据的场景，到时候可以结合校验器逻辑再加深一下理解。 结语 本文主要介绍了 System Catalog 的概念和用途，以及 Calcite System Catalog 体系中包含了哪些关键的类，他们各自有什么样的作用。然后我们结合 Calcite CsvTest 中的单测程序，具体了解了 System Catalog 初始化的逻辑。初始化时我们发现 Calcite 元数据都是通过 CalciteCatalogReader 对外提供访问方法，并介绍了主要的 getTable 和 lookupOperatorOverloads 方法，让大家对表和运算符元数据的使用有了一定的了解。 了解了 Caclite System Catalog 后，下一篇文章我们将关注 Calcite SqlValidator 的实现逻辑，一起探究下 Calcite 校验器具体校验了哪些 SQL 对象，在校验过程中它又进行了哪些处理，这些处理在后续生成关系代数表达式时有什么作用？欢迎感兴趣的朋友持续关注。 写在最后 笔者因为工作原因接触到 Calcite，前期学习过程中，深感 Calcite 学习资料之匮乏，因此创建了 Calcite 从入门到精通知识星球，希望能够将学习过程中的资料和经验沉淀下来，为更多想要学习 Calcite 的朋友提供一些帮助。 欢迎关注 欢迎关注「端小强的博客」微信公众号，会不定期分享日常学习和工作经验，欢迎大家关注交流。","tags":["Calcite"],"categories":["Calcite"]},{"title":"使用 MySQL Test Framework 测试 ShardingSphere 联邦查询","path":"/blog/use-mysql-test-framework-test-shardingsphere-sql-federation.html","content":"MySQL Test Framework 简介 TODO MySQL Test Framework 使用 TODO ShardingSphere 联邦查询测试实战 TODO 欢迎关注 欢迎关注「端小强的博客」微信公众号，会不定期分享日常学习和工作经验，欢迎大家关注交流。","tags":["ShardingSphere","MySQL"],"categories":["ShardingSphere"]},{"title":"Apache Calcite SQL Parser 原理剖析","path":"/blog/implementation-principle-of-apache-calcite-sql-parser.html","content":"注意：本文基于 Calcite 1.35.0 版本源码进行学习研究，其他版本可能会存在实现逻辑差异，对源码感兴趣的读者请注意版本选择。 前言 在 Apache Calcite 快速入门指南一文中，我们介绍了 Caclite 的执行流程，包括：Parse、Validate、Optimize 和 Execute 四个主要阶段。Parse 阶段是整个流程的基础，负责将用户输入的 SQL 字符串解析为 SqlNode 语法树，为后续的元数据校验、逻辑优化、物理优化和计划执行打好基础。 Calcite SQL 解析采用的是 JavaCC 框架，本文首先会简要介绍 JavaCC 的使用规范，并结合 Calcite 源码对 JavaCC 的使用方式进行学习。然后我们会关注 Calcite SQL Parser 的实现，以及如何使用 Freemarker 模板对 Caclite 解析进行扩展。最后我们再学习下解析后的 AST 对象——SqlNode 体系，以及基于 SqlNode 的 SQL 生成，期望通过这些内容能够帮助大家深刻理解 Caclite SQL 解析。 JavaCC 简介 Calcite SQL Parser 使用了 JavaCC 框架， 根据 JavaCC 官网介绍，JavaCC 是当前流行的解析生成器，它可以读取语法规则，并将语法规则转换为 Java 程序，通过生成的 Java 程序，可以很方便地完成语法解析过程中的词法分析和语法分析（和 JavaCC 类似，Antlr 是另外一款流行的解析器，读者感兴趣可以参考 ANTLR 基础入门）。 Java Compiler Compiler (JavaCC) is the most popular parser generator for use with Java applications. A parser generator is a tool that reads a grammar specification and converts it to a Java program that can recognize matches to the grammar. In addition to the parser generator itself, JavaCC provides other standard capabilities related to parser generation such as tree building (via a tool called JJTree included with JavaCC), actions and debugging. JavaCC 的使用和编写 Java 代码类似，开发者需要在 *.jj 文件中编写语法规则以及对应的 Java 代码处理逻辑，JavaCC 语法描述遵循以下模板结构： javacc_input ::= javacc_options PARSER_BEGIN ( IDENTIFIER ) java_compilation_unit PARSER_END ( IDENTIFIER ) ( production )+ EOF// JavaCC 配置项javacc_options ::= [ options ( option-binding )*; ]// 解析器类定义CompilationUnit ::= ( PackageDeclaration )? ( ImportDeclaration )* ( TypeDeclaration )*// 定义词法和语法规则 // 编写通用 Java 代码production ::= javacode_production | cppcode_production // 描述词法规则 | regular_expr_production | token_manager_decls // 描述语法规则 | bnf_production 大致了解 JavaCC 语法描述的基本结构后，我们结合 Calcite Parser.jj 文件，来具体看下这些规则应该如何配置，以及在 Calcite SQL Parser 中起到了什么作用。 javacc_options 规则： 用于定义 JavaCC 解析配置项，格式为 key=value，例如：IGNORE_CASE = true;，声明在解析阶段忽略大小写。STATIC = false 用于控制 JavaCC 生成的代码，成员变量和方法是否为静态方法，通常都是设置为 false。UNICODE_INPUT = true 则用于设置包括中文在内的各种字符解析。 options // JavaCC 配置项 STATIC = false; IGNORE_CASE = true; UNICODE_INPUT = true; java_compilation_unit 规则： 用于定义 JavaCC 生成解析器类的定义，该代码块包含在 PARSER_BEGIN 和 PARSER_END 中。Calcite 中使用 Freemarker 模板引擎，解析器类名由参数传入，然后继承 SqlAbstractParserImpl 抽象类，该类提供了如 createCall 等基础方法，以及 getMetadata、getPos、parseSqlStmtEof 等抽象方法。 // 解析器开始标记PARSER_BEGIN($parser.class)package $parser.package;import org.apache.calcite.avatica.util.Casing;public class $parser.class extends SqlAbstractParserImpl\t// Java 处理逻辑// 解析器结束标记PARSER_END($parser.class) production 规则： 用于定义解析中关键的词法和语法规则，JavaCC 将词法规则（如保留字、表达式）和语法规则（BNF）都统一写在一个文件中，并支持使用正则表达式，使语法描述文件易读且易于维护。production 语法规则中包含了 javacode_production、regular_expr_production 和 bnf_production 几个重要的子规则，我们结合 Calcite 的示例来学习下这些规则的使用。 javacode_production 规则： 用于编写供解析器调用的通用 Java 代码，例如：getPos 方法获取 Token 的位置，该部分代码以 JAVACODE 关键字开始。 // 公共方法，供解析器调用JAVACODE protected SqlParserPos getPos() return new SqlParserPos( token.beginLine, token.beginColumn, token.endLine, token.endColumn); regular_expr_production 规则： 用于描述词法规则，可以通过 SKIP 指定要忽略的内容（空格、换行等），通过 TOKEN 定义语法中的关键字，每个 Token 用尖括号标识，多个 Token 之间用竖线分隔。尖括号里面用冒号分隔，冒号前面是变量名，后面是对应的正则表达式。 DEFAULT, DQID, BTID, BQID，BQHID 等是词法状态，其中 DEFAULT, DQID, BTID, BQID 是 4 种正常状态，除了如何识别带引号的标识符之外，他们的行为相同。BQHID 状态仅存在于表名的开头（例如紧靠在 FROM 或 INSERT INTO 后面），一旦遇到标识符，词法状态就会转移至 BTID。 /*Lexical states:DEFAULT: Identifiers are quoted in brackets, e.g. [My Identifier]DQID: Identifiers are double-quoted, e.g. My IdentifierBTID: Identifiers are enclosed in back-ticks, escaped using back-ticks, e.g. `My ``Quoted`` Identifier`BQID: Identifiers are enclosed in back-ticks, escaped using backslash, e.g. `My \\`Quoted\\` Identifier`, and with the potential to shift into BQHID in contexts where table names are expected, and thus allow hyphen-separated identifiers as part of table namesBQHID: Identifiers are enclosed in back-ticks, escaped using backslash, e.g. `My \\`Quoted\\` Identifier` and unquoted identifiers may contain hyphens, e.g. foo-barIN_SINGLE_LINE_COMMENT:IN_FORMAL_COMMENT:IN_MULTI_LINE_COMMENT:DEFAULT, DQID, BTID, BQID are the 4 normal states. Behavior is identicalexcept for how quoted identifiers are recognized.The BQHID state exists only at the start of a table name (e.g. immediately afterFROM or INSERT INTO). As soon as an identifier is seen, the state shifts backto BTID.After a comment has completed, the lexer returns to the previous state, oneof the normal states.*/// 词法规则DEFAULT, DQID, BTID, BQID, BQHID TOKEN : HINT_BEG: /*+| COMMENT_END: */ bnf_production 规则： 用于描述语法规则，能够支持复杂的语法描述，语法规则大体上类似于 Java 代码，首先是方法声明 SqlNode ExprOrJoinOrOrderedQuery(ExprContext exprContext)，后面紧跟着冒号 : 和两对花括号，第一对花括号用于声明变量，第二对花括号则用于编写解析逻辑。 JavaCC 语法规则很灵活，可以使用正则表达式， []、() 和 | 分别表示可选、必选和分支。在解析分支语法时，可能需要通过大量的回溯操作才能完成分支的选择，JavaCC 为了优化回溯带来的性能问题，默认只向前查看一个 TOKEN（可满足大部分解析需求），可以通过 LOOKAHEAD(2) 指定向前查看的 TOKEN 数，从而做出最好的选择。 // 语法规则和 Java 处理逻辑/***************************************** * Syntactical Descriptions * *****************************************/SqlNode ExprOrJoinOrOrderedQuery(ExprContext exprContext) : SqlNode e; final ListObject list = new ArrayListObject(); // Lookhead to distinguish between TABLE emp (which will be // matched by ExplicitTable() via Query()) // and TABLE fun(args) (which will be matched by TableRef()) ( LOOKAHEAD(2) e = Query(exprContext) e = OrderByLimitOpt(e) return e; | e = TableRef1(ExprContext.ACCEPT_QUERY_OR_JOIN) ( e = JoinTable(e) )* list.add(e); ( AddSetOpQuery(list, exprContext) )* return SqlParserUtil.toTree(list); ) 以上大致介绍了 Calcite SQL Parser 使用到的 JavaCC 相关知识，如果读者对 JavaCC 感兴趣，可以查看参考资料中的官方文档以及 JavaCC 博文进行学习。下面让我们再来学习下 Calcite SQL Parser 的整体实现，如何通过 Java 代码调用解析逻辑，实现 SQL 字符串到 AST 的解析。 (adsbygoogle = window.adsbygoogle || []).push({}); 赞助商 Calcite SQL Parser 实现 Calcite SQL Parser 的核心实现在 calcite-core 模块，在 src/main 下包含了 codegen 目录，Parser.jj 文件是 SQL Parser 相关的词法和语法规则文件，并且为了实现 SQL Parser 的扩展，Calcite 采用了 Freemarker 模板引擎，config.fmpp 和 default_config.fmpp 用于定义 Freemarker 模板的属性。 Calcite SQL Parser 的入口类是 SqlParser，调用 SQLParser.create 可以快速创建解析对象，然后进行 SQL 解析。SPAN 类是 SqlParserPos 的构建器，构建的 SqlParserPos 对象主要用来记录 TOKEN 在 SQL 中的位置。SqlAbstractParserImpl 是解析的抽象类，Calcite 中生成的 SqlParserImpl、SqlBabelParserImpl 和 SqlDdlParserImpl 都继承了该抽象类。 Calcite SQL Parser 调用非常简单，按照如下示例可以快速地解析并获取 AST 对象。SqlParser.create 方法传入要解析的 SQL 字符串，以及一个 Config 对象。 String sql = select name from EMPS;SqlParser sqlParser = SqlParser.create(sql, Config.DEFAULT);SqlNode sqlNode = sqlParser.parseQuery();System.out.println(sqlNode.toSqlString(MysqlSqlDialect.DEFAULT)); Config 对象则是通过 Immutable 注解自动生成的实现类，它实现了接口方法定义的解析相关配置。例如：包含引号的标识符如何处理大小写、不包含引号的标识符如何处理大小写以及是否大小写敏感等（更多 Config 配置读者可以参考 Config 类源码）。 Config withQuotedCasing(Casing casing);Config withUnquotedCasing(Casing casing);Config withCaseSensitive(boolean caseSensitive);... Calcite 解析器核心的 SqlParser 类除了提供静态 create 方法创建解析器对象外，还提供了如下的解析方法，用于处理不同场景下的 SQL 解析。 // 解析 SQL 表达式public SqlNode parseExpression() throws SqlParseException ...// 解析 SQL 查询语句public SqlNode parseQuery() throws SqlParseException ...// 解析 SQL 查询语句public SqlNode parseQuery(String sql) throws SqlParseException ...// 解析 SQL 语句public SqlNode parseStmt() throws SqlParseException ...// 解析分号分隔的 SQL 语句public SqlNodeList parseStmtList() throws SqlParseException ... 我们以常用的 parseQuery() 方法为例，来看下方法内部调用了哪些 JavaCC 生成的方法。parseQuery 方法首先调用了 parser 对象的 parseSqlStmtEof 方法，而 parser 对象是 SqlAbstractParserImpl 抽象类的实现类，此处我们先关注 SqlParserImpl 实现类。 /** * Parses a codeSELECT/code statement. * * @return A @link org.apache.calcite.sql.SqlSelect for a regular code * SELECT/code statement; a @link org.apache.calcite.sql.SqlBinaryOperator * for a codeUNION/code, codeINTERSECT/code, or codeEXCEPT/code. * @throws SqlParseException if there is a parse error */public SqlNode parseQuery() throws SqlParseException try return parser.parseSqlStmtEof(); catch (Throwable ex) throw handleException(ex); SqlParserImpl 类是通过 JavaCC 动态生成的实现类，内部的 parseSqlStmtEof 方法定义如下，会继续调用内部的 SqlStmtEof 方法。而 SqlStmtEof 方法会调用 SqlStmt 方法，在该方法内部会判断当前 SQL 的首个 Token 的类型，查询语句会调用 OrderedQueryOrExpr(ExprContext.ACCEPT_QUERY) 方法。 // org/apache/calcite/sql/parser/impl/SqlParserImpl.java:205public SqlNode parseSqlStmtEof() throws Exception return SqlStmtEof();/** * Parses an SQL statement followed by the end-of-file symbol. */final public SqlNode SqlStmtEof() throws ParseException SqlNode stmt; stmt = SqlStmt(); jj_consume_token(0); if (true) return stmt; throw new Error(Missing return statement in function);/** * Parses an SQL statement. */final public SqlNode SqlStmt() throws ParseException SqlNode stmt; switch ((jj_ntk == -1) ? jj_ntk() : jj_ntk) case RESET: case SET: stmt = SqlSetOption(Span.of(), null); break; case ALTER: stmt = SqlAlter(); break; case A: // ... case SELECT: // ... case UNICODE_QUOTED_IDENTIFIER: stmt = OrderedQueryOrExpr(ExprContext.ACCEPT_QUERY); break; case EXPLAIN: stmt = SqlExplain(); break; case DESCRIBE: stmt = SqlDescribe(); break; case INSERT: case UPSERT: stmt = SqlInsert(); break; case DELETE: stmt = SqlDelete(); break; case UPDATE: stmt = SqlUpdate(); break; case MERGE: stmt = SqlMerge(); break; case CALL: stmt = SqlProcedureCall(); break; default: jj_la1[27] = jj_gen; jj_consume_token(-1); throw new ParseException(); if (true) return stmt; throw new Error(Missing return statement in function); OrderedQueryOrExpr 方法的定义如下，该方法主要用于处理行表达式以及包含可选 ORDER BY 的 SELECT 语句。从方法实现逻辑可以看出，它首先调用 QueryOrExpr 方法构造了 SqlSelect 对象，然后再调用 OrderByLimitOpt 方法包装成 SqlOrderBy 对象。 /** * Parses either a row expression or a query expression with an optional * ORDER BY. * * pPostgres syntax for limit: * * blockquotepre * [ LIMIT count | ALL ] * [ OFFSET start ]/pre * /blockquote * * pTrino syntax for limit: * * blockquotepre * [ OFFSET start ] * [ LIMIT count | ALL ]/pre * /blockquote * * pMySQL syntax for limit: * * blockquotepre * [ LIMIT count | start, count ]/pre * /blockquote * * pSQL:2008 syntax for limit: * * blockquotepre * [ OFFSET start ROW | ROWS ] * [ FETCH FIRST | NEXT [ count ] ROW | ROWS ONLY ]/pre * /blockquote */final public SqlNode OrderedQueryOrExpr(ExprContext exprContext) throws ParseException SqlNode e; e = QueryOrExpr(exprContext); e = OrderByLimitOpt(e); if (true) return e; throw new Error(Missing return statement in function); QueryOrExpr 方法内部会依次调用 LeafQueryOrExpr、LeafQuery 和 SqlSelect 方法，在 SqlSelect 方法内部，则会对查询语句的每个语法片段依次进行初始化，最终返回 SqlSelect 对象。SqlSelect 对象初始化的调用链路如下图所示。 Calcite SQL Parser 扩展 尽管 Calcite SQL Parser 已经支持了主流数据库的 DML 语句解析，但是考虑到数据库生态的多样性，大多数据库都提供了特有的 SQL 方言。为了能够灵活地支持数据库方言，Calcite SQL Parser 支持用户扩展，通过 Freemarker 模板可以将 Calcite 内置的解析文件和用户自定义的解析文件整合到一个 JavaCC 文件中，从而实现 SQL 解析能力的扩展。 Calcite SQL Parser 语法扩展流程如下图所示，Calcite 在 templates 目录提供了内置的 Parser.jj 模板，在 includes 目录提供了扩展的 compoundIdentifier.ftl 和 parserImpls.ftl 模板。这些模板通过 FMPP（FreeMarker Preprocessor）可以生成最终的解析文件 Parser.jj，再交由 JavaCC 编译工具生成 SqlParserImpl 类。 core 模块 build.gradle.kts 中的脚本也印证了以上的处理流程，先执行 FmppTask，再执行 JavaCCTask。 val fmppMain by tasks.registering(org.apache.calcite.buildtools.fmpp.FmppTask::class) config.set(file(src/main/codegen/config.fmpp)) templates.set(file(src/main/codegen/templates))val javaCCMain by tasks.registering(org.apache.calcite.buildtools.javacc.JavaCCTask::class) dependsOn(fmppMain) val parserFile = fmppMain.map it.output.asFileTree.matching include(**/Parser.jj) inputFile.from(parserFile) packageName.set(org.apache.calcite.sql.parser.impl) 了解了 Calcite SQL Parser 语法扩展的流程后，我们再来看一个语法扩展的例子。在 server 模块，Calcite 使用相同的扩展方法，增加了对 DDL 语句的支持。下图展示了 server 模块语法扩展使用到的文件——config.fmpp 和 parserImpls.ftl。 config.fmpp 文件（如下所示）定义了 Parser.jj 模板中需要使用的参数，如果未配置则默认会使用 default_config.fmpp 中的参数。parser 下的 package、class 和 imports 用于定义生成的解析器类的包名、类名和引入的包。keywords 用于定义扩展语法中的关键字，nonReservedKeywordsToAdd 用于定义非保留的关键字。createStatementParserMethods、dropStatementParserMethods 和 truncateStatementParserMethods 分别用于定义 DDL 语句中 CREATE、DROP 和 TRUNCATE 语句的初始化方法。implementationFiles 则用于定义这些方法的实现文件。 data: # Data declarations for this parser. # # Default declarations are in default_config.fmpp; if you do not include a # declaration (imports or nonReservedKeywords, for example) in this file, # FMPP will use the declaration from default_config.fmpp. parser: # Generated parser implementation class package and name package: org.apache.calcite.sql.parser.ddl, class: SqlDdlParserImpl, # List of import statements. imports: [ org.apache.calcite.schema.ColumnStrategy org.apache.calcite.sql.SqlCreate org.apache.calcite.sql.SqlDrop org.apache.calcite.sql.SqlTruncate org.apache.calcite.sql.ddl.SqlDdlNodes ] # List of new keywords. Example: DATABASES, TABLES. If the keyword is # not a reserved keyword, add it to the nonReservedKeywords section. keywords: [ IF MATERIALIZED STORED VIRTUAL JAR FILE ARCHIVE ] # List of non-reserved keywords to add; # items in this list become non-reserved nonReservedKeywordsToAdd: [ # not in core, added in server IF MATERIALIZED STORED VIRTUAL JAR FILE ARCHIVE ] # List of methods for parsing extensions to CREATE [OR REPLACE] calls. # Each must accept arguments (SqlParserPos pos, boolean replace). # Example: SqlCreateForeignSchema. createStatementParserMethods: [ SqlCreateForeignSchema SqlCreateMaterializedView SqlCreateSchema SqlCreateTable SqlCreateType SqlCreateView SqlCreateFunction ] # List of methods for parsing extensions to DROP calls. # Each must accept arguments (SqlParserPos pos). # Example: SqlDropSchema. dropStatementParserMethods: [ SqlDropMaterializedView SqlDropSchema SqlDropTable SqlDropType SqlDropView SqlDropFunction ] # List of methods for parsing extensions to TRUNCATE calls. # Each must accept arguments (SqlParserPos pos). # Example: SqlTruncateTable. truncateStatementParserMethods: [ SqlTruncateTable ] # List of files in @includes directory that have parser method # implementations for parsing custom SQL statements, literals or types # given as part of statementParserMethods, literalParserMethods or # dataTypeParserMethods. # Example: parserImpls.ftl. implementationFiles: [ parserImpls.ftl ] # 定义引入 Freemarker 文件的路径freemarkerLinks: includes: includes/ 以 SqlCreateForeignSchema 方法为例，它的实现逻辑在 parserImpls.ftl 中，和 Calcite 内置的语法解析逻辑类似，遵循同样的编写规则。 SqlCreate SqlCreateForeignSchema(Span s, boolean replace) : final boolean ifNotExists; final SqlIdentifier id; SqlNode type = null; SqlNode library = null; SqlNodeList optionList = null; FOREIGN SCHEMA ifNotExists = IfNotExistsOpt() id = CompoundIdentifier() ( TYPE type = StringLiteral() | LIBRARY library = StringLiteral() ) [ optionList = Options() ] return SqlDdlNodes.createForeignSchema(s.end(this), replace, ifNotExists, id, type, library, optionList); server 模块 build.gradle.kts 文件定义的 FMPP 任务稍有不同，它会指定 core 模块 templates 目录下的 Parser.jj 作为模板，扩展的语法定义会被整合到模板中，统一输出最终的 Parser.jj 文件。 val fmppMain by tasks.registering(org.apache.calcite.buildtools.fmpp.FmppTask::class) inputs.dir(src/main/codegen).withPathSensitivity(PathSensitivity.RELATIVE) config.set(file(src/main/codegen/config.fmpp)) templates.set(file($rootDir/core/src/main/codegen/templates))val javaCCMain by tasks.registering(org.apache.calcite.buildtools.javacc.JavaCCTask::class) dependsOn(fmppMain) val parserFile = fmppMain.map it.output.asFileTree.matching include(**/Parser.jj) inputFile.from(parserFile) packageName.set(org.apache.calcite.sql.parser.ddl) 想观察整个过程的读者，可以尝试执行 ServerParserTest#testCreateForeignSchema 单元测试，可以看到 build 目录生成了统一的 Parser.jj 文件。然后经过 JavaCC 编译生成了 SqlDdlParserImpl 类。 Calcite SqlNode 体系 SQL 生成 前面我们学习了 Calcite SQL Parser 的实现和扩展，在最后一个部分，我们再来了解下 Calcite SQL Parser 的最终产物——SqlNode。SqlNode 是 Calcite 中负责封装语义信息的基础类，除了在解析阶段使用外，它还在校验（validate）、转换 RelNode（convert）以及生成不同方言的 SQL（toSqlString）等阶段都发挥了重要作用。 SqlNode 是所有解析节点的父类，Calcite 中目前有 70 多个实现类，这些类共同组成了 SqlNode 体系。SqlNode 体系总体上可以分为 3 大类：SqlCall、SqlLiteral 和 SqlIdentifier。从下图中可以看出 SqlNode 抽象类定义了 validate、unparse 和 accept 等抽象方法，各实现类负责实现当前节点的处理逻辑，从而保证 SqlNode 体系能够完成元数据校验、SQL 方言生成等功能。 下面我们再来具体了解下 SqlCall、SqlLiteral 和 SqlIdentifier 这 3 类 SqlNode 分别包含了哪些子类，以及他们的具体作用。 SqlCall：代表了对 SqlOperator 的调用，Calcite 中每个操作都可以对应一个 SqlCall，例如查询操作是 SqlSelectOperator，对应的 SqlNode 是 SqlSelect。常用的 SqlCall 实现类如下图所示，包含了SqlSelect、SqlDelete、SqlUpdate、SqlInsert 和 SqlMerge 等。 SqlCall 子类体系 以 SqlSelect 为例，类中包含了查询语句涉及的子句，selectList 代表了查询中的投影列，from 代表了查询的表，where 则代表了查询条件，其他字段也都和查询语句中的子句能够一一对应。 /** * A codeSqlSelect/code is a node of a parse tree which represents a select * statement. It warrants its own node type just because we have a lot of * methods to put somewhere. */public class SqlSelect extends SqlCall //~ Static fields/initializers --------------------------------------------- // constants representing operand positions public static final int FROM_OPERAND = 2; public static final int WHERE_OPERAND = 3; public static final int HAVING_OPERAND = 5; public static final int QUALIFY_OPERAND = 7; SqlNodeList keywordList; SqlNodeList selectList; @Nullable SqlNode from; @Nullable SqlNode where; @Nullable SqlNodeList groupBy; @Nullable SqlNode having; SqlNodeList windowDecls; @Nullable SqlNode qualify; @Nullable SqlNodeList orderBy; @Nullable SqlNode offset; @Nullable SqlNode fetch; @Nullable SqlNodeList hints; 前文示例中的 select name from EMPS 语句，经过 Calcite SQL Parser 解析，最终能够得到如下的 AST 结构（SqlNode 树）： SqlIdentifier：代表 SQL 中的标识符，例如 SQL 语句中的表名、字段名。 SqlLiteral：主要用于封装 SQL 中的常量，通常也叫做字面量。 Calcite 支持了众多类型的常量，下表展示了常量类型及其含义，可供大家学习参考。 类型名称 类型含义 值类型 SqlTypeName.NULL 空值。 null SqlTypeName.BOOLEAN Boolean 类型，包含：TRUE，FALSE 或者 UNKNOWN。 Boolean 类型，null 代表 UNKNOWN。 SqlTypeName.DECIMAL 精确数值，例如：0，-.5，12345。 BigDecimal SqlTypeName.DOUBLE 近似数值，例如：6.023E-23。 BigDecimal SqlTypeName.DATE 日期，例如：DATE '1969-04'29'。 Calendar SqlTypeName.TIME 时间，例如：TIME '18:37:42.567'。 Calendar SqlTypeName.TIMESTAMP 时间戳，例如：TIMESTAMP '1969-04-29 18:37:42.567'。 Calendar SqlTypeName.CHAR 字符常量，例如：'Hello, world!'。 NlsString SqlTypeName.BINARY 二进制常量，例如：X'ABC', X'7F'。 BitString SqlTypeName.SYMBOL 符号是一种特殊类型，用于简化解析。 An Enum SqlTypeName.INTERVAL_YEAR … SqlTypeName.INTERVAL_SECOND 时间间隔，例如：INTERVAL '1:34' HOUR。 SqlIntervalLiteral.IntervalValue. 通过 SqlNode 体系的介绍，我们大致了解了不同类型 SqlNode 的用途，在 Calcite 中 SqlNode 还有一个强大的功能——SQL 生成。因为 Calcite 的目标是适配各种不同的存储引擎，提供统一的查询引擎，因此 Calcite 需要通过 SqlNode 语法树，生成不同存储引擎对应的 SQL 方言或者 DSL 语言。 在 SqlNode 中提供了 toSqlString 方法，允许用户传入不同的数据库方言，将 SqlNode 语法树转换为对应方言的 SQL 字符串。 String sql = select name from EMPS;SqlParser sqlParser = SqlParser.create(sql, Config.DEFAULT);SqlNode sqlNode = sqlParser.parseQuery();System.out.println(sqlNode.toSqlString(MysqlSqlDialect.DEFAULT));// SELECT *// FROM `T_ORDER`// WHERE `ORDER_ID` = 1 toSqlString 方法实现逻辑如下，它会调用重载方法并且额外传入参数 forceParens，该参数用于控制表达式是否需要使用括号。 public SqlString toSqlString(@Nullable SqlDialect dialect) return toSqlString(dialect, false);public SqlString toSqlString(@Nullable SqlDialect dialect, boolean forceParens) return toSqlString(c - c.withDialect(Util.first(dialect, AnsiSqlDialect.DEFAULT)) .withAlwaysUseParentheses(forceParens) .withSelectListItemsOnSeparateLines(false) .withUpdateSetListNewline(false) .withIndentation(0)); 在 toSqlString 重载方法内部，会初始化 SqlWriterConfig 参数，该参数用于控制 SQL 翻译过程中的换行、是否添加标识符引号等行为。参数初始化完成后，会将参数设置作为 Lambda 函数传递到另一个重载方法中。在该重载方法内部，会创建 SqlPrettyWriter 作为 SQL 生成的容器，它会记录 SQL 生成过程中的 SQL 字符片段。SQL 生成的核心逻辑是 unparse 方法，调用时会传入容器 writer 类。 public SqlString toSqlString(UnaryOperator SqlWriterConfig transform) final SqlWriterConfig config = transform.apply(SqlPrettyWriter.config()); SqlPrettyWriter writer = new SqlPrettyWriter(config); unparse(writer, 0, 0); return writer.toSqlString(); 示例中最外层 SqlNode 为 SqlSelect，因此调用的方法为 SqlSelect#unparse，具体逻辑如下。该方法会判断当前查询是否为子查询，是子查询则创建一个新的 SqlWriter.Frame，然后调用不同方言的 unparseCall 方法生成 SQL，如果不是子查询，则直接调用不同方言的 unparseCall 方法生成 SQL。 // Override SqlCall, to introduce a sub-query frame.@Overridepublic void unparse(SqlWriter writer, int leftPrec, int rightPrec) if (!writer.inQuery() || getFetch() != null (leftPrec SqlInternalOperators.FETCH.getLeftPrec() || rightPrec SqlInternalOperators.FETCH.getLeftPrec()) || getOffset() != null (leftPrec SqlInternalOperators.OFFSET.getLeftPrec() || rightPrec SqlInternalOperators.OFFSET.getLeftPrec()) || getOrderList() != null (leftPrec SqlOrderBy.OPERATOR.getLeftPrec() || rightPrec SqlOrderBy.OPERATOR.getRightPrec())) // If this SELECT is the topmost item in a sub-query, introduce a new // frame. (The topmost item in the sub-query might be a UNION or // ORDER. In this case, we dont need a wrapper frame.) final SqlWriter.Frame frame = writer.startList(SqlWriter.FrameTypeEnum.SUB_QUERY, (, )); writer.getDialect().unparseCall(writer, this, 0, 0); writer.endList(frame); else writer.getDialect().unparseCall(writer, this, leftPrec, rightPrec); 由于我们示例中生成的是 MySQL 的方言，因此调用的是 MysqlSqlDialect#unparseCall 方法，具体实现逻辑如下。前文我们介绍了 SqlCall 的用于，它代表了对 SqlOperator 的调用，此处为 SqlSelectOperator，它对应的 SqlKind 为 SELECT，因此会先调用 super.unparseCall 方法。可以看到，除了 default 分支外，其他分支在处理不同方言的差异，例如：将 POSITION 操作转换成 MySQL 中的 INSTR，将 LISTAGG 转换为 MySQL 中的 GROUP_CONCAT。 @Overridepublic void unparseCall(SqlWriter writer, SqlCall call, int leftPrec, int rightPrec) switch (call.getKind()) case POSITION: final SqlWriter.Frame frame = writer.startFunCall(INSTR); writer.sep(,); call.operand(1).unparse(writer, leftPrec, rightPrec); writer.sep(,); call.operand(0).unparse(writer, leftPrec, rightPrec); writer.endFunCall(frame); break; case FLOOR: if (call.operandCount() != 2) super.unparseCall(writer, call, leftPrec, rightPrec); return; unparseFloor(writer, call); break; case WITHIN_GROUP: final List SqlNode operands = call.getOperandList(); if (operands.size() = 0 || operands.get(0).getKind() != SqlKind.LISTAGG) super.unparseCall(writer, call, leftPrec, rightPrec); return; unparseListAggCall(writer, (SqlCall) operands.get(0), operands.size() == 2 ? operands.get(1) : null, leftPrec, rightPrec); break; case LISTAGG: unparseListAggCall(writer, call, null, leftPrec, rightPrec); break; default: super.unparseCall(writer, call, leftPrec, rightPrec); super.unparseCall 方法调用的是 SqlDialect#unparseCall，由于 SqlKind 不是 ROW，逻辑会走到 operator.unparse 中，即 SqlSelectOperator#unparse。 public void unparseCall(SqlWriter writer, SqlCall call, int leftPrec, int rightPrec) SqlOperator operator = call.getOperator(); switch (call.getKind()) case ROW: // Remove the ROW keyword if the dialect does not allow that. if (!getConformance().allowExplicitRowValueConstructor()) if (writer.isAlwaysUseParentheses()) // If writer always uses parentheses, it will have started parentheses // that we now regret. Use a special variant of the operator that does // not print parentheses, so that we can use the ones already started. operator = SqlInternalOperators.ANONYMOUS_ROW_NO_PARENTHESES; else // Use an operator that prints (a, b, c) rather than // ROW (a, b, c). operator = SqlInternalOperators.ANONYMOUS_ROW; // fall through default: operator.unparse(writer, call, leftPrec, rightPrec); SqlSelectOperator#unparse 方法会对 SELECT 语句按照顺序进行 SQL 生成，包括：Hint 注释、投影列、表、查询条件、分组条件等。在投影列、查询条件生成的过程中，会调用其他 SqlNode 的 unparse 方法，通过遍历语法树逐层调用，最终 writer 类获取了全部的 SQL 信息，通过 toSqlString 方法转换为最终的 SQL 字符串。SqlNode 生成 SQL 调用的节点很多，本文限于篇幅就不一一介绍了，感兴趣的朋友可以自行 DEBUG 探究一下。 @SuppressWarnings(deprecation)@Overridepublic void unparse(SqlWriter writer, SqlCall call, int leftPrec, int rightPrec) SqlSelect select = (SqlSelect) call; final SqlWriter.Frame selectFrame = writer.startList(SqlWriter.FrameTypeEnum.SELECT); // 向 writer 容器中输出 SELECT 关键字 writer.sep(SELECT); if (select.hasHints()) writer.sep(/*+); castNonNull(select.hints).unparse(writer, 0, 0); writer.print(*/); writer.newlineAndIndent(); for (int i = 0; i select.keywordList.size(); i++) final SqlNode keyword = select.keywordList.get(i); keyword.unparse(writer, 0, 0); writer.topN(select.fetch, select.offset); final SqlNodeList selectClause = select.selectList; // 向 writer 容器中输出投影列 writer.list(SqlWriter.FrameTypeEnum.SELECT_LIST, SqlWriter.COMMA, selectClause); // 向 writer 容器中输出 FROM 关键字及表名 if (select.from != null) // Calcite SQL requires FROM but MySQL does not. writer.sep(FROM); // for FROM clause, use precedence just below join operator to make // sure that an un-joined nested select will be properly // parenthesized final SqlWriter.Frame fromFrame = writer.startList(SqlWriter.FrameTypeEnum.FROM_LIST); select.from.unparse(writer, SqlJoin.COMMA_OPERATOR.getLeftPrec() - 1, SqlJoin.COMMA_OPERATOR.getRightPrec() - 1); writer.endList(fromFrame); // 向 writer 容器中输出 WHERE 关键字及查询条件 SqlNode where = select.where; if (where != null) writer.sep(WHERE); if (!writer.isAlwaysUseParentheses()) SqlNode node = where; // decide whether to split on ORs or ANDs SqlBinaryOperator whereSep = SqlStdOperatorTable.AND; if ((node instanceof SqlCall) node.getKind() == SqlKind.OR) whereSep = SqlStdOperatorTable.OR; // unroll whereClause final List SqlNode list = new ArrayList (0); while (node.getKind() == whereSep.kind) assert node instanceof SqlCall; final SqlCall call1 = (SqlCall) node; list.add(0, call1.operand(1)); node = call1.operand(0); list.add(0, node); // unparse in a WHERE_LIST frame writer.list(SqlWriter.FrameTypeEnum.WHERE_LIST, whereSep, new SqlNodeList(list, where.getParserPosition())); else where.unparse(writer, 0, 0); // 向 writer 容器中输出分组查询条件 if (select.groupBy != null) SqlNodeList groupBy = select.groupBy.size() == 0 ? SqlNodeList.SINGLETON_EMPTY : select.groupBy; // if the DISTINCT keyword of GROUP BY is present it can be the only item if (groupBy.size() == 1 groupBy.get(0) != null groupBy.get(0).getKind() == SqlKind.GROUP_BY_DISTINCT) writer.sep(GROUP BY DISTINCT); List SqlNode operandList = ((SqlCall) groupBy.get(0)).getOperandList(); groupBy = new SqlNodeList(operandList, groupBy.getParserPosition()); else writer.sep(GROUP BY); writer.list(SqlWriter.FrameTypeEnum.GROUP_BY_LIST, SqlWriter.COMMA, groupBy); if (select.having != null) writer.sep(HAVING); select.having.unparse(writer, 0, 0); if (select.windowDecls.size() 0) writer.sep(WINDOW); writer.list(SqlWriter.FrameTypeEnum.WINDOW_DECL_LIST, SqlWriter.COMMA, select.windowDecls); if (select.qualify != null) writer.sep(QUALIFY); select.qualify.unparse(writer, 0, 0); if (select.orderBy != null select.orderBy.size() 0) writer.sep(ORDER BY); writer.list(SqlWriter.FrameTypeEnum.ORDER_BY_LIST, SqlWriter.COMMA, select.orderBy); writer.fetchOffset(select.fetch, select.offset); writer.endList(selectFrame); 写在最后 笔者因为工作原因接触到 Calcite，前期学习过程中，深感 Calcite 学习资料之匮乏，因此创建了 Calcite 从入门到精通知识星球，希望能够将学习过程中的资料和经验沉淀下来，为更多想要学习 Calcite 的朋友提供一些帮助。 欢迎关注 欢迎关注「端小强的博客」微信公众号，会不定期分享日常学习和工作经验，欢迎大家关注交流。","tags":["Calcite","JavaCC"],"categories":["Calcite"]},{"title":"Apache Calcite 快速入门指南","path":"/blog/apache-calcite-quick-start-guide.html","content":"注意：本文基于 Calcite 1.35.0 版本源码进行学习研究，其他版本可能会存在实现逻辑差异，对源码感兴趣的读者请注意版本选择。 Calcite 简介 Apache Calcite 是一个动态数据管理框架，提供了：SQL 解析、SQL 校验、SQL 查询优化、SQL 生成以及数据连接查询等典型数据库管理功能。Calcite 的目标是 One Size Fits All，即一种方案适应所有需求场景，希望能为不同计算平台和数据源提供统一的查询引擎，并以类似传统数据库的访问方式（SQL 和高级查询优化）来访问不同计算平台和数据源上的数据。下图展示了 Calcite 的架构以及 Calcite 和数据处理系统的交互关系，从图中我们可以看出 Calcite 具有 4 种类型的组件。 Calcite 架构图 最外层是 JDBC Client 和数据处理系统（Data Processing System），JDBC Client 提供给用户，用于连接 Calcite 的 JDBC Server，数据处理系统则用于对接不同的数据存储引擎； 内层是 Calcite 核心架构的流程性组件，包括负责接收 JDBC 请求的 JDBC Server，负责解析 SQL 语法的 SQL Parser，负责校验 SQL 语义的 SQL Validator，以及负责构建算子表达式的 Expression Builder（可以通过 SQL 转换为关系代数，也可以通过 Expression Builder 直接构建）； 算子表达式（Operator Expressions）、元数据提供器（Metadata Providers）、可插拔优化规则（Pluggable Rules） 是用于适配不同逻辑的适配器，这些适配器都可以进行灵活地扩展； 查询优化器（Query Optimizer）是整个 Calcite 的核心，负责对逻辑执行计划进行优化，基于 RBO 和 CBO 两种优化模型，得到可执行的最佳执行计划。 另外，Calcite 还具有灵活性（Flexible）、组件可插拔（Embeddable）和可扩展（Extensible）3 大核心特性，Calcite 的解析器、优化器都可以作为独立的组件使用。目前，Calcite 作为 SQL 解析与优化引擎，已经广泛使用在 Hive、Drill、Flink、Phoenix 和 Storm 等项目中。 Calcite 入门示例 在了解了 Calcite 的基本架构和特点之后，我们以 Calcite 官方经典的 CSV 案例作为入门示例，来展示下 Calcite 强大的功能。首先，从 github 下载 calcite 项目源码，git clone https://github.com/apache/calcite.git，然后执行 cd calcite/example/csv 进入 csv 目录。 Calcite 为我们提供了内置的 sqlline 命令，可以通过 ./sqlline 快速连接到 Calcite，并使用 !connect 定义数据库连接，model 属性用于指定 Calcite 的数据模型配置文件。 ./sqlline Building Apache Calcite 1.31.0-SNAPSHOTsqlline version 1.12.0sqlline sqlline !connect jdbc:calcite:model=src/test/resources/model.json admin adminTransaction isolation level TRANSACTION_REPEATABLE_READ is not supported. Default (TRANSACTION_NONE) will be used instead.0: jdbc:calcite:model=src/test/resources/mode 连接成功后，我们可以执行一些语句来测试 SQL 执行，!tables 用于查询表相关的元数据，!columns depts 用于查询列相关的元数据。 0: jdbc:calcite:model=src/test/resources/mode !tables+-----------+-------------+------------+--------------+---------+----------+------------+-----------+---------------------------+----------------+| TABLE_CAT | TABLE_SCHEM | TABLE_NAME | TABLE_TYPE | REMARKS | TYPE_CAT | TYPE_SCHEM | TYPE_NAME | SELF_REFERENCING_COL_NAME | REF_GENERATION |+-----------+-------------+------------+--------------+---------+----------+------------+-----------+---------------------------+----------------+| | SALES | DEPTS | TABLE | | | | | | || | SALES | EMPS | TABLE | | | | | | || | SALES | SDEPTS | TABLE | | | | | | || | metadata | COLUMNS | SYSTEM TABLE | | | | | | || | metadata | TABLES | SYSTEM TABLE | | | | | | |+-----------+-------------+------------+--------------+---------+----------+------------+-----------+---------------------------+----------------+0: jdbc:calcite:model=src/test/resources/mode !columns depts+-----------+-------------+------------+-------------+-----------+-----------+-------------+---------------+----------------+----------------+----------+---------+------------+---------------+------------------+-------------------+--------------+| TABLE_CAT | TABLE_SCHEM | TABLE_NAME | COLUMN_NAME | DATA_TYPE | TYPE_NAME | COLUMN_SIZE | BUFFER_LENGTH | DECIMAL_DIGITS | NUM_PREC_RADIX | NULLABLE | REMARKS | COLUMN_DEF | SQL_DATA_TYPE | SQL_DATETIME_SUB | CHAR_OCTET_LENGTH | ORDINAL_POSI |+-----------+-------------+------------+-------------+-----------+-----------+-------------+---------------+----------------+----------------+----------+---------+------------+---------------+------------------+-------------------+--------------+| | SALES | DEPTS | DEPTNO | 4 | INTEGER | -1 | null | null | 10 | 1 | | | null | null | -1 | 1 || | SALES | DEPTS | NAME | 12 | VARCHAR | -1 | null | null | 10 | 1 | | | null | null | -1 | 2 |+-----------+-------------+------------+-------------+-----------+-----------+-------------+---------------+----------------+----------------+----------+---------+------------+---------------+------------------+-------------------+--------------+ 我们再来执行一些复杂的查询语句，看看 Calcite 是否能够真正地提供完善的查询引擎功能。通过下面的查询结果可以看出，Calcite 能够完美支持复杂的 SQL 语句。 0: jdbc:calcite:model=src/test/resources/mode SELECT * FROM DEPTS;+--------+-----------+| DEPTNO | NAME |+--------+-----------+| 10 | Sales || 20 | Marketing || 30 | Accounts |+--------+-----------+3 rows selected (0.01 seconds)0: jdbc:calcite:model=src/test/resources/mode SELECT d.name, COUNT(1) AS count FROM DEPTS d INNER JOIN EMPS e ON d.deptno = e.deptno GROUP BY d.name;+-----------+-------+| NAME | count |+-----------+-------+| Sales | 1 || Marketing | 2 |+-----------+-------+2 rows selected (0.179 seconds) 看到这里大家不禁会问，Calcite 是如何基于 CSV 格式的数据存储，来提供完善的 SQL 查询能力呢？下面我们将结合 Calcite 源码，针对一些典型的 SQL 查询语句，初步学习下 Calcite 内部的实现原理。 (adsbygoogle = window.adsbygoogle || []).push({}); 赞助商 Calcite 元数据定义 在 Caclite 集成 CSV 示例中，我们主要关注三个部分：一是 Calcite 元数据的定义，二是优化规则的管理，三是最优计划的执行。这三个部分是 Calcite 执行流程的核心，元数据主要用于对 SqlNode 语法树进行校验，并为 CBO 优化中代价的计算提供统计信息。优化规则被 Calcite 优化器使用，用来对逻辑计划进行改写，并生成最优的执行计划。最终，执行器会基于最优的执行计划，在不同的存储引擎上进行执行。 我们先关注 Calcite 元数据的定义，元数据的定义是通过 !connect jdbc:calcite:model=src/test/resources/model.json admin admin 命令，指定 model 属性对应的配置文件 model.json 来注册元数据，具体内容如下： version:1.0, // 默认 schema defaultSchema:SALES, schemas:[ // schema 名称 name:SALES, // type 定义数据模型的类型，custom 表示自定义数据模型 type:custom, // schema 工厂类 factory:org.apache.calcite.adapter.csv.CsvSchemaFactory, operand: directory:sales ] CsvSchemaFactory 类负责创建 Calcite 元数据 CsvSchema，operand 用于指定参数，directory 代表 CSV 文件的路径，flavor 则代表 Calcite 的表类型，包含了 SCANNABLE、FILTERABLE 和 TRANSLATABLE 三种类型。 /** * Factory that creates a @link CsvSchema. * * pAllows a custom schema to be included in a codeimodel/i.json/code * file. */@SuppressWarnings(UnusedDeclaration)public class CsvSchemaFactory implements SchemaFactory /** * Public singleton, per factory contract. */ public static final CsvSchemaFactory INSTANCE = new CsvSchemaFactory(); private CsvSchemaFactory() @Override public Schema create(SchemaPlus parentSchema, String name, MapString, Object operand) final String directory = (String) operand.get(directory); final File base = (File) operand.get(ModelHandler.ExtraOperand.BASE_DIRECTORY.camelName); File directoryFile = new File(directory); if (base != null !directoryFile.isAbsolute()) directoryFile = new File(base, directory); String flavorName = (String) operand.get(flavor); CsvTable.Flavor flavor; if (flavorName == null) flavor = CsvTable.Flavor.SCANNABLE; else flavor = CsvTable.Flavor.valueOf(flavorName.toUpperCase(Locale.ROOT)); return new CsvSchema(directoryFile, flavor); CsvSchema 类的定义如下，它继承了 AbstractSchema 并实现了 getTableMap 方法，getTableMap 方法根据 flavor 参数创建不同类型的表。 /** * Schema mapped onto a directory of CSV files. Each table in the schema * is a CSV file in that directory. */public class CsvSchema extends AbstractSchema private final File directoryFile; private final CsvTable.Flavor flavor; private MapString, Table tableMap; /** * Creates a CSV schema. * * @param directoryFile Directory that holds @code .csv files * @param flavor Whether to instantiate flavor tables that undergo * query optimization */ public CsvSchema(File directoryFile, CsvTable.Flavor flavor) super(); this.directoryFile = directoryFile; this.flavor = flavor; /** * Looks for a suffix on a string and returns * either the string with the suffix removed * or the original string. */ private static String trim(String s, String suffix) String trimmed = trimOrNull(s, suffix); return trimmed != null ? trimmed : s; /** * Looks for a suffix on a string and returns * either the string with the suffix removed * or null. */ private static String trimOrNull(String s, String suffix) return s.endsWith(suffix) ? s.substring(0, s.length() - suffix.length()) : null; @Override protected MapString, Table getTableMap() if (tableMap == null) tableMap = createTableMap(); return tableMap; private MapString, Table createTableMap() // Look for files in the directory ending in .csv, .csv.gz, .json, // .json.gz. final Source baseSource = Sources.of(directoryFile); File[] files = directoryFile.listFiles((dir, name) - final String nameSansGz = trim(name, .gz); return nameSansGz.endsWith(.csv) || nameSansGz.endsWith(.json); ); if (files == null) System.out.println(directory + directoryFile + not found); files = new File[0]; // Build a map from table name to table; each file becomes a table. final ImmutableMap.BuilderString, Table builder = ImmutableMap.builder(); for (File file : files) Source source = Sources.of(file); Source sourceSansGz = source.trim(.gz); final Source sourceSansJson = sourceSansGz.trimOrNull(.json); if (sourceSansJson != null) final Table table = new JsonScannableTable(source); builder.put(sourceSansJson.relative(baseSource).path(), table); final Source sourceSansCsv = sourceSansGz.trimOrNull(.csv); if (sourceSansCsv != null) final Table table = createTable(source); builder.put(sourceSansCsv.relative(baseSource).path(), table); return builder.build(); /** * Creates different sub-type of table based on the flavor attribute. */ private Table createTable(Source source) switch (flavor) case TRANSLATABLE: return new CsvTranslatableTable(source, null); case SCANNABLE: return new CsvScannableTable(source, null); case FILTERABLE: return new CsvFilterableTable(source, null); default: throw new AssertionError(Unknown flavor + this.flavor); CsvSchema#createTable 方法中定义了三种表类型，让我们来看下这三种类型的区别： CsvTranslatableTable：实现了 QueryableTable 和 TranslatableTable 接口，QueryableTable 接口会实现 asQueryable 方法，将表转化成 Queryable 实现类，从而具有 groupBy、count 等查询能力，具体可以参考 ExtendedQueryable。TranslatableTable 则用于将 RelOptTable 对象转换为 RelNode，此案例中为 CsvTableScan，后续可以使用优化规则对 CsvTableScan 进行变换从而实现下推等优化； CsvScannableTable：实现了 ScannableTable 接口，用于扫描全部数据记录，Calcite 会调用 scan 获取 csv 文件中的全部数据； CsvFilterableTable：实现了 FilterableTable 接口，可以在扫描数据过程中，根据 scan 方法传入的 ListRexNode filters 参数进行数据过滤。 前面介绍 CsvSchemaFactory 和 CsvSchema 中的元数据初始化逻辑，会在 Calcite JDBC 创建 Connection 进行初始化，具体是调用 ModelHandler 解析 JSON 格式的配置文件，然后调用 CsvSchemaFactory 创建 CsvSchema。 public void visit(JsonCustomSchema jsonSchema) try final SchemaPlus parentSchema = currentMutableSchema(sub-schema); final SchemaFactory schemaFactory = AvaticaUtils.instantiatePlugin(SchemaFactory.class, jsonSchema.factory); final Schema schema = schemaFactory.create( parentSchema, jsonSchema.name, operandMap(jsonSchema, jsonSchema.operand)); final SchemaPlus schemaPlus = parentSchema.add(jsonSchema.name, schema); populateSchema(jsonSchema, schemaPlus); catch (Exception e) throw new RuntimeException(Error instantiating + jsonSchema, e); 初始化完成后，元数据对象的结构如下，注册了 metadata 和 SALES 两个 schema。 Calcite 优化规则管理 下面我们再来看看 Calcite 是如何管理优化规则的，在 CSV 示例中我们定义了 CsvProjectTableScanRule，用于匹配在 CsvTableScan 之上的 Project 投影，并将 Project 投影下推到 CsvTableScan 中。刚接触 Calcite 的朋友可能很难理解在 CsvTableScan 之上的 Project 是什么含义？我们通过一条 SQL 来进行理解，假设我们执行的 SQL 为 select name from EMPS（读者可以使用 CsvTest#testSelectSingleProjectGz 自行测试）。 // CsvTest@Testvoid testSelectSingleProjectGz() throws SQLException sql(smart, select name from EMPS).ok(); Caclite 首先会将 SQL 解析成 SqlNode 语法树，再通过语法校验、逻辑计划生成等阶段得到如下的逻辑计划树。LogicalProject 代表了逻辑投影，会查询 SQL 中指定的投影字段 name 对应的数据列。而 LogicalProject 想要获取投影字段对应的数据，需要向下调用 CsvTableScan，CsvTableScan 则会对 EMPS 表进行扫描获取数据行，逻辑计划树中 CsvTableScan 会获取数据行中的所有行、所有列，再将数据传递给上层的 LogicalProject 按投影列进行过滤。 细心的读者可能已经发现，为什么我们指定的 SQL 中只需要查询 name 列，而逻辑计划树中的 CsvTableScan 却要扫描所有列？为了避免 CsvTableScan 扫描无用的数据列，CSV 案例中定义了 CsvProjectTableScanRule 优化规则，主要用于将 Projection 下推到 TableScan 中，在数据扫描阶段就过滤无用的数据列，从而达到减少数据传输，降低计算时占用内存的目的。可以看到，经过 CsvProjectTableScanRule 优化后，逻辑计划树中只有一个 CsvTableScan 算子，内部包含了 table 和 fields，可以在数据扫描时过滤投影列（和 Projection 下推类似，我们也可以将 Filter 下推到 TableScan 中，减少加载到内存的数据行，Filter 下推读者可以自行尝试下）。 下面展示的是 CsvProjectTableScanRule 规则的实现，它继承了 RelRule 抽象类，CsvProjectTableScanRule 构造方法会将 config 传给父类进行初始化。config 类则是由 CsvProjectTableScanRule 类的内部 Config 接口，通过 @Value.Immutable 注解动态生成的实现类，其核心逻辑定义了优化规则需要匹配的逻辑计划树结构，此处的结构为 LogicalProject 节点，下方包含一个 CsvTableScan 输入节点，而 CsvTableScan 节点则没有输入节点。 /** * Planner rule that projects from a @link CsvTableScan scan just the columns * needed to satisfy a projection. If the projections expressions are trivial, * the projection is removed. * * @see CsvRules#PROJECT_SCAN */@Value.Enclosingpublic class CsvProjectTableScanRule extends RelRuleCsvProjectTableScanRule.Config /** * Creates a CsvProjectTableScanRule. */ protected CsvProjectTableScanRule(Config config) super(config); // 匹配 config 中定义的规则后，对逻辑计划树进行 transformTo 变换 @Override public void onMatch(RelOptRuleCall call) final LogicalProject project = call.rel(0); final CsvTableScan scan = call.rel(1); int[] fields = getProjectFields(project.getProjects()); if (fields == null) // Project contains expressions more complex than just field references. return; call.transformTo(new CsvTableScan(scan.getCluster(), scan.getTable(), scan.csvTable, fields)); private static int[] getProjectFields(ListRexNode exps) final int[] fields = new int[exps.size()]; for (int i = 0; i exps.size(); i++) final RexNode exp = exps.get(i); if (exp instanceof RexInputRef) fields[i] = ((RexInputRef) exp).getIndex(); else return null; // not a simple projection return fields; /** * Rule configuration. */ @Value.Immutable(singleton = false) public interface Config extends RelRule.Config Config DEFAULT = ImmutableCsvProjectTableScanRule.Config.builder() .withOperandSupplier(b0 - b0.operand(LogicalProject.class).oneInput(b1 - b1.operand(CsvTableScan.class).noInputs())).build(); @Override default CsvProjectTableScanRule toRule() return new CsvProjectTableScanRule(this); CsvProjectTableScanRule 继承了 RelRule 抽象类，而 RelRule 抽象类又继承 RelOptRule 抽象类，继承关系如下图所示。Calcite 优化器会调用 matches 方法判断当前优化规则是否匹配，匹配则继续调用 onMatch 方法对逻辑计划树进行变换，通过代码可以看出，CSV 示例中会将投影列 fields 下推到 CsvTableScan 中。 RelOptRule 继承关系 了解了 CsvProjectTableScanRule 大致的优化逻辑后，我们再来看下 Calcite 是如何注册和执行优化规则的。在 CsvTableScan 中定义了一个 register 方法，用于注册和当前关系代数节点相关的优化规则，CsvRules.PROJECT_SCAN 是调用 toRule 方法得到的优化规则对象。入参 RelOptPlanner 是 Calcite 中的优化器对象，目前提供了 HepPlanner 和 VolcanoPlanner 两种优化器，HepPlanner 采用 RBO 模型，基于已知的优化规则进行优化，而 VolcanoPlanner 则采用 CBO 模型，基于执行计划的代价进行选择。本文由于篇幅原因，先从优化器的外部接口了解其功能，暂时不做过多的探究，在后续的文章中，我们将深入学习这两种优化器的内部实现。 // CsvTableScan@Overridepublic void register(RelOptPlanner planner) planner.addRule(CsvRules.PROJECT_SCAN);/** * Planner rules relating to the CSV adapter. */public abstract class CsvRules private CsvRules() /** * Rule that matches a @link org.apache.calcite.rel.core.Project on * a @link CsvTableScan and pushes down projects if possible. */ public static final CsvProjectTableScanRule PROJECT_SCAN = CsvProjectTableScanRule.Config.DEFAULT.toRule(); 在注册完成优化规则后，Calcite JDBC 程序会在 SQL 执行阶段，封装多个 Program 实现类，Program 接口提供了如下的 run 方法，用于对关系代数表达式 RelNode 进行变换。 /** * Program that transforms a relational expression into another relational * expression. * * pA planner is a sequence of programs, each of which is sometimes called * a phase. * The most typical program is an invocation of the volcano planner with a * particular @link org.apache.calcite.tools.RuleSet./p */public interface Program RelNode run(RelOptPlanner planner, RelNode rel, RelTraitSet requiredOutputTraits, ListRelOptMaterialization materializations, ListRelOptLattice lattices); 如下图所示，Calcite Prepare 类中默认注册了 5 个 Program，内部封装了 HepPlanner 和 VolcanoPlanner 两种优化器，以及相关子查询消除等改写逻辑，可以对查询 SQL 进行比较全面的查询优化。 调用 program.run 方法会触发 SequenceProgram 内部逻辑， 依次触发 programs 对象的 run 方法。我们以第一个 Program 为例，内部会调用 HepPlanner 优化器的 setRoot 和 findBestExp 方法，setRoot 方法用于将关系代数设置到 planner 中，而 findBestExp 方法则会调用优化器的逻辑，根据优化规则或者代价选择最优的执行计划。 // 将关系代数设置到 planner 中, findBestExp 获取最有执行计划planner.setRoot(rel);planner.findBestExp(); 优化完成后我们就得到了最优的执行计划，使用 RelOptUtil.toString(root.rel) 查看其结果为 CsvTableScan(table=[[SALES, EMPS]], fields=[[1]])，下一步我们将看看最优执行计划是如何执行得到结果的。 Calcite 最优计划执行 Calcite JDBC 执行的入口是在 Prepare#implement 方法，入参是最优的执行计划 RelRoot 类（该类对 RelNode 进行了一些包装，可以用于记录排序字段，以及投影别名处理，具体可以参考 RelRoot 类的 JavaDoc 文档），返回的结果是 PreparedResult 接口的实现类。 /** * Implements a physical query plan. * * @param root Root of the relational expression tree * @return an executable plan */protected abstract PreparedResult implement(RelRoot root); PreparedResult 接口具有如下公有方法，getFieldOrigins 方法用于返回每一个投影列的完整属性 database, schema, table, column，getParameterRowType 方法则返回行记录的类型信息，getBindable 是整个接口的核心，它返回一个可执行的类，调用 bind 方法可以获取一个 Enumerable 迭代器，从而遍历获取最终的结果。 /** * Result of a call to @link Prepare#prepareSql. */public interface PreparedResult /** * Returns the code generated by preparation. */ String getCode(); /** * Returns whether this result is for a DML statement, in which case the * result set is one row with one column containing the number of rows * affected. */ boolean isDml(); /** * Returns the table modification operation corresponding to this * statement if it is a table modification statement; otherwise null. */ TableModify.@Nullable Operation getTableModOp(); /** * Returns a list describing, for each result field, the origin of the * field as a 4-element list of (database, schema, table, column). */ List? extends @Nullable ListString getFieldOrigins(); /** * Returns a record type whose fields are the parameters of this statement. */ RelDataType getParameterRowType(); /** * Executes the prepared result. * * @param cursorFactory How to map values into a cursor * @return producer of rows resulting from execution */ Bindable getBindable(Meta.CursorFactory cursorFactory); 在生成 Bindable 接口的实现类时，会调用 EnumerableInterpretable 生成代码逻辑，并使用 Janino 将代码编译为 Java 类并创建对象。 最终返回的 PreparedResultImpl 实现类如下，getBindable 接口会返回 Janino 动态生成的 Java 对象，而 Bindable 接口调用 bind 方法即可返回 Enumerable 迭代器，Calcite JDBC 从迭代器中遍历出记录，再通过 JDBC 接口封装返回给应用程序。 return new PreparedResultImpl(resultType, requireNonNull(parameterRowType, parameterRowType), requireNonNull(fieldOrigins, fieldOrigins), root.collation.getFieldCollations().isEmpty() ? ImmutableList.of() : ImmutableList.of(root.collation), root.rel, mapTableModOp(isDml, root.kind), isDml) @Override public String getCode() throw new UnsupportedOperationException(); @Override public Bindable getBindable(Meta.CursorFactory cursorFactory) return bindable; @Override public Type getElementType() return ((Typed) bindable).getElementType(); ;/** * Statement that can be bound to a @link DataContext and then executed. * * @param T Element type of the resulting enumerable */public interface BindableT /** * Executes this statement and returns an enumerable which will yield rows. * The @code environment parameter provides the values in the root of the * environment (usually schemas). * * @param dataContext Environment that provides tables * @return Enumerable over rows */ EnumerableT bind(DataContext dataContext); 以上就是 Calcite 最优计划执行的大致流程，后面的文章中我将深入分析 Calcite 执行器代码生成的逻辑和 Enumerable 接口的具体实现。此外，我还将和大家一起探究 Presto、Drill、PolarDB-X 等框架的执行引擎逻辑，看看生产级别的执行引擎是如何高效实现的。 写在最后 笔者因为工作原因接触到 Calcite，前期学习过程中，深感 Calcite 学习资料之匮乏，因此创建了 Calcite 从入门到精通知识星球，希望能够将学习过程中的资料和经验沉淀下来，为更多想要学习 Calcite 的朋友提供一些帮助。 欢迎关注 欢迎关注「端小强的博客」微信公众号，会不定期分享日常学习和工作经验，欢迎大家关注交流。","tags":["Calcite"],"categories":["Calcite"]},{"title":"Apache Calcite 学习资料整理","path":"/blog/apache-calcite-learning-materials.html","content":"前言 Apache Calcite 是一个优秀的动态数据管理框架，提供了如：SQL 解析、SQL 校验、SQL 查询优化、SQL 生成以及数据连接查询等典型数据库管理功能。本文主要记录了学习 Calcite 过程中整理收集的资料，希望能够由点及面，逐步深入理解 Calcite 及数据库优化的精髓，也希望能够帮助其他学习 Calcite 的同学。 个人整理 Calcite 知识图谱：思维导图（密码: kiqs） Calcite 源码学习第 1 弹：Apache Calcite 快速入门指南 Calcite 源码学习第 2 弹：Apache Calcite SQL Parser 原理剖析 Calcite 源码学习第 3 弹：Apache Calcite System Catalog 实现探究 Calcite 源码学习第 4 弹：深度探究 Apache Calcite SQL 校验器实现原理 Calcite 源码学习第 5 弹：TODO Calcite 源码学习第 6 弹：深入理解 Apache Calcite HepPlanner 优化器 Calcite 源码学习第 7 弹：深入理解 Apache Calcite ValcanoPlanner 优化器 Calcite 源码学习第 8 弹：CBO 优化的基石——Apache Calcite 统计信息和代价模型详解 Calcite 源码学习第 8 弹：TODO Calcite 源码学习第 9 弹：TODO Calcite 源码学习第 10 弹：Apache Calcite Catalog 拾遗之 UDF 函数实现和扩展 Calcite 源码学习第 11 弹：Calcite UDF 实战之 ShardingSphere 联邦查询适配 MySQL BIT_COUNT Calcite 源码学习第 12 弹：TODO Calcite 项目实战第 1 弹：Apache Calcite 在 MyCat2 中的实践探究 Calcite 项目实战第 2 弹：ShardingSphere 联邦查询 GROUPING 聚合结果问题分析 官方文档 Calcite 官方文档 Calcite 官方文档中文版 Calcite 社区 Calcite Jira Calcite StackOverflow Calcite Deep Wiki 网络资料 基础介绍 Apache Calcite：Hadoop 中新型大数据查询引擎 Apache Calcite中的基本概念 Calcite 概念和架构 Apache Calcite: A Foundational Framework for Optimized Query Processing Over Heterogeneous Data Sources Apache Calcite 处理流程详解（一） Apache Calcite 框架 50 倍性能优化实践 tsangpo 总结 Calcite 学习系列 静水流深总结 Calcite 学习系列 优化器 Apache Calcite 优化器详解（二） Cascades Optimizer The Volcano/Cascades Optimizer Extensible Query Optimizers in Practice 执行器 Calcite分析——Implement Calcite分析——Linq4j TiDB 社区分享火山模型 SQL 优化之火山模型 写在最后 笔者因为工作原因接触到 Calcite，前期学习过程中，深感 Calcite 学习资料之匮乏，因此创建了 Calcite 从入门到精通知识星球，希望能够将学习过程中的资料和经验沉淀下来，为更多想要学习 Calcite 的朋友提供一些帮助。 欢迎关注 欢迎关注「端小强的博客」微信公众号，会不定期分享日常学习和工作经验，欢迎大家关注交流。","tags":["Query Optimization","Calcite"],"categories":["Calcite"]},{"title":"基于 SQLLine 快速体验 ShardingSphere JDBC","path":"/blog/quickly-start-shardingsphere-jdbc-with-sqlline.html","content":"SQLLine 简介 TODO ShardingSphere JDBC 打包 TODO SQLLine ShardingSphere JDBC 实战 TODO 欢迎关注 欢迎关注「端小强的博客」微信公众号，会不定期分享日常学习和工作经验，欢迎大家关注交流。","tags":["ShardingSphere","SQLLine"],"categories":["ShardingSphere"]},{"title":"MySQL 学习资料整理","path":"/blog/mysql-learning-materials.html","content":"前言 MySQL 是当前最流行的开源数据库，国内外众多公司基本都使用 MySQL 来支撑核心业务，学习 MySQL 能够帮助我们更好地完成日常工作，提升 SQL 开发和相关问题排查的能力。本文旨在收集整理 MySQL 学习的相关文档资料，希望能够通过这些资料，系统地学习掌握 MySQL 技术。 官方文档 MySQL 8.0 Reference Manual Planet MySQL MySQL Blog Archive Worklog Tasks MySQL 经典书籍 MySQL技术内幕：InnoDB存储引擎（第2版） 事务处理概念与技术 高性能MySQL（第4版） 网络资料 数据库内核月报 欢迎关注 欢迎关注「端小强的博客」微信公众号，会不定期分享日常学习和工作经验，欢迎大家关注交流。","tags":["MySQL"],"categories":["MySQL"]},{"title":"ShardingSphere 5.3.1 新特性之动态数据脱敏","path":"/blog/shardingsphere-5.3.1-new-feature-dynamic-data-masking.html","content":"本文首发于 Apache ShardingSphere 微信公众号，欢迎关注公众号，后续将会有更多技术分享。 背景 随着《网络安全法》的颁布施行，对个人隐私数据的保护已经上升到法律层面。传统的应用系统普遍缺少对个人隐私数据的保护措施。数据脱敏，可以实现在不需要对生产数据库中的数据进行任何改变的情况下，依据用户的角色、职责和其他定义规则，对生产数据库返回的数据进行专门的屏蔽、加密、隐藏和审计，确保业务用户、外包用户、运维人员、兼职雇员、合作伙伴、数据分析师、研发、测试和顾问，都能够恰如其分地访问生产环境的敏感数据。 根据业界的相关经验，数据脱敏通常可以分为静态脱敏和动态脱敏。静态脱敏是指通过脱敏任务，针对数据库系统使用脱敏算法对敏感数据进行遮盖、加密或替换，并将脱敏后的数据保存到目标位置。动态脱敏相对于静态脱敏则更加灵活，可以针对每次查询的数据进行脱敏，脱敏数据不需要落地保存。ShardingSphere 5.3.1 版本提供了动态数据脱敏功能，用户通过 ShardingSphere 进行查询，ShardingSphere 会根据用户预先配置的脱敏规则，在返回结果前根据脱敏算法进行处理，再将脱敏后的数据返回给用户。 实现方案 脱敏与微内核 基于 ShardingSphere 微内核及可插拔架构，数据脱敏功能只需要实现结果归并引擎 SPI 就可以实现功能的灵活扩展。如下图所示，ShardingSphere 微内核中已经包含了 SQL 解析、SQL 路由、SQL 执行等核心逻辑，ShardingSphere 5.3.1 版本提供的动态脱敏功能，只是对其他功能查询结果的增强处理，因此只需要实现归并引擎中的 ResultDecoratorEngine 和 ResultDecorator 即可实现脱敏功能。 为了实现数据脱敏功能，本次在 features 模块中增加 shardingsphere-mask 模块，该模块包含了 shardingsphere-mask-api、shardingsphere-mask-core 和 shardingsphere-mask-distsql，各个模块的作用如下： shardingsphere-mask-api：脱敏 API 模块，包含了脱敏功能的 Rule 配置，以及脱敏算法 SPI 接口； shardingsphere-mask-core：脱敏功能的核心模块，包含了 Rule 的初始化逻辑，脱敏算法实现以及结果归并装饰器实现逻辑； shardingsphere-mask-distsql：脱敏功能的 DistSQL 模块，用户可以通过 DistSQL 动态地修改脱敏规则； 除了内核流程之外，脱敏功能在内核中的定位也同样值得我们关注。我们知道，ShardingSphere 强大的可插拔架构，允许我们任意地组合叠加内核功能，新增的脱敏功能也不例外，用户可以单独使用脱敏功能，也可以将脱敏和分片、加密等功能叠加使用，组成更加完善的分布式数据库解决方案。 下图展示了目前 ShardingSphere 内核功能的关系，总体上可以将内核功能划分为三个级别：基于列级别的功能、基于表级别的功能和基于数据源级别的功能。基于列级别的功能包括了数据加密和数据脱敏，主要针对列进行增强处理，基于表级别的功能则包含了数据分片和内置的单表管理，基于数据源级别的功能目前最为丰富，包括了 SphereEx 商业版提供的双写功能，ShardingSphere 开源版本提供的读写分离、高可用发现和影子库功能，这些都是围绕数据库流量治理相关的功能。ShardingSphere 会按照这三个层级关系依次进行处理，而在每一个层级内部，则是根据 Order 进行处理，例如：当用户同时使用加密和脱敏功能时，会优先处理加密逻辑，将存储在数据库中的密文数据进行解密，然后再使用脱敏算法进行数据脱敏。 脱敏 YAML API DistSQL 介绍完脱敏和微内核的关系后，我们再来了解下脱敏功能的 API 和 DistSQL，用户可以基于 YAML 配置或者使用 DistSQL 进行脱敏规则的配置。首先，我们来了解下 YAML API 的配置方式，用户只需要在 - !MASK 下的 tables 中配置脱敏列及脱敏算法即可，maskAlgorithm 定义的脱敏算法名称，需要与 maskAlgorithms 中的名称保持一致。 脱敏 YAML API 主要配置属性如下： maskAlgorithm：指定脱敏算法，动态脱敏根据脱敏算法进行数据处理； databaseName: mask_dbdataSources: ds_0: url: jdbc:mysql://127.0.0.1:3306/demo_ds_0?serverTimezone=UTCuseSSL=false username: root password: 123456 connectionTimeoutMilliseconds: 30000 idleTimeoutMilliseconds: 60000 maxLifetimeMilliseconds: 1800000 maxPoolSize: 50 minPoolSize: 1 ds_1: url: jdbc:mysql://127.0.0.1:3306/demo_ds_1?serverTimezone=UTCuseSSL=false username: root password: 123456 connectionTimeoutMilliseconds: 30000 idleTimeoutMilliseconds: 60000 maxLifetimeMilliseconds: 1800000 maxPoolSize: 50 minPoolSize: 1 # MASK rule 配置rules:- !MASK tables: t_user: columns: password: maskAlgorithm: md5_mask email: maskAlgorithm: mask_before_special_chars_mask telephone: maskAlgorithm: keep_first_n_last_m_mask maskAlgorithms: md5_mask: type: MD5 mask_before_special_chars_mask: type: MASK_BEFORE_SPECIAL_CHARS props: special-chars: @ replace-char: * keep_first_n_last_m_mask: type: KEEP_FIRST_N_LAST_M props: first-n: 3 last-m: 4 replace-char: * 此外，考虑到一些用户存在动态更新脱敏规则的需求，ShardingSphere 5.3.1 版本同时提供了脱敏 DistSQL 的支持，满足用户在运行阶段动态更新脱敏规则的需要，脱敏 DistSQL 语法如下，包含了创建、修改、删除和查看脱敏规则等常用的 DistSQL 语句。 -- 创建脱敏规则CREATE MASK RULE t_user ( COLUMNS( (NAME=password, TYPE(NAME=MD5)), (NAME=email, TYPE(NAME=MASK_BEFORE_SPECIAL_CHARS, PROPERTIES(special-chars=@, replace-char=*))), (NAME=telephone, TYPE(NAME=KEEP_FIRST_N_LAST_M, PROPERTIES(first-n=3, last-m=4, replace-char=*))) ));-- 修改脱敏规则ALTER MASK RULE t_user ( COLUMNS( (NAME=password, TYPE(NAME=MD5, PROPERTIES(salt=123abc))), (NAME=email, TYPE(NAME=MASK_BEFORE_SPECIAL_CHARS, PROPERTIES(special-chars=@, replace-char=*))), (NAME=telephone, TYPE(NAME=TELEPHONE_RANDOM_REPLACE, PROPERTIES(network-numbers=123,180))) ));-- 删除脱敏规则DROP MASK RULE t_user;-- 查看脱敏规则SHOW MASK RULES FROM mask_db; 更多详细的 DistSQL 语法说明，请参考数据脱敏 DistSQL 文档。 内置脱敏算法 ShardingSphere 5.3.1 本次发布也包含了大量内置的脱敏算法，算法基于 MaskAlgorithm SPI 接口实现，用户可以根据自己的业务需求进行灵活扩展。 /** * Mask algorithm. * * @param I type of plain value * @param O type of mask value */public interface MaskAlgorithmI, O /** * Mask. * * @param plainValue plain value * @return mask value */ O mask(I plainValue); 内置的脱敏算法主要可以分为三类，哈希脱敏、遮盖脱敏和替换脱敏，具体算法清单如下： 分类 名称 说明 哈希脱敏 MD5 基于 MD5 的数据脱敏算法 遮盖脱敏 KEEP_FIRST_N_LAST_M 保留前 n 后 m 数据脱敏算法 KEEP_FROM_X_TO_Y 保留自 x 至 y 数据脱敏算法 MASK_FIRST_N_LAST_M 遮盖前 n 后 m 数据脱敏算法 MASK_FROM_X_TO_Y 遮盖自 x 至 y 数据脱敏算法 MASK_BEFORE_SPECIAL_CHARS 特殊字符前遮盖数据脱敏算法 MASK_AFTER_SPECIAL_CHARS 特殊字符后遮盖数据脱敏算法 替换脱敏 PERSONAL_IDENTITY_NUMBER_RANDOM_REPLACE 身份证号随机替换数据脱敏算法 MILITARY_IDENTITY_NUMBER_RANDOM_REPLACE 军官证随机替换数据脱敏算法 TELEPHONE_RANDOM_REPLACE ⼿机号随机替换数据脱敏算法 脱敏算法目前还在不断完善中，更多关于算法参数的说明，请参考脱敏算法文档，也欢迎大家积极参与贡献，一起完善脱敏算法。 (adsbygoogle = window.adsbygoogle || []).push({}); 赞助商 脱敏实战 在最后一个部分，我们通过一个实战来具体了解下数据脱敏功能。通常对于企业内部的敏感数据，我们会选择数据脱敏和数据加密配合使用，Database 层存储数据时采用数据加密进行保护，避免数据丢失造成安全问题。在数据查询阶段，则会根据规则进行数据解密和数据脱敏，避免敏感数据直接展示。因此，本文实战部分选择了数据脱敏和数据加密叠加使用的场景，通过 DistSQL 进行动态更新，向大家展示下数据脱敏功能的实际效果。 首先，我们下载 ShardingSphere Proxy 5.3.1 版本 ，并配置 server.yaml 进行空启动，然后使用 mysql -u root -h 127.0.0.1 -P 3307 -p -c -A 连接 Proxy，并执行 CREATE DATABASE mask_db; 创建脱敏逻辑数据库。 -- 创建脱敏逻辑数据库CREATE DATABASE mask_db;-- 切换到 mask_dbUSE mask_db; 创建完逻辑数据库后，我们使用 DistSQL 注册存储资源，并初始化脱敏和加密规则。 -- 注册存储资源REGISTER STORAGE UNIT ds_0 ( HOST=127.0.0.1, PORT=3306, DB=demo_ds_0, USER=root, PASSWORD=123456, PROPERTIES(maximumPoolSize=10)), ds_1 ( HOST=127.0.0.1, PORT=3306, DB=demo_ds_1, USER=root, PASSWORD=123456, PROPERTIES(maximumPoolSize=10));-- 创建脱敏规则CREATE MASK RULE t_user ( COLUMNS( (NAME=password, TYPE(NAME=MD5)), (NAME=email, TYPE(NAME=MASK_BEFORE_SPECIAL_CHARS, PROPERTIES(special-chars=@, replace-char=*))), (NAME=telephone, TYPE(NAME=KEEP_FIRST_N_LAST_M, PROPERTIES(first-n=3, last-m=4, replace-char=*))) ));-- 创建加密规则CREATE ENCRYPT RULE t_user ( COLUMNS( (NAME=user_name, CIPHER=user_name_cipher, ENCRYPT_ALGORITHM(TYPE(NAME=AES, PROPERTIES(aes-key-value=123456abc)))), (NAME=password, CIPHER =password_cipher, ENCRYPT_ALGORITHM(TYPE(NAME=AES, PROPERTIES(aes-key-value=123456abc)))), (NAME=email, CIPHER =email_cipher, ENCRYPT_ALGORITHM(TYPE(NAME=AES, PROPERTIES(aes-key-value=123456abc)))), (NAME=telephone, CIPHER =telephone_cipher, ENCRYPT_ALGORITHM(TYPE(NAME=AES, PROPERTIES(aes-key-value=123456abc)))) )); 脱敏规则和加密规则创建完成后，我们可以通过 DistSQL SHOW 语句查看脱敏和加密规则： -- 查看脱敏规则mysql SHOW MASK RULES FROM mask_db;+--------+-----------+---------------------------+-----------------------------------+| table | column | algorithm_type | algorithm_props |+--------+-----------+---------------------------+-----------------------------------+| t_user | password | MD5 | || t_user | email | MASK_BEFORE_SPECIAL_CHARS | replace-char=*,special-chars=@ || t_user | telephone | KEEP_FIRST_N_LAST_M | first-n=3,replace-char=*,last-m=4 |+--------+-----------+---------------------------+-----------------------------------+3 rows in set (0.01 sec)-- 查看加密规则mysql SHOW ENCRYPT RULES FROM mask_db;+--------+--------------+------------------+--------------+-----------------------+-------------------+----------------+-------------------------+---------------------+----------------------+-----------------+------------------+--------------------------+| table | logic_column | cipher_column | plain_column | assisted_query_column | like_query_column | encryptor_type | encryptor_props | assisted_query_type | assisted_query_props | like_query_type | like_query_props | query_with_cipher_column |+--------+--------------+------------------+--------------+-----------------------+-------------------+----------------+-------------------------+---------------------+----------------------+-----------------+------------------+--------------------------+| t_user | user_name | user_name_cipher | | | | AES | aes-key-value=123456abc | | | | | true || t_user | password | password_cipher | | | | AES | aes-key-value=123456abc | | | | | true || t_user | email | email_cipher | | | | AES | aes-key-value=123456abc | | | | | true || t_user | telephone | telephone_cipher | | | | AES | aes-key-value=123456abc | | | | | true |+--------+--------------+------------------+--------------+-----------------------+-------------------+----------------+-------------------------+---------------------+----------------------+-----------------+------------------+--------------------------+4 rows in set (0.01 sec) 创建完规则后，我们创建如下的 t_user 表并进行数据初始化： DROP TABLE IF EXISTS t_user;CREATE TABLE t_user (user_id INT PRIMARY KEY, user_name VARCHAR(50) NOT NULL, password VARCHAR(50) NOT NULL, email VARCHAR(50) NOT NULL, telephone CHAR(50) NOT NULL, creation_date DATE NOT NULL);INSERT INTO t_user(user_id, user_name, password, email, telephone, creation_date) values(10, zhangsan, 111111, zhangsan@gmail.com, 12345678900, 2017-08-08),(11, lisi, 222222, lisi@gmail.com, 12345678901, 2017-08-08),(12, wangwu, 333333, wangwu@gmail.com, 12345678902, 2017-08-08),(13, zhaoliu, 444444, zhaoliu@gmail.com, 12345678903, 2017-08-08),(14, zhuqi, 555555, zhuqi@gmail.com, 12345678904, 2017-08-08),(15, liba, 666666, liba@gmail.com, 12345678905, 2017-08-08),(16, wangjiu, 777777, wangjiu@gmail.com, 12345678906, 2017-08-08),(17, zhuda, 888888, zhuda@gmail.com, 12345678907, 2017-08-08),(18, suner, 999999, suner@gmail.com, 12345678908, 2017-08-08),(19, zhousan, 123456, zhousan@gmail.com, 12345678909, 2017-08-08),(20, tom, 234567, tom@gmail.com, 12345678910, 2017-08-08),(21, kobe, 345678, kobe@gmail.com, 12345678911, 2017-08-08),(22, jerry, 456789, jerry@gmail.com, 12345678912, 2017-08-08),(23, james, 567890, james@gmail.com, 12345678913, 2017-08-08),(24, wade, 012345, wade@gmail.com, 12345678914, 2017-08-08),(25, rose, 000000, rose@gmail.com, 12345678915, 2017-08-08),(26, bosh, 111222, bosh@gmail.com, 12345678916, 2017-08-08),(27, jack, 222333, jack@gmail.com, 12345678917, 2017-08-08),(28, jordan, 333444, jordan@gmail.com, 12345678918, 2017-08-08),(29, julie, 444555, julie@gmail.com, 12345678919, 2017-08-08); 完成了数据初始化后，我们先通过 mysql -u root -h 127.0.0.1 -P 3306 -p -c -A 直接 MySQL 数据库，查看底层数据库 t_user 表中存储的数据，可以看到 MySQL 中存储的是加密之后的数据，敏感数据在数据库存储层得到了有效的保护。 mysql SELECT * FROM t_user;+---------+--------------------------+--------------------------+----------------------------------------------+--------------------------+---------------+| user_id | user_name_cipher | password_cipher | email_cipher | telephone_cipher | creation_date |+---------+--------------------------+--------------------------+----------------------------------------------+--------------------------+---------------+| 10 | sVq8Lmm+j6bZE5EKSilJEQ== | aQol0b6th65d0aXe+zFPsQ== | WM0fHOH91JNWnHTkiqBdyNmzk4uJ7CCz4mB1va9Ya1M= | kLjLJIMnfyHT2nA+viaoaQ== | 2017-08-08 || 11 | fQ7IzBxKVuNHtUF6h6WSBg== | wuhmEKgdgrWQYt+Ev0hgGA== | svATu3uWv9KfiloWJeWx3A== | 0kDFxndQdzauFwL/wyCsNQ== | 2017-08-08 || 12 | AQRWSlufQPog/b64YRhu6Q== | x7A+2jq9B6DSOSFtSOibdA== | nHJv9e6NiClIuGHOjHLvCAq2ZLhWcqfQ8/EQnIqMx+g= | a/SzSJLapt5iBXvF2c9ycw== | 2017-08-08 || 13 | 5NqS4YvpT+mHBFqZOZ3QDA== | zi6b4xYRjjV+bBk2R4wB+w== | MLBZczLjriUXvg3aM5QPTxMJbLjNh8yeNrSNBek/VTw= | b6VVhG+F6ujG8IMUZJAIFg== | 2017-08-08 || 14 | qeIY9od3u1KwhjihzLQUTQ== | 51UmlLAC+tUvdOAj8CjWfQ== | JCmeNdPyrKO5BW5zvhAA+g== | f995xinpZdKMVU5J5/yv3w== | 2017-08-08 || 15 | VbNUtguwtpeGhHGnPJ3aXg== | +3/5CVbqoKhg3sqznKTFFQ== | T+X+e3Q3+ZNIXXmg/80uxg== | GETj+S6DrO042E7NuBXLBQ== | 2017-08-08 || 16 | U0/Ao/w1u7L5avR3fAH2Og== | jFfFMYxv02DjaFRuAoCDGw== | RNW/KRq5HeL2YTfAdXSyARMJbLjNh8yeNrSNBek/VTw= | +lbvjJwO7VO4HUKc0Mw0NA== | 2017-08-08 || 17 | zb1sgBigoMi7JPSoY4bAVw== | VFIjocgjujJCJc6waWXqJA== | 1vF/ET3nBxt7T7vVfAndZQ== | wFvs5BH6OikgveBeTEBwsQ== | 2017-08-08 || 18 | rJzNIrFEnx296kW+N1YmMw== | LaODSKGyR7vZ1IvmBOe9vA== | 5u4GIQkJsWRmnJHWaHNSjg== | uwqm2O1Lv2tNTraJX1ym7Q== | 2017-08-08 || 19 | qHwpQ9kteL8VX6iTUhNdbQ== | MyOShk4kjRnds7CZfU5NCw== | HmYCo7QBfJ2E0EvaGHBCOBMJbLjNh8yeNrSNBek/VTw= | YLNQuuUPMGA21nhKWPzzsg== | 2017-08-08 || 20 | qCCmvf7OWRxbVbtLb0az1g== | fzdTMkzpBvgNYmKSQAp8Fg== | gOoP4Mf0P4ISOJp6A4sRmg== | l4xa4HwOfs/jusoJon9Wzw== | 2017-08-08 || 21 | IYJ1COaRQ0gSjWMC/UAeMg== | 1uEDMeYh2jstbOf6kx/cqw== | tikMAFiQ37u2VgWqUT38Eg== | rGpr30UXfczXjCjdvPN+BA== | 2017-08-08 || 22 | 7wvZZ7NVHgk6m1vB/sTC1Q== | OirN3gvz9uBnrq88nfa1wQ== | T7K/Uz1O2m+3xvB0+c4nGQ== | 7+fCU+VbQZKgLJXZPTTegA== | 2017-08-08 || 23 | SbVQWl8JbnxflCfGJ7KZdA== | hWVVYdkdTUTgm08haeq+tw== | Uk3ju6GteCD1qEHns5ZhKA== | DpnV86FZefwBRmIAVBh2gg== | 2017-08-08 || 24 | fx7OfSAYqVpjNa7LoKhXvw== | N2W9ijAXNkBxhkvJiIwp0A== | lAAGItVLmb1H69++1MDrIA== | QrE62wAb8B+2cEPcs4Lm1Q== | 2017-08-08 || 25 | wH3/LdWShD9aCb8eCIm3Tg== | GDixtt6NzPOVv6H0dmov5g== | T1yfJSyVxumZUfkDnmUQxA== | iU+AsGczboCRfU+Zr7mcpw== | 2017-08-08 || 26 | GgJQTndbxyBZ2tECS8SmqQ== | gLgVFLFIyyKwdQCXaw78Ag== | O+JIn9XZ3yq6RnKElHuqlA== | kwYlbu9aF7ndvMTcj8QBSg== | 2017-08-08 || 27 | lv8w8g32kuTXNvSUUypOig== | 8i0YH2mn6kXSyvBjM5p+Yg== | gqRoJF5S66SvBalc2RCo1A== | 2ob/3UYqRsZA5VdScnaWxQ== | 2017-08-08 || 28 | P9YCbFvWCIhcS99KyKH2zA== | PRrI4z4FrWwLvcHPx9g4og== | y8q31Jj4PFSyZHiLVIxKEQq2ZLhWcqfQ8/EQnIqMx+g= | kDF2za26uOerlNYWYHRT2Q== | 2017-08-08 || 29 | 5wu9XvlJAVtjKijhxt6SQQ== | O4pgkLgz34N+C4bIUOQVnA== | UH7ihg16J61Np/EYMQnXIA== | z2hbJQD4dRkVVITNxAac5Q== | 2017-08-08 |+---------+--------------------------+--------------------------+----------------------------------------------+--------------------------+---------------+20 rows in set (0.00 sec) 确认完数据加密的效果后，我们再对数据脱敏功能进行一个简单的测试，下面的测试 CASE 包含了简单的 SELECT 查询、关联查询、子查询等用户日常操作的语句，可以看到 password 字段使用 MD5 哈希脱敏，email 字段使用了 MASK_BEFORE_SPECIAL_CHARS 遮盖脱敏，telephone 字段则使用了 KEEP_FIRST_N_LAST_M 遮盖脱敏。 -- 简单 SELECT 查询mysql SELECT * FROM t_user WHERE user_id = 10;+---------+-----------+----------------------------------+--------------------+-------------+---------------+| user_id | user_name | password | email | telephone | creation_date |+---------+-----------+----------------------------------+--------------------+-------------+---------------+| 10 | zhangsan | 96e79218965eb72c92a549dd5a330112 | ********@gmail.com | 123****8900 | 2017-08-08 |+---------+-----------+----------------------------------+--------------------+-------------+---------------+1 row in set (0.01 sec)-- 关联查询mysql SELECT u1.* FROM t_user u1 INNER JOIN t_user u2 ON u1.user_id = u2.user_id WHERE u1.user_id = 10;+---------+-----------+----------------------------------+--------------------+-------------+---------------+| user_id | user_name | password | email | telephone | creation_date |+---------+-----------+----------------------------------+--------------------+-------------+---------------+| 10 | zhangsan | 96e79218965eb72c92a549dd5a330112 | ********@gmail.com | 123****8900 | 2017-08-08 |+---------+-----------+----------------------------------+--------------------+-------------+---------------+1 row in set (0.05 sec)-- 子查询mysql SELECT * FROM (SELECT * FROM t_user) temp WHERE temp.user_id = 10;+---------+-----------+----------------------------------+--------------------+-------------+---------------+| user_id | user_name | password | email | telephone | creation_date |+---------+-----------+----------------------------------+--------------------+-------------+---------------+| 10 | zhangsan | 96e79218965eb72c92a549dd5a330112 | ********@gmail.com | 123****8900 | 2017-08-08 |+---------+-----------+----------------------------------+--------------------+-------------+---------------+1 row in set (0.03 sec)-- 子查询包含关联查询mysql SELECT * FROM (SELECT u1.* FROM t_user u1 INNER JOIN t_user u2 ON u1.user_id = u2.user_id) temp WHERE temp.user_id 15;+---------+-----------+----------------------------------+--------------------+-------------+---------------+| user_id | user_name | password | email | telephone | creation_date |+---------+-----------+----------------------------------+--------------------+-------------+---------------+| 10 | zhangsan | 96e79218965eb72c92a549dd5a330112 | ********@gmail.com | 123****8900 | 2017-08-08 || 11 | lisi | e3ceb5881a0a1fdaad01296d7554868d | ****@gmail.com | 123****8901 | 2017-08-08 || 12 | wangwu | 1a100d2c0dab19c4430e7d73762b3423 | ******@gmail.com | 123****8902 | 2017-08-08 || 13 | zhaoliu | 73882ab1fa529d7273da0db6b49cc4f3 | *******@gmail.com | 123****8903 | 2017-08-08 || 14 | zhuqi | 5b1b68a9abf4d2cd155c81a9225fd158 | *****@gmail.com | 123****8904 | 2017-08-08 |+---------+-----------+----------------------------------+--------------------+-------------+---------------+5 rows in set (0.03 sec) 我们使用 DistSQL 修改脱敏规则，将 password 字段使用的 MD5 哈希脱敏增加可选参数 salt，telephone 字段的脱敏算法修改为 TELEPHONE_RANDOM_REPLACE 手机号随机替换脱敏算法。 ALTER MASK RULE t_user ( COLUMNS( (NAME=password, TYPE(NAME=MD5, PROPERTIES(salt=123abc))), (NAME=email, TYPE(NAME=MASK_BEFORE_SPECIAL_CHARS, PROPERTIES(special-chars=@, replace-char=*))), (NAME=telephone, TYPE(NAME=TELEPHONE_RANDOM_REPLACE, PROPERTIES(network-numbers=123,180))) )); 修改完成后，再次进行查询测试，可以看到由于 salt 值的变化，password 字段 MD5 脱敏的结果发生了变化，而 telephone 字段由于使用了手机号随机替换脱敏算法，脱敏的结果也变为号段位之后随机生成。 mysql SELECT * FROM t_user WHERE user_id = 10;+---------+-----------+----------------------------------+--------------------+-------------+---------------+| user_id | user_name | password | email | telephone | creation_date |+---------+-----------+----------------------------------+--------------------+-------------+---------------+| 10 | zhangsan | 554555c0eaca7aeecada758122efd640 | ********@gmail.com | 12383015546 | 2017-08-08 |+---------+-----------+----------------------------------+--------------------+-------------+---------------+1 row in set (0.01 sec) 最后，我们执行 DROP MASK RULE t_user; 语句将脱敏规则删除，此时再进行查询可以返回原始明文结果。 mysql SELECT * FROM t_user WHERE user_id = 10;+---------+-----------+----------+--------------------+-------------+---------------+| user_id | user_name | password | email | telephone | creation_date |+---------+-----------+----------+--------------------+-------------+---------------+| 10 | zhangsan | 111111 | zhangsan@gmail.com | 12345678900 | 2017-08-08 |+---------+-----------+----------+--------------------+-------------+---------------+1 row in set (0.00 sec) 结语 Apache ShardingSphere 5.3.1 新增的动态数据脱敏功能，是对 ShardingSphere 数据安全方案的进一步补充和完善，未来数据脱敏将会尝试和用户权限、SQL 审计等功能进行结合，根据企业内部的角色划分，进行不同维度的数据脱敏处理。欢迎社区的同学积极参与进来，共同提升 Apache ShardingSphere 的脱敏功能，为社区提供更好的使用体验。 欢迎关注 欢迎关注「端小强的博客」微信公众号，会不定期分享日常学习和工作经验，欢迎大家关注交流。","tags":["ShardingSphere"],"categories":["ShardingSphere"]},{"title":"MySQL 5.7.40 安装及初始化配置指南","path":"/blog/mysql-5.7.40-install-and-initial-config-guide.html","content":"前言 MySQL 是当前主流的开源关系型数据库，学习 MySQL 能够帮助我们更好地理解关系型数据库的实现原理，在日常工作实践中也能更加从容地面对各种数据库层面的问题。本文是 MySQL 系列的第一篇，主要记录了 MySQL 5.7.40 安装及初始化配置的步骤，MySQL 安装环境为 CentOS 7，可以参考 CentOS 开发环境搭建笔记在 Virtual Box 虚拟机上搭建 CentOS 7 环境。 MySQL 下载 首先，需要从 MySQL 官网下载 MySQL 5.7.40 的通用二进制安装包，具体如下图所示，我们选择 64 位安装包： 选择通用二进制安装包，主要是考虑到这种安装方式较为简单，并且在不同的 Linux 发行版上具有很好的通用性，此外还可以灵活地指定安装路径，在一台机器上安装多个 MySQL 实例。 下载完成后，我们使用 scp 命令将二进制安装包拷贝到服务器的 /usr/local 目录下。 scp mysql-5.7.40-linux-glibc2.12-x86_64.tar.gz root@192.168.56.101:/usr/local 使用 ll 查看上传结果： -rw-r--r--. 1 root root 678001736 Aug 10 14:06 mysql-5.7.40-linux-glibc2.12-x86_64.tar.gz 上传完成后使用 md5sum 校验 md5 值，避免下载的二进制包被篡改。 md5sum mysql-5.7.40-linux-glibc2.12-x86_64.tar.gz# ce0ef7b9712597f44f4ce9b9d7414a24 mysql-5.7.40-linux-glibc2.12-x86_64.tar.gz 然后解压二进制安装包： tar zxvf mysql-5.7.40-linux-glibc2.12-x86_64.tar.gz 解压后 MySQL 安装包的目录结构如下： 目录 内容 bin mysqld 服务端，客户端以及工具程序 docs Info 格式的 MySQL 手册 man Unix 手册 include 包含（头文件）文件 lib 库 share 错误消息、字典和用于数据库安装的 SQL support-files 其他支持文件 (adsbygoogle = window.adsbygoogle || []).push({}); 赞助商 MySQL 安装 参考 MySQL 官方文档 Installing MySQL on Unix/Linux Using Generic Binaries 进行安装，安装之前需要先确保系统上没有安装过 MySQL，并且需要删除 /etc/my.cnf 和 /etc/mysql 目录中的配置。可以使用如下命令检查是否已安装 MySQL： # yum 命令yum list installed mysql*# rpm 命令rpm -qa | grep mysql* 如果已安装则通过如下命令进行卸载： # yum 命令yum -y remove mysql*# rpm 命令rpm -e mysql 删除配置： rm -rf /etc/my.cnfrm -rf /etc/mysql 此外，MySQL 依赖 libaio、libnuma 和 ncurses 库，需要在安装 MySQL 前进行安装： # search for infoyum search libaioyum search numayum search ncurses# install libraryyum install libaio.x86_64 -yyum install numactl.x86_64 -yyum install ncurses-libs.x86_64 -y 由于 MySQL 5.7.19 开始，通用二进制安装包的 tar 包格式变为了 EL6，因此 MySQL 客户端 bin/mysql 需要 libtinfo.so.5 库，需要在 64 位系统上创建一个链接来解决： ln -sf /usr/lib64/libncurses.so.5.9 /lib64/libtinfo.so.5 安装 MySQL 的操作步骤如下： # 添加 mysql 用户组$ groupadd mysql# 添加 mysql 用户，并指定用户组为 mysql$ useradd -r -g mysql -s /bin/false mysql$ cd /usr/local# 解压 mysql 安装包，前文已经操作$ tar zxvf /path/to/mysql-VERSION-OS.tar.gz# 创建链接 ln -s /usr/local/mysql-5.7.40-linux-glibc2.12-x86_64 mysql$ ln -s full-path-to-mysql-VERSION-OS mysql$ cd mysql$ mkdir mysql-files# 将 mysql-files 拥有者修改为 mysql:mysql（具体参考：https://www.runoob.com/linux/linux-comm-chown.html）$ chown mysql:mysql mysql-files# 赋予用户 mysql-files 执行权限（具体参考：https://www.runoob.com/linux/linux-comm-chmod.html）$ chmod 750 mysql-files# 初始化 mysql，该步骤会生成临时密码$ bin/mysqld --initialize --user=mysql# 可能会出现 Could not find OpenSSL on the system，需要安装 OpenSSL# yum install openssl-devel -y$ bin/mysql_ssl_rsa_setup# 使用 mysqld_safe 守护进程启动 mysql$ bin/mysqld_safe --user=mysql # Next command is optional$ cp support-files/mysql.server /etc/init.d/mysql.server 以上步骤执行完成后，需要检查如下几点确保 MySQL 已经安装成功： 生成了 data 目录，data 目录下 ib* 等数据文件； 查看安装过程中输出的日志，默认在 data 目录下的 $HOSTNAME.err 文件中，日志中不能包含 ERROR 信息； 通过 MySQL 客户端登录成功； 下图展示了 MySQL 客户端登录成功的界面，我们需要通过 SET PASSWORD = 123456; 语句重置密码。 MySQL 支持本地 Socket 登录和远程 TCP/IP 登录，本地 Socket 登录使用 mysql -S /tmp/mysql.sock -uroot -p 登录，远程 TCP/IP 使用 mysql -h 127.0.0.1 -P 3306 -uroot -p 登录。 此外，为了简化操作，我们可以将 MySQL 二进制程序的路径添加到 PATH 中，建议将 MySQL 路径添加在最左侧，避免操作系统自带的 MySQL 程序影响。 # vim /etc/profileexport PATH=/usr/local/mysql/bin:$PATHsource /etc/profileecho $PATH# /usr/local/mysql/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin 关于 MySQL 的启动，前文我们使用了 bin/mysqld_safe --user=mysql 进行启动，mysqld_safe 是一个守护进程，它会监控 mysqld 进程的运行情况，当 mysqld 进程意外停止时，mysqld_safe 会重新启动 mysqld 进程。下图中，我们通过 kill -9 pid 手动杀死了 mysqld 进程，可以看到 mysqld_safe 立即检测到并对 mysqld 进行了重启。 除了使用 mysqld_safe 启动 MySQL 外，还可以通过 Linux 启动项实现开机自启动，在前面的安装步骤中，我们执行了 cp support-files/mysql.server /etc/init.d/mysql.server 命令，mysql.server 脚本中提供了 start、stop、restart、status 等命令。 可以通过 chkconfig 实现 mysql.server 自启动，执行如下脚本，然后重启服务器测试配置是否生效。 chkconfig --add mysql.serverchkconfig --list | grep mysql# mysql.server 0:关\t1:关\t2:开\t3:开\t4:开\t5:开\t6:关# chkconfig 使用说明：https://wangchujiang.com/linux-command/c/chkconfig.html MySQL 配置 前面我们使用 MySQL 默认配置启动了 MySQL 服务，但是想要能够在生产环境运行，我们需要对 MySQL 配置进行调整，以适应生产环境的要求。笔者从互联网上找到了国内 MySQL 领域权威姜承尧老师分享的生产 MySQL 配置，该配置可以适配 MySQL 5.7 版本，如果使用最新的 MySQL 8.0，可以参考文末链接进行配置。 [client]# mysql 免密码登录配置user = rootpassword = 123456socket = /tmp/mysql.sock[mysqld]######## basic settings ########server-id = 11port = 3306user = mysqlbind_address = 10.166.224.32autocommit = 0character_set_server = utf8mb4skip_name_resolve = 1max_connections = 800max_connect_errors = 1000datadir = /data/mysql_datatransaction_isolation = READ-COMMITTEDexplicit_defaults_for_timestamp = 1join_buffer_size = 134217728tmp_table_size = 67108864tmpdir = /tmpmax_allowed_packet = 16777216sql_mode = STRICT_TRANS_TABLES,NO_ENGINE_SUBSTITUTION,NO_ZERO_DATE,NO_ZERO_IN_DATE,ERROR_FOR_DIVISION_BY_ZERO,NO_AUTO_CREATE_USERinteractive_timeout = 1800wait_timeout = 1800read_buffer_size = 16777216read_rnd_buffer_size = 33554432sort_buffer_size = 33554432######## log settings ########log_error = error.logslow_query_log = 1slow_query_log_file = slow.loglog_queries_not_using_indexes = 1log_slow_admin_statements = 1log_slow_slave_statements = 1log_throttle_queries_not_using_indexes = 10expire_logs_days = 90long_query_time = 2min_examined_row_limit = 100######## replication settings ########master_info_repository = TABLErelay_log_info_repository = TABLElog_bin = bin.logsync_binlog = 1gtid_mode = onenforce_gtid_consistency = 1log_slave_updates = 1binlog_format = row relay_log = relay.logrelay_log_recovery = 1binlog_gtid_simple_recovery = 1slave_skip_errors = ddl_exist_errors######## innodb settings ########innodb_page_size = 8192innodb_buffer_pool_size = 6Ginnodb_buffer_pool_instances = 8innodb_buffer_pool_load_at_startup = 1innodb_buffer_pool_dump_at_shutdown = 1innodb_lru_scan_depth = 2000innodb_lock_wait_timeout = 5innodb_io_capacity = 4000innodb_io_capacity_max = 8000innodb_flush_method = O_DIRECTinnodb_file_format = Barracudainnodb_file_format_max = Barracudainnodb_log_group_home_dir = /redolog/innodb_undo_directory = /undolog/innodb_undo_logs = 128innodb_undo_tablespaces = 3innodb_flush_neighbors = 1innodb_log_file_size = 4Ginnodb_log_buffer_size = 16777216innodb_purge_threads = 4innodb_large_prefix = 1innodb_thread_concurrency = 64innodb_print_all_deadlocks = 1innodb_strict_mode = 1innodb_sort_buffer_size = 67108864 ######## semi sync replication settings ########plugin_dir = /usr/local/mysql/lib/pluginplugin_load = rpl_semi_sync_master=semisync_master.so;rpl_semi_sync_slave=semisync_slave.soloose_rpl_semi_sync_master_enabled = 1loose_rpl_semi_sync_slave_enabled = 1loose_rpl_semi_sync_master_timeout = 5000[mysqld-5.7]innodb_buffer_pool_dump_pct = 40innodb_page_cleaners = 4innodb_undo_log_truncate = 1innodb_max_undo_log_size = 2Ginnodb_purge_rseg_truncate_frequency = 128binlog_gtid_simple_recovery = 1log_timestamps = systemtransaction_write_set_extraction = MURMUR32show_compatibility_56 = on 在MySQL 中，通过配置 my.cnf（Linux、Mac） 或者 my.ini（Windows） 配置文件，实现配置的自定义。可以通过 mysql --help | grep my.cnf 查看MySQL 读取的配置文件路径，MySQL 按照参数替换的原则决定最终的配置。 [root@localhost mysql]# mysql --help | grep my.cnf order of preference, my.cnf, $MYSQL_TCP_PORT,/etc/my.cnf /etc/mysql/my.cnf /usr/local/mysql/etc/my.cnf ~/.my.cnf 参考文档 Installing MySQL on Unix/Linux Using Generic Binaries MySQL 配置文件 my.cnf / my.ini 逐行详解 欢迎关注 欢迎关注「端小强的博客」微信公众号，会不定期分享日常学习和工作经验，欢迎大家关注交流。","tags":["MySQL"],"categories":["MySQL"]},{"title":"ShardingSphere Proxy 集成测试代码调试实战","path":"/blog/shardingsphere-proxy-integration-test-debug-in-action.html","content":"前言 在使用 Java 远程调试技术定位系统表加载问题一文中，我们了解了 Java 远程调试技术，以及如何使用远程调试来定位打包后的程序问题。最近，笔者在开发 ShardingSphere 过程中，又遇到了 ShardingSphere 集成测试相关的问题。ShardingSphere 集成测试使用了基于容器技术的 TestContainer，通过 TestContainer 能够快速地部署集成测试所依赖的容器，使得不同环境的测试变得简单。但是使用容器也使得代码调试变得更加困难，下面笔者将结合一个实际的社区 issue，来介绍下如何在容器中使用远程调试。 问题分析 最近，社区反馈了一个集成测试相关的异常，通过异常堆栈可以初步判断是由于 SQL Federation 测试 CASE 引起的，但是具体问题原因，还需要通过本地代码调试分析（本文使用 0a53ca3 进行演示，后续版本如有变化，请留言联系作者更新）。 首先，我们尝试本地复现这个异常，通过如下的命令构建 ShardingSphere-Proxy 镜像。 # Build Image./mvnw -B clean install -am -pl test/e2e/sql -Pit.env.docker -DskipTests -Dspotless.apply.skip=true 然后修改 test - e2e - sql 模块下的 it-env.properties 配置文件，modes 用于指定运行模式，issue 中反馈的异常是在 Cluster 模式下运行的，adapters 用于指定接入端，databases 用于指定目标数据库。 #it.modes=Standalone,Clusterit.run.modes=Clusterit.run.additional.cases=false#it.scenarios=db,tbl,readwrite_splitting,encrypt,shadow,dbtbl_with_readwrite_splitting,dbtbl_with_readwrite_splitting_and_encrypt,empty_rulesit.scenarios=tbl# it.cluster.env.type=DOCKER,NATIVEit.cluster.env.type=DOCKER# it.cluster.adapters=jdbc,proxyit.cluster.adapters=proxy# it.cluster.databases=MySQL,PostgreSQLit.cluster.databases=PostgreSQL 然后执行 GeneralDQLIT 测试程序，得到如下结果，异常信息和 issue 中反馈的一致，本地调试时发现 SQL 是连接容器中的 ShardingSphere-Proxy 服务，无法进一步定位 ShardingSphere-Proxy 中的逻辑。 本地调试无法解决，我们只能再尝试使用远程调试的方式来定位问题，首先查看 test - e2e - fixture 模块中的 start.sh 脚本，由于该问题是 Proxy 启动后的运行时问题，因此无需将 suspend 设置为 y，此外被调试程序监听的端口为 3308。 JAVA_OPTS= -Djava.awt.headless=true -Xdebug -Xrunjdwp:transport=dt_socket,server=y,suspend=n,address=3308 由于 TestContainer 对外暴露的端口是随机生成的，调试程序无法获取到动态生成的端口，我们可以在 ShardingSphereProxyClusterContainer 中增加 log 输出 3308 映射的外部端口，并且在此位置打断点保证执行 SQL 逻辑前能够获取到外部端口。 @Overridepublic DataSource getTargetDataSource(final String serverLists) DataSource dataSource = targetDataSourceProvider.get(); if (null == dataSource) targetDataSourceProvider.set(StorageContainerUtils.generateDataSource(DataSourceEnvironment.getURL(databaseType, getHost(), getMappedPort(3307), config.getProxyDataSourceName()), ProxyContainerConstants.USERNAME, ProxyContainerConstants.PASSWORD)); log.info(Mapped port 3308: + getMappedPort(3308)); return targetDataSourceProvider.get(); 配置完成后，我们再次执行 GeneralDQLIT 测试程序，在断点处获取外部映射端口。然后再配置调试程序，使用 IDEA 下载新的 ShardingSphere 源码，并在 IDEA 中添加如下的远程调试配置，然后执行远程调试程序，此时可以发现集成测试的请求已经转到调试程序中，终于可以进一步分析问题的原因了。 (adsbygoogle = window.adsbygoogle || []).push({}); 赞助商 问题解决 我们以下面的查询作为示例，使用远程调试技术来定位下问题，从异常提示来看 content 别名的使用方式在 PostgreSQL 下有语法异常，我们将 SQL 拷贝到原生 PostgreSQL 数据库执行，出现了同样的异常提示。 test-case sql=SELECT user_id, CONCAT(SUM:, total, .) content FROM (SELECT user_id, SUM(order_id_sharding) AS total FROM t_order_federate_sharding GROUP BY user_id HAVING SUM(order_id_sharding) ?) AS temp db-types=MySQL,PostgreSQL scenario-types=tbl assertion parameters=1000:int //test-case[ERROR] assertExecute[proxy: tbl - PostgreSQL - Literal - SELECT user_id, CONCAT(SUM:, total, .) content FROM (SELECT user_id, SUM(order_id_sharding) AS total FROM t_order_federate_sharding GROUP BY user_id HAVING SUM(order_id_sharding) ?) AS temp](org.apache.shardingsphere.test.integration.engine.dql.GeneralDQLIT) Time elapsed: 0.27 s ERROR!org.postgresql.util.PSQLException: ERROR: syntax error at or near content 位置：44 at org.apache.shardingsphere.test.integration.engine.dql.GeneralDQLIT.assertExecuteForStatement(GeneralDQLIT.java:108) at org.apache.shardingsphere.test.integration.engine.dql.GeneralDQLIT.assertExecute(GeneralDQLIT.java:97) 查阅 PostgreSQL SELECT 语句文档，我们可以找到如下的一段描述，你可以使用 AS output_name 为输出列指定名称，你也可以省略 AS，但是要求输出的名称不能和 PostgreSQL 的关键字冲突，而这个示例中的 content 就是一个 PostgreSQL 关键字，文档中同时也提供了解决方案，使用 AS 或者用双引号修饰输出字段。 Just as in a table, every output column of a SELECT has a name. In a simple SELECT this name is just used to label the column for display, but when the SELECT is a sub-query of a larger query, the name is seen by the larger query as the column name of the virtual table produced by the sub-query. To specify the name to use for an output column, write AS output_name after the column’s expression. (You can omit AS, but only if the desired output name does not match any PostgreSQL keyword (see Appendix C). For protection against possible future keyword additions, it is recommended that you always either write AS or double-quote the output name.) If you do not specify a column name, a name is chosen automatically by PostgreSQL. If the column’s expression is a simple column reference then the chosen name is the same as that column’s name. In more complex cases a function or type name may be used, or the system may fall back on a generated name such as ?column?. 我们为 content 别名添加 AS 之后再次进行测试，这时出现了如下异常，难道是 SQL Federation 内部逻辑有问题？ 我们都知道 SQL Federation 是面向用户的逻辑 SQL 进行执行的，通过关系代数的等价变换，对逻辑执行计划进行优化，然后生成可执行的物理执行计划，而物理执行计划中最关键的就是算子下推，从底层的数据源上获取数据，因此我们来调试下这部分逻辑是否正常。 通过 Debug 发现下推执行逻辑并无异常，返回的结果符合预期，我们再来看下集成测试断言的逻辑，ShardingSphere 集成测试在执行查询语句时，会将 ShardingSphere-Proxy 执行的结果和 PostgreSQL 上执行的结果进行对比，判断元数据以及数据行是否相同。 protected final void assertResultSet(final ResultSet actualResultSet, final ResultSet expectedResultSet) throws SQLException assertMetaData(actualResultSet.getMetaData(), expectedResultSet.getMetaData()); assertRows(actualResultSet, expectedResultSet); 通过对比结果集，发现异常是由于结果集顺序引起的，因此需要为查询语句添加 ORDER BY temp.user_id，保证结果集顺序的稳定性。 修改完测试 SQL 后再次测试，集成测试能正常通过，结果如下。这个问题也提醒大家在编写 SQL 测试用例时，需要关注测试用例的有序性，默认的排序规则通常是不稳定的，需要通过显示地声明排序规则来避免意料之外的问题。 欢迎关注 欢迎关注「端小强的博客」微信公众号，会不定期分享日常学习和工作经验，欢迎大家关注交流。","tags":["ShardingSphere","Java","Remote Debugging","Docker"],"categories":["In Action"]},{"title":"使用 Java 远程调试技术定位系统表加载问题","path":"/blog/use-java-remote-debugging-to-locate-system-table-loading-bug.html","content":"背景 在最近的工作中，笔者负责开发了 ShardingSphere 系统表 功能，该功能会在 ShardingSphere 启动时模拟不同数据库的系统表，从而兼容各种数据库客户端，避免客户端查询系统表时出现报错。按照正常开发的流程，笔者对功能进行了较为全面的测试，然后将功能提测给测试同学。本以为一切会很顺序，但在功能测试的第一步就出现了问题——ShardingSphere 打包后启动起来，Zookeeper 中无法查询到系统表的元数据。**为什么直接通过 IDEA 启动测试正常，打包之后启动就无法加载系统表呢？**为了搞清问题的原因，笔者开始了后文中的一番调查。 问题排查 根据 打包后 ShardingSphere 无法加载系统表 这个现象，首先想到的就是打包后的程序是否丢失了系统表配置文件。为了排查丢失配置文件的可能性，笔者使用 JD-GUI 对打包后的 jar 包进行反编译，得到如下结果： 从结果可以看出，ShardingSphere 加载系统表所使用的配置文件都存在，并未出现配置文件丢失的情况。为了进一步定位打包程序的问题，我们需要使用远程调试技术，了解打包程序内部运行的逻辑。 (adsbygoogle = window.adsbygoogle || []).push({}); 赞助商 远程调试 Java 远程调试技术主要是基于 JDWP（Java Debug Wire Protocol） 协议，而 JDWP 协议是 Java 语言中用于调试程序和被调试程序之间进行通信的协议。调试程序和被调试程序可以位于同一台机器上，也可以位于不同的机器上。要使用 JDWP 进行远程调试，首先需要在被调试程序中配置如下参数： java -agentlib:jdwp=transport=dt_socket,server=y,suspend=n,address=8888 -jar xxx.jar java -agentlib:jdwp 用于开启远程调试功能，它会在 address 参数对应的端口上开启监听，等待调试程序连接。JDWP 中的具体参数含义，我们可以使用 java -agentlib:jdwp=help 进行查看，具体结果如下： Java Debugger JDWP Agent Library -------------------------------- (see http://java.sun.com/products/jpda for more information)jdwp usage: java -agentlib:jdwp=[help]|[option=value, ...]Option Name and Value Description Default--------------------- ----------- -------suspend=y|n wait on startup? ytransport=name transport spec noneaddress=listen/attach address transport spec server=y|n listen for debugger? nlaunch=command line run debugger on event noneonthrow=exception name debug on throw noneonuncaught=y|n debug on any uncaught? ntimeout=timeout value for listen/attach in milliseconds nmutf8=y|n output modified utf-8 nquiet=y|n control over terminal messages nObsolete Options----------------strict=y|nstdalloc=y|nExamples-------- - Using sockets connect to a debugger at a specific address: java -agentlib:jdwp=transport=dt_socket,address=localhost:8000 ... - Using sockets listen for a debugger to attach: java -agentlib:jdwp=transport=dt_socket,server=y,suspend=y ...Notes----- - A timeout value of 0 (the default) is no timeout.Warnings-------- - The older -Xrunjdwp interface can still be used, but will be removed in a future release, for example: java -Xdebug -Xrunjdwp:[help]|[option=value, ...] transport：表示调试程序和被调试程序间的通信协议，dt_socket 表示使用 socket 方式进行通信； server：表示是否为调试程序开启监听，y 表示开启，默认为 n 表示不开启； suspend：表示是否在被调试程序启动阶段等待调试程序连接，配置成 y 可以用来调试启动流程，如果不需要调试启动流程，设置为 n 即可； address：被调试程序监听的端口； 除了上面介绍的常用配置方式之外，还可以通过 java -Xdebug -Xrunjdwp:transport 开启远程调试，这种方式是 JDK 1.5 及之前版本开启远程调试的方式，在 JDK 1.5 之后的版本，官方都推荐使用 java -agentlib:jdwp 方式。 java -Xdebug -Xrunjdwp:transport=dt_socket,server=y,suspend=n,address=8888 -jar xxx.jar 在了解了 JDWP 远程调试的使用方式后，笔者为 ShardingSphere 启动脚本添加了如下参数，由于加载系统表逻辑是在启动流程中，因此使用了 suspend=y 参数： JAVA_OPTS= -Djava.awt.headless=true -agentlib:jdwp=transport=dt_socket,server=y,suspend=y,address=8000JAVA_MEM_OPTS= -server -Xmx2g -Xms2g -Xmn1g -Xss1m -XX:AutoBoxCacheMax=4096 -XX:+UseNUMA -XX:+DisableExplicitGC -XX:LargePageSizeInBytes=128m $VERSION_OPTS -Dio.netty.leakDetection.level=DISABLED MAIN_CLASS=org.apache.shardingsphere.proxy.Bootstrapnohup java $JAVA_OPTS $JAVA_MEM_OPTS -classpath $CLASS_PATH $MAIN_CLASS $STDOUT_FILE 21 启动 ShardingSphere 后，我们在 IDEA 中配置远程调试，选择 Run/Debug Configurations - Remote JVM Debug，然后配置被调试程序的 Host 和 Port。 保存之后，我们使用 IDEA Debug 模式启动程序，可以看到在 ShardingSphere 启动过程中，程序会停在我们设置的断点处。 问题优化 通过 JDWP 远程调试技术，笔者在系统表加载的流程中发现了问题的原因，在 IDEA 中可以正常运行的 File#listFiles() 方法，打包之后返回的结果却为 null，这导致系统表元数据为空。 查阅了一些资料后发现，当源码打成 JAR 包后，由于 JAR 包是一个压缩包，无法直接使用 File API 去访问压缩包中的文件，需要使用流进行文件的读写。针对这个问题，笔者对系统表配置读取的逻辑进行了如下调整： private static CollectionInputStream getSchemaStreams(final String schemaName, final DatabaseType databaseType) SystemSchemaBuilderRule builderRule = SystemSchemaBuilderRule.valueOf(databaseType.getName(), schemaName); CollectionInputStream result = new LinkedList(); for (String each : builderRule.getTables()) // 使用 Stream 加载系统表配置文件 result.add(SystemSchemaBuilder.class.getClassLoader().getResourceAsStream(schema/ + databaseType.getName().toLowerCase() + / + schemaName + / + each + .yaml)); return result;private static ShardingSphereSchema createSchema(final CollectionInputStream schemaStreams, final TableMetaDataYamlSwapper swapper) MapString, TableMetaData tables = new LinkedHashMap(schemaStreams.size(), 1); for (InputStream each : schemaStreams) YamlTableMetaData metaData = new Yaml().loadAs(each, YamlTableMetaData.class); tables.put(metaData.getName(), swapper.swapToObject(metaData)); return new ShardingSphereSchema(tables); 经过调整之后，重新进行了打包测试，问题终于迎刃而解。通过这次问题的调查，笔者深刻认识到功能自测不能只在开发环境中进行测试，更应该考虑实际部署的环境，按照真实场景进行测试，从而充分暴露这些潜在的问题，希望后续的开发工作中能避免类似的问题。 参考文档 JDWP 官方文档 IDEA 远程调试 Java 代码指南 学习 Java 的调试技术 Java Application Remote Debugging SpringBoot 打包为 JAR 包后访问不到 Resources 下的文件问题 欢迎关注 欢迎关注「端小强的博客」微信公众号，会不定期分享日常学习和工作经验，欢迎大家关注交流。","tags":["Java","Remote Debugging"],"categories":["In Action"]},{"title":"ShardingSphere 5.1.0 执行引擎性能优化揭秘","path":"/blog/shardingsphere-5.1.0-execution-engine-performance-optimization.html","content":"本文首发于 Apache ShardingSphere 微信公众号，欢迎关注公众号，后续将会有更多技术分享。 前言 在 Apache ShardingSphere 5.1.0 前几篇技术解读文章中，我们了解了解析引擎提供的 SQL 格式化功能，以及最新优化的高可用功能。除了不断为用户提供实用的新特性外，Apache ShardingSphere 社区一直在努力提升性能。 以单库 10 分片的 t_order 表为例，max-connections-size-per-query 使用默认配置 1，如果用户执行 SELECT * FROM t_order 语句则会导致全路由。由于每个查询只允许在同一个数据库上创建一个数据库连接，因此底层真实执行的 SQL 结果会被提前加载至内存进行处理，该场景限制了数据库连接资源的消耗，但是会占用更多的内存资源。如果用户将 max-connections-size-per-query 调整为 10，则可以在执行真实 SQL 时，同时创建 10 个数据库连接，由于数据库连接能够持有结果集，因此该场景不会额外占用内存资源，但是很显然会消耗更多的数据库连接资源。 为了更好地解决数据库连接资源和内存资源占用的问题，在刚刚发布的 5.1.0 版本中，我们对 SQL 执行引擎的性能进行了优化，通过 SQL 改写引擎进行优化性改写，将同一个数据源上的多条真实 SQL，使用 UNION ALL 语句进行合并，从而有效降低了执行引擎对数据库连接资源的消耗，同时减少了内存归并的发生，大幅度提升了 OLTP 场景下 SQL 查询性能。下面我们将结合具体的 SQL 实例，为大家详细解读执行引擎性能优化的细节。 执行引擎原理 在解读执行引擎性能优化之前，让我们先来回顾下 Apache ShardingSphere 微内核及内核流程中执行引擎的原理。如下图所示，Apache ShardingSphere 微内核包含了 SQL 解析、SQL 路由、SQL 改写、SQL 执行和结果归并等核心流程。 SQL 解析引擎负责对用户输入的 SQL 语句进行解析，并生成包含上下文信息的 SQLStatement。SQL 路由引擎则根据解析上下文提取出分片条件，再结合用户配置的分片规则，计算出真实 SQL 需要执行的数据源并生成路由结果。SQL 改写引擎根据 SQL 路由引擎返回的结果，对原始 SQL 进行改写，具体包括了正确性改写和优化性改写。SQL 执行引擎则负责将 SQL 路由和改写引擎返回的真实 SQL 安全且高效地发送到底层数据源执行，执行的结果集最终会由归并引擎进行处理，生成统一的结果集返回给用户。 从整个微内核的执行流程可以看出，SQL 执行引擎直接与底层数据库交互，并负责持有执行的结果集，可以说执行引擎的性能和资源消耗，直接关系到整个 Apache ShardingSphere 的性能和资源消耗，因此 Apache ShardingSphere 内部采用了一套自动化的 SQL 执行引擎，负责在执行性能和资源消耗间进行权衡。 从执行性能的角度来看，为每个分片的执行语句分配一个独立的数据库连接，可以充分利用多线程来提升执行性能，也可以将 I/O 所产生的消耗并行处理。此外，为每个分片分配一个独立的数据库连接，还能够避免过早的将查询结果集加载至内存，独立的数据库连接，能够持有查询结果集游标位置的引用，在需要获取相应数据时移动游标即可。 从资源控制的角度来看，应当对业务访问数据库的连接数量进行限制，避免某一业务占用过多的数据库连接资源，影响其他业务的正常访问。 特别是在一个数据库实例中存在较多分表的情况下，一条不包含分片键的逻辑 SQL 将产生落在同库不同表的大量真实 SQL ，如果每条真实 SQL 都占用一个独立的连接，那么一次查询无疑将会占用过多的资源。 为了解决执行性能和资源控制的冲突问题，Apache ShardingSphere 提出了连接模式的概念，下面是 Apache ShardingSphere 源码对于连接模式的定义。 /** * Connection Mode. */public enum ConnectionMode MEMORY_STRICTLY, CONNECTION_STRICTLY 从 ConnectionMode 枚举类中成员的命名可以看出，SQL 执行引擎将数据库连接划分为 MEMORY_STRICTLY 和 CONNECTION_STRICTLY。 MEMORY_STRICTLY 代表内存限制模式，当采用内存限制模式时，对于同一个数据源，如果逻辑表对应了 10 个真实表，那么 SQL 执行引擎会创建 10 个连接并行地执行，由于每个分片的结果集都有对应的连接进行持有，因此无需将结果集提前加载到内存中，从而有效地降低了内存占用； CONNECTION_STRICTLY 代表连接限制模式，当采用连接限制模式时，SQL 执行引擎只会在同一个数据源上创建一个连接，严格控制对数据库连接资源的消耗，在真实 SQL 执行之后立即将结果集加载至内存，因此会占用部分内存空间。 那么，Apache ShardingSphere SQL 执行引擎是如何帮助用户选择连接模式的呢？SQL 执行引擎选择连接模式的逻辑可以参考下图： 用户通过配置 maxConnectionSizePerQuery 参数，可以指定每条语句在同一个数据源上最大允许的连接数。通过上面的计算公式，当每个数据库连接需执行的 SQL 数量小于等于 1 时，说明当前可以满足每条真实执行的 SQL 都分配一个独立的数据库连接，此时会选择内存限制模式，同一个数据源允许创建多个数据库连接进行并行执行。反之则会选择连接限制模式，同一个数据源只允许创建一个数据库连接进行执行，然后将结果集加载进内存结果集，再提供给归并引擎使用。 (adsbygoogle = window.adsbygoogle || []).push({}); 赞助商 执行引擎优化 在熟悉了 Apache ShardingSphere SQL 执行引擎的内部原理之后，我们发现使用内存限制模式时，会消耗更多的数据库连接，但是能够通过并发执行获得更好的性能，使用连接限制模式能够有效控制连接资源的使用，但是会占用过多的内存，执行的性能也会受到影响。 那么，有没有可能使用尽可能少的数据库连接，同时占用较少内存的执行方式呢？根据前文对 SQL 执行引擎的分析，执行模式的选择主要是根据同一个数据源上路由结果的数量，因此最直接的优化思路，就是对同一个数据源上的路由结果进行合并。SQL 语句天然支持通过 UNION ALL 对多条查询语句进行合并，因此我们采用 UNION ALL 方案，对同一个数据源中的多条真实 SQL 进行优化性改写，从而将多条真实 SQL 改写为一条 SQL，这样能够大大减少数据库连接的获取，同时也可以将内存结果集转换为流式结果集，减少内存的占用。 考虑到不同数据库方言对于 UNION ALL 语句的使用存在限制，我们调研了 MySQL、PostgreSQL、Oracle 以及 SQLServer 的官方文档，梳理之后得到了如下信息。 MySQL UNION ALL 使用规范： UNION 之后的列名使用第一个 SELECT 语句中的列名； UNION 中包含 ORDER BY 和 LIMIT 时，需要使用括号将各个查询语句括起来，UNION 无法保证最终的结果集有序，如果需要对 UNION 结果集进行排序，需要在 UNION 语句最后添加 ORDER BY LIMIT 子句； # 无法保证 UNION 结果集有序(SELECT a FROM t1 WHERE a=10 AND B=1 ORDER BY a LIMIT 10) UNION (SELECT a FROM t2 WHERE a=11 AND B=2 ORDER BY a LIMIT 10);# 保证 UNION 结果集有序(SELECT a FROM t1 WHERE a=10 AND B=1) UNION (SELECT a FROM t2 WHERE a=11 AND B=2) ORDER BY a LIMIT 10; UNION 不支持 SELECT HIGH_PRIORITY 语句和 SELECT INTO file 语句； PostgreSQL UNION ALL 使用规范： UNION 之后的列名使用第一个 SELECT 语句中的列名； UNION 中包含 ORDER BY 和 LIMIT 时，需要使用括号将各个查询语句括起来，最后一个 UNION 子句可以不使用括号，不使用括号，则 ORDER BY LIMIT 子句应用于整个 UNION 结果。 UNION 语句不支持 FOR NO KEY UPDATE、FOR UPDATE、FOR SHARE 和 FOR KEY SHARE； Oracle UNION ALL 使用规范： UNION 语句不支持 BLOB, CLOB, BFILE, VARRAY, LONG 类型或者 nested table； UNION 语句不支持 for_update_clause； UNION 语句不支持 select 子句中包含 order_by_clause，只能在 UNION 语句最后添加 order_by_clause； SELECT product_id FROM order_items UNION SELECT product_id FROM inventories ORDER BY product_id; UNION 语句不支持 SELECT 语句中包含 TABLE collection expressions； SQLServer UNION ALL 使用规范： UNION 语句中使用 ORDER BY 子句时，必须放在最后一个 select 子句之上，对 UNION 结果进行排序； 综合以上梳理的信息来看，不同的数据库方言都能够支持简单的 SELECT * FROM table WHERE 语句，对于 ORDER BY LIMIT 也能通过语法调整进行支持，只是使用上存在一些语法差异，而对于更加复杂的分组查询、子查询及关联查询，官方文档上并未进行详细描述。考虑到 SQL 优化性改写需要保证 SQL 兼容性，Apache ShardingSphere 5.1.0 只选择了简单的 SELECT * FROM table WHERE 语句进行改写，旨在快速提升 OLTP 场景下的查询性能。 下面展示了 RouteSQLRewriteEngine 改写引擎的最新逻辑，Apache ShardingSphere 5.1.0 中添加了对于 SELECT * FROM table WHERE 语句的优化性改写逻辑，首先通过 isNeedAggregateRewrite 进行判断，只有当同一个数据源中的路由结果大于 1，并且真实执行的 SQL 满足 SELECT * FROM table WHERE 结构时，才会进行 UNION ALL 改写。 /** * Rewrite SQL and parameters. * * @param sqlRewriteContext SQL rewrite context * @param routeContext route context * @return SQL rewrite result */public RouteSQLRewriteResult rewrite(final SQLRewriteContext sqlRewriteContext, final RouteContext routeContext) MapRouteUnit, SQLRewriteUnit result = new LinkedHashMap(routeContext.getRouteUnits().size(), 1); for (EntryString, CollectionRouteUnit entry : aggregateRouteUnitGroups(routeContext.getRouteUnits()).entrySet()) CollectionRouteUnit routeUnits = entry.getValue(); if (isNeedAggregateRewrite(sqlRewriteContext.getSqlStatementContext(), routeUnits)) result.put(routeUnits.iterator().next(), createSQLRewriteUnit(sqlRewriteContext, routeContext, routeUnits)); else result.putAll(createSQLRewriteUnits(sqlRewriteContext, routeContext, routeUnits)); return new RouteSQLRewriteResult(result); 由于使用了 UNION ALL 改写，归并引擎中对于 queryResults 的判断逻辑也需要同步进行调整，原先多个 queryResults 可能被 UNION ALL 合并为一个 queryResults，这种场景下仍然需要执行归并逻辑。 @Overridepublic MergedResult merge(final ListQueryResult queryResults, final SQLStatementContext? sqlStatementContext, final ShardingSphereSchema schema) throws SQLException if (1 == queryResults.size() !isNeedAggregateRewrite(sqlStatementContext)) return new IteratorStreamMergedResult(queryResults); MapString, Integer columnLabelIndexMap = getColumnLabelIndexMap(queryResults.get(0)); SelectStatementContext selectStatementContext = (SelectStatementContext) sqlStatementContext; selectStatementContext.setIndexes(columnLabelIndexMap); MergedResult mergedResult = build(queryResults, selectStatementContext, columnLabelIndexMap, schema); return decorate(queryResults, selectStatementContext, mergedResult); 为了方便大家理解优化前后的逻辑，我们使用如下分片配置，通过 SELECT * FROM t_order 来具体说明下优化的效果，示例中 max-connections-size-per-query 参数使用默认值 1。 rules:- !SHARDING tables: t_order: actualDataNodes: ds_$0..1.t_order_$0..1 tableStrategy: standard: shardingColumn: order_id shardingAlgorithmName: t_order_inline databaseStrategy: standard: shardingColumn: user_id shardingAlgorithmName: database_inline shardingAlgorithms: database_inline: type: INLINE props: algorithm-expression: ds_$user_id % 2 t_order_inline: type: INLINE props: algorithm-expression: t_order_$order_id % 2 allow-range-query-with-inline-sharding: true 在 5.0.0 版本中，我们执行 SELECT * FROM t_order 语句后，可以得到如下路由结果，结果中包含 ds_0 和 ds_1 两个数据源，并且各自包含了两个路由结果，由于 max-connections-size-per-query 设置为 1，此时无法满足每个真实执行 SQL 都有一个数据库连接，因此会选择连接限制模式。 同时由于使用了连接限制模式，在并行执行后会将结果集加载至内存中，使用 JDBCMemoryQueryResult 进行存储，当用户结果集较大时，会占用较多的内存。内存结果集的使用也会导致归并时只能使用内存归并，而无法使用流式归并。 private QueryResult createQueryResult(final ResultSet resultSet, final ConnectionMode connectionMode) throws SQLException return ConnectionMode.MEMORY_STRICTLY == connectionMode ? new JDBCStreamQueryResult(resultSet) : new JDBCMemoryQueryResult(resultSet); 在 5.1.0 版本中，我们使用了 UNION ALL 对执行的 SQL 进行优化，同一个数据源中多个路由结果会被合并为一条 SQL 执行。由于能够满足一个数据库连接持有一个结果集，因此会选择内存限制模式。在内存限制模式下，会使用流式结果集 JDBCStreamQueryResult 对象持有结果集，在需要使用数据时，可以按照流式查询的方式查询数据。 性能优化测试 从前面小节的示例中，我们可以看出使用 UNION ALL 进行优化性改写，可以有效减少对数据库连接的消耗，也能够将内存结果集转换为流式结果集，从而避免过多地占用内存。为了更加具体说明优化对于性能的提升，我们针对优化前后的逻辑进行了压测，压测所采用的软件版本如下，使用 5.0.1-SNAPSHOT 版本的 ShardingSphere-Proxy 以及 5.7.26 版本的 MySQL。 组件 版本 ShardingSphere-Proxy 5.0.1-SNAPSHOT (b073e622d58c6c3d4b79295fff261184c50d8968) MySQL 5.7.26 压测环境对应的机器配置如下： 组件 机器配置 ShardingSphere-Proxy 32C 64G MySQL 5.7 32C 64G JMH 32C 64G 我们参考 sysbench 表结构，创建了 sbtest1~sbtest10 等 10 张分片表，每个分片表又分为 5 库，每个库分为 10 张表，具体的 config-sharding.yaml 配置文件如下。 schemaName: sbtest_shardingdataSources: ds_0: url: jdbc:mysql://127.0.0.1:3306/sbtest?useSSL=falseuseServerPrepStmts=truecachePrepStmts=trueprepStmtCacheSize=8192prepStmtCacheSqlLimit=1024 username: root password: 123456 connectionTimeoutMilliseconds: 10000 idleTimeoutMilliseconds: 60000 maxLifetimeMilliseconds: 1800000 maxPoolSize: 50 minPoolSize: 1 ds_1: url: jdbc:mysql://127.0.0.1:3306/sbtest?useSSL=falseuseServerPrepStmts=truecachePrepStmts=trueprepStmtCacheSize=8192prepStmtCacheSqlLimit=1024 username: root password: 123456 connectionTimeoutMilliseconds: 10000 idleTimeoutMilliseconds: 60000 maxLifetimeMilliseconds: 1800000 maxPoolSize: 50 minPoolSize: 1 ds_2: url: jdbc:mysql://127.0.0.1:3306/sbtest?useSSL=falseuseServerPrepStmts=truecachePrepStmts=trueprepStmtCacheSize=8192prepStmtCacheSqlLimit=1024 username: root password: 123456 connectionTimeoutMilliseconds: 10000 idleTimeoutMilliseconds: 60000 maxLifetimeMilliseconds: 1800000 maxPoolSize: 50 minPoolSize: 1 ds_3: url: jdbc:mysql://127.0.0.1:3306/sbtest?useSSL=falseuseServerPrepStmts=truecachePrepStmts=trueprepStmtCacheSize=8192prepStmtCacheSqlLimit=1024 username: root password: 123456 connectionTimeoutMilliseconds: 10000 idleTimeoutMilliseconds: 60000 maxLifetimeMilliseconds: 1800000 maxPoolSize: 50 minPoolSize: 1 ds_4: url: jdbc:mysql://127.0.0.1:3306/sbtest?useSSL=falseuseServerPrepStmts=truecachePrepStmts=trueprepStmtCacheSize=8192prepStmtCacheSqlLimit=1024 username: root password: 123456 connectionTimeoutMilliseconds: 10000 idleTimeoutMilliseconds: 60000 maxLifetimeMilliseconds: 1800000 maxPoolSize: 50 minPoolSize: 1rules:- !SHARDING tables: sbtest1: actualDataNodes: ds_$0..4.sbtest1_$0..9 tableStrategy: standard: shardingColumn: id shardingAlgorithmName: table_inline_1 keyGenerateStrategy: column: id keyGeneratorName: snowflake sbtest2: actualDataNodes: ds_$0..4.sbtest2_$0..9 tableStrategy: standard: shardingColumn: id shardingAlgorithmName: table_inline_2 keyGenerateStrategy: column: id keyGeneratorName: snowflake sbtest3: actualDataNodes: ds_$0..4.sbtest3_$0..9 tableStrategy: standard: shardingColumn: id shardingAlgorithmName: table_inline_3 keyGenerateStrategy: column: id keyGeneratorName: snowflake sbtest4: actualDataNodes: ds_$0..4.sbtest4_$0..9 tableStrategy: standard: shardingColumn: id shardingAlgorithmName: table_inline_4 keyGenerateStrategy: column: id keyGeneratorName: snowflake sbtest5: actualDataNodes: ds_$0..4.sbtest5_$0..9 tableStrategy: standard: shardingColumn: id shardingAlgorithmName: table_inline_5 keyGenerateStrategy: column: id keyGeneratorName: snowflake sbtest6: actualDataNodes: ds_$0..4.sbtest6_$0..9 tableStrategy: standard: shardingColumn: id shardingAlgorithmName: table_inline_6 keyGenerateStrategy: column: id keyGeneratorName: snowflake sbtest7: actualDataNodes: ds_$0..4.sbtest7_$0..9 tableStrategy: standard: shardingColumn: id shardingAlgorithmName: table_inline_7 keyGenerateStrategy: column: id keyGeneratorName: snowflake sbtest8: actualDataNodes: ds_$0..4.sbtest8_$0..9 tableStrategy: standard: shardingColumn: id shardingAlgorithmName: table_inline_8 keyGenerateStrategy: column: id keyGeneratorName: snowflake sbtest9: actualDataNodes: ds_$0..4.sbtest9_$0..9 tableStrategy: standard: shardingColumn: id shardingAlgorithmName: table_inline_9 keyGenerateStrategy: column: id keyGeneratorName: snowflake sbtest10: actualDataNodes: ds_$0..4.sbtest10_$0..9 tableStrategy: standard: shardingColumn: id shardingAlgorithmName: table_inline_10 keyGenerateStrategy: column: id keyGeneratorName: snowflake defaultDatabaseStrategy: standard: shardingColumn: id shardingAlgorithmName: database_inline shardingAlgorithms: database_inline: type: INLINE props: algorithm-expression: ds_$id % 5 allow-range-query-with-inline-sharding: true table_inline_1: type: INLINE props: algorithm-expression: sbtest1_$id % 10 allow-range-query-with-inline-sharding: true table_inline_2: type: INLINE props: algorithm-expression: sbtest2_$id % 10 allow-range-query-with-inline-sharding: true table_inline_3: type: INLINE props: algorithm-expression: sbtest3_$id % 10 allow-range-query-with-inline-sharding: true table_inline_4: type: INLINE props: algorithm-expression: sbtest4_$id % 10 allow-range-query-with-inline-sharding: true table_inline_5: type: INLINE props: algorithm-expression: sbtest5_$id % 10 allow-range-query-with-inline-sharding: true table_inline_6: type: INLINE props: algorithm-expression: sbtest6_$id % 10 allow-range-query-with-inline-sharding: true table_inline_7: type: INLINE props: algorithm-expression: sbtest7_$id % 10 allow-range-query-with-inline-sharding: true table_inline_8: type: INLINE props: algorithm-expression: sbtest8_$id % 10 allow-range-query-with-inline-sharding: true table_inline_9: type: INLINE props: algorithm-expression: sbtest9_$id % 10 allow-range-query-with-inline-sharding: true table_inline_10: type: INLINE props: algorithm-expression: sbtest10_$id % 10 allow-range-query-with-inline-sharding: true keyGenerators: snowflake: type: SNOWFLAKE props: worker-id: 123 我们使用如下 JMH 测试程序对不同 CASE 进行测试： @State(Scope.Thread)public class QueryOptimizationTest private PreparedStatement unionAllForCaseOneStatement; private PreparedStatement unionAllForCaseTwoStatement; @Setup(Level.Trial) public void setup() throws Exception Connection connection = DriverManager.getConnection(jdbc:mysql://127.0.0.1:3307/sharding_db?useSSL=false, root, 123456); // CASE 1 unionAllForCaseOneStatement = connection.prepareStatement(SELECT COUNT(k) AS countK FROM sbtest1 WHERE id ?;); // CASE 2 unionAllForCaseTwoStatement = connection.prepareStatement(SELECT SUM(k) AS sumK FROM sbtest1 WHERE id ?;); @Benchmark public void testUnionAllForCaseOne() throws SQLException unionAllForCaseOneStatement.setInt(1, 200); unionAllForCaseOneStatement.executeQuery(); @Benchmark public void testUnionAllForCaseTwo() throws SQLException unionAllForCaseTwoStatement.setInt(1, 200); unionAllForCaseTwoStatement.executeQuery(); 性能测试会对每个 CASE 分别测试 3 组，然后取平均值，再切换到优化前的版本 aab226b72ba574061748d8f94c461ea469f9168f 进行编译打包，同样测试 3 组取平均值，最终性能测试结果如下。 Threads 数据量 CASE SQL Before Optimization After Optimization 100 100w SELECT COUNT(k) AS countK FROM sbtest1 WHERE id 200; 1954.887 10340.854 100 100w SELECT SUM(k) AS sumK FROM sbtest1 WHERE id 200; 1951.675 10218.098 CASE 1 与 CASE 2 都是基于 100 万数据量下的 sysbench 表结构进行测试，由于测试表分片数较多，整体性能提升了 4 倍左右，理论上随着分片数的增加，性能提升的效果会更加明显。 结语 Apache ShardingSphere 5.1.0 进行了大量的性能优化，针对协议层和内核层进行了全面的优化提升，本文限于篇幅只对 SQL 执行引擎进行了解读，后续的系列文章还会带来更加专业和全面的性能优化指南，希望感兴趣的同学继续关注。同时，也欢迎社区的同学积极参与进来，共同提升 Apache ShardingSphere 的性能，为社区提供更好的使用体验。 欢迎关注 欢迎关注「端小强的博客」微信公众号，会不定期分享日常学习和工作经验，欢迎大家关注交流。","tags":["ShardingSphere","Kernel"],"categories":["ShardingSphere"]},{"title":"关系系统查询优化概述论文学习","path":"/blog/an-overview-of-query-optimization-in-relational-systems.html","content":"本文翻译自论文 An Overview of Query Optimization in Relational Systems，论文介绍了 70 年代以来优化器方面的研究成果，通过本文的学习，我们可以快速了解关系系统中常见的查询优化技术，为后续深入学习 Calcite 及查询优化技术打下良好的基础。 目标 自上世纪 70 年代初以来，学术界在查询优化领域进行了大量的研究工作。在一篇简短的论文中，很难展现出这些研究工作的广度和深度。因此，我决定将重点放在关系型数据库系统的 SQL 查询优化上，并提出我对这个领域的个人看法。本文的目标不是全面的，而是介绍这个领域的基础理论，并展示这些重要工作的一些示例。我想向这个领域的贡献者们道歉，由于个人疏忽及篇幅原因，我未能明确地感谢他们的工作。此外，为了便于演示，我冒昧地牺牲了技术精度。 简介 关系查询语言提供了高级的 声明式 接口，用来访问存储在关系型数据库中的数据。随着时间的推移，SQL[1] 已经成为关系查询语言的标准。SQL 数据库系统查询计算组件的两个最关键组件是 查询优化器 和 查询执行引擎。 查询执行引擎实现了一组物理算子（physical operators）。算子负责将一个或多个数据流作为输入，并生成一个输出数据流。物理算子的例子包括：（外部）排序、顺序扫描、索引扫描、嵌套循环连接（nested-loop join） 和 排序合并连接（sort-merge join）。我将这些算子称为物理算子，因为它们不一定与关系操作符一一对应。理解物理算子最简单的方法是将它看做代码块，代码块作为基础模块实现了 SQL 查询语句的执行。这种执行的抽象表示就是物理算子树，如图 1 所示。算子树中的边表示物理算子之间的数据流。我们使用 物理算子树、执行计划（或者 简单的计划）这些可交换的术语。执行引擎负责计划的执行，并且生成查询的结果。因此，查询执行引擎的功能决定了可行的算子树的结构。读者可以参考[2]来了解查询估算技术的概要。 查询优化器负责为执行引擎生成输入。它接受一个已解析的 SQL 查询作为输入，并负责从可行的执行计划空间里，为给定的 SQL 查询生成有效的执行计划。优化器的任务并不简单，因为对于给定的 SQL 查询，可能存在大量可行的算子树： 给定查询的代数表示，可以转换为许多其他逻辑上等价的代数表示，例如： Join(Join(A,B),C) = Join(Join(B,C),A) 对于一个给定的代数表示，可能有许多实现代数表达式的算子树，例如：数据库系统通常支持多种连接算法。 此外，执行这些计划的吞吐量或响应时间可能大不相同，因此，优化器对执行计划的明智选择是极其重要的。我们可以将查询优化看作是一个困难的搜索问题。为了解决这个问题，我们需要提供： 执行计划空间（搜索空间）； 成本估算技术，以便为每个搜索空间中的执行计划分配成本（cost）。直观地说，这是对执行计划所需资源的估算； 枚举算法，用来搜索整个执行计划空间。 一个理想的优化器通常是这样的：（1）搜索空间包含低成本的执行计划；（2）成本计算技术是准确的；（3）枚举算法是有效的。这三个任务中的每一个都不是轻而易举的，这就是为什么构建一个好的优化器是一个巨大的任务。 我们首先讨论 System-R 优化框架，因为这是一种非常优雅的优化方法，有助于推动后续的优化工作。在第 4 节中，我们将讨论优化器所要考虑的搜索空间。这节将会提供一个版块，用来介绍搜索空间中包含的重要代数变换。在第 5 节中，我们将讨论成本估算的问题。在第 6 节中，我们将讨论枚举搜索空间的主题。这些小节就完成了对基本优化框架的讨论。在第 7 节中，我们将讨论查询优化方面的一些最新进展。 (adsbygoogle = window.adsbygoogle || []).push({}); 赞助商 SYSTEM-R 优化器 SYSTEM-R 项目显著提升了关系系统查询优化的状态。[3]中的思路已经被纳入到许多商业优化器中，并且仍然具有明显地关联性。在这里，我将基于 Select-Project-Join（SPJ） 查询的上下文，介绍下这些重要思路中的部分。SPJ 查询类与连接查询（conjunctive queries）密切相关，并封装了连接查询，连接查询在数据库理论中得到了广泛的研究。 在 SPJ 查询上下文中，System-R 优化器的搜索空间由算子树组成，算子树对应了线性序列的连接操作，例如：Join(Join(Join(A,B),C),D) 的序列如图 2(a) 所示。由于连接的结合性和交换性，这些序列在逻辑上是等价的。连接算子既可以使用嵌套循环（nested loop）实现，也可以使用排序合并（sort-merge）实现。每个扫描节点即可以使用索引扫描（使用聚集或非聚集索引），也可以使用顺序扫描。最后，谓词应当尽可能早地进行计算。 成本模型将估算的成本，分配给搜索空间中的任何部分或全部执行计划。它还决定了执行计划中，每个算子输出的数据流的估算大小。成本模型依赖于： 一组维护在关系和索引上的统计数据，例如：关系中数据页的数量、索引中的页数以及某一列不同值的数量； 用于估算谓词选择性以及预测每个算子节点输出数据流大小的公式。例如：通过取两个关系大小的乘积，然后使用所有适用谓词的连接选择度来估算连接输出的大小； 估算每个算子执行查询的 CPU 和 I/O 开销的公式。这些公式考虑了输入数据流的统计属性、输入数据流的既存访问方法以及数据流上的任何可用的顺序（例如，如果一个数据流是有序的，那么在该数据流上，排序合并 sort-merge 连接的成本可能会显著降低）。此外，还会检查输出数据流是否有任何顺序。 成本模型使用上述 1~3 中的信息，为执行计划中的算子以自底向上的方式进行计算并关联以下信息：（1）算子节点输出的数据流大小；（2）算子节点输出的数据流创建或维持的元组的顺序；（3）为算子估算执行成本（以及部分执行计划到目前为止的累计成本）。 System-R 优化器的枚举算法演示了两个重要的技术：使用动态规划（dynamic programming）和使用感兴趣的顺序（interesting orders）。 动态规划方法的本质是基于成本模型满足最优原则的假设。具体地说，它假设为了获得由 k 个连接组成的 SPJ 查询 Q 的最优计划，只要考虑由 (k - 1) 个连接组成的查询 Q 的子表达式的最优计划，并用一个额外的连接扩展这些计划即可。换句话说，在确定 Q 的最优方案时，不需要进一步考虑由 (k - 1) 连接组成的查询 Q 的子表达式（也称为子查询）的次优方案。因此，基于动态规划的枚举将 SPJ 查询 Q 看作一组关系 R1, . .n 的连接。枚举算法是自底向上的。在第 j 步的最后，算法为所有大小为 j 的子查询生成最优执行计划。为了获得由 (j + 1) 个关系组成的子查询的最优执行计划，我们考虑了为子查询构建执行计划的所有可能方式，通过扩展第 j 步构建的执行计划。 动态规划方法的本质是基于成本模型满足最优原则的假设。具体地说，它假设为了获得由k个连接组成的SPJ查询Q的最优计划，只考虑由(k-1)连接组成的Q子表达式的最优计划，并用一个额外的连接扩展这些计划。换句话说，在确定Q的最优方案时，不需要进一步考虑由(k-1)连接组成的Q的子表达式(也称为子查询)的次优方案。因此，基于动态规划的枚举将SPJ查询Q视为一组关系{R1, . .n}被加入。枚举算法是自底向上的。在j的最后一步,所有子查询的算法生成最优计划规模j。获得一个最优的计划组成的子查询(j + 1)关系,我们考虑所有可能的子查询的方法构造一个计划通过扩展计划建造在j - th一步。例如，{R1, R2, R3, R4}通过从最优方案中选取成本最便宜的方案来获得:(1)Join({R1, R2, R3}, R4(2)加入({R1, R2, R4}, R3(3) Join ({R1, R3, R4}, R2(4)加入({R2, R3, R4}, R1）.{R1, R2, R3, R4}可以被丢弃。动态规划方法比naïve方法快得多，因为与O(n!)计划相比，只有O(n2)计划n -1)计划需要列举。System R优化器的第二个重要方面是考虑感兴趣的订单。现在让我们考虑一个表示{R之间连接的查询1, R2, R3}和谓词R1。= R2。= R3。。我们还假设子查询{R1, R2}分别为x和y用于nested-loop和sort-merge join, x y。此时，考虑{R1, R2, R3}，我们将不考虑R1 和R2 使用排序合并连接。但是，请注意，如果使用排序合并来连接R1 和R2，则对连接的结果在a上进行排序，排序后可以显著降低与R连接的代价3．因此，裁剪表示R之间排序合并连接的计划1 和R2 可能导致全局计划的次优性。问题的出现是因为R之间的排序归并联接的结果1 和R2 元组的排序 在后续联接中有用的输出流。但是，嵌套循环连接没有这样的顺序。因此，给定一个查询，System R识别可能对查询的执行计划产生影响的元组的顺序(因此命名为感兴趣的顺序)。此外，在System R优化器中，只有当两个计划表示相同的表达式并且具有相同的感兴趣顺序时，才会比较它们。有趣顺序的概念后来在[22]中被推广到物理性质，并在现代优化器中被广泛使用。直观地说，物理属性是计划的任何特性，该特性不为同一逻辑表达式的所有计划共享，但可能影响后续操作的成本。最后，需要注意的是，System-R考虑物理属性的方法展示了一种简单的机制，可以处理任何违背最优原则的情况，而不仅仅是由于物理属性引起的。 尽管System-R方法很优雅，但是框架不能很容易地扩展到包含其他扩展搜索空间的逻辑转换(除了连接排序)。这导致了更可扩展的优化体系结构的开发。然而，基于成本的优化、动态规划和有趣的订单的使用强烈地影响了优化的后续发展。 搜索空间 统计和成本估算 枚举架构 基础优化之外的话题 参考文档 欢迎关注 欢迎关注「端小强的博客」微信公众号，会不定期分享日常学习和工作经验，欢迎大家关注交流。 Melton, J., Simon A. Understanding The New SQL: A Complete Guide. Morgan Kaufman. ↩︎ Graefe G. Query Evaluation Techniques for Large Databases. In ACM Computing Surveys: Vol 25, No 2., June 1993. ↩︎ Selinger, P.G., Astrahan, M.M., Chamberlin, D.D., Lorie, R.A., Price T.G. Access Path Selection in a Relational Database System. In Readings in Database Systems. Morgan Kaufman. ↩︎","tags":["Query Optimization","Paper"],"categories":["Paper"]},{"title":"ShardingSphere 5.0.0 内核优化及升级指南","path":"/blog/shardingsphere-5.0.0-kernel-optimization-and-upgrade-guide.html","content":"本文首发于 Apache ShardingSphere 微信公众号，欢迎关注公众号，后续将会有更多技术分享。 前言 经过将近两年时间的优化和打磨，Apache ShardingSphere 5.0.0 GA 版终于在本月正式发布，相比于 4.1.1 GA 版，5.0.0 GA 版在内核层面进行了大量的优化。首先，基于可插拔架构对内核进行了全面改造，内核中的各个功能可以任意组合并叠加使用。其次，为了提升 SQL 分布式查询能力，5.0.0 GA 版打造了全新的 Federation 执行引擎，来满足用户复杂的业务场景。此外，5.0.0 GA 版在内核功能 API 层面也进行了大量优化，旨在降低用户使用这些功能的成本。本文将为大家详细解读 5.0.0 GA 版中的这些重大内核优化，并将对比两个 GA 版本中存在的差异，以典型的数据分片、读写分离和加解密整合使用的场景为例，帮助用户更好地理解这些优化并完成版本升级。 可插拔架构内核 Apache ShardingSphere 5.0.0 GA 版提出了全新的 Database Plus 理念，目标是构架异构数据库上层标准和生态，为用户提供精准化和差异化的能力。Database Plus 具有连接、增量、可插拔的特点，具体来说，Apache ShardingSphere 能够连接不同的异构数据库，基于异构数据库的基础服务，提供诸数据分片、数据加解密及分布式事务等增量功能。另外，通过可插拔平台，Apache ShardingSphere 提供的增量功能能够无限扩展，用户也可以根据需求灵活地进行扩展。Database Plus 理念的出现，使得 Apache ShardingSphere 真正意义上，从一个分库分表中间件蜕变成为一套强大的分布式数据库生态系统。通过践行 Database Plus 理念，基于可插拔平台提供的扩展点，Apache ShardingSphere 内核也进行了全面地可插拔化改造。下图展示了全新的可插拔架构内核： Apache ShardingSphere 内核流程中的 元数据加载、SQL 解析、SQL 路由 、SQL 改写、SQL 执行 和 结果归并，都提供了丰富的扩展点，基于这些扩展点，Apache ShardingSphere 默认实现了 数据分片、读写分离、加解密、影子库压测 及 高可用 等功能。 按照扩展点是基于技术还是基于功能实现，我们可以将扩展点划分为 功能扩展点 和 技术扩展点。Apache ShardingSphere 内核流程中，SQL 解析引擎及 SQL 执行引擎的扩展点属于技术扩展点，而元数据加载、SQL 路由引擎、SQL 改写引擎及结果归并引擎的扩展点属于功能扩展点。 SQL 解析引擎扩展点，主要包括 SQL 语法树解析及 SQL 语法树遍历两个扩展点。Apache ShardingSphere 的 SQL 解析引擎，基于这两个扩展点，默认支持了 MySQL、PostgreSQL、Oracle、SQLServer、openGauss 和 SQL92 等数据库方言的解析和遍历。用户也可以基于这两个扩展点，实现 Apache ShardingSphere SQL 解析引擎暂不支持的数据库方言，以及开发诸如 SQL 审计 这样的新功能； SQL 执行引擎扩展点按照不同的执行方式来提供扩展，目前，Apache ShardingSphere SQL 执行引擎已经提供了 单线程执行引擎 和 多线程执行引擎。单线程执行引擎 主要用于处理包含事务的语句执行，多线程执行引擎则适用于不包含事务的场景，用于提升 SQL 执行的性能。未来，Apache ShardingSphere 将基于执行引擎扩展点，提供诸如 MPP 执行引擎在内的更多执行引擎，满足分布式场景下 SQL 执行的要求。 基于功能扩展点，Apache ShardingSphere 提供了 数据分片、读写分离、加解密、影子库压测 及 高可用 等功能，这些功能根据各自需求，实现了全部或者部分功能扩展点，并且在功能内部，又通过细化功能级扩展点提供了诸如分片策略、分布式 ID 生成及负载均衡算法等内部扩展点。下面是 Apache ShardingSphere 内核功能实现的扩展点： 数据分片：实现了元数据加载、SQL 路由、SQL 改写和结果归并的全部功能扩展点，在数据分片功能内部，又提供了分片算法、分布式 ID 等扩展点； 读写分离：实现了 SQL 路由的功能扩展点，功能内部提供了负载均衡算法扩展点； 加解密：实现了元数据加载、SQL 改写和结果归并的扩展点，内部提供了加解密算法扩展点； 影子库压测：实现了 SQL 路由的扩展点，在影子库压测功能内部，提供了影子算法扩展点； 高可用：实现了 SQL 路由的扩展点； 基于这些扩展点，Apache ShardingSphere 功能的可扩展空间非常大，像 多租户 和 SQL 审计 等功能，都可以通过扩展点无缝地集成到 Apache ShardingSphere 生态中。此外，用户也可以根据自己的业务需求，基于扩展点完成定制化功能开发，快速地搭建出一套分布式数据库系统。关于可插拔架构扩展点的详细说明，可以参考官网 开发者手册。 综合对比来看，5.0.0 GA 版可插拔架构内核和 4.1.1 GA 版内核主要的差异如下： 版本 4.1.1 GA 5.0.0 GA 定位 分库分表中间件 分布式数据库生态系统 功能 提供基础功能 提供基础设施和最佳实践 耦合 耦合较大，存在功能依赖 相互隔离，互无感知 组合使用 固定的组合方式，必须以数据分片为基础，叠加读写分离和加解密等功能 功能自由组合，数据分片、读写分离、影子库压测、加解密和高可用等功能可以任意叠加组合 首先，从项目定位上来看，5.0.0 GA 版借助可插拔架构实现了从分库分表中间件到分布式数据库生态系统的转变，各个功能都可以通过可插拔架构融入到分布式数据库生态系统中。其次，从项目功能上来看，4.1.1 GA 版只提供一些基础功能，而 5.0.0 GA 版则更加侧重于提供基础设施，以及一些功能的最佳实践，用户完全可以舍弃这些功能，基于内核基础设施开发定制化功能。从功能耦合的角度来看，5.0.0 GA 版的内核功能，做到了相互隔离，互无感知，这样可以最大程度地保证内核的稳定性。最后，从功能组合使用的角度来看，5.0.0 GA 版实现了功能的层级一致，数据分片、读写分离、影子库压测、加解密和高可用等功能，可以按照用户的需求任意组合，而在 4.1.1 GA 版中，用户在组合使用这些功能时，必须以数据分片为中心，再叠加使用其他功能。 通过这些对比可以看出， 5.0.0 GA 版可插拔内核进行了全方位地增强，用户可以像搭积木一样对功能进行叠加组合，从而满足更多业务需求。但是，可插拔架构的调整也导致了内核功能的使用方式出现了很大的变化，在文章的后续内容中，我们会通过实例来详细介绍在 5.0.0 GA 版中如何组合使用这些功能。 (adsbygoogle = window.adsbygoogle || []).push({}); 赞助商 Federation 执行引擎 Federation 执行引擎是 5.0.0 GA 版内核的又一大亮点功能，目标是支持那些在 4.1.1 GA 版中无法执行的分布式查询语句，例如：跨数据库实例的关联查询及子查询。Federation 执行引擎的出现，使得业务研发人员不必再关心 SQL 的使用范围，能够专注于业务功能开发，减少了业务层面的功能限制。 上图展示了 Federation 执行引擎的处理流程，总体上来看，仍然是遵循着 SQL 解析、SQL 路由、SQL 改写、SQL 执行 这几个步骤，唯一的区别是 Federation 执行引擎额外引入了 SQL 优化，对分布式查询语句进行 RBO（Rule Based Optimizer） 和 CBO（Cost Based Optimizer） 优化，从而得到代价最小的执行计划。在 SQL 路由阶段，路由引擎会根据 SQL 语句是否跨多个数据库实例，来决定 SQL 是否通过 Federation 执行引擎来执行。 Federation 执行引擎目前处于快速开发中，仍然需要大量的优化，还是一个实验性的功能，因此默认是关闭的，如果想要体验 Federation 执行引擎，可以通过配置 sql-federation-enabled: true 来开启该功能。 Federation 执行引擎主要用来支持跨多个数据库实例的关联查询和子查询，以及部分内核不支持的聚合查询。下面我们通过具体的场景，来了解下 Federation 执行引擎支持的语句。 跨库关联查询：当关联查询中的多个表分布在不同的数据库实例上时，由 Federation 执行引擎提供支持； 例如，在下面的数据分片配置中，t_order 和 t_order_item 表是多数据节点的分片表，并且未配置绑定表规则，t_user 和 t_user_role 则是分布在不同的数据库实例上的单表。 rules:- !SHARDING tables: t_order: actualDataNodes: ds_$0..1.t_order_$0..1 tableStrategy: standard: shardingColumn: order_id shardingAlgorithmName: t_order_inline t_order_item: actualDataNodes: ds_$0..1.t_order_item_$0..1 tableStrategy: standard: shardingColumn: order_id shardingAlgorithmName: t_order_item_inline 由于跨多个数据库实例，下面这些常用的 SQL，会使用 Federation 执行引擎进行关联查询。 SELECT * FROM t_order o INNER JOIN t_order_item i ON o.order_id = i.order_id WHERE o.order_id = 1;SELECT * FROM t_order o INNER JOIN t_user u ON o.user_id = u.user_id WHERE o.user_id = 1;SELECT * FROM t_order o LEFT JOIN t_user_role r ON o.user_id = r.user_id WHERE o.user_id = 1;SELECT * FROM t_order_item i LEFT JOIN t_user u ON i.user_id = u.user_id WHERE i.user_id = 1;SELECT * FROM t_order_item i RIGHT JOIN t_user_role r ON i.user_id = r.user_id WHERE i.user_id = 1;SELECT * FROM t_user u RIGHT JOIN t_user_role r ON u.user_id = r.user_id WHERE u.user_id = 1; 子查询：Apache ShardingSphere 的 Simple Push Down 引擎能够支持分片条件一致的子查询，以及路由到单个分片的子查询。对于子查询和外层查询未同时指定分片键，或分片键的值不一致的场景，需要由 Federation 执行引擎来提供支持； 下面展示了一些由 Federation 执行引擎支持的子查询场景： SELECT * FROM (SELECT * FROM t_order) o;SELECT * FROM (SELECT * FROM t_order) o WHERE o.order_id = 1;SELECT * FROM (SELECT * FROM t_order WHERE order_id = 1) o;SELECT * FROM (SELECT * FROM t_order WHERE order_id = 1) o WHERE o.order_id = 2; 聚合查询：对于 Apache ShardingSphere Simple Push Down 引擎暂不支持的一些聚合查询，我们也同样通过 Federation 执行引擎提供了支持； SELECT user_id, SUM(order_id) FROM t_order GROUP BY user_id HAVING SUM(order_id) 10;SELECT (SELECT MAX(user_id) FROM t_order) a, order_id FROM t_order;SELECT COUNT(DISTINCT user_id), SUM(order_id) FROM t_order; Federation 执行引擎的出现，使得 Apache ShardingSphere 分布式查询能力得到明显增强，未来 Apache ShardingSphere 将持续优化，有效降低 Federation 执行引擎的内存占用，不断提升分布式查询的能力。关于 Federation 执行引擎支持语句的详细清单，可有参考官方文档中的 实验性支持的 SQL。 内核功能 API 调整 为了降低用户使用内核功能的成本，5.0.0 GA 版在 API 层面也进行了大量的优化。首先，针对社区反馈较多的数据分片 API 过于复杂、难以理解的问题，经过社区充分讨论之后，在 5.0.0 GA 版中提供了全新的数据分片 API。同时，随着 Apache ShardingSphere 项目定位的变化——由传统数据库中间件蜕变为分布式数据库生态系统，实现透明化的数据分片功能也变得越发重要。因此，5.0.0 GA 版提供了自动化的分片策略，用户无需关心分库分表的细节，通过指定分片数即可实现自动分片。此外，由于可插拔架构的提出，以及影子库压测等功能的进一步增强，内核功能 API 都进行了相应的优化调整。下面我们将会从不同功能的角度，为大家详细介绍 5.0.0 GA 版 API 层面的调整。 数据分片 API 调整 在 4.x 版中，社区经常反馈数据分片的 API 过于复杂，难以理解。下面是 4.1.1 GA 版中的数据分片配置，分片策略包含了 standard、complex、inline、hint 和 none 5 中策略，不同的分片策略之间参数也大不相同，导致普通用户很难理解和使用。 shardingRule: tables: t_order: databaseStrategy: standard: shardingColumn: order_id preciseAlgorithmClassName: xxx rangeAlgorithmClassName: xxx complex: shardingColumns: year, month algorithmClassName: xxx hint: algorithmClassName: xxx inline: shardingColumn: order_id algorithmExpression: ds_$order_id % 2 none: tableStrategy: ... 5.0.0 GA 版对分片 API 中的分片策略进行了简化，首先去除了原有的 inline 策略，只保留了 standard、complex、hint 和 none 这四个分片策略，同时将分片算法从分片策略中抽取出来，放到 shardingAlgorithms 属性下进行单独配置，分片策略中通过指定 shardingAlgorithmName 属性进行引用即可。 rules:- !SHARDING tables: t_order: databaseStrategy: standard: shardingColumn: order_id shardingAlgorithmName: database_inline complex: shardingColumns: year, month shardingAlgorithmName: database_complex hint: shardingAlgorithmName: database_hint none: tableStrategy: ... shardingAlgorithms: database_inline: type: INLINE props: algorithm-expression: ds_$order_id % 2 database_complex: type: CLASS_BASED props: strategy: COMPLEX algorithmClassName: xxx database_hint: type: CLASS_BASED props: strategy: HINT algorithmClassName: xxx 上面是根据 4.1.1 GA 版分片配置修改后的配置，可以看出新的分片 API 更加简洁清晰。同时为了减少用户的配置量，Apache ShardingSphere 提供了众多内置分片算法供用户选择，用户也可以通过 CLASS_BASED 分片算法进行自定义。更多关于内置分片算法的内容，可以参考官方文档 内置算法-分片算法。 除了优化数据分片 API 之外，为了能够实现透明化数据分片，5.0.0 GA 版还提供了自动化的分片策略。下面展示了自动化分片策略配置和手动声明分片策略配置的差异： rules:- !SHARDING autoTables: # 自动分片策略 t_order: actualDataSources: ds_0, ds_1 shardingStrategy: standard: shardingColumn: order_id shardingAlgorithmName: auto_mod keyGenerateStrategy: column: order_id keyGeneratorName: snowflake shardingAlgorithms: auto_mod: type: MOD props: sharding-count: 4 tables: # 手动声明分片策略 t_order: actualDataNodes: ds_$0..1.t_order_$0..1 tableStrategy: standard: shardingColumn: order_id shardingAlgorithmName: table_inline dataBaseStrategy: standard: shardingColumn: user_id shardingAlgorithmName: database_inline 自动化分片策略，需要配置在 autoTables 属性下，用户只需要指定数据存储的数据源，同时通过自动分片算法指定分片数即可，不再需要通过 actualDataNodes 来手动声明数据分布，也无需专门设置分库策略和分表策略，Apache ShardingSphere 将自动实现数据分片管理。 此外，5.0.0 GA 版删除了数据分片 API 中的 defaultDataSourceName 配置。在 5.0.0 GA 版中，Apache ShardingSphere 定位为分布式数据库生态系统，用户可以像使用传统数据库一样，直接使用 Apache ShardingSphere 提供的服务，因此用户无需感知底层的数据库存储。Apache ShardingSphere 通过内置的 SingleTableRule 来管理数据分片之外的单表，帮助用户实现单表的自动加载和路由。 5.0.0 GA 版为了进一步简化用户配置，同时配合数据分片 API 中的 defaultDatabaseStrategy 和 defaultTableStrategy 分片策略，增加了 defaultShardingColumn 配置，作为默认的分片键。当多个表分片键相同时，用户可以不配置 shardingColumn，使用默认的 defaultShardingColumn 配置。下面的分片配置中，t_order 表的分片策略都会使用默认的 defaultShardingColumn 配置。 rules:- !SHARDING tables: t_order: actualDataNodes: ds_$0..1.t_order_$0..1 tableStrategy: standard: shardingAlgorithmName: table_inline defaultShardingColumn: order_id defaultDatabaseStrategy: standard: shardingAlgorithmName: database_inline defaultTableStrategy: none: 读写分离 API 调整 读写分离 API 的基本功能，在 5.0.0 GA 版变化不大，只是由 MasterSlave 调整为 ReadWriteSplitting，其他用法基本相同。下面是 4.1.1 GA 版和 5.0.0 GA 版读写分离 API 的对比。 # 4.1.1 GA 读写分离 APImasterSlaveRule: name: ms_ds masterDataSourceName: master_ds slaveDataSourceNames: - slave_ds_0 - slave_ds_1# 5.0.0 GA 读写分离 APIrules:- !READWRITE_SPLITTING dataSources: pr_ds: writeDataSourceName: write_ds readDataSourceNames: - read_ds_0 - read_ds_1 此外，在 5.0.0 GA 版中，基于可插拔架构开发了高可用功能，读写分离可以配合高可用功能，提供能够自动切换主从的高可用版读写分离，欢迎大家关注高可用功能后续的官方文档及技术分享。 加解密 API 调整 5.0.0 GA 版对于加解密 API 进行了小幅度优化，增加了 table 级别的 queryWithCipherColumn 属性，方便用户能够对加解密字段的明文、密文切换进行表级别的控制，其他配置和 4.1.1 GA 版基本保持一致。 rules:- !ENCRYPT encryptors: aes_encryptor: type: AES props: aes-key-value: 123456abc md5_encryptor: type: MD5 tables: t_encrypt: columns: user_id: plainColumn: user_plain cipherColumn: user_cipher encryptorName: aes_encryptor order_id: cipherColumn: order_cipher encryptorName: md5_encryptor queryWithCipherColumn: true queryWithCipherColumn: false 影子库压测 API 调整 影子库压测 API，在 5.0.0 GA 版中进行了全面调整，首先删除了影子库中的逻辑列，并增加了功能强大的影子库匹配算法，用来帮助用户实现更加灵活的路由控制。下面是 4.1.1 GA 版影子库压测的 API，总体上功能较为简单，根据逻辑列对应的值判断是否开启影子库压测。 shadowRule: column: shadow shadowMappings: ds: shadow_ds 5.0.0 GA 版中影子库压测 API 则更加强大，用户可以通过 enable 属性，控制是否开启影子库压测，同时可以按照表的维度，细粒度控制需要进行影子库压测的生产表，并支持多种不同的匹配算法，例如：列值匹配算法、列正则表达式匹配算法以及 SQL 注释匹配算法。 rules:- !SHADOW enable: true dataSources: shadowDataSource: sourceDataSourceName: ds shadowDataSourceName: shadow_ds tables: t_order: dataSourceNames: - shadowDataSource shadowAlgorithmNames: - user-id-insert-match-algorithm - simple-hint-algorithm shadowAlgorithms: user-id-insert-match-algorithm: type: COLUMN_REGEX_MATCH props: operation: insert column: user_id regex: [1] simple-hint-algorithm: type: SIMPLE_NOTE props: shadow: true foo: bar 在后续的技术分享文章中，我们会对影子库压测功能进行详细介绍，此处就不展开说明，更多影子库匹配算法可以参考官方文档 影子算法。 5.0.0 GA 升级指南 前面分别从可插拔内核架构、Federation 执行引擎以及内核功能 API 调整三个方面，详细地介绍了 5.0.0 GA 版内核的重大优化。面对两个版本存在的众多差异，大家最关心的莫过于如何从 4.1.1 GA 升级到 5.0.0 GA 版本？下面我们将基于数据分片、读写分离和加解密整合使用这样一个典型的场景，详细介绍下升级 5.0.0 GA 版本需要注意哪些问题。 在 4.1.1 GA 中，组合使用多个功能时，必须以数据分片为基础，然后叠加读写分离和加解密，因此 4.1.1 GA 版中的配置通常如下： shardingRule: tables: t_order: actualDataNodes: ms_ds_$0..1.t_order_$0..1 tableStrategy: inline: shardingColumn: order_id algorithmExpression: t_order_$order_id % 2 t_order_item: actualDataNodes: ms_ds_$0..1.t_order_item_$0..1 tableStrategy: inline: shardingColumn: order_id algorithmExpression: t_order_item_$order_id % 2 bindingTables: - t_order,t_order_item broadcastTables: - t_config defaultDataSourceName: ds_0 defaultDatabaseStrategy: inline: shardingColumn: user_id algorithmExpression: ms_ds_$user_id % 2 defaultTableStrategy: none: masterSlaveRules: ms_ds_0: masterDataSourceName: ds_0 slaveDataSourceNames: - ds_0_slave_0 - ds_0_slave_1 loadBalanceAlgorithmType: ROUND_ROBIN ms_ds_1: masterDataSourceName: ds_1 slaveDataSourceNames: - ds_1_slave_0 - ds_1_slave_1 loadBalanceAlgorithmType: ROUND_ROBIN encryptRule: encryptors: aes_encryptor: type: aes props: aes.key.value: 123456abc tables: t_order: columns: content: plainColumn: content_plain cipherColumn: content_cipher encryptor: aes_encryptor t_user: columns: telephone: plainColumn: telephone_plain cipherColumn: telephone_cipher encryptor: aes_encryptor 从上面的配置文件中可以看出，t_order 和 t_order_item 配置了分片规则，并且 t_order 表的 content 字段同时设置了加解密规则，使用 AES 算法进行加解密。t_user 则是未分片的普通表，telephone 字段也配置了加解密规则。另外需要注意的是，读写分离规则和加解密规则都是以属性的形式，配置在分片规则中，这也是 4.1.1 GA 中功能依赖的具体体现，其他功能都必须以数据分片为基础。 配置完成之后，我们启动 4.1.1 GA 版 Proxy 接入端，对 t_order、t_order_item 及 t_user 表进行初始化。初始化语句执行的结果如下： CREATE TABLE t_order(order_id INT(11) PRIMARY KEY, user_id INT(11), content VARCHAR(100));-- Logic SQL: CREATE TABLE t_order(order_id INT(11) PRIMARY KEY, user_id INT(11), content VARCHAR(100))-- Actual SQL: ds_0 ::: CREATE TABLE t_order_0(order_id INT(11) PRIMARY KEY, user_id INT(11), content VARCHAR(100))-- Actual SQL: ds_0 ::: CREATE TABLE t_order_1(order_id INT(11) PRIMARY KEY, user_id INT(11), content VARCHAR(100))-- Actual SQL: ds_1 ::: CREATE TABLE t_order_0(order_id INT(11) PRIMARY KEY, user_id INT(11), content VARCHAR(100))-- Actual SQL: ds_1 ::: CREATE TABLE t_order_1(order_id INT(11) PRIMARY KEY, user_id INT(11), content VARCHAR(100))CREATE TABLE t_order_item(item_id INT(11) PRIMARY KEY, order_id INT(11), user_id INT(11), content VARCHAR(100));-- Logic SQL: CREATE TABLE t_order_item(item_id INT(11) PRIMARY KEY, order_id INT(11), user_id INT(11), content VARCHAR(100))-- Actual SQL: ds_0 ::: CREATE TABLE t_order_item_0(item_id INT(11) PRIMARY KEY, order_id INT(11), user_id INT(11), content VARCHAR(100))-- Actual SQL: ds_0 ::: CREATE TABLE t_order_item_1(item_id INT(11) PRIMARY KEY, order_id INT(11), user_id INT(11), content VARCHAR(100))-- Actual SQL: ds_1 ::: CREATE TABLE t_order_item_0(item_id INT(11) PRIMARY KEY, order_id INT(11), user_id INT(11), content VARCHAR(100))-- Actual SQL: ds_1 ::: CREATE TABLE t_order_item_1(item_id INT(11) PRIMARY KEY, order_id INT(11), user_id INT(11), content VARCHAR(100))CREATE TABLE t_user(user_id INT(11) PRIMARY KEY, telephone VARCHAR(100));-- Logic SQL: CREATE TABLE t_user(user_id INT(11) PRIMARY KEY, telephone VARCHAR(100))-- Actual SQL: ds_0 ::: CREATE TABLE t_user(user_id INT(11) PRIMARY KEY, telephone VARCHAR(100)) t_order 表分片功能路由改写正常，但加解密功能对应的改写没有能够支持，因为 4.1.1 GA 版本不支持加解密场景下 DDL 语句的改写，因此，需要用户在底层数据库上提前创建好对应的加解密表，DDL 语句支持加解密改写在 5.0.0 GA 版已经完美支持，减少了用户不必要的操作。 t_order_item 表由于不涉及加解密，路由改写的结果正常。t_user 表同样存在加解密 DDL 语句改写的问题，并且 t_user 表被路由到了 ds_0 数据源，这是因为我们在分片规则中配置了 defaultDataSourceName: ds_0，所以对于非分片表，都会使用这个规则进行路由。 对于 t_order 表和 t_user 表，我们通过如下 SQL 在路由结果对应的底层数据库上，手动创建加解密表。 -- ds_0 创建 t_order_0、t_order_1 和 t_userCREATE TABLE t_order_0(order_id INT(11) PRIMARY KEY, user_id INT(11), content_plain VARCHAR(100), content_cipher VARCHAR(100))CREATE TABLE t_order_1(order_id INT(11) PRIMARY KEY, user_id INT(11), content_plain VARCHAR(100), content_cipher VARCHAR(100))CREATE TABLE t_user(user_id INT(11) PRIMARY KEY, telephone_plain VARCHAR(100), telephone_cipher VARCHAR(100))-- ds_1 创建 t_order_0 和 t_order_1CREATE TABLE t_order_0(order_id INT(11) PRIMARY KEY, user_id INT(11), content_plain VARCHAR(100), content_cipher VARCHAR(100))CREATE TABLE t_order_1(order_id INT(11) PRIMARY KEY, user_id INT(11), content_plain VARCHAR(100), content_cipher VARCHAR(100)) 我们重启 Proxy 并向 t_order、t_order_item 和 t_user 表添加数据。t_order 和 t_order_item 表在插入数据过程中，会根据分片键及配置的分片策略，路由到对应的数据节点。t_user 表则根据 defaultDataSourceName 配置路由到 ds_0 数据源。 INSERT INTO t_order(order_id, user_id, content) VALUES(1, 1, TEST11), (2, 2, TEST22), (3, 3, TEST33);-- Logic SQL: INSERT INTO t_order(order_id, user_id, content) VALUES(1, 1, TEST11), (2, 2, TEST22), (3, 3, TEST33)-- Actual SQL: ds_0 ::: INSERT INTO t_order_0(order_id, user_id, content_cipher, content_plain) VALUES(2, 2, mzIhTs2MD3dI4fqCc5nF/Q==, TEST22)-- Actual SQL: ds_1 ::: INSERT INTO t_order_1(order_id, user_id, content_cipher, content_plain) VALUES(1, 1, 3qpLpG5z6AWjRX2sRKjW2g==, TEST11), (3, 3, oVkQieUbS3l/85axrf5img==, TEST33)INSERT INTO t_order_item(item_id, order_id, user_id, content) VALUES(1, 1, 1, TEST11), (2, 2, 2, TEST22), (3, 3, 3, TEST33);-- Logic SQL: INSERT INTO t_order_item(item_id, order_id, user_id, content) VALUES(1, 1, 1, TEST11), (2, 2, 2, TEST22), (3, 3, 3, TEST33)-- Actual SQL: ds_0 ::: INSERT INTO t_order_item_0(item_id, order_id, user_id, content) VALUES(2, 2, 2, TEST22)-- Actual SQL: ds_1 ::: INSERT INTO t_order_item_1(item_id, order_id, user_id, content) VALUES(1, 1, 1, TEST11), (3, 3, 3, TEST33)INSERT INTO t_user(user_id, telephone) VALUES(1, 11111111111), (2, 22222222222), (3, 33333333333);-- Logic SQL: INSERT INTO t_user(user_id, telephone) VALUES(1, 11111111111), (2, 22222222222), (3, 33333333333)-- Actual SQL: ds_0 ::: INSERT INTO t_user(user_id, telephone_cipher, telephone_plain) VALUES(1, jFZBCI7G9ggRktThmMlClQ==, 11111111111), (2, lWrg5gaes8eptaQkUM2wtA==, 22222222222), (3, jeCwC7gXus4/1OflXeGW/w==, 33333333333) 然后再执行几个简单的查询语句，看下读写分离是否生效。根据日志可以看出，t_order 和 t_order_item 表，进行了加解密改写，也正确地路由到了从库。而 t_user 表仍然路由到 ds_0 数据源上执行，规则中配置的读写分离规则没有起到作用。这是由于在 4.1.1 GA 版中，读写分离和加解密都是基于分片功能进行整合，这种方案天然限制了分片之外功能的配合使用。 SELECT * FROM t_order WHERE user_id = 1 AND order_id = 1;-- Logic SQL: SELECT * FROM t_order WHERE user_id = 1 AND order_id = 1-- Actual SQL: ds_1_slave_0 ::: SELECT order_id, user_id, content_plain, content_cipher FROM t_order_1 WHERE user_id = 1 AND order_id = 1SELECT * FROM t_order_item WHERE user_id = 1 AND order_id = 1;-- Logic SQL: SELECT * FROM t_order_item WHERE user_id = 1 AND order_id = 1-- Actual SQL: ds_1_slave_1 ::: SELECT * FROM t_order_item_1 WHERE user_id = 1 AND order_id = 1SELECT * FROM t_user WHERE user_id = 1;-- Logic SQL: SELECT * FROM t_user WHERE user_id = 1-- Actual SQL: ds_0 ::: SELECT user_id, telephone_plain, telephone_cipher FROM t_user WHERE user_id = 1 5.0.0 GA 版基于可插拔架构，对内核进行了全面地升级，内核中的各个功能都可以任意组合使用。同时，5.0.0 GA 版删除了需要用户额外配置的 defaultDataSourceName，默认通过 SingleTableRule 实现单表的元数据加载及路由。下面我们来看看相同的功能，在 5.0.0 GA 版中是如何配置和使用的，具体配置如下： rules:- !SHARDING tables: t_order: actualDataNodes: ms_ds_$0..1.t_order_$0..1 tableStrategy: standard: shardingColumn: order_id shardingAlgorithmName: t_order_inline t_order_item: actualDataNodes: ms_ds_$0..1.t_order_item_$0..1 tableStrategy: standard: shardingColumn: order_id shardingAlgorithmName: t_order_item_inline bindingTables: - t_order,t_order_item broadcastTables: - t_config defaultDatabaseStrategy: standard: shardingColumn: user_id shardingAlgorithmName: database_inline defaultTableStrategy: none: shardingAlgorithms: database_inline: type: INLINE props: algorithm-expression: ms_ds_$user_id % 2 t_order_inline: type: INLINE props: algorithm-expression: t_order_$order_id % 2 t_order_item_inline: type: INLINE props: algorithm-expression: t_order_item_$order_id % 2 - !READWRITE_SPLITTING dataSources: ms_ds_0: writeDataSourceName: ds_0 readDataSourceNames: - ds_0_slave_0 - ds_0_slave_1 loadBalancerName: ROUND_ROBIN ms_ds_1: writeDataSourceName: ds_1 readDataSourceNames: - ds_1_slave_0 - ds_1_slave_1 loadBalancerName: ROUND_ROBIN - !ENCRYPT encryptors: aes_encryptor: type: AES props: aes-key-value: 123456abc tables: t_order: columns: content: plainColumn: content_plain cipherColumn: content_cipher encryptor: aes_encryptor t_user: columns: telephone: plainColumn: telephone_plain cipherColumn: telephone_cipher encryptor: aes_encryptor 首先，从配置上来看，5.0.0 GA 版和 4.1.1 GA 版最大的区别在于不同功能之间的关系，它们是一个平级关系，不存在 4.1.1 GA 中的功能依赖，每个功能都可以通过可插拔的方式灵活加载和卸载。其次，这些功能在整合使用时，使用类似于管道的传递方式，例如：读写分离规则基于两组主从关系，聚合出两个逻辑数据源，分别是 ms_ds_0 和 ms_ds_1。数据分片规则基于读写分离聚合出的逻辑数据源，配置数据分片规则，从而又聚合出逻辑表 t_order。加解密功能则关注于列和值的改写，面向数据分片功能聚合出的逻辑表，配置加解密规则。读写分离、数据分片和加解密功能层层传递，通过装饰模式，不断对功能进行增加。 为了对比 4.1.1 GA 版功能，我们执行同样的初始化语句、插入语句和查询语句对 5.0.0 GA 版进行测试。 CREATE TABLE t_order(order_id INT(11) PRIMARY KEY, user_id INT(11), content VARCHAR(100));-- Logic SQL: CREATE TABLE t_order(order_id INT(11) PRIMARY KEY, user_id INT(11), content VARCHAR(100))-- Actual SQL: ds_1 ::: CREATE TABLE t_order_0(order_id INT(11) PRIMARY KEY, user_id INT(11), content_cipher VARCHAR(100), content_plain VARCHAR(100))-- Actual SQL: ds_1 ::: CREATE TABLE t_order_1(order_id INT(11) PRIMARY KEY, user_id INT(11), content_cipher VARCHAR(100), content_plain VARCHAR(100))-- Actual SQL: ds_0 ::: CREATE TABLE t_order_0(order_id INT(11) PRIMARY KEY, user_id INT(11), content_cipher VARCHAR(100), content_plain VARCHAR(100))-- Actual SQL: ds_0 ::: CREATE TABLE t_order_1(order_id INT(11) PRIMARY KEY, user_id INT(11), content_cipher VARCHAR(100), content_plain VARCHAR(100))CREATE TABLE t_order_item(item_id INT(11) PRIMARY KEY, order_id INT(11), user_id INT(11), content VARCHAR(100));-- Logic SQL: CREATE TABLE t_order_item(item_id INT(11) PRIMARY KEY, order_id INT(11), user_id INT(11), content VARCHAR(100))-- Actual SQL: ds_1 ::: CREATE TABLE t_order_item_0(item_id INT(11) PRIMARY KEY, order_id INT(11), user_id INT(11), content VARCHAR(100))-- Actual SQL: ds_1 ::: CREATE TABLE t_order_item_1(item_id INT(11) PRIMARY KEY, order_id INT(11), user_id INT(11), content VARCHAR(100))-- Actual SQL: ds_0 ::: CREATE TABLE t_order_item_0(item_id INT(11) PRIMARY KEY, order_id INT(11), user_id INT(11), content VARCHAR(100))-- Actual SQL: ds_0 ::: CREATE TABLE t_order_item_1(item_id INT(11) PRIMARY KEY, order_id INT(11), user_id INT(11), content VARCHAR(100))CREATE TABLE t_user(user_id INT(11) PRIMARY KEY, telephone VARCHAR(100));-- Logic SQL: CREATE TABLE t_user(user_id INT(11) PRIMARY KEY, telephone VARCHAR(100))-- Actual SQL: ds_1 ::: CREATE TABLE t_user(user_id INT(11) PRIMARY KEY, telephone_cipher VARCHAR(100), telephone_plain VARCHAR(100)) 在 5.0.0 GA 版中，增加了对加解密 DDL 语句改写的支持，因此在创建 t_order 过程中，不论是数据分片、读写分离还是加解密，路由和改写都能够正常执行。t_user 表从日志来看，被路由到 ds_1 数据源执行，在 5.0.0 GA 版中，t_user 属于单表，无需用户配置数据源，在执行建表语句时，会随机选择一个数据源进行路由。对于单表，我们需要保证它在逻辑库中唯一，从而保证路由结果的准确性。 INSERT INTO t_order(order_id, user_id, content) VALUES(1, 1, TEST11), (2, 2, TEST22), (3, 3, TEST33);-- Logic SQL: INSERT INTO t_order(order_id, user_id, content) VALUES(1, 1, TEST11), (2, 2, TEST22), (3, 3, TEST33)-- Actual SQL: ds_1 ::: INSERT INTO t_order_1(order_id, user_id, content_cipher, content_plain) VALUES(1, 1, 3qpLpG5z6AWjRX2sRKjW2g==, TEST11), (3, 3, oVkQieUbS3l/85axrf5img==, TEST33)-- Actual SQL: ds_0 ::: INSERT INTO t_order_0(order_id, user_id, content_cipher, content_plain) VALUES(2, 2, mzIhTs2MD3dI4fqCc5nF/Q==, TEST22)INSERT INTO t_order_item(item_id, order_id, user_id, content) VALUES(1, 1, 1, TEST11), (2, 2, 2, TEST22), (3, 3, 3, TEST33);-- Logic SQL: INSERT INTO t_order_item(item_id, order_id, user_id, content) VALUES(1, 1, 1, TEST11), (2, 2, 2, TEST22), (3, 3, 3, TEST33)-- Actual SQL: ds_1 ::: INSERT INTO t_order_item_1(item_id, order_id, user_id, content) VALUES(1, 1, 1, TEST11), (3, 3, 3, TEST33)-- Actual SQL: ds_0 ::: INSERT INTO t_order_item_0(item_id, order_id, user_id, content) VALUES(2, 2, 2, TEST22)INSERT INTO t_user(user_id, telephone) VALUES(1, 11111111111), (2, 22222222222), (3, 33333333333);-- Logic SQL: INSERT INTO t_user(user_id, telephone) VALUES(1, 11111111111), (2, 22222222222), (3, 33333333333)-- Actual SQL: ds_1 ::: INSERT INTO t_user(user_id, telephone_cipher, telephone_plain) VALUES(1, jFZBCI7G9ggRktThmMlClQ==, 11111111111), (2, lWrg5gaes8eptaQkUM2wtA==, 22222222222), (3, jeCwC7gXus4/1OflXeGW/w==, 33333333333) 在对 t_user 表执行数据插入时，会根据元数据中存储的信息来进行自动路由，由于前一个步骤中 t_user 路由到了 ds_1 数据源，因此其他语句会根据 t_user: ds_1 这样的元数据进行路由处理。 SELECT * FROM t_order WHERE user_id = 1 AND order_id = 1;-- Logic SQL: SELECT * FROM t_order WHERE user_id = 1 AND order_id = 1-- Actual SQL: ds_1_slave_0 ::: SELECT `t_order_1`.`order_id`, `t_order_1`.`user_id`, `t_order_1`.`content_cipher` AS `content` FROM t_order_1 WHERE user_id = 1 AND order_id = 1SELECT * FROM t_order_item WHERE user_id = 1 AND order_id = 1;-- Logic SQL: SELECT * FROM t_order_item WHERE user_id = 1 AND order_id = 1-- Actual SQL: ds_1_slave_1 ::: SELECT * FROM t_order_item_1 WHERE user_id = 1 AND order_id = 1SELECT * FROM t_user WHERE user_id = 1;-- Logic SQL: SELECT * FROM t_user WHERE user_id = 1-- Actual SQL: ds_1_slave_0 ::: SELECT `t_user`.`user_id`, `t_user`.`telephone_cipher` AS `telephone` FROM t_user WHERE user_id = 1 在执行查询语句时，我们可以发现，t_user 表被路由到了 ds_1_slave_0 数据源，实现了单表的读写分离。在 5.0.0 GA 版中，Apache ShardingSphere 内核通过元数据加载，内部维护了单表的数据分布信息，并充分考虑了不同功能组合使用的场景，使得单表也能够完美支持。 5.0.0 GA 版中还有很多新功能，升级指南中的案例只是挑选了两个 GA 版本中都能够支持的一些功能进行对比，期望能够帮助大家理解新功能，并顺序地实现功能升级。如果大家对可插拔架构、Federation 执行引擎或者其他的新功能感兴趣，欢迎参考官方文档进行测试使用。 结语 历经两年时间的打磨，Apache ShardingSphere 以全新的姿态展示在大家面前，可插拔架构内核为所有的开发者提供了无限的可能性，未来，我们将基于可插拔架构内核，不断开拓新的功能，丰富 Apache ShardingSphere 生态系统。Federation 执行引擎则打开了分布式查询的大门，后续我们将专注于内存及性能的优化，为大家提供更可靠、更高效的分布式查询能力。最后，也欢迎大家能够积极地参与进来，共同推动 Apache ShardingSphere 的发展。 参考文档 Apache ShardingSphere Release Note Brand new sharding configuration API of Release 5.x Automatic Sharding Strategies for Databases and Tables 从中间件到分布式数据库生态，ShardingSphere 5.x革新变旧 ShardingSphere X openGauss，将会产生怎样的化学反应 欢迎关注 欢迎关注「端小强的博客」微信公众号，会不定期分享日常学习和工作经验，欢迎大家关注交流。","tags":["ShardingSphere","Kernel"],"categories":["ShardingSphere"]},{"title":"CentOS 开发环境搭建笔记","path":"/blog/centos-dev-environment-setup-note.html","content":"前言 虽然日常办公使用 Mac 已经非常便利，但偶尔还是需要使用 Linux 环境进行一些开发工作。为了方便使用，本文使用 VirtualBox 搭建了一个简单的 CentOS 开发环境，同时配置了 Host-Only 和 Nat 两种网络连接方式，保证了虚拟机中的 Linux 服务器，能够同时连接本机和互联网。 准备工作 搭建 CentOS 开发环境之前，需要先完成以下准备工作： 安装 VirtualBox 6.1； 下载 CentOS-7-x86_64-Minimal-1708.iso 镜像（官网下载地址）。 (adsbygoogle = window.adsbygoogle || []).push({}); 赞助商 创建虚拟机 首先，打开 VirtualBox，然后选择 新建，创建虚拟机，然后填入名称 centos7，并选择虚拟机的类型 Linux 和版本 Red Hat (64-bit)。 然后设置虚拟机的内存大小，该设置按照实际使用场景进行调整即可。 下一步，选择虚拟硬盘的文件类型，因为不需要在其他虚拟化软件中使用，所以我们选择默认的 VDI 文件类型。然后分配文件的大小，我们暂时分配 20 GB。 选择创建，这时候我们就得到了一个 centos7 虚拟机。不过在安装之前，我们还要进行一些设置，来保证虚拟机能够正常启动和运行。 设置虚拟机 首先，我们要对虚拟机的系统启动顺序进行设置。选中列表中的虚拟机，然后选择 设置-系统，并将 软驱 移动到启动顺序的最后。 然后再选择 存储-控制器-没有盘片，点击右侧的光盘图标，分配光驱，选择前面下载的 CentOS-7-x86_64-Minimal-1708.iso 镜像文件。 设置虚拟网卡 在设置虚拟网卡前，先来了解下 VirtualBox 支持的网络模式。VirtualBox 可选的网络模式有七种，分别是 Not attached、Network Address Translation (NAT)、NAT Network、Bridged networking、Internal networking、Host-only networking 和 Generic networking。 Not attached 模式相当于没插网线，因此网络是断开的，无法连接主机和外网； Network Address Translation (NAT) 模式支持访问主机和外网，但是主机和外网及其他虚拟机都不能直接访问该虚拟机，NAT 网络模式是 VirtualBox 默认的网络模式； NAT Network 模式和 Network Address Translation (NAT) 模式类似，唯一的区别是该模式下，虚拟机之间可以相互访问； Bridged networking 模式下，虚拟机相当于内网的一台机器，因此可以访问内网中的其他机器以及外网，内网中的其他机器也可以直接访问它，在该模式下，虚拟机之间也可以相互访问； Internal networking 模式下，只有虚拟机之间可以相互访问； Host-only networking 模式下，只有虚拟机和主机、虚拟机和虚拟机之间可以相互访问； Generic networking 模式很少使用，本文暂时忽略； 虚拟机可以同时设置多张网卡，例如设置两张网卡，一张网卡使用 NAT 模式，支持访问外网，另一张网卡选择 Host-only networking 模式，虚拟机、主机以及其他虚拟机可以相互访问。 在不同的网络模式下，虚拟机、主机、局域网/外网之间的可访问规则，可以参考如下的表格。 Mode VM→Host VM←Host VM1↔VM2 VM→Net/LAN VM←Net/LAN Host-only + + + – – Internal – – + – – Bridged + + + + + NAT + Port forward – + Port forward NATservice + Port forward + + Port forward 在了解了 VirtualBox 支持的网络模式后，我们来进行虚拟网卡的设置，本文采用 Host-Only 和 Nat 组合的配置方式。 选择 菜单-管理-主机网络管理器，然后创建如下的虚拟网卡。 然后配置 centos7 虚拟机的网卡，网卡 1 配置为 Host-Only，网卡 2 配置为 Nat 。 启动虚拟机 完成虚拟机配置后，点击 启动 按钮，启动虚拟机。启动过程中，需要设置 root 用户的密码，以及创建新用户。 启动安装完成之后，点击 reboot 进行重启。 然后使用 root 用户重新登录，并查看 IP 信息，发现获取不到 IP 信息。 虚拟机 IP 设置 重启虚拟机之后，发现获取不到 IP 信息，还需要配合网卡设置虚拟机 IP。首先，进入 /etc/sysconfig/network-scripts/ 目录下，查看该目录下的网卡文件，存在两个网卡 enp0s3，enp0s8。 修改 enp0s3，enp0s8 网卡配置： # Host-Only网卡，设置成静态ip，用于与主机器通信# vim /etc/sysconfig/network-scripts/ifcfg-enp0s3TYPE=EthernetPROXY_METHOD=noneBROWSER_ONLY=noBOOTPROTO=staticIPADDR=192.168.56.101DEFROUTE=yesIPV4_FAILURE_FATAL=noIPV6INIT=yesIPV6_AUTOCONF=yesIPV6_DEFROUTE=yesIPV6_FAILURE_FATAL=noIPV6_ADDR_GEN_MODE=stable-privacyNAME=enp0s3UUID=5d50000d-2081-4e44-8806-a4c1024b0d51DEVICE=enp0s3ONBOOT=yes# Nat网卡，设置成动态获取ip，用于连接互联网# vim /etc/sysconfig/network-scripts/ifcfg-enp0s8TYPE=EthernetPROXY_METHOD=noneBROWSER_ONLY=noBOOTPROTO=dhcpDEFROUTE=yesIPV4_FAILURE_FATAL=noIPV6INIT=yesIPV6_AUTOCONF=yesIPV6_DEFROUTE=yesIPV6_FAILURE_FATAL=noIPV6_ADDR_GEN_MODE=stable-privacyNAME=enp0s8UUID=27155750-2243-48d2-895d-8b79b0fd0d64DEVICE=enp0s8ONBOOT=yes 然后使用如下命令重启网络，发现并没有生效。 service network restart 查阅资料后，发现需要关闭网络管理器 NetwokManager，才能够使静态 IP 生效。 systemctl stop NetworkManagersystemctl disable NetworkManager 关闭 NetwokManager 之后，再次修改网卡配置，并重启网络服务，发现 IP 已经配置完成。 最后，使用 Mac 命令行连接虚拟机进行测试，输入 ssh root@192.168.56.101，终于成功登录上服务器。 现在，可以享受在 Linux 环境开发的乐趣了。 欢迎关注 欢迎关注「端小强的博客」微信公众号，会不定期分享日常学习和工作经验，欢迎大家关注交流。","tags":["Linux","VirtualBox","CentOS"],"categories":["Linux"]},{"title":"ANTLR 基础入门","path":"/blog/introduction-to-antlr.html","content":"什么是 ANTLR ANTLR (ANother Tool for Language Recognition) is a powerful parser generator for reading, processing, executing, or translating structured text or binary files. It’s widely used to build languages, tools, and frameworks. From a grammar, ANTLR generates a parser that can build and walk parse trees. 根据官网定义，ANTLR 是一款强大的语法分析器生成工具，可用于读取、处理、执行或者翻译结构化文本或二进制文件。ANTLR 根据语法，可以生成对应的语法分析器，并自动构建语法分析树（一种描述语法和输入文本匹配关系的数据结构），通过自动生成的语法分析树的遍历器，用户可以方便地执行自定义的业务逻辑代码。 ANTLR 被广泛应用于学术及工业领域，是众多语言、工具及框架的基石。Hive、ShardingSphere 使用 ANTLR 实现 SQL 的词法和语法解析，Hibernate 框架使用 ANTLR 来处理 HQL 语言。除了这些著名的项目之外，还可以用 ANTLR 来构建各种实用的工具，例如：配置文件读取工具、历史代码转换工具、JSON 解析器等。学习并使用 ANTLR，能够有效地提高我们工作中处理问题的效率，让我们事半功倍。 安装 ANTLR ANTLR 由 Java 语言编写，因此在安装之前需要先安装 Java，ANTLR 运行需要的 Java 版本为 1.6 及以上。安装 ANTLR 非常简单，只需要下载最新的 jar 包——antlr-4.8-complete.jar，jar 包提供了两部分功能： 一个将语法转换成词法分析器和语法分析器的工具； 生成的词法分析器、语法分析器依赖的运行时环境； 通过 ANTLR 工具，能够将用户定义的语法文件转换成可以识别该语法文件所描述语言的程序。 首先下载最新版 jar 包，并将 jar 包加入到 CLASSPATH 环境变量中： cd /usr/local/libsudo curl -O https://www.antlr.org/download/antlr-4.8-complete.jar# 设置环境变量，点号代表当前目录export CLASSPATH=.:/usr/local/lib/antlr-4.8-complete.jar:$CLASSPATH 配置完成后，可以通过如下的两种方式来检查 ANTLR 是否正确安装： # 第一种：java -jar 直接运行 ANTLR 的 jar 包java -jar /usr/local/lib/antlr-4.8-complete.jar# 第二种：直接调用 org.antlr.v4.Tool 类java org.antlr.v4.Tool# 得到以下结果代表正确安装# ANTLR Parser Generator Version 4.8# -o ___ specify output directory where all output is generated# -lib ___ specify location of grammars, tokens files# -atn generate rule augmented transition network diagrams# -encoding ___ specify grammar file encoding; e.g., euc-jp# -message-format ___ specify output style for messages in antlr, gnu, vs2005# -long-messages show exception details when available for errors and warnings# -listener generate parse tree listener (default)# -no-listener dont generate parse tree listener# -visitor generate parse tree visitor# -no-visitor dont generate parse tree visitor (default)# -package ___ specify a package/namespace for the generated code# -depend generate file dependencies# -Doption=value set/override a grammar-level option# -Werror treat warnings as errors# -XdbgST launch StringTemplate visualizer on generated code# -XdbgSTWait wait for STViz to close before continuing# -Xforce-atn use the ATN simulator for all predictions# -Xlog dump lots of logging info to antlr-timestamp.log# -Xexact-output-dir all output goes into -o dir regardless of paths/package 为了简化执行命令，可以设置如下别名，以后使用 antlr4 命令即可： alias antlr4=java -Xmx500M -cp /usr/local/lib/antlr-4.8-complete.jar:$CLASSPATH org.antlr.v4.Tool 按照惯例，让我们首先来编写一个简单的Hello World程序来初步认识 ANTLR。首先，需要创建一个语法文件HelloWorld.g4，用来描述基本的语法规范，文件内容如下： grammar HelloWorld; // 定义一个名为HelloWorld的语法r : hello ID ; // 定义一个语法规则，匹配一个关键字hello和一个紧随其后的标识符ID : [a-z]+ ; // 匹配小写字母组成的标识符WS : [ \\t\\r ]+ - skip ; // 忽略空格、Tab、换行以及\\r 文件开头的 grammar HelloWorld 定义了语法名，ANTLR 中规定语法名必须和文件名保持一致。r 为语法规则，必须以小写字母开头。ID 和 WS 为词法规则，必须以大写字母开头。定义好语法文件之后，需要使用前文定义的 antlr4 命令来生成词法分析器和语法分析器： # 生成词法分析器和语法分析器antlr4 HelloWorld.g4# 查看生成文件ls -all# drwxr-xr-x 11 duanmu staff 352 6 1 23:25 .# drwx------@ 31 duanmu staff 992 6 1 23:24 ..# -rw-r--r-- 1 duanmu staff 321 6 1 23:24 HelloWorld.g4# -rw-r--r-- 1 duanmu staff 308 6 1 23:25 HelloWorld.interp# -rw-r--r-- 1 duanmu staff 27 6 1 23:25 HelloWorld.tokens# -rw-r--r-- 1 duanmu staff 1334 6 1 23:25 HelloWorldBaseListener.java# -rw-r--r-- 1 duanmu staff 1055 6 1 23:25 HelloWorldLexer.interp# -rw-r--r-- 1 duanmu staff 3582 6 1 23:25 HelloWorldLexer.java# -rw-r--r-- 1 duanmu staff 27 6 1 23:25 HelloWorldLexer.tokens# -rw-r--r-- 1 duanmu staff 571 6 1 23:25 HelloWorldListener.java# -rw-r--r-- 1 duanmu staff 3899 6 1 23:25 HelloWorldParser.java 在 ANTLR 生成的所有文件中，主要作用如下： HelloWorldLexer.java——该文件包含了一个词法分析器类的定义，ANTLR 通过定义的词法规则，将输入字符序列解析成词汇符号，词法分析器定义如下： public class HelloWorldLexer extends Lexer ... HelloWorldParser.java——该文件包含了一个语法分析器类的定义，语法分析器专门用来识别前文定义的 'hello' ID 语法规则，语法分析器定义如下： public class HelloWorldParser extends Parser ... HelloWorld.tokens——ANTLR 会给我们定义的词法符号指定一个数字形式的类型，然后将他们的对应关系存储到该文件中，通过 tokens 中的内容，ANTLR 可以在多个小型语法间同步全部的词法符号类型，tokens 内容如下： T__0=1ID=2WS=3hello=1 HelloWorldListener.java——ANTLR 默认会生成语法规则对应的语法分析树，在遍历语法分析树时，会触发一系列事件，并通知 HelloWorldListener 监听器对象。HelloWorldBaseListener.java 是该接口的默认实现，我们只需要重写感兴趣的回调方法即可，监听器实现如下： public interface HelloWorldListener extends ParseTreeListener ... 生成文件之后，需要执行 javac *.java，将生成的文件进行编译。ANTLR 提供了一个名为 TestRig 的调试工具，可以详细列出匹配输入文本过程中的信息，该工具类似于一个 main 方法，参数中接收一个语法名和一个起始规则名，前文案例中语法名为 HelloWorld，起始规则名为 r。可以为 java org.antlr.v4.gui.TestRig 命令设置别名，方便后面操作： alias grun=java org.antlr.v4.gui.TestRig 然后执行以下命令 grun HelloWorld r -tokens 显示识别过程中生成的词法符号： # 使用 Hello 语法和 r 规则启动 TestRiggrun HelloWorld r -tokens# 输入要识别的语句hello world# 输入回车符结束输入（Mac 下输入 Crtl + D，Win 下输入 Ctrl + Z）# [@0,0:4=hello,hello,1:0]# [@1,6:10=world,ID,1:6]# [@2,12:11=EOF,EOF,2:0] 每行输出代表一个词法符号，以 world 词法符号为例，输出结果为 [@1,6:10='world',ID,1:6]，@1 表示第二个词法符号（从 0 开始），由输入文本的第 6 个位置到第 10 个位置的文本组成（从 0 开始），内容是 world，词法符号类型为 ID，位于文本的第 1 行（从 1 开始），第 6 个位置处（从 0 开始）。 使用 -gui 参数可以图形化展示出语法分析树： grun HelloWorld r -guihello world 在命令行直接输入 grun，可以查看其他参数的帮助信息： grun# java org.antlr.v4.gui.TestRig GrammarName startRuleName# [-tokens] [-tree] [-gui] [-ps file.ps] [-encoding encodingname]# [-trace] [-diagnostics] [-SLL]# [input-filename(s)]# Use startRuleName=tokens if GrammarName is a lexer grammar.# Omitting input-filename makes rig read from stdin.# -tokens 打印出词法符号流# -tree 以Lisp格式打印语法树# -gui 可视化方式展示语法树# -ps file.ps 以PostScript格式生成可视化语法树，然后存储到file.ps中# -encoding encodingname 指定编码# -trace 打印规则名称，及离开规则时的词法符号# -diagnostics 开启解析过程中的调试信息输出# -SLL 使用功能稍弱的解析策略 为了方便操作，可以将前文相关的命令添加到 .bash_profile 文件中，避免重复设置： # 设置到~/.bash_profile中sudo vim ~/.bash_profile# .bash_profile添加内容export CLASSPATH=.:/usr/local/lib/antlr-4.8-complete.jar:$CLASSPATHalias antlr4=java -Xmx500M -cp /usr/local/lib/antlr-4.8-complete.jar:$CLASSPATH org.antlr.v4.Tool# 为java TestRig命令设置别名alias grun=java -Xmx500M -cp /usr/local/lib/antlr-4.8-complete.jar:$CLASSPATH org.antlr.v4.gui.TestRig (adsbygoogle = window.adsbygoogle || []).push({}); 赞助商 理解 ANTLR 语法分析 ANTLR 语法分析流程 完成了 ANTLR 安装之后，我们来了解下 ANTLR 中的语法分析流程。ANTLR 的语法分析流程和我们大脑阅读文章的过程类似，在阅读一个句子前，我们会通过潜意识将单个字符聚集成单词，然后获取每个单词的含义，再理解整个句子的含义。 ANTLR 语法分析可以分为两个阶段： 第一个阶段为词法分析阶段，将字符聚集为单词或者符号（词法符号 token）的过程称为词法分析（lexical analysis），通常把可以将输入文本转换为词法符号的程序称为词法分析器（lexer），词法分析器可以将相关的词法符号归类，例如：INT（整数）、ID（标识符）、FLOAT（浮点数）等； 第二个阶段为语法分析阶段，输入的词法符号被消费来识别语法结构，ANTLR 生成的语法分析器会构建一个语法分析树（parse tree）数据结构，该数据结构记录了语法分析器识别出输入语句的过程，以及该结构的各组成部分； 以赋值语句 sp = 100; 为例，ANTLR 会根据如下的语法规则生成词法分析器和语法分析器： assign : ID = expr ; ; 整个语法分析的过程如下： 首先，输入的字符串 sp = 100;，经过词法分析器 lexer 可以转换为多个词法符号，再经过语法分析器 parser，生成对应的语法分析树。语法分析树的内部节点是词组名（对应语法规则中的 assign 和 expr），这些名字用于识别它们的子节点，并将子节点归类。根节点是一个抽象的名字，此处为 stat（statement 的缩写），叶子节点对应输入的词法符号。 ANTLR 工具根据前文定义的 assign 语法规则，会生成一个递归下降的语法分析器（recursive-descent parsers）。递归下降的语法分析器实际是若干递归方法的集合，每个方法对应一条规则，下降的过程就是从语法分析树的根节点开始，朝着叶子节点（词法符号）进行解析的过程。 ANTLR 根据 assign 规则生成的的方法大致实现如下： // assign : ID = expr ; ;void assign() // 根据assign规则生成的方法 match(ID); // 将当前输入的符号和ID比较，然后将其消费掉 match(=); expr(); // 通过调用expr()方法来匹配一个表达式 match(;); assign() 方法主要验证词汇符号是否存在，以及是否满足语法规定的顺序。调用 match() 方法则对应语法分析树的叶子节点。通过stat()、assign() 和 expr() 的调用描述出的调用路线图可以很好地映射到语法分析树的节点上。 在 ANTLR 中，assign 语法规则对应的语法分析树，可以映射成如下类型： 左图中 stat、assign、expr 代表的是规则节点（RuleNode），对应 ANTLR 语法定义中的规则名称，sp、100 对应的是终端节点（TerminalNode），即词法符号。 右图中的 StatContext、AssignContext、ExprContext 为 RuleNode 的子类，代表该节点的上下文信息，包括词法符号及其开始和结束位置等，同时提供了访问该节点中全部元素的方法，例如：AssignContext 类提供了方法 ID() 和方法 expr() 来访问标识符节点和代表表达式的子树。TerminalNode 则代表叶子结点信息，没有子节点。 我们可以手动编写出访问语法分析树的代码，来访问 Context 和 TerminalNode 中存储的信息，从而实现结果计算、数据结构更新、打印输出等功能。但实际上，ANTLR 已经自动生成了语法分析树的遍历器，可以直接供我们使用，下面我们就来了解下 ANTLR 提供的两种遍历树的机制。 ANTLR 语法树遍历 ANTLR 的运行库提供了两种遍历树的机制——语法分析树监听器和语法分析树访问器。ANTLR 默认会生成语法分析树监听器，内置的 ParseTreeWalker 类会进行深度优先遍历（如下图所示），遍历树的不同节点时，会触发不同的事件，语法分析树监听器会对不同的事件作出相应的处理。 ANTLR 默认为每个语法文件生成了一个 ParseTreeListener 的子类，语法中的每条规则都有对应的 enter 和 exit 方法，用户可以自行实现 ParseTreeListener 接口，来实现自己的业务逻辑。ParseTreeWalker 类对 ParseTreeListener 接口完整的调用流程如下图： 监听器机制的优势在于对语法分析树的遍历是自动的，用户无需编写遍历语法分析树的代码，也无需让监听器显示地访问子节点。 如果用户希望控制遍历语法分析树的过程，想要显示地访问子节点，那么可以使用语法分析树访问器。前文 HelloWorld 入门案例中，添加 -visitor 参数，即可生成对应的语法分析树访问器： antlr4 HelloWorld.g4 -visitor 通常，语法分析树访问器对树的遍历过程如下： ANTLR 默认会提供访问器接口及一个默认实现类，用户只需要实现自己感兴趣的方法即可。 ANTLR 实战 前文我们介绍了 ANTLR 的语法分析过程，了解了语法分析树遍历的两种机制——监听器和访问器分别是如何运行的。下面我们将通过一个简单的案例——简版计算器，来演示实战中如何使用访问器实现具体的功能，监听器使用大家可以自行参考官方文档或文末的参考书籍进行尝试。 为了方便实现，我们要实现的简版计算器功能，暂时只支持基本的整数加减乘除。下面的示例包含了计算器功能的全部特性： a = 5b = 6a + b * 2(1 + 2) * 3 根据示例，我们可以抽取出一些计算器语法规则的特征——计算器中的表达式语言由一系列的语句组成，每个语句都由换行符终止。一个语句可以是一个表达式，也可以是一个赋值语句或一个空行。 根据语法规则特征，我们可以编写出如下的语法规则： grammar Calculator;// 引入Literals词法规则import Literals;// 起始规则prog : stat+ ;stat : CLEAR NEWLINE # clear | NEWLINE # blank | expr NEWLINE # printExpr | ID = expr NEWLINE # assign ;expr : expr op=(*|/) expr # mulDiv | expr op=(+|-) expr # addSub | INT # int | ID # id | ( expr ) # parens ; Calculator 语法规则中定义 prog 为起始规则，包含了一个或多个子规则 stat+，expr 语法规则中定义了加减乘除运算规则以及括号运算。stat 和 expr 语法规则中使用|来分隔若干备选分支，由于 ANTLR 默认只会为每个规则生成一个方法，不方便对每个备选分支进行操作，因此需要给每个备选分支加上标签，标签以 # 开头，可以是任意符号，但是不能与规则名冲突，加上标签后，就可以方便地获取每个备选分支对应的事件。 语法中使用圆括号()可以把一些符合组合成子规则，例如：op=('*'|'/')，其中 op 为词法符号标签，('*'|'/') 为乘法或除法组合的子规则。 Calculator 语法规则中使用了 import 语法导入，语法导入适合用于将非常大的语法拆分成较小的逻辑单元，通常是将语法拆分成两部分：语法分析器语法和词法分析器语法。通常，语法分析器语法定义使用 grammar 进行声明，而词法分析器语法定义则使用 lexer grammar 声明。Calculator 语法规则中导入的词法分析器 Literals 如下，定义了计算器程序中所需的整数、ID、换行符、加减乘除符号等词法符号： lexer grammar Literals;import Alphabet;// 清除中间变量CLEAR : C L E A R ;// 整数词法符号INT : [0-9]+ ;// ID词法符号ID : [a-zA-Z]+ ;// 换行符NEWLINE : \\r? ;// 忽略空白符WS : [ \\t\\r ]+ - skip ;// 乘法MUL : * ;// 除法DIV : / ;// 加法ADD : + ;// 减法SUB : - ; 区别于前文直接使用命令行的方式，本例中会使用 IDEA 进行计算器程序开发，首先需要在 IDEA 中安装 ANTLR v4 插件，该插件可以快速地对语法规则进行解析，生成语法分析树。 以前文的语法规则为例，我们使用 ANTLR v4 插件进行语法解析，例如：选中语法规则 stat，然后右击，选择 Test Rule stat，出现 ANTLR Preview 界面，输入 a = 5，右侧能够实时显示出对应的语法分析树，语法调试非常方便。 其次，我们需要在项目中引入antlr4-maven-plugin插件，该插件规定了语法规则文件的路径，默认路径如下： src/main/ | +--- antlr4/... .g4 files organized in the required package structure | +--- imports/ .g4 files that are imported by other grammars. 引入该插件只需要在 maven 中添加如下插件，libDirectory 指定需要语法导入的规则所在的路径，listener 和 visitor 分别对应是否生成语法分析树监听器和访问器： build plugins plugin groupIdorg.antlr/groupId artifactIdantlr4-maven-plugin/artifactId executions execution idantlr/id configuration libDirectorysrc/main/antlr4/imports//libDirectory listenerfalse/listener visitortrue/visitor /configuration goals goalantlr4/goal /goals /execution /executions /plugin /plugins/build 安装好插件后，执行 mvn package 会自动生成语法分析树访问器，生成的代码位于 target/generated-sources 目录下： 下面，我们需要编写一个计算器表达式解析程序的访问器，由于返回的结果为只包含整数，因此泛型可以声明为 Integer，访问器代码实现如下： /** * Desc: Calculator 访问器 * Date: 2020/7/4 * * @author duanzhengqiang */public class CalculatorEvalVisitor extends CalculatorBaseVisitorInteger /** * 计算器程序中间变量存储 */ private final MapString, Integer calculatorMemory = new HashMap(); /** * 解析 expr NEWLINE 规则 * * @param ctx * @return */ @Override public Integer visitPrintExpr(CalculatorParser.PrintExprContext ctx) // 解析expr子节点的值 Integer value = visit(ctx.expr()); // 打印结果并返回一个假值 System.out.println(value); return 0; /** * 解析 ID = expr NEWLINE 规则 * * @param ctx * @return */ @Override public Integer visitAssign(CalculatorParser.AssignContext ctx) String id = ctx.ID().getText(); // 解析expr子节点的值 Integer value = visit(ctx.expr()); calculatorMemory.put(id, value); return 0; /** * 解析 NEWLINE * * @param ctx * @return */ @Override public Integer visitBlank(CalculatorParser.BlankContext ctx) System.out.println(); return 0; /** * 解析 CLEAR * * @param ctx * @return */ @Override public Integer visitClear(CalculatorParser.ClearContext ctx) calculatorMemory.clear(); System.out.println(clear success!); return 0; /** * 解析 expr (*|/) expr 规则 * * @param ctx * @return */ @Override public Integer visitMulDiv(CalculatorParser.MulDivContext ctx) Integer leftValue = visit(ctx.expr(0)); Integer rightValue = visit(ctx.expr(1)); // 判断操作符控制乘除法 if (ctx.op.getType() == CalculatorParser.MUL) return leftValue * rightValue; return leftValue / rightValue; /** * 解析 expr op=(+|-) expr 规则 * * @param ctx * @return */ @Override public Integer visitAddSub(CalculatorParser.AddSubContext ctx) Integer leftValue = visit(ctx.expr(0)); Integer rightValue = visit(ctx.expr(1)); // 判断操作符控制加减法 if (ctx.op.getType() == CalculatorParser.ADD) return leftValue + rightValue; return leftValue - rightValue; /** * 解析 INT * * @param ctx * @return */ @Override public Integer visitInt(CalculatorParser.IntContext ctx) return Integer.parseInt(ctx.INT().getText()); /** * 解析 ID * * @param ctx * @return */ @Override public Integer visitId(CalculatorParser.IdContext ctx) // 获取中间存储的变量 String id = ctx.ID().getText(); return calculatorMemory.getOrDefault(id, 0); /** * 解析 ( expr ) 规则 * * @param ctx * @return */ @Override public Integer visitParens(CalculatorParser.ParensContext ctx) // 返回子表达式的值 return visit(ctx.expr()); 对于赋值语句（ID '=' expr NEWLINE），会将解析结果存储到计算器的内存中，当用户输入对应的变量名时（ID），会从内存中获取该值，如果用户输入 CLEAR，则会清空内存中存储的值。对于乘除法，由于位于语法规则的最左边，处理时优先执行，根据词法符号标签op的类型，执行相应的乘法或除法操作，加减法解析与乘除法则类似。 最后，我们需要编写一个测试程序，来验证计算器程序是否正常运行。我们从 calculator_test.txt 文件中读取一些常见的计算操作，然后转换为字符流输入到词法解析器中，词法解析器负责将字符流拆分为词法符号，并且存储到 tokenStream 中，最后语法解析器会将词法符号解析成语法分析树。语法分析器访问器 CalculatorEvalVisitor 对语法分析树进行解析，返回对应的结果。 /** * Desc: 计算器程序测试类 * Date: 2020/7/4 * * @author duanzhengqiang */public class CalculatorTest @Test public void testCalculatorVisitor() throws IOException InputStream inputStream = CalculatorTest.class.getClassLoader().getResourceAsStream(calculator_test.txt); // 字符流 CharStream charStream = CharStreams.fromStream(Objects.requireNonNull(inputStream)); // 词法分析器拆分词法符号 CalculatorLexer lexer = new CalculatorLexer(charStream); // 词法符号存储到tokenStream中 CommonTokenStream commonTokenStream = new CommonTokenStream(lexer); // 语法解析器接受词法符号，构建AST CalculatorParser parser = new CalculatorParser(commonTokenStream); // 语法解析 ParseTree parseTree = parser.prog(); // 创建语法树访问器执行计算器逻辑 CalculatorEvalVisitor visitor = new CalculatorEvalVisitor(); visitor.visit(parseTree); 执行测试程序，根据文本中输入的表达式，可以计算得到如下结果： # 输入文本a = 5b = 6a + b * 2(1 + 2) * 3b = 9(a * b) / (a + 1)CLEARab# 输出结果clear success! 本案例源码参考 Github 仓库中的 antlr-learn/calculator。ANTLR 的功能非常强大，上面的示例只是一个入门的应用，大家如果对 ANTLR 感兴趣，欢迎参与 ShardingSphere 项目的 SQL 解析，在实战中加深对 ANTLR 的理解。 欢迎关注 欢迎关注「端小强的博客」微信公众号，会不定期分享日常学习和工作经验，欢迎大家关注交流。","tags":["Antlr"],"categories":["Antlr"]},{"title":"探秘 ShardingSphere 5.0.0 beta 版内核增强","path":"/blog/explore-shardingsphere-5.0.0-beta-kernel-enhancement.html","content":"本文首发于 Apache ShardingSphere 微信公众号，欢迎关注公众号，后续将会有更多技术分享。 前言 在去年 10 月 5.0.0-alpha 版发布之后，Apache ShardingSphere 经历了长达 8 个多月的持续开发与优化，终于在 6 月 25 日正式迎来了 5.0.0-beta 版本的发布。本次 5.0.0-beta 版除了提供 DistSQL 这样的新特性外，对 ShardingSphere 内核也进行了增强，主要体现在 SQL 基础解析能力增强、SQL 标准路由能力提升和 SQL 分布式查询能力增强这三方面。通过这三方面优化，不仅进一步提高了对 MySQL，PostgreSQL，SQLServer 和 Oracle 数据库的基础 SQL 解析能力，而且大幅度提高了对用户 SQL 的支持度，特别针对跨数据库实例的关联 SQL 进行了更有针对性的优化。本文将带领大家一起，探秘 5.0.0-beta 版内核增强特性。 内核原理 在探秘 5.0.0-beta 版内核增强之前，让我们先来回顾下 ShardingSphere 的内核原理。如下图所示，ShardingSphere 内核主要由 解析引擎、路由引擎、改写引擎、Standard 执行引擎、Federate 执行引擎、归并引擎 等组成，Federate 执行引擎是本次 5.0.0-beta 版本引入的新功能，用于增强分布式查询能力。 解析引擎：解析引擎负责进行 SQL 解析，具体可以分为词法分析和语法分析。词法分析负责将 SQL 语句拆分为一个个不可再分的单词，然后语法分析器对 SQL 进行理解，并最终得到解析上下文。解析上下文包括表、选择项、排序项、分组项、聚合函数、分页信息、查询条件以及可能需要修改的占位符标记； 路由引擎：路由引擎根据解析上下文，匹配用户配置的分片策略，并生成路由结果，目前支持分片路由和广播路由； 改写引擎：改写引擎负责将 SQL 改写为在真实数据库中可以正确执行的语句，SQL 改写可以分为正确性改写和优化改写； Standard 执行引擎：Standard 执行引擎负责将路由和改写完成之后的真实 SQL 安全且高效地发送到底层数据源执行； Federate 执行引擎：Federate 执行引擎负责处理跨多个数据库实例的分布式查询，底层使用的 Calcite 基于关系代数和 CBO 优化，通过最优执行计划查询出结果； 归并引擎：归并引擎负责将从各个数据节点获取的多数据结果集，组合成为一个结果集并正确的返回至请求客户端； 在回顾了 ShardingSphere 内核原理后，下面让我们来具体看看 5.0.0-beta 版内核增强。 (adsbygoogle = window.adsbygoogle || []).push({}); 赞助商 SQL 基础解析能力增强 SQL 解析引擎是 ShardingSphere 项目的基石，也是项目中最稳定的基础设施。在 5.0.0-alpha 版中，我们将 SQL 解析引擎与主项目完全剥离，为开发者提供了一套独立的 SQL 解析引擎组件，相比其他老牌 SQL 解析引擎，ShardingSphere SQL 解析引擎具有易于扩展和更完善的 SQL 方言支持等特性。目前，用户可将 ShardingSphere SQL 解析引擎作为独立解析器，进行 SQL 解析，详见官网链接。 在本次发布的 5.0.0-beta 中，我们更加关注 SQL 解析引擎最重要的两个衡量指标——性能和 SQL 支持度。对于性能问题，ShardingSphere 已通过缓存将 SQL 解析的性能损耗降至最低。对于社区一直关注的 SQL 支持度问题，ShardingSphere 结合多个不同反馈渠道，在本次发布的 5.0.0-beta 版中进行了大量的 SQL 解析优化和支持度提升。 首先是 ShardingSphere 社区通过协议层反推过来的 SQL 优化，在 SQL 支持度提升的同时，Proxy 接入端也越来越稳定，特别是 ShardingSphere-Proxy PostgreSQL 5.0.0-beta 版，在各个方面都有较大提升，欢迎大家下载使用。此外，针对 MySQL，PostgreSQL，openGauss 数据库的 Proxy 接入端介绍，也会在后续为大家带来技术分享。 其次是 SphereEx 性能测试团队，在使用 sysbench 和 tpcc 进行压测过程中，反馈了很多测试用例中不支持的 SQL。针对 SphereEx 性能测试团队反馈的 SQL 不支持项，我们在 5.0.0-beta 版进行了针对性优化，目前已经全部支持。 针对社区反馈问题较多的 PostgreSQL，SQLServer 和 Oracle 等数据库中的 SQL 支持度问题，ShardingSphere 社区通过核心团队成员领导支持、社区同学大规模参与的方式进行提升。特别是在本次作为 Apache 优秀社区参加的 Google Summer Code 中，海外同学做出了较大贡献。 在众多社区贡献者的努力之下，ShardingSphere 5.0.0-beta 版的 SQL 支持度取得了大幅度提升，为了打造更好的项目基石，我们会持续提升优化 SQL 支持度，期待有更多的贡献者可以参与到这项工作中来，一起提升 SQL 支持度。 SQL 标准路由能力提升 在 SQL 支持度提升的基础上，ShardingSphere 5.0.0-beta 版也对 SQL 路由逻辑进行了增强，重点优化了 DDL 语句 和 DQL 语句的路由逻辑。 在 5.0.0-beta 版优化 DDL 语句路由逻辑前，路由引擎只能处理 DDL 语句中单表的路由，对于包含多表的场景，路由处理并不是很完善。 以 ALTER TABLE 语句为例，假设 t_order 和 t_order_item 为分片表，并且未设置为绑定表关系。在优化前执行如下 SQL 会抛出 Table t_order_item does not exist. 异常，路由逻辑只会针对 t_order 表进行路由，忽视了 t_order_item 表的数据分布情况。 ALTER TABLE t_order ADD CONSTRAINT t_order_fk FOREIGN KEY (order_id) REFERENCES t_order_item (order_id); 想要支持 DDL 语句多表组合路由，需要考虑许多复杂的组合场景。按照 ShardingSphere 中对于表的分类，我们可以将表划分为分片表（sharding table）、广播表（broadcast table）和单表（single table），分片表又可以组成绑定表（binding table）。关于表的详细概念可以参考下面的说明。 分片表（sharding table）：又叫逻辑表，水平拆分的数据库（表）的相同逻辑和数据结构表的总称。例：订单数据根据主键尾数拆分为 10 张表，分别是 t_order_0 到 t_order_9，他们的逻辑表名为 t_order； 绑定表（binding table）：指分片规则一致的主表和子表。例如：t_order 表和 t_order_item 表，均按照 order_id 分片，则此两张表互为绑定表关系； 广播表（broadcast table）：指所有的分片数据源中都存在的表，表结构和表中的数据在每个数据库中均完全一致。适用于数据量不大且需要与海量数据的表进行关联查询的场景，例如：字典表； 单表（single table）：指所有的分片数据源中只存在唯一一张的表。适用于数据量不大且不需要做任何分片操作的场景； 对于以上三种主要类型的表进行排列组合，可以得到如下 9 种组合场景。 针对这 9 种表的组合场景，ShardingSphere 5.0.0-beta 版对 ShardingTableBroadcastRoutingEngine 路由引擎进行了增强，完全支持分片表/广播表和其他类型表的组合路由。当 SQL 语句中包含的表都为分片表，并且都是绑定表关系时，会按照原有主表驱动路由的方式进行处理。当 SQL 语句中包含的表都为分片表，但不是绑定表关系时，或者 SQL 语句中的部分表为分片表时，路由引擎会按照表所属的数据源先取交集，然后再对同数据源的物理表计算笛卡尔积，得到最终的路由结果。 由于表的组合关系复杂，路由结果也存在多种情况。当分片表只配置了单个数据节点，并且分布在同一数据源时，DDL 语句多表组合的笛卡尔积路由结果是合法的，而当分片表配置了多个数据节点时，笛卡尔积路由结果往往是非法的。路由引擎需要能够判断出合法路由结果和非法路由结果，对于非法的路由结果，路由引擎需要抛出合适的异常信息。 为了保证用户使用 ShardingSphere 的安全性，针对不支持的 SQL 或非法 SQL，ShardingSphere 引入了前置校验（pre validate）和后置校验（post validate）。前置校验主要用于校验 SQL 语句的基本信息是否合法，如：表是否存在、索引是否存在、多个单表是否存在于同一个数据源中。后置校验主要用于校验路由的结果是否合法，如：在 ALTER TABLE 语句中添加外键约束时，我们认为所有的主表（primary table）都成功添加外键约束为合法路由结果，否则将抛出异常信息。 对于 DQL 语句路由逻辑的优化，主要是针对跨数据库实例 JOIN 及子查询进行的。路由引擎在处理 DQL 语句时，如果当前语句中的表跨多个数据库实例，则会使用 ShardingFederatedRoutingEngine 路由引擎来处理。在下面一个部分，将会对 SQL 分布式查询能力增强进行介绍。 SQL 分布式查询能力增强 在 ShardingSphere 5.0.0-beta 版前，跨数据库实例进行 JOIN 及子查询一直是令用户头疼的问题。在同时使用多个数据库实例时，业务研发人员需要时刻注意查询 SQL 的使用范畴，尽量避免跨数据库实例进行 JOIN 及子查询，这使得业务层面的功能受到了数据库限制。 在 ShardingSphere 5.0.0-beta 版中，借助于 Apache Calcite 和 ShardingSphere 自身的解析、路由和执行能力，通过路由引擎进行判断，将跨数据实例的分布式查询 SQL，交由 Federate 执行引擎处理，完美支持了跨数据库实例的 JOIN 及子查询。 同时，针对 ShardingSphere 尚不支持的一些复杂查询语句，我们也在最新的 master 分支进行了尝试，使用 Federate 执行引擎进行处理，目前已经取得了良好的效果。例如：查询语句使用 Having 过滤，子查询使用聚合函数，多聚合函数组合查询等语句，已经得到了支持，支持的 SQL 样例如下。 SELECT user_id, SUM(order_id) FROM t_order GROUP BY user_id HAVING SUM(order_id) 10;SELECT (SELECT MAX(user_id) FROM t_order) a, order_id FROM t_order;SELECT COUNT(DISTINCT user_id), SUM(order_id) FROM t_order; ShardingSphere 最新 SQL 语句支持情况可以参考官方文档。5.0.0-beta 版对于分布式查询能力的增强是一个良好的开端，未来 ShardingSphere 将持续优化，不断增强分布式查询能力。 结语 Apache ShardingSphere 项目仍然在快速发展中，在后续的版本中，我们将持续提升各种数据库的 SQL 支持度，不断完善内核功能，努力为社区提供更多强大的功能，欢迎持续关注并积极参与社区任务。 参考文档 新版发布｜ShardingSphere 5.0.0-beta 来了 细数 ShardingSphere 5.0.0-beta 版本新功能 从中间件到分布式数据库生态，ShardingSphere 5.x革新变旧 欢迎关注 欢迎关注「端小强的博客」微信公众号，会不定期分享日常学习和工作经验，欢迎大家关注交流。","tags":["ShardingSphere","Kernel"],"categories":["ShardingSphere"]},{"title":"Charles 抓包下载钉钉群直播视频","path":"/blog/download-ding-talk-video-by-charles.html","content":"前言 作为一名爱学习的技术青年，博主经常会加入一些技术讨论群，参与技术大佬的直播分享，但是由于日常工作繁忙，经常会错过很多精彩的直播，因此想将直播视频下载下来，充分利用上下班的时间进行回看。但是往往事与愿违，大部分群管理员都会设置不允许下载回放 ( 如下图 ) ，幸好强大的互联网提供了各种 NB 工具，最终使用 Charles 和 FFmpeg 工具，成功实现了钉钉直播视频下载。 Charles 配置 要下载钉钉群直播视频，我们需要先使用 Charles 抓包，获取直播视频的下载链接。如果本地没有安装过 Charles，需要从 官网 下载并安装，安装完成后可使用如下注册码： Registered Name: https://zhile.ioLicense Key: 48891cf209c6d32bf4 安装完成后，需要配置 Charles 代理，首先选择 Proxy - macOS Proxy 菜单开启代理。 然后再选择 Proxy - Proxy Settings 菜单，对代理进行配置，需要开启 HTTP 代理——选择 Use HTTP proxy。 由于钉钉群直播使用了 HTTPS 协议，因此需要安装 Charles 根证书，并设置 SSL 代理，支持加密数据的获取。安装 Charles 根证书操作很简单，选择 Help - SSL Proxying - Install Charles Root Certificate 即可完成安装。通常会出现如下界面 ( 未出现可自行打开 Mac 系统自带软件——钥匙串访问 ) ，如果证书显示不被信任，则双击进行设置，设置为始终信任。 最后再设置 SSL 代理，选择 Proxy - SSL Proxying Settings 菜单，出现如下界面后，选中 Enable SSL Proxying，然后添加一个代理规则，Host 设置为 *，由于是抓取 HTTPS 协议请求，Port 设置为 443。 Charles 抓包 Charles 配置完成后，打开钉钉群直播视频，然后观察 Charles 抓包内容，获取到如下请求信息，其中 *.alicdn.com 格式的请求，为钉钉群视频直播地址。展开抓取到的请求信息后，发现了完整的视频地址，最后我们要做的就是想办法下载 m3u8 格式的视频。 FFmpeg 下载视频 下载视频之前，也许有小伙伴会好奇什么是 M3U8？参考网络上的相关博客，可以得到如下信息： M3U8 是 Unicode 版本的 M3U，用 UTF-8 编码。M3U 和 M3U8 文件都是苹果公司使用的 HTTP Live Streaming 协议格式的基础，这种协议格式可以在 iPhone 和 Macbook 等设备播放。 HLS 的工作原理是把整个流分成一个个小的基于 HTTP 的文件来下载，每次只下载一些。当媒体流正在播放时，客户端可以选择从许多不同的备用源中以不同的速率下载同样的资源，允许流媒体会话适应不同的数据速率。 那么如何下载 M3U8 格式的视频呢？我们可以借助强大的 FFmpeg 工具来下载，通过 homebrew 可以快速在 Mac 上安装 FFmpeg。 brew install ffmpeg 安装过程中，如果出现 LibreSSL SSL_connect: SSL_ERROR_SYSCALL in connection to github.com:443 错误，可以执行以下命令关闭 IPV6 网络 ( 参考 文档 ) ： networksetup -setv6off Wi-Fi 安装完成后，我们只需要执行以下命令，即可下载 M3U8 格式视频，并转化为 MP4 格式。 ffmpeg -i https://lzdliving.alicdn.com/live_hp/2fa194dc-044e-43f6-b964-3a09a43a3594_merge.m3u8?app_type=macauth_key=1616907608-0-0-fbfbee74d55b0a048ccc2f0e8920e6dbcid=038194bc5bde7a7bf9c1b126d48869e6token=320f03dcb114f8f29e8c91a9427170f2sKmfXOG-gLUuCtVdMRfWRg21jiq2T6lwgb42XfmE2d6coCLyz7G1xNXtbbBvlxOsoKTqzHQNo002uoxS1IcHoxbzpEciQpOou8zu98qyQ_I=token2=64f9833b15d8c3a85b466e4826bd8243HLTpjGeXPtMi9cWSIC0qXEDmDYjrQ7LPfJ3rwNMcHsqxiRko0EXzbbsEGe7KiSV92saXKu8Lp8QjI-WHrlFopaW-cSar4_kpYJcom0FY9gAversion=6.0.0 ~/Downloads/数据集成 Elasticsearch 实时同步.mp4 下载完成后，我们可以在 Download 文件夹找到对应的视频文件，使用 ffplay ~/Downloads/数据集成 Elasticsearch 实时同步.mp4 命令进行播放测试，可以看到效果非常完美。 参考文档 钉钉群直播提取视频文件 搞定 m3u8 视频下载 m3u8 文件格式详解 欢迎关注 欢迎关注「端小强的博客」微信公众号，会不定期分享日常学习和工作经验，欢迎大家关注交流。","tags":["Charles","FFmpeg"],"categories":["Tool"]},{"title":"Java8 新特性实战","path":"/blog/java-8-new-features-in-action.html","content":"前言 Java8 是 Oracle 公司在 2014 年 3 ⽉发布的版本，是 Java5 之后最重要的版本，带来了诸多⽅⾯的新特性，包括语⾔、类库、编译器及 JVM 等新特性。本⽂重点介绍 Java8 中语法相关的新特性，主要包括 Lambda 表达式、Stream API、New Date API、Optional 等。 Lambda 表达式 什么是 Lambda Lambda expression is a new feature which is introduced in Java 8. A lambda expression is an anonymous function. A function that doesn’t have a name and doesn’t belong to any class. The concept of lambda expression was first introduced in LISP programming language. Lambda 这个概念最早起源于 LISP 语言，Java8 中引入了这个特性，Lambda 表达式本质上是一个匿名函数，这个函数没有名称并且不属于任何类。 为什么需要 Lambda 在 Java8 之前的版本中，如果我们需要实现 行为参数化，必须将特定的行为包装在某个类中，然后将对象传递给具体方法进行执行。使用匿名内部类的实现如下： ExecutorService executorService = Executors.newFixedThreadPool(2);executorService.execute(new Runnable() @Override public void run() System.out.println(查询材料库存数！); );executorService.execute(new Runnable() @Override public void run() System.out.println(查询材料门店销量！); ); 这种方式写出的代码十分冗长，实际我们想要执行的其实就是 run 方法中的功能，Java8 提供了 Lambda 表达式来解决这个问题，经过 Lambda 表达式改造的代码清晰简洁： ExecutorService executorService = Executors.newFixedThreadPool(2);executorService.execute(() - System.out.println(查询材料库存数！));executorService.execute(() - System.out.println(查询材料门店销量！)); 如何使用 Lambda Lambda 表达式由参数、箭头和主体组成。Lambda 的基本语法如下，有两种基本形式： // 单行语句(parameters) - expressionexecutorService.execute(() - System.out.println(查询材料库存数！));// 多行语句(parameters) - statements; executorService.execute(() - System.out.println(查询材料门店销量！); System.out.println(查询材料门店销量！);); 参数列表：函数式接口 中的抽象方法对应的参数列表，前文例子中函数式接口为 Runnable 接口，抽象方法为 run 方法，为空参数方法； 箭头：Lambda 表达式的标志符号，用来分隔参数列表和 Lambda 主体； Lambda 主体：功能代码块，多行需要使用 花括号 ； 那么究竟在哪里可以使用 Lambda 表达式——在函数式接口上使用 Lambda 表达式，前文示例中的 Runnable 接口就是一个函数式接口。 @FunctionalInterfacepublic interface Runnable /** * When an object implementing interface codeRunnable/code is used * to create a thread, starting the thread causes the objects * coderun/code method to be called in that separately executing * thread. * p * The general contract of the method coderun/code is that it may * take any action whatsoever. * * @see java.lang.Thread#run() */ public abstract void run(); 函数式接⼝ 那么什么是函数式接口呢？简单来说，就是 只定义了一个抽象方法的接口。Lambda 表达式允许直接以内联的方式为函数式接口的抽象方法提供实现，并把整个表达式作为函数式接口的实例。 在 Java8 中，提供了 @FunctionalInterface 注解来专门表示函数式接口，该注解不是必须的，但是添加了该注解编译器会进行语法检查，保证接口中只能包含一个抽象方法。 观察下如下接口是否符合函数式接口的定义？ @FunctionalInterfacepublic interface Runnable public abstract void run();@FunctionalInterfacepublic interface CallableV V call() throws Exception;@FunctionalInterfacepublic interface ComparatorT int compare(T o1, T o2); boolean equals(Object obj); default ComparatorT reversed() return Collections.reverseOrder(this); ... public static T extends Comparable? super T ComparatorT reverseOrder() return Collections.reverseOrder(); ... 为什么 Comparator 接口也是函数式接口？可以参考 FunctionalInterface 注解的 javadoc： If an interface declares an abstract method overriding one of the public methods of java.lang.Object, that also does not count toward the interface’s abstract method count since any implementation of the interface will have an implementation from java.lang.Object or elsewhere. Note that instances of functional interfaces can be created with lambda expressions, method references, or constructor references. 如果一个接口中定义了一个抽象方法——重写 Object 基类中的公有方法，那么这个抽象方法不会被算入接口抽象方法的计数中，因为任何一个这个接口的实现类本来就会通过继承 Object 基类来实现该方法。 Java8 新增了 java.util.function 包，在 function 包中引入了一些常用的函数式接口： .tg {border-collapse:collapse;border-color:#ccc;border-spacing:0;} .tg td{background-color:#fff;border-color:#ccc;border-style:solid;border-width:1px;color:#333; font-family:Arial, sans-serif;font-size:14px;overflow:hidden;padding:10px 5px;word-break:normal;} .tg th{background-color:#f0f0f0;border-color:#ccc;border-style:solid;border-width:1px;color:#333; font-family:Arial, sans-serif;font-size:14px;font-weight:normal;overflow:hidden;padding:10px 5px;word-break:normal;} .tg .tg-baqh{text-align:center;vertical-align:top} .tg .tg-0lax{text-align:left;vertical-align:top} 函数式接口 函数描述符 原始类型特化 说明 PredicateT T - boolean IntPredicate, LongPredicate, DoublePredicate 断言型接口 ConsumerT T - void IntConsumer, LongConsumer, DoubleConsumer 消费型接口 FunctionT, R T - R IntFunctionR, IntToDoubleFunction, IntToLongFunction, LongFunctionR, LongToDoubleFunction, LongToIntFunction, DoubleFunctionR, ToIntFunctionT, ToDoubleFunctionT, ToLongFunctionT 函数型接口 SupplierT () - T BooleanSupplier, IntSupplier, LongSupplier, DoubleSupplier 供给型接口 UnaryOperatorT T - T IntUnaryOperator, LongUnaryOperator, DoubleUnaryOperator 一元操作型接口 BinaryOperatorT (T, T) - T IntBinaryOperator, LongBinaryOperator, DoubleBinaryOperator 二元操作型接口 BiPredicateL, R (L, R) - boolean 二元断言型接口 BiConsumerT, U (T, U) - void ObjIntConsumerT, ObjLongConsumerT, ObjDoubleConsumerT 二元消费型接口 BiFunctionT, U, R (T, U) - R ToIntBiFunctionT, U, ToLongBiFunctionT, U, ToDoubleBiFunctionT, U 二元函数型接口 PredicateT：断言型接口，抽象方法为 boolean test(T t)，传入一个参数，返回一个布尔值。 // 断言型接口PredicateInteger predicate = t - t.equals(30);System.out.println(predicate.test(35));// 断言型接口原始类型特化IntPredicate intPredicate = t - t 30;System.out.println(intPredicate.test(25)); ConsumerT：消费型接口，抽象方法为 void accept(T t)，传入一个参数，没有返回值。 // 消费型接口// ConsumerString consumer = t - System.out.println(t);ConsumerString consumer = System.out::println;consumer.accept(张三); FunctionT,R：函数型接口，抽象方法为 R apply(T t)，传入一个参数，返回另一个值。 // 函数型接口// FunctionInteger, String function = (t) - String.valueOf(t);FunctionInteger, String function = String::valueOf;System.out.println(function.apply(2020)); SupplierT：供给型接口，抽象方法为 T get()，不传入参数，返回一个结果。 // 生产型接口SupplierString supplier = () - 2020年世界和平！;System.out.println(supplier.get()); UnaryOperatorT：一元操作型接口，继承自 FunctionT, T接口，传入一个参数，返回该参数。 // 一元操作型接口// UnaryOperatorString unaryOperator = t - t.toUpperCase();UnaryOperatorInteger unaryOperator = t - t + 1;System.out.println(unaryOperator.apply(99)); ⽅法引⽤构造⽅法引⽤ 方法引用是特殊场景下 Lambda 表达式的一种简洁写法。如果某个方法刚好满足了 Lambda 表达式的形式，那么就可以用方法引用来表示 Lambda 表达式。 方法引用构造方法引用有四类： 类名::静态方法名——在 lambda 表达式中，调用了某个类的静态方法； 对象::实例方法名——在 lambda 表达式中，调用了某个外部对象的实例方法； 类名::实例方法名——在 lambda 表达式中，调用了 lambda 参数列表中的对象实例方法； 类名::new——在 lambda 表达式中，调用了构造方法创建对象； ListString nums = Lists.newArrayList(-11, 111, 23, 14, 6, 18);// 类名::静态方法名，在 lambda 表达式中，调用了某个类的静态方法// nums.sort(Comparator.comparing(num - Integer.valueOf(num)));nums.sort(Comparator.comparing(Integer::valueOf));System.out.println(--类名::静态方法名-- + nums);// 对象::实例方法名，在 lambda 表达式中，调用了某个外部对象的实例方法// SupplierInteger supplier = () - nums.size();SupplierInteger supplier = nums::size;System.out.println(supplier.get());// 类名::实例方法名，在 lambda 表达式中，调用了 lambda 参数列表中的对象实例方法// nums.sort(Comparator.comparing(num - num.length()));nums.sort(Comparator.comparing(String::length));System.out.println(--类名::实例方法名-- + nums);// 类名::new，在 lambda 表达式中，调用了构造方法创建对象/*FunctionString, BigInteger function = new FunctionString, BigInteger() @Override public BigInteger apply(String s) return new BigInteger(s); ;*/FunctionString, BigInteger function = BigInteger::new;System.out.println(function.apply(12345678901234567890)); (adsbygoogle = window.adsbygoogle || []).push({}); 赞助商 Stream API 什么是 Stream Stream API 是对集合功能的增强，借助于 Lambda 表达式，能够极大地提高编程效率和程序可读性。Stream 处理集合数据时，将要处理的元素看做一种流，流在管道中传输，并且可以在管道的节点上处理，包括筛选、去重、排序、聚合等。元素流在管道中经过中间操作的处理，最后由结束操作得到处理结果。 使用 Stream API 具有以下优势： 提升性能——Stream 会记录下过程操作、并对这些操作进行叠加，最后在一个迭代循环中执行所有叠加的操作，减少迭代次数； 代码简洁——函数式编程风格的代码简洁、意图明确； 多核友好——只需调用 parallel()方法，即可实现并行程序，简化编码； 使用 Stram API 前的编码风格： ListStaff staffs = Lists.newArrayList(Staff.builder().name(james).age(35).build(), Staff.builder().name(wade).age(37).build(), Staff.builder().name(kobe).age(41).build(), Staff.builder().name(rose).age(31).build());ListStaff results = Lists.newArrayList();// 筛选出年龄大于35岁的员工for (Staff staff : staffs) if (staff.getAge() = 35) continue; results.add(staff);System.out.println(results); 使用 Stram API 后的编码风格： ListStaff staffs = Lists.newArrayList(Staff.builder().name(james).age(35).build(), Staff.builder().name(wade).age(37).build(), Staff.builder().name(kobe).age(41).build(), Staff.builder().name(rose).age(31).build());// 使用 Stream API 进行筛选ListStaff streamResults = staffs.stream().filter(staff - staff.getAge() 35).collect(Collectors.toList());System.out.println(streamResults); 如何创建 Stream 通常创建 Stream 都是调用集合（Collection）类中的 stream()方法或者 parallelStream()方法，可以对应生成串行流和并行流。 // 从 List 创建 StreamLists.newArrayList(123, 11, 323, 2).stream().map(num - num * 2).forEach(System.out::println);// 直接从 Stream 创建Stream.of(123, 11, 323, 2).map(num - num * 2).forEach(System.out::println); 也可以使用 IntStream、LongStream、DoubleStream 从基本类型创建 Stream，基本类型创建的 Stream 支持一些特殊的结束操作——sum()、average()、max()。 // 通过 IntStream 直接创建System.out.println(IntStream.of(123, 11, 323, 2).max().orElse(-1)); Stream 和 IntStream、LongStream、DoubleStream 之间可以相互装换： // 从 Stream 转换成 IntStreamStream.of(123, 11, 323, 2).mapToInt(Integer::parseInt).average().ifPresent(System.out::println);// 从 IntStream 转换成 StreamIntStream.of(123, 11, 323, 2).mapToObj(num - f6 + num).forEach(System.out::println); 常⽤ Stream 操作 Stream 操作具有如下特点： Stream 操作不会修改原始的数据； 操作无状态，不依赖外部变量，在 Stream 操作内部引用外部非 final 变量会报错（外部变量默认 final，修改之后会报错）； Stream 中记录中间操作，并对这些操作进行叠加，最后在一个迭代循环中执行所有叠加的操作，生成结果； 根据 Stream 操作的执行阶段，可以分为两类： 中间操作：总是会惰式执行，调用中间操作只会生成一个标记了该操作的新 Stream，中间操作的结果仍然是 Stream，可以继续使用 Stream API 连续调用。中间操作又可以分为 有状态操作 和 无状态操作，有状态操作是指该操作只有拿到所有元素之后才能继续执行，而无状态操作则不受之前元素的影响； 结束操作：会触发实际计算，计算发生时会把所有中间操作以 pipeline 的方式执行，这样可以减少迭代次数。结束操作的结果通常是一个非 Stream 结果，计算完成之后 Stream 就会失效（只能遍历一次）； 常用的 Stream 操作如下图： .tg {border-collapse:collapse;border-color:#ccc;border-spacing:0;} .tg td{background-color:#fff;border-color:#ccc;border-style:solid;border-width:1px;color:#333; font-family:Arial, sans-serif;font-size:14px;overflow:hidden;padding:10px 5px;word-break:normal;} .tg th{background-color:#f0f0f0;border-color:#ccc;border-style:solid;border-width:1px;color:#333; font-family:Arial, sans-serif;font-size:14px;font-weight:normal;overflow:hidden;padding:10px 5px;word-break:normal;} .tg .tg-baqh{text-align:center;vertical-align:top} .tg .tg-0lax{text-align:left;vertical-align:top} Stream 操作分类 中间操作 无状态 unordered(), filter(), map(), mapToInt(), mapToLong(), mapToDobule(), flatMap(), flatMapToInt(), flatMapToLong(), flatMapToDobule(), peek() 有状态 distinct(), sorted(), limit(), skip() 结束操作 非短路操作 forEach(), forEachOrdered(), toArray(), reduce(), collect(), max(), min(), count() 短路操作 anyMatch(), allMatch(), noneMatch(), findFirst(), findAny() 常⽤的中间操作 filter——根据 Predicate 条件，过滤出符合条件的元素： // StreamT filter(Predicate? super T predicate)// 过滤绝对值开根号大于15的数字Lists.newArrayList(112, 131, 323, 234, 730, 177, -226, 434) .stream().filter(num - Math.sqrt(Math.abs(num)) 15).forEach(System.out::println); sorted——对集合中的元素进行排序： // StreamT sorted(Comparator? super T comparator)// 字符串装换成数字排序ListString nums = Lists.newArrayList(112, 131, 323, 234, 730, 177, -226, 434);nums.stream().sorted(Comparator.comparingInt(Integer::parseInt)).forEach(System.out::println); map——对集合中的每个元素按照 mapper 操作进行转换，转换前后 Stream 中元素的个数不会改变，但元素的类型取决于转换之后的类型： // StreamR map(Function? super T,? extends R mapper)// map操作：对集合中的每个元素按照 mapper 操作进行转换Lists.newArrayList(a1, a2, a3, a4, a5).stream().map(String::toUpperCase).forEach(System.out::println); flatMap——map 方法只能把一个对象转换成另一个对象，如果需要将一个对象转换成多个，需要使用 flatMap： // StreamR flatMap(Function? super T,? extends Stream? extends R mapper)// flatMap 操作：找出所有员工的兴趣爱好ListStaff staffs = Lists.newArrayList(Staff.builder().name(张三).age(18).hobbies(Lists.newArrayList(篮球, 足球, 围棋)).build(), Staff.builder().name(李四).age(27).hobbies(Lists.newArrayList(书法, 围棋, 乒乓球)).build(), Staff.builder().name(王五).age(33).hobbies(Lists.newArrayList(品茶, 读书, 篮球)).build());SetString hobbies = staffs.stream().map(Staff::getHobbies).flatMap(Collection::stream).collect(Collectors.toSet());System.out.println(hobbies); 常用的结束操作 forEach——对每一个元素的执行指定的 action 操作： // void forEach(Consumer? super T action)// forEach 操作：对每一个元素的执行指定的 action 操作Lists.newArrayList(112, 131, 323, 234, 730, 177, -226, 434).forEach(System.out::println); collect——collect 方法接收一个 Collector 参数，Collector 可以将 Stream 转换成集合，如 List、Set 或 Map。JDK 内置了很多常用的 Collector，大多数场景下不需要自己实现： // R, A R collect(Collector? super T, A, R collector);// collect 操作：将 Stream 转换成集合，如：List、Set、Map// MapString, Staff staffMap = staffs.stream().collect(Collectors.toMap((Staff::getName), Function.identity()));MapString, Staff staffMap = staffs.stream().collect(Collectors.toMap((Staff::getName), Function.identity(), (oldValue, newValue) - newValue));System.out.println(staffMap); 将 Stream 元素转换成 map 的时候，需要特别注意：key 必须是唯一的，否则会抛出 IllegalStateException 。如果想主动规避这个问题，需要我们传入一个 merge function，来指定重复的元素映射的方式。也可以使用 Collectors.groupingBy()，按照指定 key 分组的方式来代替： // collect 操作：按照指定 key 分组MapString, ListStaff staffsMap = staffs.stream().collect(Collectors.groupingBy((Staff::getName)));System.out.println(staffsMap); reduce——reduce 操作可以实现从一组元素中生成一个值，sum()、max()、min()、count()等都是 reduce 操作，将他们单独设为函数方便日常使用。redeue 方法定义了三种重载形式： 第一种方法声明为：OptionalT reduce(BinaryOperatorT accumulator); 参数为累加器，返回值为 Optional 对象，通过 accumulator 计算得到一个最终结果，通过 Optional 对象中返回： // 实现#号拼接字符串// 第一次执行时第一个参数是 Stream 中的第一个元素，第二个参数是 Stream 参数中的第二个元素// 后面每次执行的中间结果赋给第一个参数，然后第二个参数为 Stream 中的下一个元素，依次执行，最后返回一个 OptionalLists.newArrayList(d4, c3, a1, b2, f5).stream().sorted().reduce((s1, s2) - System.out.println(s1: + s1); System.out.println(s2: + s2); System.out.println(--------); return s1 + # + s2;).ifPresent(System.out::println);// 执行结果s1:a1s2:b2--------s1:a1#b2s2:c3--------s1:a1#b2#c3s2:d4--------s1:a1#b2#c3#d4s2:f5--------a1#b2#c3#d4#f5 第二种方法声明为：T reduce(T identity, BinaryOperatorT accumulator); 新增了一个初始化类型。 // 第一次执行时第一个参数是指定的初始对象，第二个参数是 Stream 参数中的第一个元素// 后面每次执行的中间结果赋给第一个参数，然后第二个参数为 Stream 中的下一个元素，依次执行，最后返回一个和初始值类型相同的结果System.out.println(Stream.of(1, 2, 3, 4, 5).reduce(10, (p1, p2) - System.out.println(p1: + p1); System.out.println(p2: + p2); System.out.println(--------); return p1 + p2;));// 执行结果p1:10p2:1--------p1:11p2:2--------p1:13p2:3--------p1:16p2:4--------p1:20p2:5--------25 第三种方法声明为：U U reduce(U identity, BiFunctionU, ? super T, U accumulator, BinaryOperatorU combiner); 在初始对象和累加器基础上，添加了组合器 combiner。 // 第三种方式，求单词长度之和，使用串行流和并行流分别执行System.out.println(Stream.of(d4, c3, a1, b2, f5).reduce(0, (o1, o2) - String threadName = Thread.currentThread().getName(); System.out.println(BiFunction-- + threadName); System.out.println(o1: + o1 + -- + threadName); System.out.println(o2: + o2 + -- + threadName); return o1 + o2.length();, (o1, o2) - String threadName = Thread.currentThread().getName(); System.out.println(BinaryOperator-- + threadName); System.out.println(o1: + o1 + -- + threadName); System.out.println(o2: + o2 + -- + threadName); return o1 + o2;));// 执行结果BiFunction--maino1:0--maino2:d4--mainBiFunction--maino1:2--maino2:c3--mainBiFunction--maino1:4--maino2:a1--mainBiFunction--maino1:6--maino2:b2--mainBiFunction--maino1:8--maino2:f5--main10 执行以上的案例发现 BinaryOperator 并没有执行，此时的操作与第二种方式类似，我们将 Stream 转换为并行流再尝试一下： BiFunction--maino1:0--maino2:a1--mainBiFunction--ForkJoinPool.commonPool-worker-3o1:0--ForkJoinPool.commonPool-worker-3o2:d4--ForkJoinPool.commonPool-worker-3BiFunction--ForkJoinPool.commonPool-worker-2o1:0--ForkJoinPool.commonPool-worker-2o2:f5--ForkJoinPool.commonPool-worker-2BiFunction--ForkJoinPool.commonPool-worker-3o1:0--ForkJoinPool.commonPool-worker-3o2:b2--ForkJoinPool.commonPool-worker-3BinaryOperator--ForkJoinPool.commonPool-worker-3o1:2--ForkJoinPool.commonPool-worker-3o2:2--ForkJoinPool.commonPool-worker-3BinaryOperator--ForkJoinPool.commonPool-worker-3o1:2--ForkJoinPool.commonPool-worker-3o2:4--ForkJoinPool.commonPool-worker-3BiFunction--ForkJoinPool.commonPool-worker-1o1:0--ForkJoinPool.commonPool-worker-1o2:c3--ForkJoinPool.commonPool-worker-1BinaryOperator--ForkJoinPool.commonPool-worker-1o1:2--ForkJoinPool.commonPool-worker-1o2:2--ForkJoinPool.commonPool-worker-1BinaryOperator--ForkJoinPool.commonPool-worker-1o1:4--ForkJoinPool.commonPool-worker-1o2:6--ForkJoinPool.commonPool-worker-110 发现在并行流中，BinaryOperator 执行了，查阅资料发现，为了避免并行竞争，将每个线程的任务单独维护了一个结果，然后通过组合器 combiner 进行最终结果的合并。 match——用来判断某一种规则是否与流对象匹配。所有的匹配操作都是结束操作，只返回一个 boolean 类型的结果。 // match操作：用来判断某一种规则是否与流对象匹配boolean anyMatch = staffs.stream().anyMatch((staff) - staff.getName().startsWith(张));System.out.println(anyMatch);boolean allMatch = staffs.stream().allMatch((staff) - staff.getAge().equals(34));System.out.println(allMatch);boolean noneMatch = staffs.stream().noneMatch((staff) - staff.getAge().equals(34));System.out.println(noneMatch); New Date API Java8 另一项新特性是新的时间和日期 API，它们被包含在 java.time 包中。借助新的时间和日期 API 可以更简洁地处理时间和日期。 为什么需要 New Date API 在 Java8 之前的时间和日期 API 有很多缺陷，具体如下： Java 的 java.util.Date 和 java.util.Calendar 类易用性差，而且不是线程安全的； 对日期的计算方式繁琐，容易出错——月份是从 0 开始的，从 Calendar 中获取的月份需要加一才能表示当前月份； 由于以上这些问题，Java 社区出现了一些第三方时间日期库——Joda-Time，Java8 充分借鉴了 Joda 库的一些优点，提供了一套新的时间和日期 API。 日期时间类 Java8 中常用的日期和时间类主要有 LocalDate、LocalTime、LocalDateTime、Instant、Duration 和 Period。 LocalDate、LocalTime、LocalDateTime LocalDate 类表示一个具体的日期，但不包含具体时间，也不包含时区信息。可以通过 LocalDate 的静态方法 of() 创建一个实例，LocalDate 也包含一些方法用来获取年份、月份、天、星期几等： // 初始化日期LocalDate localDate = LocalDate.of(2020, 1, 10);// 年份 2020System.out.println(localDate.getYear());// 年份中第几天 10System.out.println(localDate.getDayOfYear());// 月份 JANUARYMonth month = localDate.getMonth();System.out.println(month);// 月份中的第几天 10System.out.println(localDate.getDayOfMonth());// 一周的第几天：FRIDAYSystem.out.println(localDate.getDayOfWeek());// 月份的天数 31System.out.println(localDate.lengthOfMonth());// 是否为闰年 trueSystem.out.println(localDate.isLeapYear()); LocalTime 和 LocalDate 类似，他们之间的区别在于 LocalDate 不包含具体时间，而 LocalTime 包含具体时间： // 初始化一个时间：17:50:40LocalTime localTime = LocalTime.of(17, 50, 40);// 时：17System.out.println(localTime.getHour());// 分：50System.out.println(localTime.getMinute());// 秒：40System.out.println(localTime.getSecond()); LocalDateTime 类是 LocalDate 和 LocalTime 的结合体，可以通过 of()方法直接创建，也可以调用 LocalDate 的 atTime() 方法或 LocalTime 的 atDate() 方法将 LocalDate 或 LocalTime 合并成一个 LocalDateTime： LocalDateTime localDateTime = LocalDateTime.of(2020, Month.JANUARY, 10, 17, 50, 40);LocalDate localDate = LocalDate.of(2020, Month.JANUARY, 10);LocalTime localTime = LocalTime.of(17, 50, 40);LocalDateTime combineLocalDateTime = localDate.atTime(localTime);// LocalDateTime combineLocalDateTime = localTime.atDate(localDate);// 从 LocalDateTime 中获取年月日时分秒System.out.println(combineLocalDateTime.getYear());System.out.println(combineLocalDateTime.getMonth());System.out.println(combineLocalDateTime.getDayOfMonth());System.out.println(combineLocalDateTime.getHour());System.out.println(combineLocalDateTime.getMinute());System.out.println(combineLocalDateTime.getSecond());// LocalDateTime 转化成 LocalDate 或 LocalTimeLocalDate transferLocalDate = localDateTime.toLocalDate();LocalTime transferLocalTime = localDateTime.toLocalTime(); Instant——Instant 用于表示一个时间戳，可以精确到纳秒，可以使用 now() 方法创建，也可以通过 ofEpochSecond() 方法创建。 // Instant可以使用 now() 方法创建，也可以通过 ofEpochSecond 方法创建Instant now = Instant.now();// 2020-01-12T16:16:41.723ZSystem.out.println(now);// ofEpochSecond 方法第一个参数表示从 1970-01-01 00:00:00 开始到现在的秒数// ofEpochSecond 方法第二个参数表示纳秒数，0~999,999,999Instant instant = Instant.ofEpochSecond(9999, 1000);// 1970-01-01T02:46:39.000001ZSystem.out.println(instant); Duration——Duration 表示一个时间段，可以通过 Duration.between() 或 Duration.of() 方法创建。 // 使用 of 创建 Duration，统一一个单位设置Duration duration1 = Duration.of(7, ChronoUnit.DAYS);Duration duration2 = Duration.of(3000, ChronoUnit.SECONDS);// 2018-07-03 09:00:00LocalDateTime start = LocalDateTime.of(2018, Month.JULY, 3, 9, 0, 0);// 2020-01-13 18:00:00LocalDateTime end = LocalDateTime.of(2020, Month.JANUARY, 13, 18, 0, 0);Duration duration = Duration.between(start, end);// 总天数System.out.println(duration.toDays());// 总小时数System.out.println(duration.toHours());// 总分钟数System.out.println(duration.toMinutes());// 总秒数System.out.println(duration.getSeconds()); Period——Period 和 Duration 类似，不同之处在于 Period 是以年月日来衡量一个时间段。 // 创建2年3个月6天的范围，年月日单独字段设置Period period1 = Period.of(2, 3, 6);// 从 2018-07-03 到 2020-01-13Period period2 = Period.between(LocalDate.of(2018, 7, 3), LocalDate.of(2020, 1, 13));System.out.println(period2.getYears());System.out.println(period2.getMonths());System.out.println(period2.getDays()); 日期操作和格式化 日期操作——常用的日期操作有增减天数、月数，查找本月最后一个周五等操作： // 2019-12-01LocalDate date = LocalDate.of(2019, 12, 1);// 修改日期为 2020-01-13 2020-01-13LocalDate newDate = date.withYear(2020).withMonth(1).withDayOfMonth(13);System.out.println(newDate);// 增加一年，减一个月，加十天 2020-12-23LocalDate localDate = newDate.plusYears(1).minusMonths(1).plus(10, ChronoUnit.DAYS);System.out.println(localDate);// 查找本月最后一个周五 2020-01-31System.out.println(LocalDate.now().with(TemporalAdjusters.lastInMonth(DayOfWeek.FRIDAY))); 日期格式化——新的日期 API 中提供了一个 DateTimeFormatter 类用于处理日期格式化操作，日期类中调用 format() 方法，传入 DateTimeFormatter 参数： // 20200113System.out.println(LocalDateTime.now().format(DateTimeFormatter.BASIC_ISO_DATE));// 2020-01-13System.out.println(LocalDateTime.now().format(DateTimeFormatter.ISO_LOCAL_DATE));// 11:02:38.148System.out.println(LocalDateTime.now().format(DateTimeFormatter.ISO_LOCAL_TIME));// 2020-01-13System.out.println(LocalDateTime.now().format(DateTimeFormatter.ofPattern(yyyy-MM-dd))); Optional 什么是 Optional 在 Optional 出现之前，Java 的 NullPointerException 问题令人头疼，我们需要手动添加很多判空逻辑： 为了减少这样的 null 值判断，Java8 借鉴了 Guava Optional，提供了新的 Optional 容器。根据官方文档定义，Optional 是一个容器对象，容器中可能包含也可能不包含一个非空对象。如果对象存在，isPresent() 将会返回 true，get()方法将会返回一个值。 A container object which may or may not contain a non-null value. If a value is present, isPresent() will return true and get() will return the value. 如何使用 Optional of、ofNullable——分别为非 null 值和可为 null 值创建一个 Optional： // 使用 of 为非 null 值创建 Optional，ofNullableString name = 张三;Integer age = LocalDate.now().isAfter(LocalDate.of(2020, 1, 10)) ? null : 0;OptionalString nameOptional = Optional.of(name);OptionalInteger ageOptional = Optional.ofNullable(age); isPresent——判断 Optional 中是否存在值，存在则返回 true，不存在则返回 false： // 使用 isPresent 判断 Optional 是否存在值System.out.println(nameOptional.isPresent());System.out.println(ageOptional.isPresent()); ifPresent——如果存在值则执行函数式接口 Consumer 中的逻辑，否则不操作： // nameOptional.ifPresent(value - System.out.println(value));nameOptional.ifPresent(System.out::println);ageOptional.ifPresent(System.out::println);// 执行结果// 张三 get——如果有值直接返回，否则抛出 NoSuchElementException 异常： System.out.println(nameOptional.get());// NoSuchElementException: No value present// System.out.println(ageOptional.get()); orElse、orElseGet、orElseThrow——orElse 有值则直接返回，为 null 时返回参数设置的默认值；orElseGet 方法与 orElse 方法类似，只是提供了一个函数式接口 Supplier，用来生成默认值；orElseThrow 允许传入一个 Lambda 表达式，来指定为空时抛出异常信息： // orElse 设置为空时的默认值System.out.println(nameOptional.orElse(李四));System.out.println(ageOptional.orElse(20));// orElseGet 设置为空时的默认值System.out.println(ageOptional.orElseGet(() - 20));// orElseThrow 设置为空时抛出的异常System.out.println(ageOptional.orElseThrow(RuntimeException::new)); map、flatMap——map 允许传入一个 Function 对原始值进行转化，生成一个新的值，然后返回 Optional；flatMap 用法类似，只是传入的 lambda 表达式要求返回值为 Optional： // 使用 map、flatMap 映射得到 OptionalnameOptional.map(value - value.replace(三, 四)).ifPresent(System.out::println);nameOptional.flatMap(value - Optional.of(value.replace(三, 四))).ifPresent(System.out::println); filter——通过传入的条件 Predicate 对原始值进行过滤，然后返回 Optional： // 使用 filter 对原始值进行过滤System.out.println(nameOptional.filter(value - value.length() 2).isPresent()); 使用 Optional 的注意事项 不要将 Optional 作为方法参数传递——使用 Optional 作为方法参数传递，如果使用方法时传递了 null，那么这时候就会 NullPointerException，我们不得不加上非空判断，这样就违背了引入 Optional 的初衷； /** * 根据名称过滤员工 * * @param staffs * @param name * @param age * @return */public static ListStaff filterStaffByNameAndAge(ListStaff staffs, String name, OptionalInteger age) return staffs.stream() .filter(p - p.getName().equals(name)) .filter(p - p.getAge() = age.orElse(0)) .collect(Collectors.toList());// 使用 Optional 的注意事项——不要作为方法参数传递ListStaff staffs = Lists.newArrayList(Staff.builder().name(张三).age(18).build(), Staff.builder().name(李四).age(27).hobbies(Lists.newArrayList(书法, 围棋, 乒乓球)).build(), Staff.builder().name(王五).age(35).hobbies(Lists.newArrayList(读书, 篮球, 爬山)).build());filterStaffByNameAndAge(staffs, 李四, null); 不要将 Optional 作为类中的成员变量，因为 Optional 不支持序列化； // 使用 Optional 的注意事项——不要作为类中的字段，不支持序列化Staff staff = Staff.builder().name(张三).telephoneNumber(Optional.of(12345678900)).build();try // java.io.NotSerializableException: java.util.Optional ObjectOutputStream outputStream = new ObjectOutputStream(new FileOutputStream(object.txt)); outputStream.writeObject(staff); catch (Exception e) System.out.println(e.toString()); 参考文档 Java 8 的新特性—终极版 Java8 新特性，你应该了解这些 Guide To Java 8 Optional 一文带你玩转 Java8 Stream 流，从此操作集合 So Easy 欢迎关注 欢迎关注「端小强的博客」微信公众号，会不定期分享日常学习和工作经验，欢迎大家关注交流。","tags":["Java8"],"categories":["Java"]},{"path":"/more/index.html","content":"关于博主我的动态 关于博主 大家好，我是端正强，目前是 Apache ShardingSphere 社区的 PMC 成员，并担任 SphereEx 高级中间件工程师职位。从 2018 年开始接触 Apache ShardingSphere 中间件，曾主导公司内部海量数据的分库分表，有着丰富的实践经验。本人热爱开源，乐于分享，目前专注于 Apache ShardingSphere 内核模块开发，业余时间喜欢研究 Apache Calcite 查询优化技术，期待能够和大家多多交流，共同进步。 分享经历 2023/04/27 openGauss 特性直播课参与 openGauss 特性直播课，分享 ShardingSphere 联邦查询引擎演进与实战 主题演讲，介绍了 ShardingSphere 联邦查询引擎和 openGauss 数据库的适配，通过一些实例指导大家如何使用联邦查询引擎，欢迎观看直播详解。PPT 下载地址，视频观看地址。2022/11/14 亚马逊云科技中国峰会 Dev Day参与 亚马逊云科技中国峰会 Dev Day，分享 Apache ShardingSphere 分布式数据库生态的云原生实践 主题演讲，介绍了 Apache ShardingSphere 和 AWS Aurora 联合打造的分布式数据库生态，以及 ShardingSphere 的云原生实践。PPT 下载地址，视频观看地址。2021/12/14~17 PostgresConf.CN & PGConf.Asia2021参与 2021 年度 PostgreSQL 亚洲技术大会，分享 PostgreSQL 增量服务生态实践 主题演讲，介绍基于 Apache ShardingSphere 打造的 PostgreSQL 增量服务生态。PPT 下载地址，视频观看地址。2021/12/11「TUG 企业行 - 武汉站」走进神州数码，聊聊开源数据架构参与 TUG 企业行武汉站，走进神州数码，聊聊开源数据架构。分享 Apache ShardingSphere 5.0.0 全新 Database Plus 架构演进，介绍 5.0.0 Database Plus 理念下的产品架构和技术架构。PPT 下载地址，视频观看地址。2021/9/8 技术琐话大咖来了参与 B 站技术琐话大咖来了直播，分享 Apache ShardingSphere 可插拔内核与动态组合配置，介绍 Apache ShardingSphere 5.x 最新的可插拔架构及内核动态组合配置。PPT 下载地址，视频观看地址。2021/5/16 Apache DolphinScheduler - ShardingSphere Global Online Co-MeetUp参与 Apache DolphinScheduler 和 ShardingSphere 全球线上联合 MeetUp，分享 Insight Into Apache ShardingSphere 主题演讲，向海外同学介绍 Apache ShardingSphere 项目的概况和核心特性。PPT 下载地址，视频观看地址。 联系方式 Emailmailto:duanzhengqiang@apache.org Githubhttps://github.com/strongduanmu 欢迎关注 欢迎关注「端小强的博客」微信公众号，会不定期分享日常学习和工作经验，欢迎大家关注交流。 .tag-plugin.gallery.grid-box .grid-cell { background: none; padding: 4px; }"},{"title":"Docker","path":"/notes/docker.html","content":"常用数据库 Docker 命令 MySQL # 查找 MySQL 镜像docker search mysql# 拉取最新版 MySQL 镜像，可以指定其他版本docker pull mysql:latest# 查看本地镜像docker images# 运行容器# -i：以交互模式运行，通常配合 -t# -t：为容器重新分配一个伪输入终端，通常配合 -i# -d：后台运行容器# -p：端口映射，格式为主机端口:容器端口# -e：设置环境变量，此处设置 root 密码# --name：设置容器别名docker run -itd -p 3306:3306 -e MYSQL_ROOT_PASSWORD=123456 --name mysql mysql# 挂载本地 my.cnf 运行容器，避免中文乱码docker run -itd -p 3306:3306 -v /Users/duanzhengqiang/softs/mysql/my.cnf:/etc/mysql/my.cnf -e MYSQL_ROOT_PASSWORD=123456 --name mysql_8.0 mysql:8.0# 查看容器运行状态docker ps# 在容器中执行命令docker exec -it mysql /bin/bash# 在容器中执行命令，连接本机 ShardingSphere-Proxy，后端为 MySQLdocker exec -it mysql mysql -u sharding -h host.docker.internal -P 3307 -p 解决 MySQL 中文乱码的 my.cnf 参考配置： [client]default-character-set=utf8mb4[mysql]default-character-set=utf8mb4[mysqld]port=3306# character-set-client-handshake=FALSEcharacter-set-server=utf8mb4character-set-filesystem=utf8mb4collation-server=utf8mb4_general_ciinit-connect=SET NAMES utf8mb4# 解决数据库读取区分大小写问题lower-case-table-names=1 PostgreSQL # 查找 PostgreSQL 镜像docker search postgres# 拉取最新版 PostgreSQL 镜像，默认拉取最新版docker pull postgres# 运行容器docker run -d -p 5432:5432 -e POSTGRES_PASSWORD=123456 --name postgres postgres# 在容器中执行命令，连接本机 ShardingSphere-Proxy，后端为 PostgreSQLdocker exec -it postgres psql -U sharding -d sharding_db -h host.docker.internal -p 3307 openGauss # 官方文档：https://hub.docker.com/r/enmotech/opengauss# 查找 openGauss 镜像docker search opengauss# 拉取 openGauss 镜像docker pull enmotech/opengauss# 运行容器docker run --privileged=true -d -e GS_PASSWORD=Sphere@123 -p 5432:5432 --name opengauss enmotech/opengauss# 在容器中执行命令，连接本机 ShardingSphere-Proxy，后端为 openGaussdocker run --rm -it enmotech/opengauss gsql -U sharding -d sharding_db -Wsharding -h host.docker.internal -p 3307 Oracle # 查找 Oracle 镜像docker search oracle-19c# 拉取 Oracle 镜像docker pull doctorkirk/oracle-19c# 创建数据文件目录mkdir -p /Users/strongduanmu/softs/oracle/oracle_19c_data# 授权chmod 777 /Users/strongduanmu/softs/oracle/oracle_19c_data# 运行容器docker run -d \\-p 1521:1521 -p 5500:5500 \\-e ORACLE_SID=ORCLSID \\-e ORACLE_PDB=ORCLPDB \\-e ORACLE_PWD=123456 \\-e ORACLE_EDITION=standard \\-e ORACLE_CHARACTERSET=AL32UTF8 \\-v /Users/strongduanmu/softs/oracle/oracle_19c_data \\--name oracle_19c doctorkirk/oracle-19c# 查看运行日志docker logs -ft oracle_19c# 在容器中执行命令，连接 Oracledocker exec -it oracle_19c /bin/bashsqlplus / as sysdbashow pdbs;# GUI 连接账号# username：sys as sysdba# password：123456# sid：ORCLSID StarRocks # 拉取镜像docker pull starrocks/allin1-ubuntu:2.5.8docker run --name starrocks -p 9030:9030 -p 8030:8030 -p 8040:8040 -itd starrocks/allin1-ubuntu:2.5.8mysql -P9030 -h127.0.0.1 -uroot --prompt=StarRocks Zookeeper docker pull zookeeperdocker run -d --name zookeeper --privileged=true -p 2181:2181 zookeeper Jenkins docker pull jenkinsci/blueoceandocker run --name jenkins -u root -d -p 8080:8080 -p 50000:50000 -v /Users/duanzhengqiang/.m2:/root/.m2 -v /var/run/docker.sock:/var/run/docker.sock jenkinsci/blueocean# 浏览器访问管理界面localhost:8080# 日志中查看密码docker logs jenkins ShardingSphere # 安装 socatbrew install socatsocat TCP-LISTEN:2375,reuseaddr,fork UNIX-CLIENT:/var/run/docker.sock# 在创建镜像的窗口中执行export DOCKER_HOST=tcp://127.0.0.1:2375# 然后执行 ShardingSphere Proxy 镜像打包命令./mvnw -B clean install -am -pl test/e2e/sql -Pit.env.docker -DskipTests -Dspotless.apply.skip=true -T 1C all predefined address pools have been fully subnetted 执行 ShardingSphere E2E 程序，Docker 启动容器时出现 BadRequestException: Status 400: message:all predefined address pools have been fully subnetted 异常，参考 issues#3529，需要修改 etc/docker/daemon.json 文件，增加 default-address-pools 配置，具体配置如下： log-level: warn, log-driver: json-file, log-opts: max-size: 10m, max-file: 5 , default-address-pools: [ base: 172.16.0.0/12, size: 24 ] 增加完成后，重启 Docker 服务，再次执行 E2E 成功。"},{"title":"Git","path":"/notes/git.html","content":"常用 Git 命令 # 添加远程仓库git remote add upstream https://github.com/apache/shardingsphere.git# 查看远程仓库信息git remote -v# origin\thttps://github.com/strongduanmu/shardingsphere.git (fetch)# origin\thttps://github.com/strongduanmu/shardingsphere.git (push)# upstream\thttps://github.com/apache/shardingsphere.git (fetch)# upstream\thttps://github.com/apache/shardingsphere.git (push)# 拉取远程库 PR 代码到 dev-0705 分支git fetch upstream pull/11150/head:dev-0705# From https://github.com/apache/shardingsphere# * [new ref] refs/pull/11150/head - dev-0705# 拉取远程库 TAG 代码到 5.3.0-test 分支git fetch upstream refs/tags/5.3.0:5.3.0-test# 和上游 master 分支同步（先拉取 upstream）git fetch upstreamgit rebase upstream/master# 修改 commit 信息git commit --amend# 根据指定 commitId 创建新分支git checkout commitId -b branchName# 批量删除 dev* 分支git branch -a | grep ^ dev* | xargs git branch -D# 恢复误删除分支## 查看被删除分支对应的 commit idgit log -g## 根据 commit id 创建 recover_branch 分支git branch recover_branch 34fd566205a34a2842111331449b47c39ef7fa6e Git 合并本地多次提交 本地开发时，可能会存在多次提交的情况，为了保证 Git log 的整洁，需要对多次提交进行合并，通过 git rebase 命令可以快速完成这个目标。参考 Git 合并多个 commit 文章，可以执行如下命令进行合并： # 从 HEAD 版本开始，合并过去的 3 个版本git rebase -i HEAD~3# 合并 3a4226b 之前的版本，3a4226b 不参与合并git rebase -i 3a4226b 执行 rebase 后，会出现如下的窗口，展示了需要合并的 commit 记录，根据下面的命令提示，我们可以将 pick 修改为 squash 或 s，squash 命令会将当前提交合并到前一次提交，并允许修改提交信息。 pick c144c143930 Add DatabaseConnector interface, and move execute logic to StandardDatabaseConnectorpick 26eb5180e88 fix unit test# Rebase 8e684ae6bdf..26eb5180e88 onto 8e684ae6bdf (2 commands)## Commands:# p, pick commit = use commit# r, reword commit = use commit, but edit the commit message# e, edit commit = use commit, but stop for amending# s, squash commit = use commit, but meld into previous commit# f, fixup [-C | -c] commit = like squash but keep only the previous# commits log message, unless -C is used, in which case# keep only this commits message; -c is same as -C but# opens the editor# x, exec command = run command (the rest of the line) using shell# b, break = stop here (continue rebase later with git rebase --continue)# d, drop commit = remove commit# l, label label = label current HEAD with a name# t, reset label = reset HEAD to a label# m, merge [-C commit | -c commit] label [# oneline]# create a merge commit using the original merge commits# message (or the oneline, if no original merge commit was# specified); use -c commit to reword the commit message# u, update-ref ref = track a placeholder for the ref to be updated# to this position in the new commits. The ref is# updated at the end of the rebase## These lines can be re-ordered; they are executed from top to bottom. 修改后交互式信息显示如下，然后保存退出，此时如果有冲突则需要修改，修改的时候需要注意，保留最新的历史，否则我们的修改就会丢失。 pick c144c143930 Add DatabaseConnector interface, and move execute logic to StandardDatabaseConnectors 26eb5180e88 fix unit tests 967ddd22efb fix unit test 修改完成后，需要执行以下命令，将修改添加进来，并继续 rebase 流程，如果处理不了冲突，则可以中断 rebase 流程。 git add .# 继续 rebasegit rebase --continue# 中断 rebasegit rebase --abort 如果没有冲突，或者已经解决了冲突，则会显示如下的内容，内容中包含了合并提交的 commit 信息，不需要的信息可以使用 # 注释，也可以修改最终保留的 commit 信息。 # This is a combination of 3 commits.# This is the 1st commit message:Add DatabaseConnector interface, and move execute logic to StandardDatabaseConnector# This is the commit message #2:fix unit test# This is the commit message #3:fix unit test# Please enter the commit message for your changes. Lines starting# with # will be ignored, and an empty message aborts the commit.## Date: Wed Nov 13 10:57:57 2024 +0800## interactive rebase in progress; onto 8e684ae6bdf# Last commands done (3 commands done):# squash 26eb5180e88 fix unit test# squash 967ddd22efb fix unit test# No commands remaining.# You are currently rebasing branch dev-1113 on 8e684ae6bdf.## Changes to be committed:# modified: proxy/backend/core/src/main/java/org/apache/shardingsphere/proxy/backend/connector/DatabaseConnector.java 此处我们将 fix unit test 都注释掉，并修改第一个 commit 信息，增加 modify test，修改完成后退出保存。 # This is a combination of 3 commits.# This is the 1st commit message:Add DatabaseConnector interface, and move execute logic to StandardDatabaseConnector modify test# This is the commit message #2:# fix unit test# This is the commit message #3:# fix unit test 此时会提示 rebase 成功，并显示了新的 HEAD commit 信息。 [detached HEAD 552f1459fd4] Add DatabaseConnector interface, and move execute logic to StandardDatabaseConnector modify test Date: Wed Nov 13 10:57:57 2024 +0800 6 files changed, 433 insertions(+), 392 deletions(-) create mode 100644 proxy/backend/core/src/main/java/org/apache/shardingsphere/proxy/backend/connector/StandardDatabaseConnector.java rename proxy/backend/core/src/test/java/org/apache/shardingsphere/proxy/backend/connector/DatabaseConnectorTest.java = StandardDatabaseConnectorTest.java (91%)Successfully rebased and updated refs/heads/dev-1113. Github 配置 HTTPS 最近使用 SSH 访问 Github 仓库，经常出现 push 卡顿的情况，使用 ssh -T -p 443 git@ssh.github.com 测试可用性时，无法正常获取响应结果，ssh 服务可用时，应当返回如下结果。 duanzhengqiang@duanzhengqiang-ubuntu:~/blog$ ssh -T -p 443 git@ssh.github.comThe authenticity of host [ssh.github.com]:443 ([20.205.243.160]:443) cant be established.ED25519 key fingerprint is SHA256:+DiY3wvvV6TuJJhbpZisF/zLDA0zPMSvHdkr4UvCOqU.This host key is known by the following other names/addresses: ~/.ssh/known_hosts:1: [hashed name]Are you sure you want to continue connecting (yes/no/[fingerprint])? yesWarning: Permanently added [ssh.github.com]:443 (ED25519) to the list of known hosts.Hi strongduanmu! Youve successfully authenticated, but GitHub does not provide shell access. 为了不影响工作效率，尝试将 SSH 替换为 HTTPS，参考 Managing your personal access tokens，点击个人头像下的 Settings，然后选择 Developer settings - Personal access tokens - Fine-grained personal access tokens Beta - Generate new token，生成 token 如下图所示。 生成完成后，执行 git fetch 等命令时，输入用户名和密码（生成的 token）。 Username for https://github.com: strongduanmuPassword for https://strongduanmu@github.com: token 此外，为了避免频繁输入用户名和密码，可以执行 git config --global credential.helper store ，将认证信息存储下来，这样后续执行就无需重复输入了。 Git 提交超大文件 当 PR 中包含超大文件时（超过 50 MB），此时无法将这些文件提交到仓库中，需要通过大文件提交方式进行处理。 ~/IdeaProjects/dbplus-engine-honor │ dev-5.5.1-fix-0104 *34 +10 !9  git push  PIPE ✘ │ 5m 52s Uploading LFS objects: 100% (2/2), 439 MB | 0 B/s, done. Enumerating objects: 762, done.Counting objects: 100% (541/541), done.Delta compression using up to 12 threadsCompressing objects: 100% (142/142), done.Writing objects: 100% (193/193), 38.50 MiB | 1.89 MiB/s, done.Total 193 (delta 77), reused 0 (delta 0), pack-reused 0 (from 0)remote: Resolving deltas: 100% (77/77), completed with 32 local objects.remote: warning: File test/e2e/sql/src/test/resources/env/scenario/sphereex_db_tbl_sql_federation_honor/data/expected/init-sql/mysql/02-expected-prps_asc_stock_keep_list_t-init.sql is 80.21 MB; this is larger than GitHubs recommended maximum file size of 50.00 MBremote: error: Trace: 28a0369f24fb235ec54356e77d75564ec23819e687079bf342ad247ff6bd5ca2remote: error: See https://gh.io/lfs for more information.remote: error: File test/e2e/sql/src/test/resources/env/scenario/sphereex_db_tbl_sql_federation_honor/data/expected/init-sql/mysql/02-expected-rps_actual_material_consume_rate_t-init.sql is 337.76 MB; this exceeds GitHubs file size limit of 100.00 MBremote: error: GH001: Large files detected. You may want to try Git Large File Storage - https://git-lfs.github.com.To github.com:strongduanmu/dbplus-engine-honor.git ! [remote rejected] dev-5.5.1-fix-0104 - dev-5.5.1-fix-0104 (pre-receive hook declined)error: failed to push some refs to github.com:strongduanmu/dbplus-engine-honor.git 首先，安装 git-lfs 工具，并在代码仓库目录下执行 git lfs install 进行初始化。 brew install git-lfsgit lfs install 然后使用如下的命令追踪大文件： git lfs track test/e2e/sql/src/test/resources/env/scenario/sphereex_db_tbl_sql_federation_honor/data/expected/init-sql/mysql/02-expected-prps_asc_stock_keep_list_t-init.sqlgit lfs track test/e2e/sql/src/test/resources/env/scenario/sphereex_db_tbl_sql_federation_honor/data/expected/init-sql/mysql/02-expected-rps_actual_material_consume_rate_t-init.sql git lfs track 命令会自动生成 / 修改 .gitattributes 文件，我们需要将这个文件提交到仓库： git add .gitattributes 最后，我们需要撤销上一次提交，因为大文件已经被纳入了之前的提交记录，执行如下的命令先撤销提交，再重新处理并 push。 git reset --soft HEAD~1# 修改成你要提交的具体文件git add .git commit -m commit messagegit push"},{"title":"IDEA","path":"/notes/idea.html","content":"IDEA File size exceeds configured limit IDEA 为了保护内存，对关联的文件大小做了限制，默认值为 2500kb。文件过大时，选择 Help - Edit Custom Properties...，然后设置如下参数即可。 idea.max.intellisense.filesize=999999 IDEA Maven pom 文件变灰如何处理 正常情况下，pom 文件是蓝色图标和黑色文字。当 pom 文件变为灰色图标和文字，并且文字上出现删除线时，我们该如何处理呢？ pom 文件变灰 查阅资料发现 pom 文件变灰，是由于 IDEA 将该 pom 文件添加到了忽略文件清单中，我们可以通过 Preferences - Build, Execution, Deployment - Build Tools - Maven - Ignored Files 去除该忽略文件，然后保存并重新导入 Maven 即可。"},{"title":"Common","path":"/notes/index.html","content":"SQL 数据库 数据库管理系统清华大学李国良教授出品《数据库管理系统》课程。Modern SQL介绍自 SQL 92 以来提出的现代 SQL。SQL 标准文档SQL 标准文档。 技术论坛 HeapDumpHeapDump 性能社区并发编程网并发编程网即时通讯网即时通讯网设计模式深入设计模式在线书籍 技术工具 ArthasArthas - Java 应用诊断利器GCEasyGCEasy - Java GC 日志分析工具FastThreadFastThread - Java 线程分析工具 日常工具 Tables Generator表格生成器Profile Readme GeneratorGithub 主页生成器"},{"title":"Mac","path":"/notes/mac.html","content":"Mac 安装提示：已损坏，无法打开 解决步骤： # 1. 在命令行执行以下命令sudo spctl --master-disable# 2. 打开系统设置，点击安全与隐私，在软件来源处选择任意来源# 3. 执行以下命令xattr -rc /Applications/prettyZoo.app Mac 快速开启 HTTP 服务 # 启动 Apache 服务sudo apachectl start# 重启 Apache 服务sudo apachectl restart# 关闭 Apache 服务sudo apachectl stop# 修改端口：打开 /etc/apache2/httpd.conf# 将默认 80 端口改为 8080open /etc/apache2# 打开 HTTP 服务根目录open /Library/WebServer/Documents# 访问 localhost:8080 会出现 It works! 提示"},{"title":"Shell","path":"/notes/shell.html","content":"scp # 复制本地文件到远程，命令格式：scp [-Pport] local_file_path username@host:remote_directoryscp -P22 ~/Downloads/perf.svg root@100.75.35.101:/root# 复制远程文件到本地，命令格式：scp [-Pport] username@host:remote_file_path local_directoryscp -P22 root@100.75.35.101:/root/perf.svg ~/Downloads# 复制远程文件到其他主机，命令格式：scp [-Pport] remote_file_path username@host:remote_directoryscp -P22 root@100.75.35.101:/root/perf.svg root@100.75.35.102:/root rename # 批量重命名，将 tcl 目录下 *.java 文件名称中的 MySQL 替换为 Dorisrename s/MySQL/Doris/ tcl/*.java"},{"title":"VirtualBox","path":"/notes/virtual_box.html","content":"Kernel driver not installed (rc=-1908) Mac 上使用 VirtualBox 虚拟机安装系统出现如下异常： Kernel driver not installed (rc=-1908)Make sure the kernel module has been loaded successfully.where: suplibOsInit what: 3 VERR_VM_DRIVER_NOT_INSTALLED (-1908) - The support driver is not installed. On linux, open returned ENOENT. 首先需要在 系统偏好设置-安全性与隐私 中，允许 VirtualBox 加载，然后再执行如下命令重启 VirtualBox。 sudo /Library/Application\\ Support/VirtualBox/LaunchDaemons/VirtualBoxStartup.sh restart 重启 VirtualBox 后再次尝试，可以正常安装或使用系统了。"},{"title":"WireShark","path":"/notes/wireshark.html","content":"Mac Wireshark 无法抓包，出现无权限异常 Mac Wireshark 权限异常 sudo chmod 777 /dev/bpf*"},{"path":"/ads/google/article_ads.html","content":"(adsbygoogle = window.adsbygoogle || []).push({}); 赞助商"},{"path":"/more/news/index.html","content":"关于博主我的动态 2024-02-09新年到，龙年，祝大家龙马精神，龙年大吉！👏👏👏"},{"title":"背景","path":"/wiki/avatica/background.html","content":"原文链接：https://calcite.apache.org/avatica/docs/ Avatica 是一个用于构建数据库 JDBC 和 ODBC 驱动程序，以及 RPC 有线协议的框架。 Avatica 的 Java 绑定依赖性非常小。尽管它是 Apache Calcite 的一部分，但它并不依赖于 Calcite 的其他部分。它仅依赖于 JDK 8+ 和 Jackson。 Avatica 的有线协议是 JSON 或 HTTP 上的协议缓冲区。JSON 协议的 Java 实现使用 Jackson，将请求命令对象转换为 JSON，或从 JSON 转换为响应命令对象。 Avatica-Server 是 Avatica RPC 的 Java 实现。 核心概念： Meta 是一个本地 API，通过它能够实现任何 Avatica provider 提供程序； AvaticaFactory 在 Meta 之上创建 JDBC 类的实现； Service 是一个接口，它实现了 Meta 在请求和响应命令对象方面的功能。 JDBC Avatica 通过 AvaticaFactory 实现 JDBC。AvaticaFactory 的实现在 Meta 之上创建 JDBC 类 (Driver、Connection、Statement、ResultSet) 的实现。 ODBC Avatica ODBC 的工作尚未开始。 Avatica ODBC 将使用相同的有线协议，并可以使用 Java 中的相同服务器实现。ODBC 客户端将用 C 或 C++ 编写。 由于 Avatica 协议抽象了 provider 提供程序之间的许多差异，因此相同的 ODBC 客户端可用于不同的数据库。 虽然 Avatica 项目不包含 ODBC 驱动程序，但是有基于 Avatica 协议编写的 ODBC 驱动程序，例如 Apache Phoenix 的 ODBC 驱动程序。 HTTP 服务 Avatica 服务端嵌入了 Jetty HTTP 服务器，提供了一个实现 Avatica RPC 协议的 HttpServer 类，可以作为独立的 Java 应用程序运行。 如果需要，可以通过扩展 HttpServer 类，并重写其 configureConnector() 方法，来配置 HTTP 服务器中的连接器。例如，用户可以将 requestHeaderSize 设置为 64K 字节，如下所示： HttpServer server = new HttpServer(handler) @Override protected ServerConnector configureConnector( ServerConnector connector, int port) HttpConnectionFactory factory = (HttpConnectionFactory) connector.getDefaultConnectionFactory(); factory.getHttpConfiguration().setRequestHeaderSize(64 10); return super.configureConnector(connector, port); ;server.start(); 项目结构 我们知道客户端库具有最小的依赖性非常重要。 Avatica 是 Apache Calcite 的一个子项目，在一个单独的存储库中维护。它不依赖于 Calcite 的任何其他部分。 软件包： org.apache.calcite.avatica 核心框架； org.apache.calcite.avatica.remote 使用远程过程调用的 JDBC 驱动程序； org.apache.calcite.avatica.server HTTP 服务器； org.apache.calcite.avatica.util 实用程序。 状态 已实现的 创建连接 create connection、创建语句 create statement、元数据 metadata、准备 prepare、绑定 bind、执行 execute、获取 fetch； 通过 HTTP 使用 JSON 格式进行 RPC 调用； 本地实现； 通过现有的 JDBC 驱动程序实现； 复合 RPC 调用（将多个请求组合成一次往返）： 执行 - 获取； 元数据获取（元数据调用，例如 getTables 返回所有行）。 未实现的 ODBC； RPC 调用： CloseStatement； CloseConnection； 复合 RPC 调用： CreateStatement - Prepare； CloseStatement - CloseConnection； 准备 - 执行 - 获取（Statement.executeQuery 应该获取前 N 行）。 从语句表中删除语句； DML (INSERT, UPDATE, DELETE)； Statement.execute 应用于 SELECT 语句。 客户端 以下是可用的 Avatica 客户端列表，其中一些客户端是 Apache Phoenix 的适配器，但也与其他 Avatica 后端兼容。非常欢迎为其他语言的客户端做出贡献！ 适用于 Apache Phoenix 查询服务器的 Microsoft .NET 驱动程序 主页 语言：C# 许可证：Apache 2.0 Avatica 版本 1.2.0 及以上 维护者：Microsoft Azure Apache Phoenix/Avatica SQL 驱动程序 主页 语言：Go 许可证：Apache 2.0 Avatica 版本 1.8.0 及以上 维护者：Boostport 和 Apache Calcite 社区 Avatica thin 客户端 主页 语言：Java 许可证：Apache 2.0 任何 Avatica 版本 维护者：Apache Calcite 社区 适用于 Python 的 Apache Phoenix 数据库适配器 主页 语言：Python 许可证：Apache 2.0 Avatica 版本 1.2.0 及以上 维护者：Apache Phoenix 社区 JavaScript 绑定到 Calcite Avatica 服务器 主页 语言：JavaScript 许可证：MIT 任何 Avatica 版本 维护者：Waylay.io Calcite Avatica CLI：基于 Go 的工具 主页 语言：Go 许可证：Apache 2.0 Avatica 版本 1.8.0 及以上 维护者：Satya Kommula 写在最后 笔者因为工作原因接触到 Calcite，前期学习过程中，深感 Calcite 学习资料之匮乏，因此创建了 Calcite 从入门到精通知识星球，希望能够将学习过程中的资料和经验沉淀下来，为更多想要学习 Calcite 的朋友提供一些帮助。"},{"title":"路线图","path":"/wiki/avatica/roadmap.html","content":"原文链接：https://calcite.apache.org/avatica/docs/roadmap.html 状态 已实现 创建连接 create connection、创建语句 create statement、元数据 metadata、准备 prepare、绑定 bind、执行 execute、获取 fetch； 通过 HTTP 使用 JSON 进行 RPC 调用； 本地实现； 通过现有的 JDBC 驱动程序实现； 复合 RPC 调用（将多个请求组合成一次往返）： 执行 - 获取； 元数据获取（元数据调用，例如 getTables 返回所有行）。 未实现 ODBC RPC 调用： CloseStatement； CloseConnection。 复合 RPC 调用： CreateStatement - Prepare； CloseStatement - CloseConnection； 准备 - 执行 - 获取（Statement.executeQuery 应该获取前 N 行）； 从语句表中删除语句； DML (INSERT, UPDATE, DELETE)； Statement.execute 应用于 SELECT 语句。 写在最后 笔者因为工作原因接触到 Calcite，前期学习过程中，深感 Calcite 学习资料之匮乏，因此创建了 Calcite 从入门到精通知识星球，希望能够将学习过程中的资料和经验沉淀下来，为更多想要学习 Calcite 的朋友提供一些帮助。"},{"title":"学习资料","path":"/wiki/c/index.html","content":"C 语言基础教程学习过程中参考了如下资料，如有其他推荐的参考资料，欢迎大家留言补充。 学习资料 C 语言精讲教程 深入 C 语言和程序运行原理"},{"title":"C 语言入门","path":"/wiki/c/introduction.html","content":"初识 C 语言 1969 年，美国贝尔实验室的肯-汤普森（Ken Thompson）与丹尼斯-里奇（Dennis Ritchie）一起开发了 Unix 操作系统。Unix 是用汇编语言写的，依赖于计算机硬件。为了程序的 可读性 和 可移植性，他们决定使用高级语言重写。但是，当时的高级语言无法满足他们的要求，汤普森就在 BCPL 语言的基础上发明了 B 语言。1972 年，丹尼斯-里奇（Dennis Ritchie）在 B 语言的基础上重新设计了一种新语言，这种新语言取代了 B 语言，称为 C 语言。1973 年， 整个 Unix 系统都使用 C 语言重写。 此后，这种语言快速流传，广泛用于各种操作系统和系统软件的开 发。如 UNIX、MS-DOS、Microsoft Windows 及 Linux 等。1988 年，美国国家标准协会（ANSI）正式将 C 语言标准化 ，标志着 C 语言开始稳定和规范化。 为什么要学习 C 语言 C 语言具有可移植性好、跨平台的特点，用 C 编写的代码可以在不同的操作系统和硬件平台上编译和运行； C 语言在许多领域应用广泛； 操作系统：C 广泛用于开发操作系统，如 Unix、Linux 和 Windows； 嵌入式系统：C 是一种用于开发嵌入式系统（如微控制器、微处理器和其他电子设备）的流行语言； 系统软件：C 用于开发设备驱动程序、编译器和汇编器等系统软件； 网络：C 语言广泛用于开发网络应用程序，例如 Web 服务器、 网络协议和网络驱动程序； 数据库系统：C 用于开发数据库系统，例如 Oracle、MySQL 和 PostgreSQL； 游戏：由于 C 能够处理低级硬件交互，因此经常用于开发计算机游戏； 人工智能：C 用于开发人工智能和机器学习应用程序，例如神经网络和深度学习算法； 科学应用：C 用于开发科学应用程序，例如仿真软件和数值分析工具； 金融应用：C 用于开发股票市场分析和交易系统等金融应用； C 语言能够直接对硬件进行操作、管理内存、跟操作系统对话，这使得它是一种非常接近底层的语言，非常适合写需要跟硬件交互、有极高性能要求的程序； 学习 C 语言有助于快速上手其他编程语言，比如 C++（原先是 C 语言的一个扩展，在 C 语言的基础上嫁接了面向对象编程）、C#、 Java、PHP、Javascript、Perl 等，这些语言都继承或深受 C 语言的影响和启发； C 语言长盛不衰。至今，依然是最广泛使用、最流行的编程语言之一。包括很多大学将 C 语言作为计算机教学的入门语言，拥有庞大 而活跃的用户社区，这意味着有许多资源和库可供开发人员使用。 (adsbygoogle = window.adsbygoogle || []).push({}); 赞助商 C 语言的版本选择 随着微型计算机的日益普及，出现了许多 C 语言版本。 版本 1——KR C： KR C 指的是 C 语言的原始版本。1978 年，C 语言的发明者布莱恩-柯林（Brian Kernighan）和丹尼斯-里奇（Dennis Ritchie）合写了一本著名的教材《C 编程语言》（The C programming language）。由于 C 语言还没有成文的语法标准，这本书就成了公认标准，以两位作者的姓氏首字母作为版本简称 KR C。 版本 2——ANSI C（又称 C89 或 C90）： C 语言的原始版本非常简单，对很多情况的描述非常模糊，加上 C 语法依然在快速发展，要求将 C 语言标准化的呼声越来越高。1989 年，美国国家标准协会（ANSI）制定了一套 C 语言标准，并于次年被国际标准化组织（ISO）通过。它被称为 ANSI C，也可以按照发布年份，称为 C89 或 C90。 版本 3——C99： C 语言标准的第一次大型修订，发生在 1999 年，增加了许多语言特性，比如双斜杠（//）的注释语法，可变长度数组、灵活的数组成 员、复数、内联函数和指定的初始值设定项。这个版本称为 C99，是目前最流行的 C 版本。 版本 4——C11： 2011 年，标准化组织再一次对 C 语言进行修订，增加了_Generic、 static_assert 和原子类型限定符，这个版本称为 C11。 需要强调的是，修订标准的原因不是因为原标准不能用，而是需要跟进新的技术。 版本 5——C17： C11 标准在 2017 年进行了修补，但发布是在 2018 年。新版本只是解决了 C11 的一些缺陷，没有引入任何新功能。这个版本称为 C17。 版本 6——C23： 2023 年预计发布，计划进一步增强安全性，消除实现定义的行为，引入模块化语言概念等新特性，使 C 语言在安全和可靠性方面有重大 提高。 第一个 C 语言程序——Hello World C 语言的源代码文件，以后缀名 .c 结尾，下面是我们学习的第一个 C 语言程序——Hello World。 // 引入 C 语言标准输入输出头文件#include stdio.h// C 语言入口 main 函数int main() // 通过 printf 输出字符串 printf(Hello, World! ); return 0; 我们将它保存在 HelloWorld.c 文件中，并使用 gcc 编译器进行编译，-o 参数指定了输出二进制文件的名称，此外，还可以指定 -std 参数指定编译的 C 语言标准： gcc -o HelloWorld HelloWorld.cgcc -std=c99 -o HelloWorld HelloWorld.c 编译完成后，我们就得到了一个可执行程序，使用如下的命令执行，可以看到输出了 Hello, World!： ./HelloWorldHello, World! 使用 CLion IDE 开发 C 程序 IDE（Integrated Development Environment，集成开发环境）相较于文本开发工具，它可以把代码编写、编译、执行、调试等多种功能综合到一起，有效地提升开发效率。CLion 是一款由 JetBrains 推出的跨平台 C/C++ 集成开发环境，它具有智能编辑器、CMake 构建支持、调试器、单元测试、代码分析等功能，可以极大提高 C/C++ 开发效率。 下图展示了使用 CLion 创建项目，用户可以选择创建 C 可执行文件，还是 C 库文件，并可以选择对应的 C 语言标准。 创建完成后，项目中自带了一个 main.c 文件，直接选择右上角的 Run 或 Debug 按钮，可以执行 C 程序，下方的 Debug 窗口展示了执行结果。 为了方便后续 C 语言的学习，我们需要在 c_lecture 项目中创建多个子目录，如下图所示，我们创建了 hello_world 目录，并将前文练习的 HelloWorld 源码复制过来，可以发现执行出现了报错，这是因为一个 C 程序中只允许存在一个 main 函数。 为了解决这个问题，我们需要安装 C/C++ Single File Execution 插件，然后在需要执行的代码中右键选择 Add executable for single c/cpp file，此时 CMakeLists.txt 文件中多处了一行 add_executable(HelloWorld hello_world/HelloWorld.c)，然后我们再右击项目文件夹，选择 Reload CMake Project 进行刷新，此时再次执行 HelloWorld 程序，发现可以正常执行。 cmake_minimum_required(VERSION 3.28)project(c_lecture C)set(CMAKE_C_STANDARD 99)add_executable(main main.c)add_executable(HelloWorld hello_world/HelloWorld.c) C 程序的运行流程 C 程序从编写到执行总共需要 4 个步骤：编辑、编译、链接和执行，编辑指的是编写 C 源码，并将源码存储为 .c 源文件的过程。编译则是使用编译器，将源码转换为目标程序的过程，如果程序没有任何报错，则会生成一个扩展名为 .obj 的二进制文件。由于 C 程序中需要引入其他依赖库，因此链接会将编译好的目标程序，以及其他依赖的程序库链接到一起，形成统一的可执行二进制程序。有了可执行程序，最终我们可以直接在命令行中执行程序。 C 语言的注释 C 语言中支持两种注释类型： 单行注释： // 单行注释 多行注释（或块注释）： /*这是第一行注释 这是第二行注释 这是第三行注释*/"},{"title":"适配器","path":"/wiki/calcite/adapters.html","content":"原文链接：https://calcite.apache.org/docs/adapter.html 模式适配器 模式适配器允许 Calcite 读取特定类型的数据，并将这些数据显示为模式中的表。 主要适配器 Arrow 适配器（calcite-arrow） Cassandra 适配器（calcite-cassandra）； CSV 适配器（示例/csv）； Druid 适配器（calcite-druid）； Elasticsearch 适配器（calcite-elasticsearch）； 文件适配器（calcite-file）； Geode 适配器（calcite-geode）； InnoDB 适配器（calcite-innodb）； JDBC 适配器（calcite-core 的一部分）； MongoDB 适配器（calcite-mongodb）； 操作系统适配器（calcite-os）； Pig 适配器（calcite-pig）； Redis 适配器（calcite-redis）； Solr cloud 适配器（solr-sql）； Spark 适配器（calcite-spark）； Splunk 适配器（calcite-splunk）； Eclipse 内存分析器 (MAT) 适配器（mat-calcite-plugin）； Apache Kafka 适配器。 其他语言接口 Piglet（calcite-piglet）在 Pig Latin 的子集中运行查询； 引擎 许多项目和产品使用 Apache Calcite 进行 SQL 解析、查询优化、数据虚拟化/联邦查询和物化视图重写。他们中的一些列在了由 Calcite 提供支持页面上。 (adsbygoogle = window.adsbygoogle || []).push({}); 赞助商 驱动 驱动允许你从应用程序连接到 Calcite。 JDBC 驱动程序； JDBC 驱动由 Avatica 提供支持。连接可以是本地连接或远程连接（基于 HTTP 协议传输的 JSON 或 Protobuf）。 JDBC 连接字符串的基本格式如下： jdbc:calcite:property=value;property2=value2 其中 property，property2 是下面描述的这些属性。连接字符串遵循 OLE DB 连接字符串语法，由 Avatica 的 ConnectStringParser 实现。 JDBC 连接字符串参数 属性 描述 approximateDecimal 是否可以接受 DECIMAL 类型聚合函数返回近似结果。 approximateDistinctCount 是否可以接受 COUNT(DISTINCT ...) 聚合函数返回近似结果。 approximateTopN 是否可以接受 Top N 查询（ORDER BY aggFun() DESC LIMIT n）返回近似结果。 caseSensitive 标识符匹配是否区分大小写。如果未指定，将会使用 lex 中的值。 conformance SQL 一致性级别。包含如下值：DEFAULT（默认值，类似于 PRAGMATIC_2003）、LENIENT、MYSQL_5、ORACLE_10、ORACLE_12、PRAGMATIC_99、PRAGMATIC_2003、STRICT_92、STRICT_99、STRICT_2003、SQL_SERVER_2008。 createMaterializations Calcite 是否应该创建物化视图。默认为 false。 defaultNullCollation 如果查询中既未指定 NULLS FIRST 也未指定 NULLS LAST，应该如何对 NULL 值进行排序。默认值为 HIGH，对 NULL 值的排序与 Oracle 相同。 druidFetch 执行 SELECT 查询时，Druid 适配器应当一次获取多少行记录。 forceDecorrelate 优化器是否应该尽可能地尝试去除相关子查询。默认为 true。 fun 内置函数和运算符的集合。有效值为 standard（默认值）、oracle、spatial，并且可以使用逗号组合，例如 oracle,spatial。 lex 词法分析策略。有效值为 BIG_QUERY、JAVA、MYSQL、MYSQL_ANSI、ORACLE（默认）、SQL_SERVER。 materializationsEnabled Calcite 是否应该使用物化视图。默认为 false。 model JSON/YAML 模型文件的 URI 或内联的 JSON（例如：inline:...） 、内联的 YAML（例如： inline:...）。 parserFactory 解析器工厂。实现 interface SqlParserImplFactory 并具有公共默认构造函数或 INSTANCE 常量的类的名称。 quoting 如何引用标识符。值为 DOUBLE_QUOTE、BACK_TICK、BACK_TICK_BACKSLASH、BRACKET。如果未指定，则使用 lex 中的值。 quotedCasing 如果标识符被引用，设置如何存储标识符。值为 UNCHANGED、TO_UPPER、TO_LOWER。如果未指定，则使用 lex 中的值。 schema 初始模式的名称。 schemaFactory 模式工厂。实现 interface SchemaFactory 并具有公共默认构造函数或 INSTANCE 常量的类的名称。如果指定了 model 则忽略该参数。 schemaType 模式类型。值必须是 MAP（默认值）、JDBC 或 CUSTOM（如果指定了 schemaFactory 则隐式设置为 CUSTOM）。如果指定了 model 则忽略该参数。 spark 指定是否应使用 Spark 作为引擎来处理无法推送到源系统的处理。如果为 false（默认值），Calcite 会生成实现 Enumerable 接口的代码。 timeZone 时区，例如 gmt-3。默认是 JVM 的时区。 typeSystem 类型系统。实现 interface RelDataTypeSystem 并具有公共默认构造函数或 INSTANCE 常量的类的名称。 unquotedCasing 如果标识符未加引号，设置如何存储标识符。值为 UNCHANGED、TO_UPPER、TO_LOWER。如果未指定，则使用 lex 中的值。 typeCoercion sql 节点校验时，如果类型不匹配是否进行隐式类型强转，默认为 true。 要基于内置模式类型连接到单个模式，你不需要指定 model 参数。例如，通过映射到 foodmart 数据库的 JDBC 模式适配器创建一个模式，并使用这个模式创建一个数据库连接。 jdbc:calcite:schemaType=JDBC; schema.jdbcUser=SCOTT; schema.jdbcPassword=TIGER; schema.jdbcUrl=jdbc:hsqldb:res:foodmart 同样，你可以基于用户定义的模式适配器连接到单个模式。例如： jdbc:calcite:schemaFactory=org.apache.calcite.adapter.cassandra.CassandraSchemaFactory; schema.host=localhost; schema.keyspace=twissandra 与 Cassandra 适配器建立连接，可以通过编写如下的模型文件实现： version: 1.0, defaultSchema: foodmart, schemas: [ type: custom, name: twissandra, factory: org.apache.calcite.adapter.cassandra.CassandraSchemaFactory, operand: host: localhost, keyspace: twissandra ] 请注意 operand 部分中的每个键，在连接字符串中使用都需要加上 schema. 前缀。 服务器 Calcite 的核心模块 (calcite-core) 支持 SQL 查询 (SELECT) 和 DML 操作 (INSERT， UPDATE， DELETE， MERGE)，但不支持 CREATE SCHEMA 或 CREATE TABLE 等 DDL 操作。正如我们将看到的，DDL 使元数据库中的状态模型变得复杂，并使解析器更难以扩展，因此我们将 DDL 排除在核心之外。 服务器模块 (calcite-server) 为 Calcite 添加了 DDL 支持。它扩展了 SQL 解析器，使用与子项目相同的机制，添加了一些 DDL 命令： CREATE 和 DROP SCHEMA； CREATE 和 DROP FOREIGN SCHEMA； CREATE 和 DROP TABLE（包括 CREATE TABLE ... AS SELECT）； CREATE 和 DROP MATERIALIZED VIEW； CREATE 和 DROP VIEW； CREATE 和 DROP FUNCTION； CREATE 和 DROP TYPE。 SQL 参考中描述了这些命令。 要启用 Calite 服务器模块，请将 calcite-server.jar 包含在你的类路径中，并添加 parserFactory=org.apache.calcite.sql.parser.ddl.SqlDdlParserImpl#FACTORY 到 JDBC 连接字符串（请参阅连接字符串属性 parserFactory）。下面是一个使用 sqlline shell 的示例。 $ ./sqllinesqlline version 1.3.0 !connect jdbc:calcite:parserFactory=org.apache.calcite.sql.parser.ddl.SqlDdlParserImpl#FACTORY sa CREATE TABLE t (i INTEGER, j VARCHAR(10));No rows affected (0.293 seconds) INSERT INTO t VALUES (1, a), (2, bc);2 rows affected (0.873 seconds) CREATE VIEW v AS SELECT * FROM t WHERE i 1;No rows affected (0.072 seconds) SELECT count(*) FROM v;+---------------------+| EXPR$0 |+---------------------+| 1 |+---------------------+1 row selected (0.148 seconds) !quit calcite-server 模块是可选的。它的目标之一是使用可以从 SQL 命令行尝试的简单示例，来展示 Calcite 的功能（例如物化视图、外部表和自动生成列）。 calcite-server 使用的所有功能都可以通过 calcite-core 中的 API 获得。 如果你是子项目的作者，你的语法扩展不太可能与 calcite-server 中的语法扩展匹配，因此我们建议你通过扩展核心解析器来添加 SQL 语法扩展。如果你需要 DDL 命令，你可以将 calcite-server 复制粘贴到你的项目中。 目前，元数据库尚未持久化。当你执行 DDL 命令时，你正在通过添加和删除可从根 Schema 访问的对象，来修改内存元数据库。同一 SQL 会话中的所有命令都将看到这些对象。你可以通过执行相同的 SQL 命令脚本在将来的会话中创建相同的对象。 Calcite 还可以充当数据虚拟化或联邦查询的服务器：Calcite 管理多个外部模式中的数据，但对于客户端而言，这些数据似乎都在同一个地方。Calcite 选择应在何处进行处理，以及是否创建数据副本以提高效率。calcite-server 模块是朝着这一目标迈出的一步；行业级解决方案需要进一步打包（使 Calcite 作为服务运行）、元数据库持久性、授权和安全性。 可扩展性 还有许多其他 API 允许你扩展 Calcite 的功能。 在本节中，我们将简要描述这些 API，让你了解可能发生的情况。要充分使用这些 API，你需要阅读其他文档，例如接口的 javadoc，并可能查找我们为它们编写的测试。 函数和运算符 有多种方法可以向 Calcite 添加运算符或函数。我们将首先描述最简单的（也是最不强大的）。 用户定义的函数是最简单的（但功能最弱）。它们编写起来很简单（你只需编写一个 Java 类并将其注册到你的模式中），但在参数的数量和类型、解析重载函数或派生的返回类型方面没有提供太多灵活性。 如果你想要这种灵活性，你可能需要编写一个用户定义的运算符（请参考 interface SqlOperator）。 如果你的运算符不遵守标准 SQL 函数语法 f(arg1, arg2, ...)，那么你需要去扩展解析器。 测试中有很多好的例子：class UdfTest 测试了用户定义函数和用户定义聚合函数。 聚合函数 用户定义的聚合函数与用户定义的函数类似，但每个函数都有几个相应的 Java 方法，用于聚合生命周期中的每个阶段： init 创建一个累加器； add 将一行的值添加到累加器中； merge 将两个累加器合二为一； result 完成累加器并将其转换为结果。 举个例子，SUM(int) 的方法（伪代码）如下： struct Accumulator final int sum;Accumulator init() return new Accumulator(0);Accumulator add(Accumulator a, int x) return new Accumulator(a.sum + x);Accumulator merge(Accumulator a, Accumulator a2) return new Accumulator(a.sum + a2.sum);int result(Accumulator a) return a.sum; 以下是计算列值为 4 和 7 的两行之和的调用序列： a = init() # a = 0a = add(a, 4) # a = 4a = add(a, 7) # a = 11return result(a) # returns 11 窗口函数 窗口函数类似于聚合函数，但它应用于由 OVER 子句而不是 GROUP BY 子句收集的一组行。每个聚合函数都可以用作窗口函数，但存在一些关键区别。窗口函数看到的行可能是有序的，并且依赖于顺序的窗口函数（例如 RANK ）不能用作聚合函数。 另一个区别是窗口可以是相交的（non-disjoint）：特定行可以出现在多个窗口中。例如，10:37 既可以出现在 9:00-10:00 时间段，也可以出现在 9:15-9:45 时间段。 窗口函数是动态计算的：当时钟从 10:14 跳转到 10:15 时，可能有两行进入窗口，而三行离开。为此，窗口函数有一个额外的生命周期操作： remove 从累加器中删除一个值。 它的伪代码 SUM(int) 是： Accumulator remove(Accumulator a, int x) return new Accumulator(a.sum - x); 以下是计算前 2 行动态求和（SUM）的调用顺序，其中 4 行的数值为 4、7、2 和 3： a = init() # a = 0a = add(a, 4) # a = 4emit result(a) # emits 4a = add(a, 7) # a = 11emit result(a) # emits 11a = remove(a, 4) # a = 7a = add(a, 2) # a = 9emit result(a) # emits 9a = remove(a, 7) # a = 2a = add(a, 3) # a = 5emit result(a) # emits 5 分组窗口函数 分组窗口函数是操作 GROUP BY 子句并将记录聚集成集合的函数。内置的分组窗口函数是 HOP、TUMBLE 和 SESSION。你可以通过实现 interface SqlGroupedWindowFunction 来定义其他函数。 表函数和表宏 用户自定义表函数的定义方式，与常用的标量用户自定义函数类似，但在查询的 FROM 子句中使用。以下查询使用名为 Ramp 的表函数： SELECT * FROM TABLE(Ramp(3, 4)) 用户自定义表宏使用与表函数相同的 SQL 语法，但定义不同。它们不是生成数据，而是生成关系表达式。在查询准备期间调用表宏，然后可以优化它们生成的关系表达式。（Calcite 的视图实现使用表宏） class TableFunctionTest 测试了表函数并包含几个有用的示例。 扩展解析器 假设你需要在保持语法兼容的情况下，扩展 Calcite 的 SQL 语法。在你的项目中复制 Parser.jj 语法文件将是愚蠢的，因为语法经常被编辑。 幸运的是，Parser.jj 实际上是一个 Apache FreeMarker 模板，其中包含可以替换的变量。calcite-core 中的解析器使用变量的默认值（通常为空）实例化模板，但你也可以覆盖这些变量。如果你的项目需要不同的解析器，你可以提供自己的 config.fmpp 和 parserImpls.ftl 文件，从而生成扩展解析器。 calcite-server 模块是在 CALCITE-707 中创建的，并添加了 DDL 语句，例如 CREATE TABLE，是你可以参考的示例。另外可以参考 class ExtensionSqlParserTest。 自定义接受和生成的 SQL 方言 要自定义解析器应接受的 SQL 扩展，请实现 interface SqlConformance 或使用 enum SqlConformanceEnum. 要控制如何为外部数据库生成 SQL（通常通过 JDBC 适配器），请使用 class SqlDialect。方言还描述了引擎的功能，例如它是否支持 OFFSET 和 FETCH 子句。 定义自定义模式 要定义自定义模式，你需要实现 interface SchemaFactory。 在查询准备期间，Calcite 将调用此接口，来查找自定义模式包含哪些表和子模式。当查询引用了模式中的表时，Calcite 将要求自定义模式创建 interface Table。 表将被包装在 TableScan 中，并将经历查询优化过程。 反射模式 反射模式（class ReflectiveSchema）是一种包装 Java 对象以使其显示为模式的方法。其中的集合字段将展示为表格。 它不是一个模式工厂，而是一个实际的模式。你必须创建对象并通过调用 API 将其包装在模式中。 参考 class ReflectiveSchemaTest。 定义自定义表 要定义自定义表，你需要实现 interface TableFactory。模式工厂是一组命名表，而表工厂在绑定到具有特定名称（以及可选的一组额外操作数）的模式时会生成单个表。 修改数据 如果你的表要支持 DML 操作（INSERT、UPDATE、DELETE、MERGE），则你的 interface Table 实现类必须同时实现 interface ModifiableTable。 流式操作 如果你的表支持流式查询，则你的 interface Table 实现类必须实现 interface StreamableTable。 请参考 class StreamTest 示例。 将操作下推到你的表中 如果你希望将处理逻辑下推到自定义表的源系统，请考虑实现 interface FilterableTable 或 interface ProjectableFilterableTable。 如果你想要更多的控制，你应该写一个优化规则。这将允许你下推表达式，并基于代价做出关于是否下推处理的决定，以及下推更复杂的操作，例如：连接、聚合和排序。 类型系统 你可以通过实现 interface RelDataTypeSystem 来自定义类型系统的某些方面。 关系运算符 所有关系运算符都实现 interface RelNode，并且大多数扩展了 class AbstractRelNode。最核心的运算符（被 SqlToRelConverter 使用并覆盖了常规的关系代数）是 TableScan， TableModify， Values， Project， Filter， Aggregate， Join， Sort， Union， Intersect， Minus， Window 和 Match。 其中每一个都有一个纯逻辑子类， LogicalProject 等。任何给定的适配器都会有对应的操作，其引擎可以有效地实现。例如，Cassandra 适配器有 CassandraProject 但没有 CassandraJoin。 你可以定义自己的 RelNode 子类来添加新运算符，或在特定引擎中添加现有运算符实现。 为了使运算符有用且强大，你需要将优化器规则与现有运算符相结合（并且还提供元数据，见下文）。这些是关系代数，它们的效果是组合的：你虽然编写了少量的规则，但它们组合起来能够处理指数数量的查询模式。 如果可能，让你的运算符成为现有运算符的子类；那么你也许就可以重新使用或调整他们对应的规则。更好的是，如果你的运算符是一个可以根据现有运算符重写（再次通过优化器规则）的逻辑运算符，那么你应该这样做。你将无需额外工作即可重复使用这些运算符的规则、元数据和实现。 优化规则 优化器规则 (class RelOptRule) 将关系表达式转换为等效的关系表达式。 优化器引擎注册了许多优化器规则，并触发它们从而将输入的查询转换为更有效的查询。因此，优化器规则是优化过程的核心，但令人惊讶的是，每个优化器规则本身并不关心代价。优化器引擎负责按顺序触发规则以产生最佳计划，但每个单独的规则只关心自己的正确性。 Calcite 有两个内置的优化器引擎：class VolcanoPlanner 使用动态规划，它适用于穷举搜索，而 class HepPlanner 以更固定的顺序触发一系列规则。 调用约定 调用约定是特定数据引擎使用的协议。例如，Cassandra 引擎有一组关系运算符，CassandraProject，CassandraFilter 等，并且这些运算符可以相互连接，而无需将数据从一种格式转换成另一种格式。 如果数据需要从一种调用约定转换为另一种调用约定，Calcite 使用称为转换器的特殊关系表达式子类（请参阅 interface Converter）。但当然，转换数据有运行时的成本。 在优化器使用多个引擎进行查询时，Calcite 根据调用约定对关系表达式树的区域进行着色。优化器通过触发规则将操作推送到数据源中。如果引擎不支持特定操作，则不会触发规则。有时一项操作可能会发生在多个地方，最终会根据代价选择最佳方案。 调用约定是一个实现 interface Convention 的类 、一个辅助接口（例如 interface CassandraRel），以及一组为核心关系运算符而实现 class RelNode 接口的子类（Project、 Filter、 Aggregate 等）。 内置 SQL 实现 如果适配器没有实现所有核心关系运算符，Calcite 如何实现 SQL？ 答案是特定的内置调用约定 EnumerableConvention。Enumerable 约定的关系表达式作为内置实现：Calcite 生成 Java 代码，对其进行编译，并在其自己的 JVM 中执行。Enumerable 约定的效率低于运行在面向列的数据文件上的分布式引擎，但它可以实现所有核心关系运算符以及所有内置 SQL 函数和运算符。如果数据源无法实现关系运算符，则可以使用枚举约定。 统计和代价 Calcite 有一个元数据系统，允许你定义有关关系运算符的代价函数和统计信息，统称为元数据。每种元数据都有一个单方法的接口（通常）。例如，选择性由class RelMdSelectivity 和 getSelectivity(RelNode rel, RexNode predicate) 方法定义。 有许多种内置的元数据，包括：排序规则、 列来源、 列唯一性、 唯一行数、 分布、 执行计划可见性、 表达式血缘、 最大行数、 节点类型、 并行度、 原始行百分比、 总体大小、 谓词、 行数、 选择性、 大小、 表引用 和 唯一键。你也可以定义自己的元数据。 然后，你可以提供一个元数据提供程序，为 RelNode 的特定子类计算此类元数据。元数据提供程序可以处理内置和扩展元数据类型，以及内置和扩展 RelNode 类型。在准备查询时，Calcite 结合了所有适用的元数据提供者并维护一个缓存，以便给定的元数据（例如特定 Filter 运算符中条件 x 10 的选择性）仅计算一次。 写在最后 笔者因为工作原因接触到 Calcite，前期学习过程中，深感 Calcite 学习资料之匮乏，因此创建了 Calcite 从入门到精通知识星球，希望能够将学习过程中的资料和经验沉淀下来，为更多想要学习 Calcite 的朋友提供一些帮助。"},{"title":"代数","path":"/wiki/calcite/algebra.html","content":"原文链接：https://calcite.apache.org/docs/algebra.html 关系代数是 Calcite 的核心。每个查询都可以表示为一个关系运算符树。你可以将 SQL 转换为关系代数，也可以直接构建关系运算符树。 优化器规则使用保持相同语义的数学恒等式来变换表达式树。例如，如果过滤器没有引用其他输入中的列，那么将过滤器推入到内部关联的输入则是有效的。 Calcite 通过反复地将优化器规则应用于关系表达式来优化查询。成本模型指导该过程，优化器引擎生成与原始语义相同，但成本较低的替代表达式。 优化过程是可扩展的。你可以添加自己的关系运算符、优化器规则、成本模型和统计信息。 代数构建器 构建关系表达式的最简单方法是使用代数构建器 RelBuilder。下面是一个例子： 表扫描 final FrameworkConfig config;final RelBuilder builder = RelBuilder.create(config);final RelNode node = builder.scan(EMP).build();System.out.println(RelOptUtil.toString(node)); 你可以在 RelBuilderExample.java 中找到这个例子和其他例子的完整代码。这段代码打印如下： LogicalTableScan(table=[[scott, EMP]]) 它创建了对 EMP 表的扫描，相当于如下 SQL： SELECT * FROM scott.EMP; 添加投影 现在，让我们添加一个投影，相当于如下 SQL： SELECT ename, deptno FROM scott.EMP; 我们只需要在调用 build 方法前，添加一个 project 方法调用： final RelNode node = builder.scan(EMP).project(builder.field(DEPTNO), builder.field(ENAME)).build();System.out.println(RelOptUtil.toString(node)); 输出结果如下： LogicalProject(DEPTNO=[$7], ENAME=[$1]) LogicalTableScan(table=[[scott, EMP]]) 对 builder.field 的两次调用创建了简单表达式，这些表达式从输入的关系表达式中返回字段。那也就是说，scan 方法的调用创建了 TableScan。Calcite 将它们转换为按序号的字段引用，例如：$7 和 $1。 添加过滤和聚合 下面是一个包含聚合和过滤的查询语句： final RelNode node = builder.scan(EMP).aggregate(builder.groupKey(DEPTNO), builder.count(false, C), builder.sum(false, S, builder.field(SAL))).filter(builder.call(SqlStdOperatorTable.GREATER_THAN, builder.field(C), builder.literal(10))).build();System.out.println(RelOptUtil.toString(node)); 相当于如下 SQL： SELECT deptno, count(*) AS c, sum(sal) AS s FROM emp GROUP BY deptno HAVING count(*) 10 并生成如下结果： LogicalFilter(condition=[($1, 10)]) LogicalAggregate(group=[7], C=[COUNT()], S=[SUM($5)]) LogicalTableScan(table=[[scott, EMP]]) 压栈和出栈 构建器使用 堆栈 来存储第一步生成的关系表达式，并将它作为输入传递给下一步。这允许生成关系表达式的方法生成一个构建器。 在大多数情况下，你只需要使用 build() 这个堆栈方法，用来获取最后一个关系表达式，也就是树的根节点。 有时候堆栈会嵌套得非常深，以至于令人困惑。为了让这些事情清楚明了，你可以从堆栈中去除些表达式。例如，我们正在构建下面这个复杂的连接查询： join / \\ join join / \\ / \\CUSTOMERS ORDERS LINE_ITEMS PRODUCTS 我们分三个阶段进行构建。先将中间结果存储在 left 和 right 变量中，然后使用 push() 方法，在创建最终的 Join 对象时，将它们放回堆栈中： final RelNode left = builder.scan(CUSTOMERS).scan(ORDERS).join(JoinRelType.INNER, ORDER_ID).build();final RelNode right = builder.scan(LINE_ITEMS).scan(PRODUCTS).join(JoinRelType.INNER, PRODUCT_ID).build();final RelNode result = builder.push(left).push(right).join(JoinRelType.INNER, ORDER_ID).build(); 转换约定 默认的 RelBuilder 会创建没有约定的逻辑 RelNode。但你可以通过 adoptConvention() 来进行切换，从而使用不同的约定： final RelNode result = builder.push(input).adoptConvention(EnumerableConvention.INSTANCE).sort(toCollation).build(); 在这个案例中，我们在 input RelNode 之上创建了一个 EnumerableSort。 字段名称和序号 你可以通过名称或序号来引用一个字段。 序号是从零开始的。每个运算符保证它输出字段出现的顺序。例如，Project 返回每个标量表达式生成的字段。 运算符的字段名称需要保证是唯一的，但有时这也意味着，名称并不完全符合你的预期。例如，当你对 EMP 和 DEPT 进行关联时，其中一个输出字段会叫做 DEPTNO，而另一个输出字段则会叫做类似 DEPTNO_1 的名称。 一些关系表达式方法让你能够更好地控制字段名称： project 允许你使用 alias(expr, fieldName) 来包装表达式。它删除了包装器，但保留了建议的名称（只要它是唯一的）； values(String[] fieldNames, Object... values) 接受一个字段名称数组。如果数组中的任何元素为空，构建器将会生成一个唯一的名称； 如果一个表达式投影成输入字段，或投影成输入字段的一个转换，那么它将使用输入字段的名称。 一旦唯一的字段名称完成了分配，这些名称就是不可变的。如果你有一个特定的 RelNode 实例，你可以依赖字段名称的不变性。事实上，整个关系表达式也是不可变的。 但是，如果一个关系表达式已经通过了多个重写规则（参考 RelOptRule），结果表达式的字段名称可能看起来与原始表达式不太一样。这种情况下，最好按照序号来引用字段。 当你正在构建一个接受多个输入的关系表达式时，你需要考虑到那些点，从而构建字段引用。这在构建关联条件时经常出现。 假设你正在 EMP 和 DEPT 上构建一个关联查询，EMP 有 8 个字段 EMPNO、ENAME、JOB、MGR、HIREDATE、SAL、COMM、DEPTNO，DEPT 有 3 个字段 DEPTNO、DNAME、LOC。在内部，Calcite 使用偏移量来表示这些字段，存储在一个包含 11 个字段的组合输入行中：左侧输入的第一个字段是 #0（请记住，序号从 0 开始），右侧输入的第一个字段是 #8。 通过构建器 API，你可以指定哪个输入的哪个字段。要引用内部字段序号是 #5 的 SAL，可以写成 builder.field(2, 0, SAL)，builder.field(2, EMP, SAL) 或 builder.field(2, 0, 5)。这个写法表示，在两个输入中，#0 输入的 #5 字段。为什么它需要知道有两个输入？因为它们存储在堆栈中，#1 输入位于堆栈顶部，#0 输入在其下方。如果我们不告诉构建器是两个输入，它不知道 #0 输入的深度。 类似地，要引用内部字段是 #9 (8 + 1) 的 DNAME，可以写成 builder.field(2, 1, DNAME)，builder.field(2, DEPT, DNAME) 或 builder.field(2, 1, 1)。 递归查询 警告：当前 API 是实验性的，如有变更不会另行通知。 下面是一个递归查询的 SQL，用于生成 1, 2, 3, ...10 这样的序列： WITH RECURSIVE aux(i) AS (VALUES (1) UNION ALL SELECT i + 1 FROM aux WHERE i 10) SELECT * FROM aux 可以对 TransientTable 和 RepeatUnion 进行表扫描，来生成这个 SQL： final RelNode node = builder.values(new String[] i, 1).transientScan(aux).filter(builder.call(SqlStdOperatorTable.LESS_THAN, builder.field(0), builder.literal(10))).project(builder.call(SqlStdOperatorTable.PLUS, builder.field(0), builder.literal(1))).repeatUnion(aux, true).build();System.out.println(RelOptUtil.toString(node)); 生成结果如下： LogicalRepeatUnion(all=[true]) LogicalTableSpool(readType=[LAZY], writeType=[LAZY], tableName=[aux]) LogicalValues(tuples=[[ 1 ]]) LogicalTableSpool(readType=[LAZY], writeType=[LAZY], tableName=[aux]) LogicalProject($f0=[+($0, 1)]) LogicalFilter(condition=[($0, 10)]) LogicalTableScan(table=[[aux]]) 接口摘要 关系运算符 以下方法会创建一个关系表达式 RelNode，并将它压入堆栈中，然后返回 RelBuilder。 方法 描述 scan(tableName) 创建一个 TableScan。 functionScan(operator, n, expr...) functionScan(operator, n, exprList) 创建 n 个最新的关系表达式 TableFunctionScan。 transientScan(tableName [, rowType]) 在给定类型的 TransientTable 上创建 TableScan（如果未指定，将使用最新的关系表达式类型）。 values(fieldNames, value...) values(rowType, tupleList) 创建一个 Values。 filter([variablesSet, ] exprList) filter([variablesSet, ] expr...) 在给定谓词的 AND 上创建 过滤器（如果 variablesSet 指定，谓词可以引用这些变量）。 project(expr...) project(exprList [, fieldNames]) 创建一个投影。如果要覆盖默认名称，请使用 alias 来包装表达式，或指定 fieldNames 参数。 projectPlus(expr...) projectPlus(exprList) project 的变体，保留了原始字段，并添加给定的表达式。 projectExcept(expr...) projectExcept(exprList) project 的变体，保留了原始字段，并删除给定的表达式。 permute(mapping) 创建一个使用 mapping 重新排列字段的投影。 convert(rowType [, rename]) 创建一个将字段转换为指定类型，或者重命名这些字段的投影。 aggregate(groupKey, aggCall...) aggregate(groupKey, aggCallList) 创建一个聚合。 distinct() 创建一个消除重复记录的聚合。 pivot(groupKey, aggCalls, axes, values) 添加旋转（pivot 行转列）操作，该操作使用每个度量和值的组合列，生成一个聚合来实现。 unpivot(includeNulls, measureNames, axisNames, axisMap) 添加逆旋转（unpivot 列转行）操作，该操作为每个 Values 生成一个 Join，从而将每行转换为多行来实现。 sort(fieldOrdinal...) sort(expr...) sort(exprList) 创建一个 Sort。在第一种形式中，字段序号是从 0 开始的，负数序号表示降序。例如，-2 表示字段 1 降序。在其它的形式中，你可以将表达式包装在 as，nullsFirst 或 nullsLast 中。 sortLimit(offset, fetch, expr...) sortLimit(offset, fetch, exprList) 创建一个带有 offset 和 limit 的 Sort。 limit(offset, fetch) 创建一个不排序的 Sort，只适用于 offset 和 limit。 exchange(distribution) 创建一个 Exchange。 sortExchange(distribution, collation) 创建一个 SortExchange。 correlate(joinType, correlationId, requiredField...) correlate(joinType, correlationId, requiredFieldList) 使用两个最新的关系表达式，创建一个 Correlate，它包含了一个可变名称以及左侧关联关系需要的字段表达式。 join(joinType, expr...) join(joinType, exprList)join(joinType, fieldName...) 使用两个最新的关系表达式，创建一个 Join。第一种形式，在布尔表达式上进行关联（使用 AND 组合多个条件）。最后一个形式，在命名字段上进行关联，每边必须有一个各自名称的字段。 semiJoin(expr) 使用两个最新的关系表达式，创建一个半连接类型的 Join。 antiJoin(expr) 使用两个最新的关系表达式，创建一个反连接类型的 Join。 union(all [, n]) 使用 n（默认两个）个最新的关系表达式，创建一个 Union。 intersect(all [, n]) 使用 n（默认两个）个最新的关系表达式，创建一个 Intersect。 minus(all) 使用两个最新的关系表达式，创建一个 Minus。 repeatUnion(tableName, all [, n]) 创建与 TransientTable （使用两个最新的关系表达式创建）相关联的 RepeatUnion，它具有 n 个最大迭代次数（默认为 -1，即没有限制）。 snapshot(period) 创建指定的快照时间段的 Snapshot。 match(pattern, strictStart, strictEnd, patterns, measures, after, subsets, allRows, partitionKeys, orderKeys, interval) 创建一个 Match。 参数类型： expr，interval：RexNode； expr...， requiredField...：RexNode 数组； exprList，measureList，partitionKeys，orderKeys， requiredFieldList：可迭代的 RexNode； fieldOrdinal：行内字段的序号（从 0 开始）； fieldName：字段名称，在行内唯一； fieldName...：字符串数组； fieldNames：可迭代的字符串； rowType：RelDataType； groupKey：RelBuilder.GroupKey； aggCall...：RelBuilder.AggCall 数组； aggCallList：可迭代的 RelBuilder.AggCall； value...：对象数组； value：对象； tupleList：可迭代的 RexLiteral 集合； all，distinct，strictStart，strictEnd，allRows：布尔值； alias：字符串； correlationId：CorrelationId； variablesSet：可迭代的 CorrelationId； varHolder：RexCorrelVariable Holder； patterns：键为字符串，值为 RexNode 的 Map； subsets：键为字符串，值为字符串有序集合的 Map； distribution：RelDistribution； collation：RelCollation； operator：SqlOperator； joinType：JoinRelType； builder 方法执行了各种优化，具体包括： 如果要求按顺序投影所有列，project 则返回它的输入； filter 会打平条件表达式，所以，一个 AND 和 OR 可能有 2 个以上的子节点。filter 也会进行简化，例如将 x = 1 AND TRUE 转化为 x = 1； 如果你先使用 sort，然后使用 limit 时，效果就像你调用了 sortLimit 一样； 有一些注解方法，可以向堆栈顶部的关系表达式添加信息： 方法 描述 as(alias) 为堆栈顶部的关系表达式分配一个表别名。 variable(varHolder) 创建一个引用顶部关系表达式的相关变量。 堆栈方法 方法 描述 build() 从堆栈中弹出最新创建的关系表达式。 push(rel) 将关系表达式压入堆栈。前面提到的关系方法，例如 scan，会调用这个方法，但是用户代码一般不会调用。 pushAll(collection) 将一组关系表达式压入堆栈。 peek() 返回最新放入堆栈的关系表达式，但不删除它。 标量表达式方法 以下方法返回标量表达式 RexNode。许多方法使用堆栈的内容。例如，field(DEPTNO) 返回被添加到堆栈中的关系表达式的 DEPTNO 字段的引用。 方法 描述 literal(value) 常量。 field(fieldName) 按照名称引用关系表达式最顶层的字段。 field(fieldOrdinal) 按照顺序引用关系表达式最顶层的字段。 field(inputCount, inputOrdinal, fieldName) 按照名称引用关系表达式第 inputCount - inputOrdinal 个字段。 field(inputCount, inputOrdinal, fieldOrdinal) 按照序号引用关系表达式第 inputCount - inputOrdinal 个字段。 field(inputCount, alias, fieldName) 按照表别名和字段名称，引用堆栈顶部最多 inputCount - 1 个元素的字段。 field(alias, fieldName) 按照表别名和字段名称，引用关系表达式最顶层的字段。 field(expr, fieldName) 按照名称引用记录值（record-valued）表达式字段。 field(expr, fieldOrdinal) 按照序号引用记录值（record-valued）表达式字段。 fields(fieldOrdinalList) 按照序号引用输入字段的表达式列表。 fields(mapping) 按照给定映射引用输入字段的表达式列表。 fields(collation) 表达式列表 exprList，sort(exprList) 将复制排序规则。 call(op, expr...) call(op, exprList) 调用函数或运算符。 and(expr...) and(exprList) 逻辑与。会打平嵌套的 AND，并优化涉及 TRUE 和 FALSE 的情况。 or(expr...) or(exprList) 逻辑或。会打平嵌套的 OR，并优化涉及 TRUE 和 FALSE 的情况。 not(expr) 逻辑非。 equals(expr, expr) 等于。 isNull(expr) 检查表达式是否为空。 isNotNull(expr) 检查表达式是否为非空。 alias(expr, fieldName) 重命名表达式（仅作为 project 的参数时有效）。 cast(expr, typeName) cast(expr, typeName, precision) cast(expr, typeName, precision, scale) 将表达式转换为指定类型。 desc(expr) 将排序方向改为降序（仅作为 sort 或 sortLimit 的参数时有效）。 nullsFirst(expr) 将排序顺序改为空值最先（仅作为 sort 或 sortLimit 的参数时有效）。 nullsLast(expr) 将排序顺序改为空值最后（仅作为 sort 或 sortLimit 的参数时有效）。 cursor(n, input) 引用第 input 个（从 0 开始）关系输入，关系输入是有 n 个输入的 TableFunctionScan（参考 functionScan）。 模式方法 以下方法会返回用于 match 中的模式。 方法 描述 patternConcat(pattern...) 连接模式 patternAlter(pattern...) 替换模式 patternQuantify(pattern, min, max) 量化模式 patternPermute(pattern...) 重新排列模式 patternExclude(pattern) 排除模式 分组键方法 以下方法会返回一个 RelBuilder.GroupKey。 方法 描述 groupKey(fieldName...)groupKey(fieldOrdinal...) groupKey(expr...) groupKey(exprList) 创建一个指定表达式的分组键。 groupKey(exprList, exprListList) 创建一个使用分组集合的指定表达式的分组键。 groupKey(bitSet [, bitSets]) 创建一个指定输入列的分组键，如果指定了 bitSets，则指定输入列包含多个分组集合。 聚合调用方法 以下方法会返回一个 RelBuilder.AggCall。 方法 描述 aggregateCall(op, expr...)aggregateCall(op, exprList) 为指定的聚合函数创建一个调用。 count([ distinct, alias, ] expr...)count([ distinct, alias, ] exprList) 为 COUNT 聚合函数创建一个调用。 countStar(alias) 为 COUNT(*) 聚合函数创建一个调用。 sum([ distinct, alias, ] expr) 为 SUM 聚合函数创建一个调用。 min([ alias, ] expr) 为 MIN 聚合函数创建一个调用。 max([ alias, ] expr) 为 MAX 聚合函数创建一个调用。 如果想要进一步地修改 AggCall，可以调用如下方法： 方法 描述 approximate(approximate) 允许聚合的近似值 approximate。 as(alias) 为表达式分配一个列别名（请参考 SQL AS）。 distinct() 在聚合之前消除重复值（请参考 SQL DISTINCT）。 distinct(distinct) 如果配置了 distinct，则在聚合之前消除重复值。 filter(expr) 在聚合之前过滤行（请参考 SQL FILTER (WHERE ...)）。 sort(expr...) sort(exprList) 在聚合之前对行进行排序（请参考 SQL WITHIN GROUP）。 unique(expr...) unique(exprList) 在聚合之前使行唯一（请参考 SQL WITHIN DISTINCT）。 over() 将这个 AggCall 转换为窗口聚合（参考下面的 OverCall）。 窗口聚合调用方法 为了创建一个 RelBuilder.OverCall（它代表对窗口聚合函数的调用）， 需要先创建一个聚合调用，然后调用它的 over() 方法，例如：count().over()。 如果想要进一步地修改 OverCall，可以调用如下方法： 方法 描述 rangeUnbounded() 创建一个无界的、基于范围的窗口，RANGE BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING。 rangeFrom(lower) 创建一个基于范围的、有下界的窗口，RANGE BETWEEN lower AND CURRENT ROW。 rangeTo(upper) 创建一个基于范围的、有上界的窗口，RANGE BETWEEN CURRENT ROW AND upper。 rangeBetween(lower, upper) 创建一个基于范围的窗口，RANGE BETWEEN lower AND upper。 rowsUnbounded() 创建一个无界的、基于行的窗口，ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING。 rowsFrom(lower) 创建一个基于行的、有下界的窗口，ROWS BETWEEN lower AND CURRENT ROW。 rowsTo(upper) 创建一个基于行的、有上界的窗口，ROWS BETWEEN CURRENT ROW AND upper。 rowsBetween(lower, upper) 创建一个基于行的窗口，ROWS BETWEEN lower AND upper。 partitionBy(expr...) partitionBy(exprList) 根据指定的表达式对窗口进行分区（请参考 SQL PARTITION BY）。 orderBy(expr...) sort(exprList) 对窗口中的行进行排序（请参考 SQL ORDER BY）。 allowPartial(b) 设置是否允许部分宽度的窗口，默认为 true。 nullWhenCountZero(b) 设置如果窗口中没有数据行时，聚合函数是否应该计算为空，默认 false。 as(alias) 分配列别名（请参考 SQL AS），并将 OverCall 转换为 RexNode。 toRex() 将 OverCall 转换为 RexNode。 写在最后 笔者因为工作原因接触到 Calcite，前期学习过程中，深感 Calcite 学习资料之匮乏，因此创建了 Calcite 从入门到精通知识星球，希望能够将学习过程中的资料和经验沉淀下来，为更多想要学习 Calcite 的朋友提供一些帮助。"},{"title":"背景","path":"/wiki/calcite/background.html","content":"原文链接：https://calcite.apache.org/docs/ Apache Calcite 是一个动态数据管理框架。它包含了构成典型数据库管理系统的许多部分，但省略了一些关键功能，如：数据存储、处理数据的算法以及用于存储元数据的仓库。 Calcite 有意置身于存储和处理数据业务之外。正如我们将看到的，这使得 Calcite 成为应用程序与数据存储和数据处理引擎之间进行中转的绝佳选择。 它也是构建数据库的完美基础：只需添加数据。为了说明这一点，让我们创建一个 Calcite 的空实例，然后将其指向一些数据。 public static class HrSchema public final Employee[] emps = 0; public final Department[] depts = 0;Class.forName(org.apache.calcite.jdbc.Driver);Properties info = new Properties();info.setProperty(lex, JAVA);Connection connection = DriverManager.getConnection(jdbc:calcite:, info);CalciteConnection calciteConnection = connection.unwrap(CalciteConnection.class);SchemaPlus rootSchema = calciteConnection.getRootSchema();Schema schema = new ReflectiveSchema(new HrSchema());rootSchema.add(hr, schema);Statement statement = calciteConnection.createStatement();ResultSet resultSet = statement.executeQuery(SELECT d.deptno, min(e.empid) FROM hr.emps AS e JOIN hr.depts AS d ON e.deptno = d.deptno GROUP BY d.deptno HAVING COUNT(*) 1);print(resultSet);resultSet.close();statement.close();connection.close(); 数据库在哪里？其实根本没有数据库。这个连接也完全是空的，直到 new ReflectiveSchema 注册了一个 Java 对象作为 schema， 并将它的集合字段 emps 和 depts 作为表，这时连接才有了数据。 Calcite 不想拥有数据，它甚至没有一个最喜欢的数据格式。这个示例使用了内存数据集，并使用 linq4j 库中的 groupBy 和 join 算子处理这些数据。但是 Calcite 也可以处理其他数据格式的数据，例如 JDBC。在第一个示例中，将 Schema schema = new ReflectiveSchema(new HrSchema()); 替换成 Class.forName(com.mysql.jdbc.Driver);BasicDataSource dataSource = new BasicDataSource();dataSource.setUrl(jdbc:mysql://localhost);dataSource.setUsername(username);dataSource.setPassword(password);Schema schema = JdbcSchema.create(rootSchema, hr, dataSource, null, name); Calcite 将在 JDBC 中执行相同的查询。对应用来说，数据和 API 是一样的，但背后的实现方式却大不相同。Calcite 使用优化器规则将 JOIN 和 GROUP BY 运算推送到源数据库。 内存和 JDBC 只是两个常见的例子。Calcite 可以处理任何数据源和数据格式。为了添加一个数据源，你需要编写一个适配器，来告诉 Calcite 应该将数据源中的哪些集合作为表。 对于更高级的集成，你可以编写优化器规则。优化器规则允许 Calcite 访问新格式的数据，允许你注册新的算子（比如一个更好的连接算法），也允许 Calcite 对如何将查询转换为算子进行优化。Calcite 会将你定义的规则、算子与内置的规则、算子相结合，使用基于成本的优化模型，生成一个高效的执行计划。 编写一个适配器 example/csv 子项目提供了一个功能齐全、可用于应用程序的 CSV 适配器。它也足够简单，如果您正在编写自己的适配器，它可以作为一个很好的模板。 关于使用 CSV 适配器和编写其他适配器的信息，请参阅教程。 关于使用其他适配器以及常规使用 Calcite 的更多信息，请参阅如何去做。 状态 以下功能是已经完成。 查询解析器、校验器和优化器； 支持读取 JSON 格式的模型； 许多标准函数和聚合函数； 针对 Linq4j 和 JDBC 后端的 JDBC 查询； Linq4j 前端； SQL 特性：SELECT、FROM（包括 JOIN 语法）、WHERE、GROUP BY（包括 GROUPING SETS）、聚合函数（包括 COUNT(DISTINCT ...) 和 FILTER）、HAVING、ORDER BY（包括 NULLS FIRST/LAST）、集合操作（UNION、INTERSECT、MINUS)、子查询（包括相关子查询）、窗口聚合、LIMIT（如 Postgres 语法）——更多详细信息参考 SQL 参考； 本地和远程 JDBC 驱动程序——参考 Avatica； 多个适配器； 写在最后 笔者因为工作原因接触到 Calcite，前期学习过程中，深感 Calcite 学习资料之匮乏，因此创建了 Calcite 从入门到精通知识星球，希望能够将学习过程中的资料和经验沉淀下来，为更多想要学习 Calcite 的朋友提供一些帮助。"},{"title":"Cassandra 适配器","path":"/wiki/calcite/cassandra-adapter.html","content":"原文链接：https://calcite.apache.org/docs/cassandra_adapter.html 有关下载和构建 Calcite 的说明，请从教程开始。 一旦你成功编译了项目，就可以返回到这里，并开始使用 Calcite 查询 Cassandra。首先，我们需要一个模型定义，模型为 Calcite 创建 Cassandra 适配器实例，提供了必要的参数。需要注意，虽然模型可以包含物化视图的定义，但适配器将尝试自动填充 Cassandra 中定义的任何物化视图。 下面是模型文件的基本示例： version: 1.0, defaultSchema: twissandra, schemas: [ name: twissandra, type: custom, factory: org.apache.calcite.adapter.cassandra.CassandraSchemaFactory, operand: host: localhost, keyspace: twissandra ] 需要注意，如果你的服务器需要身份验证，你可以和 host 和 keyspace 一起，再指定 username 和 password 属性。假设此文件存储为 model.json，你可以通过 sqlline 连接到 Cassandra，如下所示： $ ./sqllinesqlline !connect jdbc:calcite:model=model.json admin admin 现在 sqlline 将会接收你执行的 CQL 表相关的 SQL 查询，你不仅能执行 CQL 表相关的查询，Calcite 也允许你执行复杂的操作，例如聚合或连接。适配器将尽可能尝试在 Cassandra 中直接利用过滤和排序，将查询编译为最有效的 CQL。 例如，在示例数据集中有一个名为 timeline 的 CQL 表，其中 username 是分区键，time 是聚类键。 我们可以通过编写标准 SQL，发出简单查询来获取用户最新的推文 ID： sqlline SELECT tweet_id FROM timeline WHERE username = JmuhsAaMdw ORDER BY time DESC LIMIT 1;+--------------------------------------+| tweet_id |+--------------------------------------+| f3d3d4dc-d05b-11e5-b58b-90e2ba530b12 |+--------------------------------------+ 在执行此查询时，Cassandra 适配器能够识别 username 是分区键，可以被 Cassandra 过滤。它还识别聚类键 time，并将排序下推到 Cassandra。下面是下发到 Cassandra 的最终 CQL 查询： SELECT username, time, tweet_idFROM timelineWHERE username = JmuhsAaMdwORDER BY time DESC ALLOW FILTERING; 在提高适配器的灵活性和性能方面，目前仍有大量工作要做，但如果你正在寻找一种快速方法，来获得对存储在 Cassandra 中的数据的额外查询，Calcite 应该会证明自己是有价值的。 写在最后 笔者因为工作原因接触到 Calcite，前期学习过程中，深感 Calcite 学习资料之匮乏，因此创建了 Calcite 从入门到精通知识星球，希望能够将学习过程中的资料和经验沉淀下来，为更多想要学习 Calcite 的朋友提供一些帮助。"},{"title":"开发 Calcite","path":"/wiki/calcite/develop.html","content":"原文链接：https://calcite.apache.org/develop/ 想要帮助添加功能或修复错误吗？ 源代码 你可以通过下载 Release 版本或从源代码控制获取源代码。 Calcite 使用 git 进行版本控制。标准源位于 Apache，但大多数人发现 Github 镜像更加用户友好。 下载源码、构建并运行测试 前提条件是你的路径上有 Git 和 Java（JDK 8u220 或更高版本，首选 11）。 注意：早期的 OpenJDK 1.8 版本（例如 1.8u202 之前的版本）已知在为类型注释生成字节码时存在问题（请参阅JDK-8187805、 JDK-8187805、 JDK-8210273、 JDK-8160928、 JDK-8144185），因此请确保你使用的是最新的 Java。 创建 Git 存储库的本地副本 cd 到其根目录，然后使用 Gradle 进行构建： $ git clone https://github.com/apache/calcite.git$ cd calcite$ ./gradlew build HOWTO 描述了如何从源代码发行版进行构建、设置用于贡献的 IDE、运行更多或更少的测试以及运行集成测试。 (adsbygoogle = window.adsbygoogle || []).push({}); 赞助商 JIRA 帐户 Calcite 使用 JIRA 进行问题/案例管理。你必须拥有 JIRA 帐户才能记录案例和问题。 我已经有一个 ASF JIRA 帐户并希望添加为贡献者 如果你已有 ASF JIRA 帐户，则无需注册新帐户。请使用以下模板发送电子邮件至 jira-requests@calcite.apache.org，以便我们将你的帐户添加到 JIRA 的贡献者列表中： [在你的电子邮件客户端中打开模板](mailto:jira-requests@calcite.apache.org?subject=Add me as a contributor to JIRAbody=Hello, Please add me as a contributor to JIRA. My JIRA username is: INSERT YOUR JIRA USERNAME HERE Thanks, INSERT YOUR NAME HERE) Subject: Add me as a contributor to JIRAHello,Please add me as a contributor to JIRA.My JIRA username is: [INSERT YOUR JIRA USERNAME HERE]Thanks,[INSERT YOUR NAME HERE] 我没有 ASF JIRA 帐户，想要申请一个帐户并添加为贡献者 请使用 ASF 的自助服务设施申请帐户。 贡献 我们欢迎贡献。 如果你打算做出重大贡献，请先与我们联系！它有助于就总体方法达成一致。为你提议的功能记录 JIRA 案例或在开发列表上开始讨论。 在打开新的 JIRA 案例之前，请查看现有问题。你计划处理的功能或错误可能已经存在。 如果需要创建一个新问题，提供简洁且有意义的摘要行非常重要。它应该暗示最终用户试图做什么、在哪个组件中以及看到了什么问题。如果不清楚所需的行为是什么，请改写：例如，验证器关闭模型文件为验证器不应关闭模型文件。 该案例的贡献者应随时重新措辞并澄清摘要内容。如果你在澄清时删除了信息，请将其放入案例描述中。 设计讨论可能发生在不同的地方（电子邮件线程、Github 评论），但 JIRA 案例是这些讨论的典型场所。链接到它们或在案例中总结它们。 在实现案例时，尤其是新功能时，请确保案例包含变更的功能规范。例如，在 CREATE TABLE 命令中添加 IF NOT EXISTS 子句；如果表已经存在，则该命令是无操作的。如果规范在设计讨论或实施期间发生变化，请更新描述。 在实现功能或修复错误时，请在开始处理代码之前尝试创建 JIRA 案例。这让其他人有机会在你走得太远（审阅者认为是）错误的道路之前塑造该功能。 寻求与问题相关的反馈的最佳位置是开发人员列表。请避免在 JIRA 案例中标记特定人员以寻求反馈。这阻碍了其他贡献者参与讨论并提供有价值的反馈。 如果存在似乎与特定提交相关的回归，请随时在讨论中标记相应的贡献者。 如果你要立即处理该问题，请立即将其分配给你自己。要将问题分配给自己，你必须在 JIRA 中注册为贡献者。为此，请按照 JIRA 帐户部分中概述的说明进行操作。 如果你致力于在即将发布的版本之前解决问题，请相应地设置修复版本（例如 1.20.0），否则将其留空。 如果你发现一个现有问题，请将其标记为正在进行，并在完成后将其标记为 pull-request-available。 如果出于任何原因你决定某个问题不能进入正在进行的版本，请将修复版本重置为空白。 在发布期间，发布经理会将当前版本未完成的问题更新到下一个版本。 在某些情况下，JIRA 问题可能会在讨论（或其他原因）中得到解决，而无需进行更改。在这种情况下，参与讨论的贡献者应该： 解决问题（不要关闭它）； 选择适当的解决原因（重复、无效、无法修复等）； 如果不明显，请添加带有推理的评论。 Fork GitHub 存储库，并为你的功能创建一个分支。 开发你的功能和测试用例，并确保 ./gradlew build 成功（如果你的更改需要的话，请运行额外的测试）。 将更改提交到你的分支，并使用以 JIRA 案例编号开头的注释，如下所示： [CALCITE-345] AssertionError in RexToLixTranslator comparing to date literal 如果你的更改有多个提交，请使用 git rebase -i main 将它们压缩为单个提交，并使你的代码与主线上的最新代码保持同步。 为了保持提交历史记录的干净和统一，你应该遵守以下准则。 阅读以前提交的消息，并遵循他们的风格； 提交消息的第一行必须是对更改的简洁且有用的描述； 该消息通常（但并非总是）与 JIRA 主题相同。如果 JIRA 主题不清楚，请更改它（如果澄清的话，可以将原始主题移至 JIRA 案例的描述中）； 在 JIRA id 后保留一个空格字符； 以大写字母开头； 不要以句号结束； 使用祈使语气（添加处理程序…… Add a handler …）而不是过去时（添加处理程序…… Added a handler …）或现在时（添加处理程序…… Adds a handler …）； 如果可能，请描述你更改的用户可见行为（FooCommand 现在创建目录，如果它不存在），而不是实现（为 FileNotFound 添加处理程序）； 如果你正在修复错误，那么描述该错误就足够了（如果用户未知，则出现 NullPointerException），人们会正确地推测你的更改的目的是修复该错误。 然后将你的提交推送到 GitHub，并创建从你的分支到方解石主分支的拉取请求。更新 JIRA 案例以引用你的拉取请求，提交者将审查你的更改。 拉取请求可能需要更新（提交后），主要原因有以下三个： 你在提交拉取请求后发现了问题； 审稿人要求进一步修改； CI 构建失败，并且失败不是由你的更改引起的。 为了更新拉取请求，你需要在分支中提交更改，然后将提交推送到 GitHub。我们鼓励你在以前现有的提交之上使用常规（non-rebased）提交。 将更改推送到 GitHub 时，你应该避免使用 --force 参数及其替代方案。你可以选择在某些条件下强制推送更改： 拉取请求是在不到 10 分钟前提交的，并且没有与之相关的待讨论（在 PR 和/或 JIRA 中）； 审阅者明确要求你执行一些需要使用 --force 选项的修改。 在特殊情况下，CI 构建失败，并且失败不是由你的更改引起的，创建一个空提交 ( git commit --allow-empty ) 并推送它。 空安全 Apache Calcite 使用 Checker Framework 来避免意外的 NullPointerExceptions 。你可以在 https://checkerframework.org/ 找到详细的文档。 注意：目前仅验证主代码，因此在测试代码中不强制执行 nullness 注释。 要在本地执行 Checker 框架，请使用以下命令： ./gradlew -PenableCheckerframework :linq4j:classes :core:classes 下面简单介绍一下空安全编程： 默认情况下，参数、返回值和字段不可为 null，因此请不要使用 @NonNull； 局部变量从表达式推断为空，因此你可以编写 Object v = ... 而不是 @Nullable Object v = ...； 避免使用 javax.annotation.* 注释。 jsr305 中的注释不支持 List@Nullable String 等情况，因此最好坚持使用 org.checkerframework.checker.nullness.qual.Nullable 。不幸的是，Guava（从 29-jre 开始）同时具有 jsr305 和 checker-qual 依赖项，因此你可能需要配置 IDE 以排除 javax.annotation.* 来自代码完成的注释； Checker 框架逐个验证代码。这意味着，它无法解释方法执行顺序。这就是为什么 @Nullable 字段应该在使用它们的每个方法中进行验证。如果将逻辑拆分为多个方法，你可能需要验证一次 null，然后通过不可为 null 的参数传递它。对于以 null 开头并随后变为非 null 的字段，请使用 @MonotonicNonNull 。对于已检查是否为 null 的字段，请使用 @RequiresNonNull； 如果你绝对确定该值不为空，则可以使用 org.apache.calcite.linq4j.Nullness.castNonNull(T) 。 castNonNull 背后的意图就像 trustMeThisIsNeverNullHoweverTheVerifierCantTellYet(...)； 但是，如果表达式可为空，则需要将其传递给非空方法，请使用 Objects.requireNonNull 。它允许获得包含上下文信息的更好的错误消息； Checker Framework 附带了带注释的 JDK，但是，可能存在无效注释。在这种情况下，可以将存根文件放置到 /src/main/config/checkerframework 以覆盖注释。文件具有 .astub 扩展名非常重要，否则它们将被忽略； 在数组类型中，类型注释紧邻其引用的类型组件（数组或数组组件）之前出现。 Java 语言规范对此进行了解释。 String nonNullable;@Nullable String nullable;java.lang.@Nullable String fullyQualifiedNullable;// array and elements: non-nullableString[] x;// array: nullable, elements: non-nullableString @Nullable [] x;// array: non-nullable, elements: nullable@Nullable String[] x;// array: nullable, elements: nullable@Nullable String @Nullable [] x;// arrays: nullable, elements: nullable// x: non-nullable// x[0]: non-nullable// x[0][0]: nullable@Nullable String[][] x;// x: nullable// x[0]: non-nullable// x[0][0]: non-nullableString @Nullable [][] x;// x: non-nullable// x[0]: nullable// x[0][0]: non-nullableString[] @Nullable [] x; 默认情况下，泛型参数可以为可为空和不可为空： class HolderT // can be both nullable final T value; T get() return value; // works int hashCode() return value.hashCode(); // error here since T can be nullable 但是，默认边界是不可为空的，因此如果你编写 T extends Number ，那么它与 T extends @NonNull Number 相同： class HolderT extends Number // note how this T never permits nulls final T value; Holder(T value) this.value = value; static T HolderT empty() return new Holder(null); // fails since T must be non-nullable 如果你需要“可为空或不可为空 Number ”，则使用 T extends @Nullable Number。如果需要确保类型始终可为空，请使用 @Nullable T ，如下所示： class Holder@Nullable T // note how this requires T to always be nullable protected T get() // Default implementation. // Default implementation returns null, so it requires that T must always be nullable return null; static void useHolder() // T is declared as @Nullable T, so HolderString would not compile Holder@Nullable String holder = ...; String value = holder.get(); 持续集成测试 Calcite 利用 GitHub Actions 进行持续集成测试。 入门 Calcite 是一个社区，因此加入该项目的第一步是自我介绍。加入开发人员列表并发送电子邮件。 如果你有机会参加聚会，或在会议上与社区成员见面，那也很棒。 选择要执行的初始任务。它应该是非常简单的事情，例如错误修复或我们标记为新手的 Jira 任务。请遵循贡献指南来承诺你的更改。 我们重视所有有助于建立充满活力的社区的贡献，而不仅仅是代码。你可以通过测试代码、帮助验证版本、编写文档、改进网站或仅回答列表中的问题来做出贡献。 在你做出一些有用的贡献后，我们可能会邀请你成为提交者。 写在最后 笔者因为工作原因接触到 Calcite，前期学习过程中，深感 Calcite 学习资料之匮乏，因此创建了 Calcite 从入门到精通知识星球，希望能够将学习过程中的资料和经验沉淀下来，为更多想要学习 Calcite 的朋友提供一些帮助。"},{"title":"Druid 适配器","path":"/wiki/calcite/druid-adapter.html","content":"原文链接：https://calcite.apache.org/docs/druid_adapter.html Druid 是一个快速的，面向列的分布式数据存储。它允许你通过基于 JSON 的查询语言执行查询，特别是 OLAP 风格的查询。Druid 可以以批处理模式，或连续加载加载模式执行。Druid 的关键特性之一，是它能够从流源（如 Kafka）加载数据，并在几毫秒内使数据可用于查询。 Calcite 的 Druid 适配器，允许你使用 SQL 查询数据，并将其与其他 Calcite 模式中的数据结合使用。 首先，我们需要定义一个模型。模型为 Calcite 创建 Druid 适配器实例，提供了必要的参数。 下面是模型文件的基本示例： version: 1.0, defaultSchema: wiki, schemas: [ type: custom, name: wiki, factory: org.apache.calcite.adapter.druid.DruidSchemaFactory, operand: url: http://localhost:8082, coordinatorUrl: http://localhost:8081 , tables: [ name: wiki, factory: org.apache.calcite.adapter.druid.DruidTableFactory, operand: dataSource: wikiticker, interval: 1900-01-09T00:00:00.000Z/2992-01-10T00:00:00.000Z, timestampColumn: name: time, type: timestamp , dimensions: [ channel, cityName, comment, countryIsoCode, countryName, isAnonymous, isMinor, isNew, isRobot, isUnpatrolled, metroCode, namespace, page, regionIsoCode, regionName ], metrics: [ name: count, type: count , name: added, type: longSum, fieldName: added , name: deleted, type: longSum, fieldName: deleted , name: delta, type: longSum, fieldName: delta , name: user_unique, type: hyperUnique, fieldName: user_id ], complexMetrics: [ user_id ] ] ] 此文件存储为 druid/src/test/resources/druid-wiki-model.json，因此你可以通过 sqlline 连接到 Druid，如下所示： $ ./sqllinesqlline !connect jdbc:calcite:model=druid/src/test/resources/druid-wiki-model.json admin adminsqlline select countryName, cast(count(*) as integer) as c from wiki group by countryName order by c desc limit 5;+----------------+------------+| countryName | C |+----------------+------------+| | 35445 || United States | 528 || Italy | 256 || United Kingdom | 234 || France | 205 |+----------------+------------+5 rows selected (0.279 seconds)sqlline 该查询显示了 2015-09-12（wikiticker 数据集涵盖的日期）维基百科编辑的前 5 个来源国家/地区。 现在让我们看看这个查询是如何计划执行的： sqlline !set outputformat csvsqlline explain plan for select countryName, cast(count(*) as integer) as c from wiki group by countryName order by c desc limit 5;PLANEnumerableInterpreter BindableProject(countryName=[$0], C=[CAST($1):INTEGER NOT NULL]) BindableSort(sort0=[$1], dir0=[DESC], fetch=[5]) DruidQuery(table=[[wiki, wiki]], groups=[4], aggs=[[COUNT()]])1 row selected (0.024 seconds) 该计划显示 Calcite 能够将查询的 GROUP BY 部分下推到 Druid，包括 COUNT(*) 函数，但不能下推 ORDER BY ... LIMIT（我们计划取消此限制；请参阅 CALCITE-1206）。 复杂指标 Druid 有特殊的指标，可以产生快速但近似的结果。目前有两种类型： hyperUnique - HyperLogLog 数据草图，用于估算维度的基数； thetaSketch - Theta 草图，也用于估算维度的基数，但也可以用于执行集合操作。 在模型定义中，有一个名为 complexMetrics 的字符串数组，用于声明每个复杂指标的别名。该别名在 SQL 中使用，但是当 Calcite 为 Druid 生成 JSON 查询时，将使用其真实列名。 Foodmart 数据集 测试 VM 还包括一个数据集，该数据集将 Foodmart 模式的销售、产品和客户表非规范化为一个名为 “foodmart” 的单个 Druid 数据集。 你可以通过 druid/src/test/resources/druid-foodmart-model.json 模型访问它。 简化模型 如果在模型中提供的元数据较少，Druid 适配器可以自动从 Druid 发现它。以下是与前一个模式等效的模式，但删除了 dimensions、metrics 和 timestampColumn： version: 1.0, defaultSchema: wiki, schemas: [ type: custom, name: wiki, factory: org.apache.calcite.adapter.druid.DruidSchemaFactory, operand: url: http://localhost:8082, coordinatorUrl: http://localhost:8081 , tables: [ name: wiki, factory: org.apache.calcite.adapter.druid.DruidTableFactory, operand: dataSource: wikiticker, interval: 1900-01-09T00:00:00.000Z/2992-01-10T00:00:00.000Z ] ] Calcite 向 Druid 调度 segmentMetadataQuery 以发现表的列。 现在，让我们取出 tables 元素： version: 1.0, defaultSchema: wiki, schemas: [ type: custom, name: wiki, factory: org.apache.calcite.adapter.druid.DruidSchemaFactory, operand: url: http://localhost:8082, coordinatorUrl: http://localhost:8081 ] Calcite 通过 /druid/coordinator/v1/metadata/datasources REST 调用发现 “wikiticker” 数据源。现在 “wiki” 表元素已被删除，该表称为 “wikiticker”。Druid 中存在的任何其他数据源也将显示为表。 我们的模型现在是一个基于自定义模式工厂的模式，只有两个操作数，所以我们可以省略模型，并将操作数作为连接字符串的一部分提供： jdbc:calcite:schemaFactory=org.apache.calcite.adapter.druid.DruidSchemaFactory; schema.url=http://localhost:8082; schema.coordinatorUrl=http://localhost:8081 事实上，这些是操作数的默认值，所以我们可以省略它们： jdbc:calcite:schemaFactory=org.apache.calcite.adapter.druid.DruidSchemaFactory 现在，我们可以使用一个非常简单的连接字符串连接到 sqlline，并列出可用的表： $ ./sqllinesqlline !connect jdbc:calcite:schemaFactory=org.apache.calcite.adapter.druid.DruidSchemaFactory admin adminsqlline !tables+-----------+-------------+------------+--------------+| TABLE_CAT | TABLE_SCHEM | TABLE_NAME | TABLE_TYPE |+-----------+-------------+------------+--------------+| | adhoc | foodmart | TABLE || | adhoc | wikiticker | TABLE || | metadata | COLUMNS | SYSTEM_TABLE || | metadata | TABLES | SYSTEM_TABLE |+-----------+-------------+------------+--------------+ 我们看到两个系统表（TABLES 和 COLUMNS），以及 Druid 中的两个表（foodmart 和 wikiticker）。 写在最后 笔者因为工作原因接触到 Calcite，前期学习过程中，深感 Calcite 学习资料之匮乏，因此创建了 Calcite 从入门到精通知识星球，希望能够将学习过程中的资料和经验沉淀下来，为更多想要学习 Calcite 的朋友提供一些帮助。"},{"title":"Elasticsearch 适配器","path":"/wiki/calcite/elasticsearch-adapter.html","content":"原文链接：https://calcite.apache.org/docs/elasticsearch_adapter.html 有关下载和构建 Calcite 的说明，请从教程开始。 一旦你成功编译了项目，就可以返回到这里，并开始使用 Calcite 查询 Elasticsearch。首先，我们需要定义一个模型。模型为 Calcite 创建 Elasticsearch 适配器实例，提供了必要的参数。模型可以包含物化视图的定义。模型定义中定义的表的名称，对应于 Elasticsearch 中的索引。 下面是模型文件的基本示例： version: 1.0, defaultSchema: elasticsearch, schemas: [ type: custom, name: elasticsearch, factory: org.apache.calcite.adapter.elasticsearch.ElasticsearchSchemaFactory, operand: coordinates: 127.0.0.1: 9200 ] 假设此文件存储为 model.json，你可以通过 sqlline 连接到 Elasticsearch，如下所示： $ ./sqllinesqlline !connect jdbc:calcite:model=model.json admin admin 你也可以指定索引名称和路径前缀，它们由模型定义中的 index 和 pathPrefix 参数表示： ... operand: coordinates: 127.0.0.1: 9200, index: usa, pathPrefix: path ... 现在 sqlline 将接收访问 Elasticsearch 的 SQL 查询。此适配器的目的是，尽可能直接利用 Elasticsearch 中的过滤和排序，将查询编译为最有效的 Elasticsearch SEARCH JSON。 我们可以执行一个简单查询，来获取存储在索引 usa 中的所有州名称。 sqlline SELECT * from usa; _MAP=pop=13367, loc=[-72.505565, 42.067203], city=EAST LONGMEADOW, id=01028, state=MA_MAP=pop=1652, loc=[-72.908793, 42.070234], city=TOLLAND, id=01034, state=MA_MAP=pop=3184, loc=[-72.616735, 42.38439], city=HATFIELD, id=01038, state=MA_MAP=pop=43704, loc=[-72.626193, 42.202007], city=HOLYOKE, id=01040, state=MA_MAP=pop=2084, loc=[-72.873341, 42.265301], city=HUNTINGTON, id=01050, state=MA_MAP=pop=1350, loc=[-72.703403, 42.354292], city=LEEDS, id=01053, state=MA_MAP=pop=8194, loc=[-72.319634, 42.101017], city=MONSON, id=01057, state=MA_MAP=pop=1732, loc=[-72.204592, 42.062734], city=WALES, id=01081, state=MA_MAP=pop=9808, loc=[-72.258285, 42.261831], city=WARE, id=01082, state=MA_MAP=pop=4441, loc=[-72.203639, 42.20734], city=WEST WARREN, id=01092, state=MA 在执行此查询时，Elasticsearch 适配器能够识别 city 可以被 Elasticsearch 过滤，state 可以被 Elasticsearch 按升序排序。 下面是下发给 Elasticsearch 的最终 JSON 查询： query: constant_score: filter: bool: must: [ term: city: springfield ] , fields: [ city, state ], script_fields: , sort: [ state: asc ] 你也可以在没有预先视图定义的情况下，查询 Elasticsearch 索引： sqlline SELECT _MAP[city], _MAP[state] from elasticsearch.usa order by _MAP[state]; 使用滚动 API 对于没有聚合函数（如 COUNT、MAX 等）的查询，Elastic 适配器默认使用滚动 API。这确保向最终用户返回一致且完整的数据集（惰性地并以批处理方式）。请注意，当所有查询结果被消费完时，滚动会自动清除（删除）。 支持的版本 目前，此适配器支持 ElasticSearch 6.x 版本（或更新版本）。通常，我们会遵循官方支持计划。此外，不支持类型（此适配器仅支持索引）。 写在最后 笔者因为工作原因接触到 Calcite，前期学习过程中，深感 Calcite 学习资料之匮乏，因此创建了 Calcite 从入门到精通知识星球，希望能够将学习过程中的资料和经验沉淀下来，为更多想要学习 Calcite 的朋友提供一些帮助。"},{"title":"文件适配器","path":"/wiki/calcite/file-adapter.html","content":"原文链接：https://calcite.apache.org/docs/file_adapter.html 概述 文件适配器能够读取多种格式的文件，也可以通过各种协议（如 HTTP）读取文件。 例如，如果你定义： States - https://en.wikipedia.org/wiki/List_of_states_and_territories_of_the_United_States Cities - https://en.wikipedia.org/wiki/List_of_United_States_cities_by_population 然后你可以编写如下查询： select count(*) City Count, sum(100 * c.Population / s.Population) Pct State Populationfrom Cities c, States swhere c.State = s.State and s.State = California; 并了解加州有 69 个人口 10 万或更多的城市，占该州人口的近 1/2： +---------------------+----------------------+| City Count | Pct State Population |+---------------------+----------------------+| 69 | 48.574217177106576 |+---------------------+----------------------+ 对于 CSV 等简单文件格式，文件是自我描述的，你甚至不需要模型。请参阅 CSV 文件和无模型浏览。 一个简单的例子 让我们从一个简单的例子开始。首先，我们需要一个模型定义，如下所示。 version: 1.0, defaultSchema: SALES, schemas: [ name: SALES, type: custom, factory: org.apache.calcite.adapter.file.FileSchemaFactory, operand: tables: [ name: EMPS, url: file:file/src/test/resources/sales/EMPS.html , name: DEPTS, url: file:file/src/test/resources/sales/DEPTS.html ] ] 模式被定义为表列表，每个表至少包含一个表名称和一个 URL。如果一个页面有多个表，你可以在表定义中包含 selector 和 index 字段来指定所需的表。如果没有表规范，文件适配器会选择页面上最大的表。 EMPS.html 包含一个 HTML 表格： html body table thead tr thEMPNO/th thNAME/th thDEPTNO/th /tr /thead tbody tr td100/td tdFred/td td30/td /tr tr td110/td tdEric/td td20/td /tr tr td110/td tdJohn/td td40/td /tr tr td120/td tdWilma/td td20/td /tr tr td130/td tdAlice/td td40/td /tr /tbody /table /body/html 模型文件存储为 file/src/test/resources/sales.json，因此你可以通过 sqlline 连接，如下所示： $ ./sqllinesqlline !connect jdbc:calcite:model=file/src/test/resources/sales.json admin adminsqlline select * from sales.emps;+-------+--------+------+| EMPNO | DEPTNO | NAME |+-------+--------+------+| 100 | 30 | Fred || 110 | 20 | Eric || 110 | 40 | John || 120 | 20 | Wilma || 130 | 40 | Alice |+-------+--------+------+5 rows selected 映射表 现在让我们看一个更复杂的例子。这次我们通过 HTTP 连接到 Wikipedia，读取美国州和城市的页面，并从这些页面上的 HTML 表格中提取数据。表格具有更复杂的格式，文件适配器帮助我们定位和解析这些表格中的数据。 可以简单定义表以立即获得满足感： tableName: RawCities, url: https://en.wikipedia.org/wiki/List_of_United_States_cities_by_population 并随后进行改进以获得更好的可用性/查询： tableName: Cities, url: https://en.wikipedia.org/wiki/List_of_United_States_cities_by_population, path: #mw-content-text table.wikitable.sortable, index: 0, fieldDefs: [ th: 2012 rank, name: Rank, type: int, pattern: (\\\\d+), matchGroup: 0, th: City, selector: a, selectedElement: 0, th: State[5], name: State, selector: a:eq(0), th: 2012 estimate, name: Population, type: double, th: 2010 Census, skip: true, th: Change, skip: true, th: 2012 land area, name: Land Area (sq mi), type: double, selector: :not(span), th: 2012 population density, skip: true, th: ANSI, skip: true ] 连接并执行查询，如下所示。 $ ./sqllinesqlline !connect jdbc:calcite:model=file/src/test/resources/wiki.json admin adminsqlline select * from wiki.RawCities;sqlline select * from wiki.Cities; 请注意，Cities 比 RawCities 更容易使用，因为它的表定义有一个字段列表。 文件适配器使用 Jsoup 进行 HTML DOM 导航；表和字段的选择器遵循 Jsoup 选择器规范。 字段定义可用于重命名或跳过源字段，选择和调节单元格内容，以及设置数据类型。 解析单元格内容 文件适配器可以选择单元格内的 DOM 节点，替换所选元素内的文本，在所选文本内进行匹配，并为结果数据库列选择数据类型。处理步骤按描述的顺序应用，替换和匹配模式基于 Java 正则表达式。 更多示例 还有更多示例，形式为脚本： $ ./sqlline -f file/src/test/resources/webjoin.sql 运行 webjoin.sql 时，你将看到每个包含连接的查询的一些警告消息。这些是预期的，不会影响查询结果。这些消息将在下一个版本中被抑制。 CSV 文件和无模型浏览 有些文件描述自己的模式，对于这些文件，我们不需要模型。例如，DEPTS.csv 有一个整数 DEPTNO 列和一个字符串 NAME 列： DEPTNO:int,NAME:string10,Sales20,Marketing30,Accounts 你可以启动 sqlline，并将文件适配器指向该目录，每个 CSV 文件都变成一个表： $ ls file/src/test/resources/sales-csv -rw-r--r-- 1 jhyde jhyde 62 Mar 15 10:16 DEPTS.csv -rw-r--r-- 1 jhyde jhyde 262 Mar 15 10:16 EMPS.csv.gz$ ./sqlline -u jdbc:calcite:schemaFactory=org.apache.calcite.adapter.file.FileSchemaFactory;schema.directory=file/src/test/resources/sales-csvsqlline !tables+-----------+-------------+------------+------------+| TABLE_CAT | TABLE_SCHEM | TABLE_NAME | TABLE_TYPE |+-----------+-------------+------------+------------+| | adhoc | DEPTS | TABLE || | adhoc | EMPS | TABLE |+-----------+-------------+------------+------------+sqlline select distinct deptno from depts;+--------+| DEPTNO |+--------+| 20 || 10 || 30 |+--------+3 rows selected (0.985 seconds) JSON 文件和无模型浏览 有些文件描述自己的模式，对于这些文件，我们不需要模型。例如，DEPTS.json 有一个整数 DEPTNO 列和一个字符串 NAME 列： [ DEPTNO: 10, NAME: Sales , DEPTNO: 20, NAME: Marketing , DEPTNO: 30, NAME: Accounts ] 你可以启动 sqlline，并将文件适配器指向该目录，每个 JSON 文件都变成一个表： $ ls file/src/test/resources/sales-json -rw-r--r-- 1 jhyde jhyde 62 Mar 15 10:16 DEPTS.json$ ./sqlline -u jdbc:calcite:schemaFactory=org.apache.calcite.adapter.file.FileSchemaFactory;schema.directory=file/src/test/resources/sales-jsonsqlline !tables+-----------+-------------+------------+------------+| TABLE_CAT | TABLE_SCHEM | TABLE_NAME | TABLE_TYPE |+-----------+-------------+------------+------------+| | adhoc | DATE | TABLE || | adhoc | DEPTS | TABLE || | adhoc | EMPS | TABLE || | adhoc | EMPTY | TABLE || | adhoc | SDEPTS | TABLE |+-----------+-------------+------------+------------+sqlline select distinct deptno from depts;+--------+| DEPTNO |+--------+| 20 || 10 || 30 |+--------+3 rows selected (0.985 seconds) 未来改进 我们正在继续增强适配器，并欢迎贡献新的解析能力（例如解析 JSON 文件），以及动态生成 URL 以向下推送过滤器的功能。 写在最后 笔者因为工作原因接触到 Calcite，前期学习过程中，深感 Calcite 学习资料之匮乏，因此创建了 Calcite 从入门到精通知识星球，希望能够将学习过程中的资料和经验沉淀下来，为更多想要学习 Calcite 的朋友提供一些帮助。"},{"title":"Geode 适配器","path":"/wiki/calcite/geode-adapter.html","content":"原文链接：https://calcite.apache.org/docs/geode_adapter.html 有关下载和构建 Calcite 的说明，请从教程开始。 可选：在 maven 构建中添加 -Puberjdbc 以创建一个独立的自包含 Geode JDBC 适配器 jar。 一旦你成功编译了项目，就可以回到这里，并开始使用 Calcite 查询 Apache Geode。首先，我们需要定义一个模型。模型为 Calcite 创建 Geode 适配器实例，提供了必要的参数。模型可以包含物化视图的定义，模型定义中的表的名称对应于 Geode 中的区域。 下面给出了模型文件的基本示例： version: 1.0, defaultSchema: geode, schemas: [ name: geode_raw, type: custom, factory: org.apache.calcite.adapter.geode.rel.GeodeSchemaFactory, operand: locatorHost: localhost, locatorPort: 10334, regions: Zips, pdxSerializablePackagePath: .* ] 此适配器针对 Geode 1.3.x，regions 字段允许列出（逗号分隔）所有 Geode 区域以显示为关系表。 假设此文件存储为 model.json，你可以通过 sqlline 连接到 Geode，如下所示： $ ./sqllinesqlline !connect jdbc:calcite:model=model.json admin admin 现在 sqlline 将接收使用 OQL 访问区域的 SQL 查询，而且你不仅能执行 OQL 支持的查询，Calcite 还允许你执行复杂的操作，例如聚合或连接。适配器将尽可能尝试在 Geode 中直接利用过滤、排序和聚合，将查询编译为最有效的 OQL。 例如，在示例 Bookshop 数据集中有一个区域 BookMaster。 我们可以执行一个 SQL 查询，来获取按成本排序的年度零售成本： sqlline SELECT yearPublished, SUM(retailCost) AS totalCost FROM TEST.BookMaster GROUP BY yearPublished ORDER BY totalCost;+---------------+--------------------+| yearPublished | totalCost |+---------------+--------------------+| 1971 | 11.989999771118164 || 2011 | 94.9800033569336 |+---------------+--------------------+ 在执行此查询时，Geode 适配器能够识别出投影、分组和排序，这些运算符可以由 Geode 本地执行。 下面是下推到 Geode 的最终 OQL 查询： SELECT yearPublished AS yearPublished, SUM(retailCost) AS totalCostFROM /BookMasterGROUP BY yearPublishedORDER BY totalCost ASC Geode 中不支持的操作由 Calcite 本身处理。例如，以下在同一 Bookshop 数据集上的 JOIN 查询： sqlline SELECT i.itemNumber, m.author, m.retailCost FROM TEST.BookInventory i JOIN TEST.BookMaster m ON i.itemNumber = m.itemNumber WHERE m.retailCost 20;+------------+----------------+------------+| itemNumber | author | retailCost |+------------+----------------+------------+| 123 | Daisy Mae West | 34.99 |+------------+----------------+------------+ 将生成两个单独的下推 OQL 查询： SELECT itemNumber AS itemNumber, retailCost AS retailCost, author AS authorFROM /BookMasterWHERE retailCost 20;SELECT itemNumber AS itemNumberFROM /BookInventory; 然后查询结果将在 Calcite 中进行连接。要选择 Geode 数组字段中的特定项目，请使用 fieldName[index] 语法： sqlline SELECT loc [0] AS lon, loc [1] AS lat FROM geode.ZIPS 要选择嵌套字段，请使用映射 fieldName[nestedFiledName] 语法： sqlline SELECT primaryAddress [postalCode] AS postalCode FROM TEST.BookCustomer WHERE primaryAddress [postalCode] 0; 这条 SQL 将投影 BookCustomer.primaryAddress.postalCode 值字段。 以下演示和视频教程提供了有关 Geode 适配器的更多详细信息： 使用 Apache Calcite 启用对 Apache Geode/GemFire 的 SQL/JDBC 访问（GeodeSummit/SpringOne 2017） 通过 SQL/JDBC 访问 Apache Geode/GemFire 使用 IntelliJ SQL/Database 工具探索 Geode GemFire 数据 使用 Apache Zeppelin 通过 SQL/JDBC 进行高级 Apache Geode 数据分析 对 Geode/Greenplum/… 的统一访问 Apache Calcite 用于启用对 NoSQL 数据系统（如 Apache Geode）的 SQL 访问（ApacheCon Big Data，2016） 在提高适配器的灵活性和性能方面，仍有大量工作要做，但如果你正在寻找一种快速方法，来获得对存储在 Geode 中的数据的额外查询，Calcite 应该会证明自己是有价值的。 写在最后 笔者因为工作原因接触到 Calcite，前期学习过程中，深感 Calcite 学习资料之匮乏，因此创建了 Calcite 从入门到精通知识星球，希望能够将学习过程中的资料和经验沉淀下来，为更多想要学习 Calcite 的朋友提供一些帮助。"},{"title":"如何参与","path":"/wiki/calcite/howto.html","content":"原文链接：https://calcite.apache.org/docs/howto.html 以下是有关使用 Calcite 及其各种适配器的一些杂项文档。 从分发的源代码构建 先决条件是你的路径上有 Java（JDK 8、9、10、11、12、13、14、15、16、17、18 或 19）和 Gradle（版本 7.6.1）。 解压分发的源代码 .tar.gz 文件， cd 到解压源文件的根目录，然后使用 Gradle 进行构建： $ tar xvfz apache-calcite-1.36.0-src.tar.gz$ cd apache-calcite-1.36.0-src$ gradle build 运行测试描述了如何运行更多或更少的测试（但你应该使用 gradle 命令而不是 ./gradlew ）。 从 Git 构建 先决条件是你的路径上有 git 和 Java（JDK 8、9、10、11、12、13、14、15、16、17、18 或 19）。 创建 GitHub 存储库的本地副本，然后 cd 到其根目录，再使用包含的 Gradle 包装器进行构建： $ git clone https://github.com/apache/calcite.git$ cd calcite$ ./gradlew build Calcite 包含许多机器生成的代码。默认情况下，它们会在每次构建时重新生成，但这会产生负面影响，即在非机器生成的代码未更改时导致整个项目重新编译。 通常，当相关模板发生更改时，会自动调用重新生成，并且它应该透明地工作。但是，如果你的 IDE 不生成源（例如 core/build/javacc/javaCCMain/org/apache/calcite/sql/parser/impl/SqlParserImpl.java ），那么你可以手动调用 ./gradlew generateSources 任务。 运行测试描述了如何运行更多或更少的测试。 (adsbygoogle = window.adsbygoogle || []).push({}); 赞助商 Gradle 与 Gradle 包装器 Calcite 使用 Gradle Wrapper 来创建一致的构建环境。在典型情况下，你不需要手动安装 Gradle， ./gradlew 会为你下载正确的版本并验证预期的校验和。 如果你愿意，可以手动安装 Gradle，但请注意这可能会导致版本不匹配。 有关 Gradle 的更多信息，请查看以下链接：Gradle 五件事 和 Gradle 多项目构建。 升级 Gradle 和 Gradle 包装器 Gradle 的文档提供了有关如何升级 Gradle 的详细信息。以下是步骤列表： 运行 ./gradlew help --warning-mode=all 以查明你是否正在使用任何已弃用的功能； 修复弃用的问题并重复上一步以确认它们已修复。 Gradle 文档在这一步可能非常有帮助，因为它包含有关弃用以及如何处理它们的信息； 运行 ./gradlew wrapper --gradle-version new_gradle_version 来升级 Gradle。如有必要，它还会升级 Gradle Wrapper。此步骤还会更新 gradle/wrapper/gradle-wrapper.properties ，包括校验和； 如果需要，检查并更新 gradle.properties 中的 Kotlin 版本。应根据 Kotlin 兼容性矩阵进行检查； 步骤 3 将从 gradle/wrapper/gradle-wrapper.properties 中删除标头，因此现在运行 ./gradlew autostyleApply 将其添加回来； 根据官方 Gradle 版本校验和检查 gradle/wrapper/gradle-wrapper.properties 中更新的 Gradle 版本和校验和； 尝试构建项目并运行测试；使用故障排除指南调试任何错误； 更新本指南中的 Gradle 版本。 运行测试 构建时测试套件将默认运行，除非你指定 -x test： $ ./gradlew assemble # build the artifacts$ ./gradlew build -x test # build the artifacts, verify code style, skip tests$ ./gradlew check # verify code style, execute tests$ ./gradlew test # execute tests$ ./gradlew style # update code formatting (for auto-correctable cases) and verify style$ ./gradlew autostyleCheck checkstyleAll # report code style violations$ ./gradlew -PenableErrorprone classes # verify Java code with Error Prone compiler, requires Java 11 你可以使用 ./gradlew assemble 构建工件并跳过所有测试和验证。 还有其他选项可以控制运行哪些测试以及在什么环境中运行，如下所示。 -Dcalcite.test.db=DB （其中 DB 为 h2 、 hsqldb 、 mysql 或 postgresql ）允许你更改 JDBC 测试套件的数据源。 Calcite 的测试套件需要一个填充有 foodmart 数据集的 JDBC 数据源。 hsqldb 默认使用 hsqldb 内存数据库； 所有其他都访问测试虚拟机（请参阅下面的集成测试）。 mysql 和 postgresql 可能比 hsqldb 快一些，但你需要填充它（即配置虚拟机）。 -Dcalcite.debug 将额外的调试信息打印到标准输出； -Dcalcite.test.splunk 启用针对 Splunk 运行的测试。Splunk 必须已安装并正在运行； ./gradlew testSlow 运行需要更长时间执行的测试。例如，有些测试可以在内存中创建虚拟 TPC-H 和 TPC-DS 模式，并根据这些基准运行测试。 注意：测试是在分叉的 JVM 中执行的，因此使用 Gradle 运行测试时不会自动传递系统属性。默认情况下，构建脚本传递以下 -D... 属性（请参阅 build.gradle.kts 中的 passProperty ）： java.awt.headless； junit.jupiter.execution.parallel.enabled， 默认：true； junit.jupiter.execution.timeout.default， 默认：5 m； user.language， 默认：TR； user.country， 默认：tr； calcite.** （启用 calcite.test.db 及上述其他内容）。 运行集成测试 为了测试 Calcite 的外部适配器，应使用测试虚拟机。 VM 包括 Cassandra、Druid、H2、HSQLDB、MySQL、MongoDB 和 PostgreSQL。 测试虚拟机需要 5GiB 磁盘空间，构建需要 30 分钟。 注意：你可以使用 calcite-test-dataset 填充你自己的数据库，但建议使用测试虚拟机，以便可以重现测试环境。 虚拟机准备 安装依赖项：Vagrant 和 VirtualBox； 在与 Calcite 存储库相同的级别克隆：https://github.com/vlsi/calcite-test-dataset.git 仓库。例如： code +-- calcite +-- calcite-test-dataset 注意：集成测试搜索 ../calcite-test-dataset 或 ../../calcite-test-dataset。你可以通过 calcite.test.dataset 系统属性指定完整路径。 构建并启动 VM： cd calcite-test-dataset mvn install 虚拟机管理 测试虚拟机由 Vagrant 配置，因此应使用常规 Vagrant vagrant up 和 vagrant halt 来启动和停止虚拟机。 calcite-test-dataset 自述文件中列出了不同数据库的连接字符串。 建议的测试流程 注意：测试虚拟机应在启动集成测试之前启动。Calcite 本身不会启动/停止虚拟机。 命令行： 执行常规单元测试（不需要外部数据）：没有变化。 ./gradlew test 或 ./gradlew build； 对所有数据库执行所有测试： ./gradlew test integTestAll； 仅执行外部数据库的测试，不包括单元测试： ./gradlew integTestAll； 执行 PostgreSQL JDBC 测试： ./gradlew integTestPostgresql； 仅执行 MongoDB 测试： ./gradlew :mongo:build。 在 IDE 中执行： 执行常规单元测试：没有变化； 执行 MongoDB 测试：使用 calcite.integrationTest=true 系统属性运行 MongoAdapterTest.java； 执行 MySQL 测试：使用设置 -Dcalcite.test.db=mysql 运行 JdbcTest 和 JdbcAdapterTest； 执行 PostgreSQL 测试：使用设置 -Dcalcite.test.db=postgresql 运行 JdbcTest 和 JdbcAdapterTest。 集成测试技术细节 使用外部数据的测试是在 Gradle 的集成测试阶段执行的。我们目前不使用集成前测试/集成后测试，但是，我们将来可以使用它。构建通过/失败的验证是在验证阶段执行的。集成测试应命名为 ...IT.java ，因此在单元测试执行时不会拾取它们。 贡献 请参阅开发者指南-贡献。 入门 请参阅开发者指南-入门。 设置 IDE 进行贡献 设置 IntelliJ IDEA 下载高于 (2018.X) 的 IntelliJ IDEA 版本。版本 2019.2 和 2019.3 已经过社区成员的测试，看起来很稳定。对于不使用 Gradle 构建（版本 1.21.0 及之前版本）的方解石源，旧版本的 IDEA 仍然可以正常工作。 按照安装 IDEA 的标准步骤并设置 Calcite 目前支持的 JDK 版本之一。 首先从命令行构建 Calcite。 转到 文件 打开... 并打开 Calcite 的根 build.gradle.kts 文件。当 IntelliJ 询问你是否要将其作为项目或文件打开时，请选择项目。另外，当它询问你是否想要一个新窗口时，请选择是。 IntelliJ 的 Gradle 项目导入器应该处理其余的事情。 你可以在 GitHub 上导入部分实现的 IntelliJ 代码样式配置。它并没有做让 Calcite 的样式检查器满意所需的一切，但它做了相当多的事情。要导入，请转至首选项 编辑器 代码样式，单击方案旁边的齿轮，然后单击导入方案 IntelliJ IDEA 代码样式 XML。 导入程序完成后，测试项目设置。例如，使用 Navigate Symbol 导航到方法 JdbcTest.testWinAgg 并输入 testWinAgg 。右键单击并选择运行（或等效的键盘快捷键）来运行 testWinAgg 。 设置 NetBeans 从主菜单中，选择 文件 打开项目，然后导航到带有小 Gradle 图标的项目名称 (Calcite)，然后选择打开。等待 NetBeans 完成所有依赖项的导入。 为了确保项目配置成功，请导航到 org.apache.calcite.test.JdbcTest 中的方法 testWinAgg 。右键单击该方法并选择运行重点测试方法。 NetBeans 将运行 Gradle 进程，你应该在命令输出窗口中看到一行 Running org.apache.calcite.test.JdbcTest ，后跟 BUILD SUCCESS 。 注意：尚不清楚 NetBeans 是否在项目导入时自动生成相关源，因此你可能需要在导入项目之前（以及更新模板解析器源和项目版本时）运行 ./gradlew generateSources。 追踪（Tracing） 要启用追踪，请将以下标志添加到 java 命令行： -Dcalcite.debug=true 第一个标志使 Calcite 将其生成的 Java 代码（以执行查询）打印到 stdout。如果你正在调试像这样的神秘问题，它特别有用： Exception in thread main java.lang.ClassCastException: Integer cannot be cast to Long at Baz$1$1.current(Unknown Source) 默认情况下，Calcite 使用 SLF4J 的 Log4j 绑定。提供了一个配置文件，它将 INFO 级别的日志记录输出到 core/src/test/resources/log4j.properties 中的控制台。你可以修改 rootLogger 的级别以增加详细程度，或者更改特定类的级别（如果你愿意）。 # Change rootLogger level to WARNlog4j.rootLogger=WARN, A1# Increase level to DEBUG for RelOptPlannerlog4j.logger.org.apache.calcite.plan.RelOptPlanner=DEBUG# Increase level to TRACE for HepPlannerlog4j.logger.org.apache.calcite.plan.hep.HepPlanner=TRACE 在 Intellij 中调试生成的类 Calcite 使用 Janino 生成 Java 代码。生成的类可以交互式调试（请参阅 Janino 教程）。 要调试生成的类，请在启动 JVM 时设置两个系统属性： -Dorg.codehaus.janino.source_debugging.enable=true； -Dorg.codehaus.janino.source_debugging.dir=C:\\tmp （此属性是可选的；如果未设置，Janino 将在系统的默认临时文件位置创建临时文件，例如基于 Unix 的系统上的 /tmp）。 代码生成后，可以进入 Intellij 将包含生成的临时文件的文件夹标记为生成的源根或源根，也可以在启动 JVM 时直接将 org.codehaus.janino.source_debugging.dir 的值设置为现有的源根。 CSV 适配器 请参阅教程。 MongoDB 适配器 首先，下载并安装 Calcite，然后安装 MongoDB。 注意：你可以从上面的集成测试虚拟机使用 MongoDB。 将 MongoDB 的邮政编码数据集导入 MongoDB： $ curl -o /tmp/zips.json https://media.mongodb.org/zips.json$ mongoimport --db test --collection zips --file /tmp/zips.jsonTue Jun 4 16:24:14.190 check 9 29470Tue Jun 4 16:24:14.469 imported 29470 objects 登录 MongoDB 以检查它是否存在： $ mongoMongoDB shell version: 2.4.3connecting to: test db.zips.find().limit(3) city : ACMAR, loc : [ -86.51557, 33.584132 ], pop : 6055, state : AL, _id : 35004 city : ADAMSVILLE, loc : [ -86.959727, 33.588437 ], pop : 10616, state : AL, _id : 35005 city : ADGER, loc : [ -87.167455, 33.434277 ], pop : 3205, state : AL, _id : 35006 exitbye 使用 mongo-model.json Calcite 模型进行连接： $ ./sqllinesqlline !connect jdbc:calcite:model=mongodb/src/test/resources/mongo-model.json admin adminConnecting to jdbc:calcite:model=mongodb/src/test/resources/mongo-model.jsonConnected to: Calcite (version 1.x.x)Driver: Calcite JDBC Driver (version 1.x.x)Autocommit status: trueTransaction isolation: TRANSACTION_REPEATABLE_READsqlline !tables+------------+--------------+-----------------+---------------+| TABLE_CAT | TABLE_SCHEM | TABLE_NAME | TABLE_TYPE |+------------+--------------+-----------------+---------------+| null | mongo_raw | zips | TABLE || null | mongo_raw | system.indexes | TABLE || null | mongo | ZIPS | VIEW || null | metadata | COLUMNS | SYSTEM_TABLE || null | metadata | TABLES | SYSTEM_TABLE |+------------+--------------+-----------------+---------------+sqlline select count(*) from zips;+---------+| EXPR$0 |+---------+| 29467 |+---------+1 row selected (0.746 seconds)sqlline !quitClosing: org.apache.calcite.jdbc.FactoryJdbc41$CalciteConnectionJdbc41$ Splunk 适配器 要针对 Splunk 运行测试套件和示例查询，请按照 Splunk 教程中的说明加载 Splunk 的 tutorialdata.zip 数据集。 （此步骤是可选的，但它为示例查询提供了一些有趣的数据。如果你打算使用 -Dcalcite.test.splunk=true 运行测试套件，则这也是必要的。） 实现一个适配器 可以通过实现 CalcitePrepare.Context 创建新的适配器： import org.apache.calcite.adapter.java.JavaTypeFactory;import org.apache.calcite.jdbc.CalcitePrepare;import org.apache.calcite.jdbc.CalciteSchema;public class AdapterContext implements CalcitePrepare.Context @Override public JavaTypeFactory getTypeFactory() // adapter implementation return typeFactory; @Override public CalciteSchema getRootSchema() // adapter implementation return rootSchema; 用 Java 测试适配器 下面的示例显示了如何使用自定义上下文（在本例中为 AdapterContext ）将 SQL 查询提交到 CalcitePrepare 。 Calcite 使用 Context 提供的资源准备并实现查询执行。 CalcitePrepare.PrepareResult 提供对底层枚举和枚举方法的访问。可枚举本身自然可以是某些适配器特定的实现。 import org.apache.calcite.jdbc.CalcitePrepare;import org.apache.calcite.prepare.CalcitePrepareImpl;import org.junit.Test;public class AdapterContextTest @Test public void testSelectAllFromTable() AdapterContext ctx = new AdapterContext(); String sql = SELECT * FROM TABLENAME; Class elementType = Object[].class; CalcitePrepare.PrepareResultObject prepared = new CalcitePrepareImpl().prepareSql(ctx, sql, null, elementType, -1); Object enumerable = prepared.getExecutable(); // etc. 面向开发者的高级主题 如果你要向代码库的特定部分添加功能，则可能会对以下部分感兴趣。如果你只是从源代码构建并运行测试，则不需要了解这些主题。 Java 类型工厂 当 Calcite 比较类型（ RelDataType 的实例）时，它要求它们是同一对象。如果有两个不同的类型实例引用相同的 Java 类型，Calcite 可能无法识别它们是否匹配。建议： 在 Calcite 上下文中使用 JavaTypeFactory 的单个实例； 存储类型，以便始终为相同类型返回相同的对象。 重建生成的 Protocol Buffer 代码 Calcite 的 Avatica Server 组件支持使用 Protocol Buffers 的 RPC 序列化。在 Avatica 的上下文中，Protocol Buffers 可以生成由模式定义的消息集合。该库本身可以使用新模式解析旧的序列化消息。在不保证客户端和服务器具有相同版本的对象的环境中，这是非常需要的。 通常，Protocol Buffers 库生成的代码不需要仅在每次构建时重新生成，仅当架构更改时才需要重新生成。 首先，安装Protobuf 3.0： $ wget https://github.com/google/protobuf/releases/download/v3.0.0-beta-1/protobuf-java-3.0.0-beta-1.tar.gz$ tar xf protobuf-java-3.0.0-beta-1.tar.gz cd protobuf-3.0.0-beta-1$ ./configure$ make$ sudo make install 然后，重新生成编译后的代码： $ cd avatica/core$ ./src/main/scripts/generate-protobuf.sh 创建优化器规则 创建一个扩展 RelRule 的类（或者偶尔是一个子类）。 /** Planner rule that matches a @link Filter and futzes with it. * * @see CoreRules#FILTER_FUTZ */class FilterFutzRule extends RelRuleFilterFutzRule.Config /** Creates a FilterFutzRule. */ protected FilterFutzRule(Config config) super(config); @Override onMatch(RelOptRuleCall call) final Filter filter = call.rels(0); final RelNode newRel = ...; call.transformTo(newRel); /** Rule configuration. */ interface Config extends RelRule.Config Config DEFAULT = EMPTY.as(Config.class) .withOperandSupplier(b0 - b0.operand(LogicalFilter.class).anyInputs()) .as(Config.class); @Override default FilterFutzRule toRule() return new FilterFutzRule(this); 类名应指示匹配的基本 RelNode 类型，有时后跟规则的作用，然后是单词 Rule 。示例： ProjectFilterTransposeRule 、 FilterMergeRule 。 该规则必须有一个以 Config 作为参数的构造函数。它应该是 protected ，并且只会从 Config.toRule() 调用。 该类必须包含一个名为 Config 的接口，该接口扩展 RelRule.Config （或规则的超类的配置）。 Config 必须实现 toRule 方法并创建规则。 Config 必须有一个名为 DEFAULT 的成员来创建典型配置。至少，它必须调用 withOperandSupplier 来创建典型的算子树。 该规则不应具有静态 INSTANCE 字段。持有者类中应该有一个规则实例，例如 CoreRules 或 EnumerableRules ： public class CoreRules ... /** Rule that matches a @link Filter and futzes with it. */ public static final FILTER_FUTZ = FilterFutzRule.Config.DEFAULT.toRule(); 持有者类可以包含具有不同参数的规则的其他实例（如果常用）。 如果规则是使用多种操作数模式实例化的（例如，使用相同 RelNode 基类的不同子类，或者使用不同的谓词），则配置可能包含一个方法 withOperandFor 以使其更容易构建常见的操作数模式（参见 FilterAggregateTransposeRule 示例）。 提交者的高级主题 以下部分是 Calcite 提交者，特别是发布经理感兴趣的。 通过 GitHub 管理 Calcite 仓库 提交者拥有对 Calcite 的 ASF git 存储库的写入权限，该存储库托管项目的源代码以及网站。 GitBox 上的所有存储库都可以在 GitHub 上使用，并启用写入访问权限，包括打开/关闭/合并拉取请求和解决问题的权限。 为了利用 GitHub 服务，提交者应通过帐户链接页面链接其 ASF 和 GitHub 帐户。 步骤如下： 将你的 GitHub 用户名设置到你的 Apache 配置文件中； 在你的 GitHub 帐户上启用 GitHub 2FA； 激活 GitHub 2FA 会更改身份验证过程，并可能影响你访问 GitHub 的方式。你可能需要建立个人访问令牌或将公共 SSH 密钥上传到 GitHub，具体取决于你使用的协议（HTTPS 与 SSH）； 使用帐户链接页面合并你的 Apache 和 GitHub 帐户（你应该在 GitBox 中看到 3 个绿色对勾）； 至少等待 30 分钟，你将收到邀请你加入 Apache GitHub 组织的电子邮件； 接受邀请并验证你是团队成员。 合并拉取请求 这些是针对 Calcite 提交者的说明，他已审查了贡献者的拉取请求，发现它令人满意，并将其合并到 main。通常贡献者不是提交者（否则，在你在审查中批准后，他们会自己提交）。 有些类型的持续集成测试不会针对 PR 自动运行。可以通过向 PR 添加适当的标签来显式触发这些测试。例如，你可以通过添加 slow-tests-needed 标签来运行慢速测试。由你决定是否需要在合并之前运行这些附加测试。 如果 PR 有多个提交，请将它们压缩为单个提交。提交消息应遵循贡献指南中概述的约定。如果存在冲突，最好要求贡献者执行此步骤，否则最好手动执行此操作，因为这样可以节省时间，也可以避免向 GitHub 上的许多人发送不必要的通知消息。 如果通过命令行（而不是通过 GitHub Web 界面）执行合并，请确保消息包含一行 Close apache/calcite#YYY，其中 YYY 是 GitHub 拉取请求标识符。 当 PR 合并并推送后，请务必更新 JIRA 案例。你必须： 解决问题（不要关闭它，因为这将由发布经理完成）； 选择已修复作为解决原因； 在修复版本字段中标记适当的版本（例如 1.20.0）； 添加评论（例如，已修复……），其中包含指向解决问题的提交的超链接（在 GitHub 或 GitBox 中），并感谢贡献者的贡献（如果贡献者是已经是提交者了）。提供的超链接应该是相对于主分支的。你应该能够通过浏览来识别提交——https://github.com/apache/calcite/commits/main/。 设置 PGP 签名密钥 按照此处的说明创建密钥对（在 macOS 上，我执行了 brew install gpg 和 gpg --full-generate-key）。 按照 KEYS 文件中的说明将你的公钥添加到 KEYS 文件中。如果你没有更新 KEYS 文件的权限，请向 PMC 寻求帮助（ KEYS 文件不存在于 git 存储库或发布 tar 球中，因为这是多余的）。 为了能够制作候选版本，请确保将密钥上传到 https://keyserver.ubuntu.com 和/或 http://pool.sks-keyservers.net:11371（Nexus 使用的密钥服务器）。 设置 Nexus 存储库凭据 Gradle 提供了多种配置项目属性的方法。例如，你可以更新 $HOME/.gradle/gradle.properties。 注意：构建脚本会打印缺少的属性，因此你可以尝试运行它并让它抱怨缺少的属性。 使用以下选项： asfCommitterId=asfNexusUsername=asfNexusPassword=asfSvnUsername=asfSvnPassword=asfGitSourceUsername=asfGitSourcePassword= 注意： asfNexusUsername 和 asfSvnUsername 都是你的 apache id， asfNexusPassword 和 asfSvnPassword 是相应的密码； Git 源帐户可以配置为 Gitbox（默认）或 Github。对于 Gitbox， asfGitSourceUsername 是你的 apache id， asfGitSourcePassword 是相应的密码。对于 Github， asfGitSourceUsername 是你的 GitHub id，而 asfGitSourcePassword 不是你的 GitHub 密码，你需要在 https://github.com/settings/tokens 中选择 Personal access tokens。 当使用 asflike-release-environment 时，凭据取自 asfTest... （例如 asfTestNexusUsername=test ） 注意：如果你想使用 gpg-agent ，你需要传递一些更多的属性： useGpgCmd=truesigning.gnupg.keyName=signing.gnupg.useLegacyGpg= 制作快照 在你开始之前： 确保你使用的是 JDK 8。注意：如果你使用基于 OpenJDK 的 Java，则需要 Java 8u202 或更高版本。 使用 -Dcalcite.test.db=hsqldb （默认）确保构建和测试成功。 # Make sure that there are no junk files in the sandboxgit clean -xn# Publish snapshot artifacts./gradlew clean publish -Pasf 制作候选版本 注意：发布构建（dist.apache.org 和 repository.apache.org）通过 stage-vote-release-plugin 进行管理。 在你开始之前： 请查阅发布仪表板以快速了解发布状态，并采取适当的操作来解决待处理的票证或将其移至另一个版本/待办事项； 发送电子邮件至 dev@calcite.apache.org 通知 RC 构建过程正在启动，因此 main 分支处于代码冻结状态，直至另行通知； 如上所述设置签名密钥； 确保你使用的是 JDK 8（而不是 9 或 10）； 检查 README 和 site/_docs/howto.md 的版本号是否正确； 检查 site/_docs/howto.md 是否具有正确的 Gradle 版本； 检查 NOTICE 是否具有当前版权年份； 检查 calcite.version 在 /gradle.properties 中是否具有正确的值； 确保构建和测试成功； 确保 ./gradlew javadoc 成功（即没有给出错误；警告也可以）； 使用 ./gradlew dependencyCheckUpdate dependencyCheckAggregate 生成依赖项之间发生的漏洞的报告。如果在依赖项中发现新的严重漏洞，请向 private@calcite.apache.org 报告； 确定JDK、操作系统和Guava支持的配置。这些可能与先前版本的发行说明中描述的相同。将它们记录在发行说明中。要测试 Guava 版本 x.y，请指定 -Pguava.version=x.y； 使用属性的可选测试： -Dcalcite.test.db=mysql； -Dcalcite.test.db=hsqldb； -Dcalcite.test.mongodb； -Dcalcite.test.splunk。 使用任务的可选测试： ./gradlew testSlow； 将发行说明添加到 site/_docs/history.md 。如果要发布的版本已存在发行说明，但已被注释掉，请删除注释（ % comment %` 和 `% endcomment % ）。包括提交历史记录、对该版本做出贡献的人员姓名，并说明该版本针对哪些版本的 Java、Guava 和操作系统进行了测试； 确保每个已解决的 JIRA 案例（包括重复的案例）都分配了一个修复版本（很可能是我们即将发布的版本）。 生成贡献者列表： # Commits since 1.35range=calcite-1.35.0..HEAD# distinct authorsgit log --abbrev-commit --pretty=format:%aN, $range | sort -u# most prolific authorsgit log --abbrev-commit --pretty=format:%aN $range | sort | uniq -c | sort -nr# number of JIRA casesgit log --abbrev-commit --pretty=format:%f $range | awk -F- $1 == CALCITE print $2 | sort -u | wc 使用 Spatial 和 Oracle 函数表进行冒烟测试 sqlline ： $ ./sqlline !connect jdbc:calcite:fun=spatial,oracle sa SELECT NVL(ST_Is3D(ST_PointFromText(POINT(-71.064544 42.28787))), TRUE);+--------+| EXPR$0 |+--------+| false |+--------+1 row selected (0.039 seconds) !quit 候选版本进程不会添加提交，因此即使失败也不会造成任何影响。它可能会留下 -rc 标记，如果需要可以将其删除。 如果你愿意，你可以在 asflike-release-environment 的帮助下执行试运行发布；它会执行相同的步骤，但会将更改推送到模拟 Nexus、Git 和 SVN 服务器。 如果任何步骤失败，请解决问题，然后从头开始。 开始候选版本构建 选择一个候选版本索引并确保它不会干扰该版本之前的候选版本。 # Tell GPG how to read a password from your terminalexport GPG_TTY=$(tty)# Make sure that there are no junk files in the sandboxgit clean -xn# Dry run the release candidate (push to asf-like-environment)./gradlew prepareVote -Prc=0# Push release candidate to ASF servers# If you prefer to use Github account, change pushRepositoryProvider to GITHUB./gradlew prepareVote -Prc=0 -Pasf -Pasf.git.pushRepositoryProvider=GITBOX 故障排除 net.rubygrapefruit.platform.NativeException: Could not start 'svnmucc' ：确保你的计算机中安装了 svnmucc 命令； Execution failed for task ':closeRepository' ... Possible staging rules violation. Check repository status using Nexus UI ：登录 Nexus UI 查看实际错误。如果是 Failed: Signature Validation. No public key: Key with id: ... was not able to be located ，请确保你已将密钥上传到 Nexus 使用的密钥服务器，请参阅上文； [CALCITE-5573] 签署构建时 GradleprepareVote 失败； [VLSI-RELEASE-PLUGINS-64] 由于缺少 nexus.txt，任务 :releaseRepository 执行失败。 检查构建 release/build/distributions 目录中应包含以下 3 个文件，其中包括： apache-calcite-X.Y.Z-src.tar.gz； apache-calcite-X.Y.Z-src.tar.gz.asc； apache-calcite-X.Y.Z-src.tar.gz.sha512。 请注意，文件名以 apache-calcite- 开头； 在源发行版 .tar.gz （当前没有二进制发行版）中，检查所有文件是否属于名为 apache-calcite-X.Y.Z-src 的目录； 该目录必须包含文件 NOTICE 、 LICENSE 、 README 、 README.md； 检查 README 中的版本是否正确； 检查 NOTICE 中的版权年份是否正确； 检查 LICENSE 是否与签入 git 的文件相同。 确保以下文件不会出现在源发行版中： KEYS 、 gradlew 、 gradlew.bat 、 gradle-wrapper.jar 、 gradle-wrapper.properties； 确保源发行版中没有 KEYS 文件； 在每个 .jar（例如 core/build/libs/calcite-core-X.Y.Z.jar 和 mongodb/build/libs/calcite-mongodb-X.Y.Z-sources.jar ）中，检查 META-INF 目录是否包含 LICENSE 、 NOTICE； 检查 PGP，按照此文档。 验证 Nexus 存储库中的暂存构建： 访问 https://repository.apache.org/ 并登录； 在 Build Promotion 下，单击 Staging Repositories； 在 Staging Repositories 选项卡中应该有一行包含配置文件 org.apache.calcite 和状态 closed； 浏览工件树并确保 .jar、.pom、.asc 文件存在。 尝试发布失败后进行清理 如果某些内容不正确，你可以修复它，提交它，并为下一个候选人做好准备。候选版本标签可能会保留一段时间。 验证发布 # Check that the signing key (e.g. DDB6E9812AD3FAE3) is pushedgpg --recv-keys key# Check keyscurl -O https://dist.apache.org/repos/dist/release/calcite/KEYS# Sign/check sha512 hashes# (Assumes your O/S has a shasum command.)function checkHash() cd $1 for i in *.pom,gz; do if [ ! -f $i ]; then continue fi if [ -f $i.sha512 ]; then if [ $(cat $i.sha512) = $(shasum -a 512 $i) ]; then echo $i.sha512 present and correct else echo $i.sha512 does not match fi else shasum -a 512 $i $i.sha512 echo $i.sha512 created fi donecheckHash apache-calcite-X.Y.Z-rcN 通过 Apache 投票流程获得发布批准 通过向开发者列表发送电子邮件来开始投票。如果成功完成，Gradle prepareVote 任务会在最后打印草稿邮件。你可以在 /build/prepareVote/mail.txt 中找到草稿。 投票结束后，发送结果： Subject: [RESULT] [VOTE] Release apache-calcite-X.Y.Z (release candidate N)To: dev@calcite.apache.orgThanks to everyone who has tested the release candidate and giventheir comments and votes.The tally is as follows.N binding +1s:namesN non-binding +1s:namesNo 0s or -1s.Therefore, I am delighted to announce that the proposal to releaseApache Calcite X.Y.Z has passed.Thanks everyone. We’ll now roll the release out to the mirrors.There was some feedback during voting. I shall open a separatethread to discuss.Julian 发布版本 成功发布投票后，我们需要将版本推送到镜像和其他任务。 选择发布日期。这是基于你预计宣布发布的时间。这通常是投票结束后的一天。请记住，UTC 日期在太平洋时间下午 4 点更改。 # Dry run publishing the release (push to asf-like-environment)./gradlew publishDist -Prc=0# Publish the release to ASF servers# If you prefer to use Github account, change pushRepositoryProvider to GITHUB./gradlew publishDist -Prc=0 -Pasf -Pasf.git.pushRepositoryProvider=GITBOX 如果由于某种原因 publishDist 任务失败（例如未能发布 nexus 存储库，仍然可以手动执行发布任务。如果你不确定需要做什么，请在开发列表中寻求帮助。 如果 releaseRepository 任务打印如下内容： Task :releaseRepositoryInitialized stagingRepositoryId orgapachecalcite-1219 for repository nexusGET request failed. 404: Not Found, body: [errors:[[id:*, msg:No such repository: orgapachecalcite-1219]]]Requested operation was executed successfully in attempt 83 (maximum allowed 601) 很可能存储库已成功发布，你可以在 ASF Nexus 中检查它。 Svnpubsub 将发布到发布存储库并几乎立即传播到镜像。因此无需等待超过十五分钟即可宣布发布。 如果现在有超过 2 个版本，请清除最旧的版本： cd ~/dist/release/calcitesvn rm apache-calcite-X.Y.Zsvn ci 旧版本将保留在版本存档中。 你应该会收到一封来自 Apache Reporter Service 的电子邮件。请务必在电子邮件中链接的网站上添加最新版本的版本号和日期。 一旦发布提交/标签到达 ASF 远程并触发相应的 Github 工作流程，新版本的发行说明和 javadoc 将自动部署到网站。 通过复制 site/_posts/2016-10-12-release-1.10.0.md 添加发布公告，并根据需要调整 history.md 中的发布日期。按照 site/README.md 中的说明在本地预览更改，然后提交更改并将其推送到 main 分支。请注意，由于 CALCITE-5584，该提交应作为最后一次提交推送到 Github，不要将其与准备下一次开发迭代提交链接。 确保正确显示网站的所有更改（新闻、发行说明、javadoc）。 在 JIRA 中，搜索此版本中解决的所有问题，然后进行批量更新（选择 transition issues 选项），将其状态更改为“已关闭”，并添加更改注释“已在版本 X.Y.Z (YYYY-MM) 中解决” -DD)”（适当填写版本号和日期）。取消选中“发送此更新的邮件”。在 Calcite 项目的发布选项卡下，将发布 X.Y.Z 标记为已发布。如果尚不存在，请为下一个版本创建一个新版本（例如，X.Y+1.Z）。为了使发布仪表板反映下一个版本的状态，请更改为仪表板提供支持的 JIRA 过滤器中的修复版本并保存更改。 增加 /gradle.properties 中的 calcite.version 值，提交并推送更改，并显示消息“准备下一次开发迭代”（请参阅 ed1470a 作为参考）。 重新打开 main 分支。发送电子邮件至 dev@calcite.apache.org 通知 main 代码冻结已结束并且可以恢复提交。 通过使用 @apache.org 地址向 announce@apache.org 发送电子邮件来宣布发布。你可以使用1.20.0 公告作为模板。请务必包含项目的简短描述。 发布网站 请参阅 site/README.md 中的说明 。 PMC 成员的高级主题 处理 JIRA 帐户请求 以下是一些在处理添加 JIRA 帐户作为贡献者的请求时可以使用的电子邮件模板。 帐户已添加到贡献者列表 Hello [INSERT NAME HERE],Thanks for your interest in becoming a Calcite contributor! I have added your username ([INSERT USERNAME HERE])to the contributors group in JIRA. Happy contributing!If you have not subscribed to our development list (dev@calcite.apache.org) yet, I encourage you to do so byemailing dev-subscribe@calcite.apache.org. Further information about our mailing lists is available here:https://calcite.apache.org/community/#mailing-listsBest regards,[INSERT YOUR NAME HERE] 找不到帐户 Hello [INSERT NAME HERE],Thanks for your interest in becoming a Calcite contributor! I am sorry to inform you that I was unable tofind your account ([INSERT USERNAME HERE]) in JIRA and was not able to add you to the contributors group.Please let me know the correct username by return email and I will process your request again.If you do not have an ASF JIRA account, please follow the instructions here to request one:https://calcite.apache.org/develop/#i-do-not-have-an-asf-jira-account-want-to-request-an-account-and-be-added-as-a-contributorBest regards,[INSERT YOUR NAME HERE] 写在最后 笔者因为工作原因接触到 Calcite，前期学习过程中，深感 Calcite 学习资料之匮乏，因此创建了 Calcite 从入门到精通知识星球，希望能够将学习过程中的资料和经验沉淀下来，为更多想要学习 Calcite 的朋友提供一些帮助。"},{"title":"InnoDB 适配器","path":"/wiki/calcite/innodb-adapter.html","content":"原文链接：https://calcite.apache.org/docs/innodb_adapter.html MySQL 是最流行的开源 SQL 数据库管理系统，由 Oracle Corporation 开发、分发和支持。InnoDB 是 MySQL 中一种通用的存储引擎，自 5.6 起成为默认的 MySQL 存储引擎，平衡了高可靠性和高性能。 Calcite 的 InnoDB 适配器允许你直接基于 InnoDB 数据文件查询数据，如下图所示，数据文件也称为 .ibd 文件。它利用了 innodb-java-reader。该适配器与 JDBC 适配器不同，后者映射 JDBC 数据源中的模式，需要 MySQL 服务器来提供响应。 SQL query | | / \\ --------- --------- | | v v+-------------------------+ +------------------------+| MySQL Server | | Calcite InnoDB Adapter || | +------------------------+| +---------------------+ | +--------------------+| |InnoDB Storage Engine| | | innodb-java-reader || +---------------------+ | +--------------------++-------------------------+-------------------- File System -------------------- +------------+ +-----+ | .ibd files | ... | | InnoDB Data files +------------+ +-----+ 有了 .ibd 文件和相应的 DDL，InnoDB 适配器充当一个简单的 “MySQL 服务器”：它接受 SQL 查询，并尝试基于 innodb-java-reader 提供的 InnoDB 文件访问 API 来编译每个查询。它尽可能在 InnoDB 数据文件中直接进行投影、过滤和排序。 更重要的是，通过 DDL 语句，适配器是 “索引感知的”。它利用规则来选择要扫描的适当索引，例如，使用主键或辅助键来查找数据，然后尝试将某些条件下推到存储引擎。适配器还支持提示，以便用户可以告诉优化器使用特定的索引。 下面给出一个模型文件的基本示例，此模式从 MySQL “scott” 数据库读取： version: 1.0, defaultSchema: scott, schemas: [ name: scott, type: custom, factory: org.apache.calcite.adapter.innodb.InnodbSchemaFactory, operand: sqlFilePath: [ /path/scott.sql ], ibdDataFileBasePath: /usr/local/mysql/data/scott ] sqlFilePath 是一个 DDL 文件列表，你可以通过在命令行中执行 mysqldump 来生成表定义： mysqldump -d -uusername -ppassword -h hostname dbname /path/scott.sql 的文件内容如下： CREATE TABLE `DEPT`( `DEPTNO` TINYINT NOT NULL, `DNAME` VARCHAR(50) NOT NULL, `LOC` VARCHAR(20), UNIQUE KEY `DEPT_PK` (`DEPTNO`)) ENGINE=InnoDB DEFAULT CHARSET=latin1;CREATE TABLE `EMP`( `EMPNO` INT(11) NOT NULL, `ENAME` VARCHAR(100) NOT NULL, `JOB` VARCHAR(15) NOT NULL, `AGE` SMALLINT, `MGR` BIGINT, `HIREDATE` DATE, `SAL` DECIMAL(8,2) NOT NULL, `COMM` DECIMAL(6,2), `DEPTNO` TINYINT, `EMAIL` VARCHAR(100) DEFAULT NULL, `CREATE_DATETIME` DATETIME, `CREATE_TIME` TIME, `UPSERT_TIME` TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP, PRIMARY KEY (`EMPNO`), KEY `ENAME_KEY` (`ENAME`), KEY `HIREDATE_KEY` (`HIREDATE`), KEY `CREATE_DATETIME_JOB_KEY` (`CREATE_DATETIME`, `JOB`), KEY `CREATE_TIME_KEY` (`CREATE_TIME`), KEY `UPSERT_TIME_KEY` (`UPSERT_TIME`), KEY `DEPTNO_JOB_KEY` (`DEPTNO`, `JOB`), KEY `DEPTNO_SAL_COMM_KEY` (`DEPTNO`, `SAL`, `COMM`), KEY `DEPTNO_MGR_KEY` (`DEPTNO`, `MGR`), KEY `AGE_KEY` (`AGE`)) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4; ibdDataFileBasePath 是 .ibd 文件的父文件路径。 假设模型文件存储为 model.json，你可以通过 sqlline 连接到 InnoDB 数据文件以执行查询，如下所示： sqlline !connect jdbc:calcite:model=model.json admin admin 我们可以通过编写标准 SQL 查询所有员工： sqlline select empno, ename, job, age, mgr from EMP;+-------+--------+-----------+-----+------+| EMPNO | ENAME | JOB | AGE | MGR |+-------+--------+-----------+-----+------+| 7369 | SMITH | CLERK | 30 | 7902 || 7499 | ALLEN | SALESMAN | 24 | 7698 || 7521 | WARD | SALESMAN | 41 | 7698 || 7566 | JONES | MANAGER | 28 | 7839 || 7654 | MARTIN | SALESMAN | 27 | 7698 || 7698 | BLAKE | MANAGER | 38 | 7839 || 7782 | CLARK | MANAGER | 32 | 7839 || 7788 | SCOTT | ANALYST | 45 | 7566 || 7839 | KING | PRESIDENT | 22 | null || 7844 | TURNER | SALESMAN | 54 | 7698 || 7876 | ADAMS | CLERK | 35 | 7788 || 7900 | JAMES | CLERK | 40 | 7698 || 7902 | FORD | ANALYST | 28 | 7566 || 7934 | MILLER | CLERK | 32 | 7782 |+-------+--------+-----------+-----+------+ 在执行此查询时，InnoDB 适配器使用主键扫描 InnoDB 数据文件 EMP.ibd，主键在 MySQL 中也称为聚类 B+ 树索引，并且能够将投影下推到底层存储引擎。投影可以减少从存储引擎获取的数据大小。 我们可以通过过滤来查找一个员工。InnoDB 适配器通过在 model.json 中提供的 DDL 文件检索所有索引。 sqlline select empno, ename, job, age, mgr from EMP where empno = 7782;+-------+-------+---------+-----+------+| EMPNO | ENAME | JOB | AGE | MGR |+-------+-------+---------+-----+------+| 7782 | CLARK | MANAGER | 32 | 7839 |+-------+-------+---------+-----+------+ InnoDB 适配器识别 empno 是主键，并使用聚类索引执行点查找，而不是全表扫描。 我们还可以对主键进行范围查询： sqlline select empno, ename, job, age, mgr from EMP where empno 7782 and empno 7900; 请注意，这种具有可接受范围的查询在 MySQL 使用 InnoDB 存储引擎时通常很高效，因为对于聚类 B+ 树索引，索引中接近的记录在数据文件中也接近，这有利于扫描。 我们可以通过辅助键查找员工。例如，在以下查询中，过滤条件是类型为 VARCHAR 的字段 ename。 sqlline select empno, ename, job, age, mgr from EMP where ename = smith;+-------+-------+-------+-----+------+| EMPNO | ENAME | JOB | AGE | MGR |+-------+-------+-------+-----+------+| 7369 | SMITH | CLERK | 30 | 7902 |+-------+-------+-------+-----+------+ InnoDB 适配器在 MySQL 中几乎所有常用数据类型上都能很好地工作，有关支持的数据类型的更多信息，请参阅 innodb-java-reader。 我们可以通过组合键查询。例如，给定 DEPTNO_MGR_KEY 的辅助索引。 sqlline select empno, ename, job, age, mgr from EMP where deptno = 20 and mgr = 7566;+-------+-------+---------+-----+------+| EMPNO | ENAME | JOB | AGE | MGR |+-------+-------+---------+-----+------+| 7788 | SCOTT | ANALYST | 45 | 7566 || 7902 | FORD | ANALYST | 28 | 7566 |+-------+-------+---------+-----+------+ InnoDB 适配器利用匹配的键 DEPTNO_MGR_KEY 将过滤条件 deptno = 20 and mgr = 7566 下推。 在某些情况下，由于底层存储引擎 API 的限制，只能下推部分条件；其余条件保留在计划的其余部分。给定以下 SQL，只有 deptno = 20 被下推。 select empno, ename, job, age, mgr from EMP where deptno = 20 and upsert_time 2018-01-01 00:00:00; innodb-java-reader 仅支持使用索引进行具有上下限的范围查询，而不完全支持 索引条件下推（ICP）。存储引擎返回一系列行，Calcite 从获取的行中评估其余的 WHERE 条件。 对于以下 SQL，有多个索引满足左前缀索引规则：可能的索引是 DEPTNO_JOB_KEY、DEPTNO_SAL_COMM_KEY 和 DEPTNO_MGR_KEY。InnoDB 适配器根据 DDL 中定义的顺序选择其中一个；只有 deptno = 20 条件被下推，其余的 WHERE 条件由 Calcite 的内置执行引擎处理。 sqlline select empno, deptno, sal from EMP where deptno = 20 and sal 2000;+-------+--------+---------+| EMPNO | DEPTNO | SAL |+-------+--------+---------+| 7788 | 20 | 3000.00 || 7902 | 20 | 3000.00 || 7566 | 20 | 2975.00 |+-------+--------+---------+ 通过辅助键访问行需要通过辅助索引扫描并检索回 InnoDB 中的聚类索引，对于 “大” 扫描，这将引入许多随机 I/O 操作，因此性能通常不够好。请注意，上面的查询可以通过使用 DEPTNO_SAL_COMM_KEY 索引来获得更高的性能，因为覆盖索引不需要检索回聚类索引。我们可以通过提示强制使用 DEPTNO_SAL_COMM_KEY 索引，如下所示。 sqlline select empno, deptno, sal from EMP/*+ index(DEPTNO_SAL_COMM_KEY) */ where deptno = 20 and sal 2000; 提示可以在 SqlToRelConverter 中配置，要启用提示，你应该在 SqlToRelConverter.ConfigBuilder 中为 TableScan 注册 index HintStrategy。索引提示对基本 TableScan 关系节点生效，如果有条件匹配索引，索引条件也可以下推。对于以下 SQL，虽然没有任何索引可以使用，但通过利用覆盖索引，性能比全表扫描更好，我们可以强制使用 DEPTNO_MGR_KEY 在辅助索引中扫描。 sqlline select empno,mgr from EMP/*+ index(DEPTNO_MGR_KEY) */ where mgr = 7839; 如果排序与使用的索引的自然排序匹配，则可以下推。 sqlline select deptno,ename,hiredate from EMP where hiredate 2020-01-01 order by hiredate desc;+--------+--------+------------+| DEPTNO | ENAME | HIREDATE |+--------+--------+------------+| 20 | ADAMS | 1987-05-23 || 20 | SCOTT | 1987-04-19 || 10 | MILLER | 1982-01-23 || 20 | FORD | 1981-12-03 || 30 | JAMES | 1981-12-03 || 10 | KING | 1981-11-17 || 30 | MARTIN | 1981-09-28 || 30 | TURNER | 1981-09-08 || 10 | CLARK | 1981-06-09 || 30 | WARD | 1981-02-22 || 30 | ALLEN | 1981-02-20 || 20 | JONES | 1981-02-04 || 30 | BLAKE | 1981-01-05 || 20 | SMITH | 1980-12-17 |+--------+--------+------------+ 关于时区 MySQL 将 TIMESTAMP 值从当前时区转换为 UTC 进行存储，并从 UTC 转换回当前时区进行检索。因此在此适配器中，MySQL 的 TIMESTAMP 被映射到 Calcite 的 TIMESTAMP WITH LOCAL TIME ZONE。每个会话的时区设置可以在 Calcite 连接配置 timeZone 中配置，这告诉 MySQL 服务器 TIMESTAMP 值所在的时区。目前 InnoDB 适配器无法将属性传递到底层存储引擎，但你可以在 model.json 中指定 timeZone，如下所示。请注意，只有在连接配置中设置了 timeZone 并且它与 InnoDB 适配器运行的系统默认时区不同时，才需要指定该属性。 version: 1.0, defaultSchema: test, schemas: [ name: test, type: custom, factory: org.apache.calcite.adapter.innodb.InnodbSchemaFactory, operand: sqlFilePath: [src/test/resources/data_types.sql], ibdDataFileBasePath: src/test/resources/data, timeZone: America/Los_Angeles ] 限制 innodb-java-reader 对 .ibd 文件有一些先决条件。 支持 COMPACT 和 DYNAMIC 行格式。不支持 COMPRESSED、REDUNDANT 和 FIXED。 innodb_file_per_table 应设置为 ON，innodb_file_per_table 在 MySQL 5.6 及更高版本中默认启用。 页面大小应设置为 16K，这也是默认值。 有关更多信息，请参阅先决条件。 在数据一致性方面，你可以将适配器视为一个简单的 MySQL 服务器，能够直接通过 InnoDB 数据文件查询，通过从 MySQL 卸载来转储数据。如果页面没有从 InnoDB 缓冲池刷新到磁盘，则结果可能不一致（.ibd 文件中的 LSN 可能小于内存中的页面）。InnoDB 就性能而言利用预写日志，因此没有可用于刷新所有脏页的命令。只有内部机制管理何时何地将页面持久化到磁盘，例如页面清理线程、自适应刷新等。 目前 InnoDB 适配器不知道 .ibd 数据文件的行数和基数，因此它依赖简单的规则来执行优化。如果将来底层存储引擎可以提供此类指标和元数据，则可以通过利用基于成本的优化将其集成到 Calcite 中。 写在最后 笔者因为工作原因接触到 Calcite，前期学习过程中，深感 Calcite 学习资料之匮乏，因此创建了 Calcite 从入门到精通知识星球，希望能够将学习过程中的资料和经验沉淀下来，为更多想要学习 Calcite 的朋友提供一些帮助。"},{"title":"Kafka 适配器","path":"/wiki/calcite/kafka-adapter.html","content":"原文链接：https://calcite.apache.org/docs/kafka_adapter.html 注意： KafkaAdapter 是一个实验性功能，预计公共 API 和使用方式会发生变化。 有关下载和构建 Calcite 的说明，请从教程开始。 Kafka 适配器将 Apache Kafka 主题公开为 STREAM 表，因此可以使用 Calcite Stream SQL 进行查询。请注意，适配器不会尝试扫描所有主题，而是需要用户手动配置表，一个 Kafka 流表映射到一个 Kafka 主题。 下面给出一个模型文件的基本示例： version: 1.0, defaultSchema: KAFKA, schemas: [ name: KAFKA, tables: [ name: TABLE_NAME, type: custom, factory: org.apache.calcite.adapter.kafka.KafkaTableFactory, row.converter: com.example.CustKafkaRowConverter, operand: bootstrap.servers: host1:port,host2:port, topic.name: kafka.topic.name, consumer.params: key.deserializer: org.apache.kafka.common.serialization.ByteArrayDeserializer, value.deserializer: org.apache.kafka.common.serialization.ByteArrayDeserializer ] ] 请注意： 由于 Kafka 消息是无模式的，因此需要 KafkaRowConverter 来显式指定行模式（使用参数 row.converter），以及如何将 Kafka 消息解码为 Calcite 行。如果未提供，则使用 KafkaRowConverterImpl； 可以在参数 consumer.params 中添加更多消费者设置； 假设此文件存储为 kafka.model.json，你可以通过 sqlline 连接到 Kafka，如下所示： $ ./sqllinesqlline !connect jdbc:calcite:model=kafka.model.json admin admin 现在 sqlline 将接受访问你的 Kafka 主题的 SQL 查询。 使用上面模型中配置的 Kafka 表。我们可以运行一个简单查询来获取消息： sqlline SELECT STREAM * FROM KAFKA.TABLE_NAME;+---------------+---------------------+---------------------+---------------+-----------------+| MSG_PARTITION | MSG_TIMESTAMP | MSG_OFFSET | MSG_KEY_BYTES | MSG_VALUE_BYTES |+---------------+---------------------+---------------------+---------------+-----------------+| 0 | -1 | 0 | mykey0 | myvalue0 || 0 | -1 | 1 | mykey1 | myvalue1 |+---------------+---------------------+---------------------+---------------+-----------------+ Kafka 表是一个流表，它连续运行。 如果你希望查询快速结束，请添加 LIMIT，如下所示： sqlline SELECT STREAM * FROM KAFKA.TABLE_NAME LIMIT 5; 写在最后 笔者因为工作原因接触到 Calcite，前期学习过程中，深感 Calcite 学习资料之匮乏，因此创建了 Calcite 从入门到精通知识星球，希望能够将学习过程中的资料和经验沉淀下来，为更多想要学习 Calcite 的朋友提供一些帮助。"},{"title":"Lattice 格","path":"/wiki/calcite/lattice.html","content":"原文链接：https://calcite.apache.org/docs/lattice.html Lattice 格是用于创建和填充物化视图以及识别物化视图可用于解决特定查询的框架。 概念 Lattice 格代表星形（或雪花）模式，而不是一般模式。特别是，所有关系都必须是多对一的，从星形中心的事实表开始。 该名称源自数学：格是一个部分有序的集合，其中任何两个元素都有唯一的最大下界和最小上界。 HRU96[1] 观察到数据立方体的可能物化集合形成了一个格，并提出了一种算法来选择一组好的物化。Calcite的推荐算法就是由此衍生出来的。 Lattice 格定义使用 SQL 语句来表示星形。 SQL 是一种有用的速记方式，可以表示连接在一起的多个表，并为列名分配别名（它比发明一种新语言来表示关系、连接条件和基数更方便）。 与常规 SQL 不同，顺序很重要。如果在 FROM 子句中将 A 放在 B 之前，并在 A 和 B 之间进行联接，则表示从 A 到 B 存在多对一外键关系（例如，在示例格中，Sales 事实表出现在时间维度表之前和产品维度表之前。产品维度表出现在产品类外部维度表之前，位于雪花分支的下方）。 Lattice 格意味着约束。在 A 到 B 的关系中，A 上有一个外键（即 A 的外键的每个值在 B 的键中都有一个对应的值），并且 B 上有一个唯一的键（即没有一个键值出现超过一次）。这些约束非常重要，因为它允许优化器删除与未使用列的表的连接，并知道查询结果不会改变。 Calcite 不检查这些限制。如果违反，Calcite 将返回错误的结果。 Lattice 格是一个大的虚拟连接视图。它没有具体化（由于非规范化，它会比星型模式大几倍），并且你可能不想查询它（列太多）。那么它有什么用呢？正如我们上面所说： 格子声明了一些非常有用的主键和外键约束； 它帮助查询优化器将用户查询映射到过滤器连接聚合物化视图（DW 查询最有用的物化视图类型） ； 为 Calcite 提供了一个框架，用于收集有关数据量和用户查询的统计信息； 允许 Calcite 自动设计和填充物化视图。 大多数星型模式模型都会强制您选择列是维度还是度量。在格子中，每一列都是一个维度列（也就是说，它可以成为 GROUP BY 子句中的列之一，以查询特定维度的星型模式）。任何列也可以用于度量，你可以通过提供列和聚合函数来定义度量。 如果 unit_sales 更常被用作度量而不是维度，那也没关系。Calcite 的算法应该注意到它很少聚合，并且不倾向于创建在其上聚合的图块（我所说的应该是指可以而且有一天会，该算法目前在设计图块时不考虑查询历史记录）。 但有人可能想知道少于 5 件商品的订单比 100 件以上的订单利润更高还是更低。突然间，unit_sales 变成了一个维度。如果将列声明为维度列的成本几乎为零，我想让我们将它们全部设为维度列。 该模型允许使用不同的表别名多次使用特定的表。您可以使用它来建模 OrderDate 和 ShipDate，并对时间维度表有两种用途。 大多数 SQL 系统要求视图中的列名是唯一的。这在网格中很难实现，因为您经常在连接中包含主键列和外键列。所以 Calcite 允许您以两种方式引用列。如果该列是唯一的，您可以使用其名称 [unit_sales]。无论它在格中是否唯一，它在其表中也将是唯一的，因此你可以通过其表别名来限定使用它。例如： [销售额，单位销售额]； [发货日期，时间 ID]； [订单日期，时间 ID]。 Tile 块是 Lattice 格中的物化表格，具有特定的维度。Lattice 格 JSON 元素中的 tiles 属性定义了一组要具体化的初始图块。 示范 创建一个包含 Lattice 格的模型： version: 1.0, defaultSchema: foodmart, schemas: [ type: jdbc, name: foodmart, jdbcUser: FOODMART, jdbcPassword: FOODMART, jdbcUrl: jdbc:hsqldb:res:foodmart, jdbcSchema: foodmart , name: adhoc, lattices: [ name: star, sql: [ select 1 from \\foodmart\\.\\sales_fact_1997\\ as \\s\\, join \\foodmart\\.\\product\\ as \\p\\ using (\\product_id\\), join \\foodmart\\.\\time_by_day\\ as \\t\\ using (\\time_id\\), join \\foodmart\\.\\product_class\\ as \\pc\\ on \\p\\.\\product_class_id\\ = \\pc\\.\\product_class_id\\ ], auto: true, algorithm: true, rowCountEstimate: 86837, defaultMeasures: [ agg: count ] ] ] 这是 hsqldb-foodmart-lattice-model.json 的精简版本，不包含 tiles 属性，因为我们将自动生成 Tile 块。下面让我们登录 sqlline 并连接到此模式： $ sqlline version 1.3.0sqlline !connect jdbc:calcite:model=core/src/test/resources/hsqldb-foodmart-lattice-model.json sa 你会注意到连接需要几秒钟。Calcite 正在运行优化算法，并创建和填充物化视图。让我们运行一个查询并检查其计划： sqlline select the_year,the_month, count(*) as c. . . . from sales_fact_1997. . . . join time_by_day using (time_id). . . . group by the_year,the_month;+----------+-----------+------+| the_year | the_month | C |+----------+-----------+------+| 1997 | September | 6663 || 1997 | April | 6590 || 1997 | January | 7034 || 1997 | June | 6912 || 1997 | August | 7038 || 1997 | February | 6844 || 1997 | March | 7710 || 1997 | October | 6479 || 1997 | May | 6866 || 1997 | December | 8717 || 1997 | July | 7752 || 1997 | November | 8232 |+----------+-----------+------+12 rows selected (0.147 seconds)sqlline explain plan for. . . . select the_year,the_month, count(*) as c. . . . from sales_fact_1997. . . . join time_by_day using (time_id). . . . group by the_year,the_month;+--------------------------------------------------------------------------------+| PLAN |+--------------------------------------------------------------------------------+| EnumerableCalc(expr#0..2=[inputs], the_year=[$t1], the_month=[$t0], C=[$t2]) || EnumerableAggregate(group=[3, 4], C=[$SUM0($7)]) || EnumerableTableScan(table=[[adhoc, m16, 17, 27, 31, 32, 36, 37]]) |+--------------------------------------------------------------------------------+ 查询给出了正确的答案，但计划却有些令人惊讶。它不读取 sales_fact_1997 或 time_by_day 表，而是从名为 m16, 17, 27, 31, 32, 36, 37 的表中读取。这是在连接开始时创建的 Tile 块之一。 这是一个真实的表，你甚至可以直接查询它。它只有 120 行，因此是返回查询的更有效方法： sqlline !describe adhoc.m16, 17, 27, 31, 32, 36, 37+-------------+-------------------------------+--------------------+-----------+-----------------+| TABLE_SCHEM | TABLE_NAME | COLUMN_NAME | DATA_TYPE | TYPE_NAME |+-------------+-------------------------------+--------------------+-----------+-----------------+| adhoc | m16, 17, 27, 31, 32, 36, 37 | recyclable_package | 16 | BOOLEAN || adhoc | m16, 17, 27, 31, 32, 36, 37 | low_fat | 16 | BOOLEAN || adhoc | m16, 17, 27, 31, 32, 36, 37 | product_family | 12 | VARCHAR(30) || adhoc | m16, 17, 27, 31, 32, 36, 37 | the_month | 12 | VARCHAR(30) || adhoc | m16, 17, 27, 31, 32, 36, 37 | the_year | 5 | SMALLINT || adhoc | m16, 17, 27, 31, 32, 36, 37 | quarter | 12 | VARCHAR(30) || adhoc | m16, 17, 27, 31, 32, 36, 37 | fiscal_period | 12 | VARCHAR(30) || adhoc | m16, 17, 27, 31, 32, 36, 37 | m0 | -5 | BIGINT NOT NULL |+-------------+-------------------------------+--------------------+-----------+-----------------+sqlline select count(*) as c. . . . from adhoc.m16, 17, 27, 31, 32, 36, 37;+-----+| C |+-----+| 120 |+-----+1 row selected (0.12 seconds) 让我们列出表格，您将看到更多的 Tile 块。还有模式表 foodmart、系统表 TABLES 和 COLUMNS，以及 Lattice 格本身，它显示的名称为 star。 sqlline !tables+-------------+-------------------------------+--------------+| TABLE_SCHEM | TABLE_NAME | TABLE_TYPE |+-------------+-------------------------------+--------------+| adhoc | m16, 17, 18, 32, 37 | TABLE || adhoc | m16, 17, 19, 27, 32, 36, 37 | TABLE || adhoc | m4, 7, 16, 27, 32, 37 | TABLE || adhoc | m4, 7, 17, 27, 32, 37 | TABLE || adhoc | m7, 16, 17, 19, 32, 37 | TABLE || adhoc | m7, 16, 17, 27, 30, 32, 37 | TABLE || adhoc | star | STAR || foodmart | customer | TABLE || foodmart | product | TABLE || foodmart | product_class | TABLE || foodmart | promotion | TABLE || foodmart | region | TABLE || foodmart | sales_fact_1997 | TABLE || foodmart | store | TABLE || foodmart | time_by_day | TABLE || metadata | COLUMNS | SYSTEM_TABLE || metadata | TABLES | SYSTEM_TABLE |+-------------+-------------------------------+--------------+ (adsbygoogle = window.adsbygoogle || []).push({}); 赞助商 统计信息 选择要具体化 Lattice 格的哪些 Tile 块的算法取决于大量统计信息。它需要知道它正在考虑实现的每个列组合 (a, b, c) 的 select count(distinct a, b, c) from star 。因此，该算法在具有许多行和列的模式上需要很长时间。 我们正在开发 数据分析器 来解决这个问题。 Lattice 格建议器 如果你定义了一个 Lattice 格，Calcite 将在该 Lattice 格内进行自调整。但是如果你还没有定义格子怎么办？ 输入格子建议器，它根据传入的查询构建 Lattice 格，并创建一个具有以下架构的模型 autoLattice: true： version: 1.0, defaultSchema: foodmart, schemas: [ type: jdbc, name: foodmart, jdbcUser: FOODMART, jdbcPassword: FOODMART, jdbcUrl: jdbc:hsqldb:res:foodmart, jdbcSchema: foodmart , name: adhoc, autoLattice: true ] 这是 hsqldb-foodmart-lattice-model.json 的精简版本，当你执行查询时，Calcite 将开始根据这些查询构建 Lattice 格。每个格都基于特定的事实表。当它在该事实表上看到更多查询时，它将演化 Lattice 格，将更多维度表连接到星形，并添加度量。 然后，每个网格将根据数据和查询进行自身优化。目标是创建相当小的汇总表（Tile 块），但基于更常用的属性和度量。 该功能仍处于实验阶段，但有可能使数据库比以前更加自动优化。 未来规划 以下是一些尚未实施的想法： 构建 Lattice 格的算法，考虑过去查询的日志； 物化视图管理器查看传入的查询并为它们构建 Tile 块。 物化视图管理器会删除未主动使用的 Tile 块； Lattice 格建议器根据传入查询添加 Lattice 格，将 Tile 块从现有 Lattice 格转移到新 Lattice 格，并删除不再使用的 Lattice 格； Tile 盖覆表水平拆分，重写算法可以通过将多个 Tile 块拼接在一起，并使用原始数据来填补漏洞来返回查询； 当底层数据发生更改时，用于使 Tile 块或 Tile 块水平分片失效的 API。 写在最后 笔者因为工作原因接触到 Calcite，前期学习过程中，深感 Calcite 学习资料之匮乏，因此创建了 Calcite 从入门到精通知识星球，希望能够将学习过程中的资料和经验沉淀下来，为更多想要学习 Calcite 的朋友提供一些帮助。 参考文档 [HRU96] V. Harinarayan, A. Rajaraman and J. Ullman. Implementing data cubes efficiently. In Proc. ACM SIGMOD Conf., Montreal, 1996. ↩︎"},{"title":"物化视图","path":"/wiki/calcite/materialized-views.html","content":"原文链接：https://calcite.apache.org/docs/materialized_views.html 有几种不同的方法可以利用 Calcite 中的物化视图。 由 Calcite 维护的物化视图 有关详细信息，请参考 Lattice 格。 将物化视图暴露给 Calcite 一些 Calcite 适配器以及依赖 Calcite 的项目都有自己的物化视图概念。 例如，Apache Cassandra 允许用户基于自动维护的现有表定义物化视图。 Cassandra 适配器自动将这些物化视图公开给 Calcite。 另一个例子是 Apache Hive。当在Hive中创建物化视图时，用户可以指定该视图是否可以用于查询优化。如果用户选择这样做，物化视图将注册到 Calcite。 通过在 Calcite 中注册物化视图，优化器有机会自动重写查询以使用这些视图。 基于视图的查询重写 基于视图的查询重写，可以获取预先存在的视图来返回给输入的查询，并重写查询以利用该视图。目前，Calcite 有两种基于视图的查询重写实现。 通过规则转换进行替换 第一种方法基于视图替换。 SubstitutionVisitor 及其扩展的 MaterializedViewSubstitutionVisitor，会使用物化视图的等效表达式替换部分关系代数树。对物化视图的扫描和物化视图的定义都注册在优化器中，然后，优化器会尝试统一计划中被触发表达式的转换规则。表达式不需要等效于被替换的部分，如果需要，访问者可以在表达式之上添加剩余谓词。 以下示例取自 SubstitutionVisitor 的文档： 查询：SELECT a, c FROM t WHERE x = 5 AND b = 4； 目标（物化视图定义）：SELECT a, b, c FROM t WHERE x = 5； 结果：SELECT a, c FROM mv WHERE b = 4。 请注意， result 使用物化视图表 mv 和简化条件 b = 4 。 虽然这种方法可以完成大量重写，但它有一些局限性。由于该规则依赖于转换规则来创建查询中的表达式与物化视图之间的等价性，因此它可能需要详尽地枚举给定表达式的所有可能的等价重写，以找到物化视图替换。然而，在存在复杂视图（例如具有任意数量的连接运算符的视图）的情况下，这是不可扩展的。 使用计划结构信息重写 反过来，又提出了一种替代规则，尝试通过提取有关要替换的表达式的一些结构信息来将查询与视图进行匹配。 MaterializedViewRule 建立在 GL01[1] 中提出的想法之上，并引入了一些额外的扩展。该规则可以重写包含任意连接、过滤和投影运算符的表达式。此外，该规则可以重写以聚合运算符为根的表达式，并在必要时向上滚动聚合。反过来，如果可以从视图部分返回查询，它还可以使用 Union 运算符生成重写。 为了产生大量重写，该规则依赖于作为在数据库表上定义的约束而公开的信息，例如：外键、主键、唯一键或非空。 重写覆盖范围 让我们用一些例子来说明 MaterializedViewRule 中实现的视图重写算法的覆盖范围。这些示例基于以下数据库模式。 CREATE TABLE depts( deptno INT NOT NULL, deptname VARCHAR(20), PRIMARY KEY (deptno));CREATE TABLE locations( locationid INT NOT NULL, state CHAR(2), PRIMARY KEY (locationid));CREATE TABLE emps( empid INT NOT NULL, deptno INT NOT NULL, locationid INT NOT NULL, empname VARCHAR(20) NOT NULL, salary DECIMAL (18, 2), PRIMARY KEY (empid), FOREIGN KEY (deptno) REFERENCES depts(deptno), FOREIGN KEY (locationid) REFERENCES locations(locationid)); 范围一：Join 重写 重写可以处理查询和视图定义中的不同连接顺序。此外，该规则尝试检测何时可以使用补偿谓词来使用视图来生成重写。 查询： SELECT empidFROM deptsJOIN ( SELECT empid, deptno FROM emps WHERE empid = 1) AS subqON depts.deptno = subq.deptno 物化视图定义： SELECT empidFROM empsJOIN depts USING (deptno) 重写： SELECT empidFROM mvWHERE empid = 1 范围二：聚合重写 查询： SELECT deptnoFROM empsWHERE deptno 10GROUP BY deptno 物化视图定义： SELECT empid, deptnoFROM empsWHERE deptno 5GROUP BY empid, deptno 重写： SELECT deptnoFROM mvWHERE deptno 10GROUP BY deptno 范围三：聚合重写（使用聚合汇总） 查询： SELECT deptno, COUNT(*) AS c, SUM(salary) AS sFROM empsGROUP BY deptno 物化视图定义： SELECT empid, deptno, COUNT(*) AS c, SUM(salary) AS sFROM empsGROUP BY empid, deptno 重写： SELECT deptno, SUM(c), SUM(s)FROM mvGROUP BY deptno 范围四：查询部分重写 通过声明的约束，该规则可以检测仅附加列而不改变元组重数的连接，并产生正确的重写。 查询： SELECT deptno, COUNT(*)FROM empsGROUP BY deptno 物化视图定义： SELECT empid, depts.deptno, COUNT(*) AS c, SUM(salary) AS sFROM empsJOIN depts USING (deptno)GROUP BY empid, depts.deptno 重写： SELECT deptno, SUM(c)FROM mvGROUP BY deptno 范围五：视图部分重写 查询： SELECT deptname, state, SUM(salary) AS sFROM empsJOIN depts ON emps.deptno = depts.deptnoJOIN locations ON emps.locationid = locations.locationidGROUP BY deptname, state 物化视图定义： SELECT empid, deptno, state, SUM(salary) AS sFROM empsJOIN locations ON emps.locationid = locations.locationidGROUP BY empid, deptno, state 重写： SELECT deptname, state, SUM(s)FROM mvJOIN depts ON mv.deptno = depts.deptnoGROUP BY deptname, state 范围六：联合 Union 重写 查询： SELECT empid, deptnameFROM empsJOIN depts ON emps.deptno = depts.deptnoWHERE salary 10000 物化视图定义： SELECT empid, deptnameFROM empsJOIN depts ON emps.deptno = depts.deptnoWHERE salary 12000 重写： SELECT empid, deptnameFROM mvUNION ALLSELECT empid, deptnameFROM empsJOIN depts ON emps.deptno = depts.deptnoWHERE salary 10000 AND salary = 12000 范围七：使用聚合进行联合 Union 重写 查询： SELECT empid, deptname, SUM(salary) AS sFROM empsJOIN depts ON emps.deptno = depts.deptnoWHERE salary 10000GROUP BY empid, deptname 物化视图定义： SELECT empid, deptname, SUM(salary) AS sFROM empsJOIN depts ON emps.deptno = depts.deptnoWHERE salary 12000GROUP BY empid, deptname 重写： SELECT empid, deptname, SUM(s)FROM ( SELECT empid, deptname, s FROM mv UNION ALL SELECT empid, deptname, SUM(salary) AS s FROM emps JOIN depts ON emps.deptno = depts.deptno WHERE salary 10000 AND salary = 12000 GROUP BY empid, deptname) AS subqGROUP BY empid, deptname 使用限制 使用计划结构信息重写仍然存在一些使用限制。特别是，重写规则尝试将所有视图与每个查询进行匹配。我们计划实施更精细的过滤技术，例如 GL01[1:1] 中描述的技术。 写在最后 笔者因为工作原因接触到 Calcite，前期学习过程中，深感 Calcite 学习资料之匮乏，因此创建了 Calcite 从入门到精通知识星球，希望能够将学习过程中的资料和经验沉淀下来，为更多想要学习 Calcite 的朋友提供一些帮助。 (adsbygoogle = window.adsbygoogle || []).push({}); 赞助商 参考文档 [GL01] Jonathan Goldstein and Per-åke Larson. Optimizing queries using materialized views: A practical, scalable solution. In Proc. ACM SIGMOD Conf., 2001. ↩︎ ↩︎"},{"title":"JSON/YAML 模型","path":"/wiki/calcite/model.html","content":"原文链接：https://calcite.apache.org/docs/model.html Calcite 模型可以表示为 JSON/YAML 文件。本文描述了这些文件的结构。还可以使用 Schema SPI 编程的方式构建模型。 根节点 JSON： version: 1.0, defaultSchema: mongo, schemas: [ Schema... ], types: [ Type... ] YAML： version: 1.0defaultSchema: mongoschemas:- [Schema...]types:- [Type...] version：必填字符串，必须设置 version 属性值 1.0； defaultSchema：可选字符串，如果指定，它将作为此模型中定义的模式名称（区分大小写），并将成为使用此模型 Calcite 连接的默认模式； schemas：模式元素的可选列表； types：所有模式共享的类型元素可选列表。 模式 配置在 root.schemas 节点中。 JSON： name: foodmart, path: [lib], cache: true, materializations: [ Materialization... ] YAML： name: foodmartpath: libcache: truematerializations:- [ Materialization... ] name：必填字符串，模式的名称； type：可选字符串，默认为 map，表示子类型。可选值为： map 用于配置 Map 模式； custom 用于配置自定义模式； jdbc 用于配置 JDBC 模式。 path：可选列表，用于解析此模式中使用函数的 SQL 路径。如果指定，它必须是一个列表，并且列表的每个元素必须是字符串或字符串列表。例如， JSON： path: [ [usr, lib], lib ] YAML： path:- [usr, lib]- lib 声明一个包含两个元素的路径：模式 /usr/lib 和模式 /lib。大多数模式都位于顶层，对于这些模式，你可以使用字符串。 materializations （可选的物化视图列表）定义此模式中作为查询物化视图的表。 cache （可选布尔值，默认 true）告诉 Calcite 是否缓存此模式生成的元数据（包括表、函数和子模式）。 如果设置为 false ，Calcite 将在每次需要元数据时访问模式，例如，每次需要表列表以验证某个模式中的查询时； 如果设置为 true ，Calcite 将在第一次读取元数据时缓存元数据。这可以带来更好的性能，特别是在名称匹配不区分大小写的情况下。 然而，这也导致了缓存陈旧的问题。特定模式实现可以重写 Schema.contentsHaveChangedSince 方法，来告诉 Calcite 何时应考虑缓存过期。 在模式中显式创建的表、函数、类型和子模式不受此缓存机制的影响。它们总是立即出现在模式中，并且永远不会被刷新。 (adsbygoogle = window.adsbygoogle || []).push({}); 赞助商 Map 模式 与基类 Schema 一样，Map 模式同样出现在 root.schemas 中。 JSON： name: foodmart, type: map, tables: [ Table... ], functions: [ Function... ], types: [ Type... ] YAML： name: foodmarttype: maptables:- [ Table... ]functions:- [ Function... ]types:- [ Type... ] name 、 type 、 path 、 cache 和 materializations 都继承自 Schema。 tables （表元素的可选列表）定义模式中的表； functions（函数元素的可选列表）定义模式中的函数； types 定义模式中的类型。 自定义模式 与基类 Schema 一样，自定义模式出现在 root.schemas 中。 JSON： name: mongo, type: custom, factory: org.apache.calcite.adapter.mongodb.MongoSchemaFactory, operand: host: localhost, database: test YAML： name: mongotype: customfactory: org.apache.calcite.adapter.mongodb.MongoSchemaFactoryoperand: host: localhost database: test name 、 type 、 path 、 cache 和 materializations 都继承自 Schema。 factory（必填字符串）是该模式的工厂类的名称。必须实现接口 org.apache.calcite.schema.SchemaFactory 并具有公共默认构造函数； operand（可选映射）包含要传递给工厂的属性。 JDBC 模式 与基类 Schema 一样，JDBC 模式出现在 root.schemas 中。 JSON： name: foodmart, type: jdbc, jdbcDriver: TODO, jdbcUrl: TODO, jdbcUser: TODO, jdbcPassword: TODO, jdbcCatalog: TODO, jdbcSchema: TODO YAML： name: foodmarttype: jdbcjdbcDriver: TODOjdbcUrl: TODOjdbcUser: TODOjdbcPassword: TODOjdbcCatalog: TODOjdbcSchema: TODO name 、 type 、 path 、 cache 和 materializations 继承自 Schema。 jdbcDriver（可选字符串）是 JDBC 驱动程序类的名称。如果未指定，则使用 JDBC DriverManager 选择的任何类； jdbcUrl（可选字符串）是 JDBC 连接字符串，例如：jdbc:mysql://localhost/foodmart； jdbcUser（可选字符串）是 JDBC 用户名； jdbcPassword（可选字符串）是 JDBC 密码； jdbcCatalog（可选字符串）是 JDBC 数据源中初始目录的名称； jdbcSchema（可选字符串）是 JDBC 数据源中初始模式的名称。 物化视图 出现在 root.schemas.materializations 中。 JSON： view: V, table: T, sql: select deptno, count(*) as c, sum(sal) as s from emp group by deptno YAML： view: Vtable: Tsql: select deptno, count(*) as c, sum(sal) as s from emp group by deptno view（可选字符串）是视图的名称。null 表示该表已经存在，并且填充了正确的数据； table（必填字符串）是在查询中物化数据表的名称。如果 view 不为空，则该表可能不存在，如果不存在，Calcite 将创建并填充内存中的表； sql（可选字符串，或者可选的多行连接字符串列表）是具体化的 SQL 定义。 表 出现在 root.schemas.tables 中。 JSON： name: sales_fact, columns: [ Column... ] YAML： name: sales_factcolumns: [ Column... ] name（必填字符串）是该表的名称。在模式中必须是唯一的； type（可选字符串，默认为 custom）表示子类型。值为： custom 用于自定义表； view 用于视图。 columns（Column 元素列表，对于某些类型的表是必需的，对于其他类型的表是可选的，例如 View）。 视图 与基类 Table 一样，出现在 root.schemas.tables 中。 JSON： name: female_emps, type: view, sql: select * from emps where gender = F, modifiable: true YAML： name: female_empstype: viewsql: select * from emps where gender = Fmodifiable: true name 、 type 、 columns 都继承自 Table。 sql（必填字符串，或者必填的多行字符串的字符串列表）是视图的 SQL 定义； path（可选列表）代表解析查询的 SQL 路径。如果未指定，则默认为当前模式； modifiable（可选布尔值）代表视图是否可修改。如果为 null 或未指定，Calcite 会推断视图是否可修改。 如果视图仅包含 SELECT、FROM、WHERE（无 JOIN、聚合或子查询）并且每列满足以下条件，则视图是可修改的： 在 SELECT 子句中指定一次； 出现在带有 column = literal 谓词的 WHERE 子句中； 可以为空。 第二个子句允许 Calcite 自动为隐藏列提供正确的值。它在多租户环境中很有用，其中 tenantId 列是隐藏的、强制的（NOT NULL），并且对于特定视图具有常量值。 有关可修改视图的错误： 如果视图标记为 modifiable: true 并且不可修改，Calcite 在读取模式时会抛出错误； 如果你向不可修改的视图提交 INSERT、UPDATE 或 UPSERT 命令，Calcite 在验证语句时会抛出错误； 如果 DML 语句创建的行不会出现在视图中（例如，上面 female_emps 中的行，带有 gender = 'M' ），Calcite 在执行该语句时会抛出错误。 自定义表 与基类 Table 类似，出现在 root.schemas.tables 中。 JSON： name: female_emps, type: custom, factory: TODO, operand: todo: TODO YAML： name: female_empstype: customfactory: TODOoperand: todo: TODO name 、 type 、 columns 继承自 Table。 factory （必填字符串）是该表的工厂类的名称。必须实现接口 org.apache.calcite.schema.TableFactory 并具有公共默认构造函数； operand（可选映射）包含要传递给工厂的属性。 流式查询 有关表是否允许流式传输的信息。出现在 root.schemas.tables.stream。 JSON： stream: true, history: false YAML： stream: truehistory: false stream（可选；默认 true）表示表是否允许流式传输； history（可选；默认 false）表示流的历史记录是否可用。 列 出现在 root.schemas.tables.columns 中。 JSON： name: empno YAML： name: empno name（必填字符串）代表该列的名称。 函数 出现在 root.schemas.functions 中。 JSON： name: MY_PLUS, className: com.example.functions.MyPlusFunction, methodName: apply, path: [] YAML： name: MY_PLUSclassName: com.example.functions.MyPlusFunctionmethodName: applypath: name（必填字符串）表示该函数的名称； className（必填字符串）表示实现此函数类的名称； methodName（可选字符串）表示实现此功能方法的名称。 如果指定了 methodName ，则该方法必须存在（区分大小写），并且 Calcite 将创建一个标量函数。该方法可以是静态或非静态的，但如果是非静态的，则该类必须具有不带参数的公共构造函数。 如果 methodName 是 *，Calcite 会为类中的每个方法创建一个函数。 如果未指定 methodName ，Calcite 会查找名为 eval 的方法，如果找到，则创建表宏或标量函数。它还查找方法 init、add、merge、result，如果找到，则创建一个聚合函数。 path （可选字符串列表）是解析此函数的路径。 类型 出现在 root.types 和 root.schemas.types 中。 JSON： name: mytype1, type: BIGINT, attributes: [ name: f1, type: BIGINT ] YAML： name: mytype1type: BIGINTattributes:- name: f1 type: BIGINT name（必填字符串）表示该类型的名称； type（可选）表示 SQL 类型； attributes（可选）表示该类型的属性列表。如果 attributes 和 type 两者存在于同一级别，type 则优先。 Lattice 格 出现在 root.schemas.lattices 中。 JSON： name: star, sql: [ select 1 from foodmart.sales_fact_1997 as s, join foodmart.product as p using (product_id), join foodmart.time_by_day as t using (time_id), join foodmart.product_class as pc on p.product_class_id = pc.product_class_id ], auto: false, algorithm: true, algorithmMaxMillis: 10000, rowCountEstimate: 86837, defaultMeasures: [ agg: count ], tiles: [ dimensions: [ the_year, [t, quarter] ], measures: [ agg: sum, args: unit_sales , agg: sum, args: store_sales , agg: count ] ] YAML： name: starsql: select 1 from foodmart.sales_fact_1997 as s, join foodmart.product as p using (product_id), join foodmart.time_by_day as t using (time_id), join foodmart.product_class as pc on p.product_class_id = pc.product_class_idauto: falsealgorithm: truealgorithmMaxMillis: 10000rowCountEstimate: 86837defaultMeasures:- agg: counttiles:- dimensions: [ the_year, [t, quarter] ] measures: - agg: sum args: unit_sales - agg: sum args: store_sales - agg: count name（必填字符串）表示该格的名称； sql（必填字符串，或者必填的多行字符串的字符串列表）表示定义该格的事实表、维度表和连接路径的 SQL 语句； auto（可选布尔值，默认 true）表示执行查询时，是否根据需要物化块； algorithm（可选布尔值，默认 false）表示是否使用优化算法，来建议和填充初始块集； algorithmMaxMillis（可选长整型，默认为 -1，表示无限制）表示运行算法的最大毫秒数。在此之后，获取算法迄今为止得出的最佳结果； rowCountEstimate（可选 double，默认 1000.0）表示网格中的估计行数； tiles（Tile 元素的可选列表）表示要预先创建的物化聚合的列表； defaultMeasures（可选的度量元素列表）表示图块默认应具有的度量列表。tiles 中定义的任何图块仍然可以定义自己的度量，包括不在此列表中的度量。如果未指定，默认的度量列表只是 count(*)： JSON： [ name: count ] YAML： name: count statisticProvider （实现 org.apache.calcite.materialize.LatticeStatisticProvider 的类的可选名称）提供每列中不同值数量的估计。 你可以使用类名，或类加静态字段。例子： statisticProvider: org.apache.calcite.materialize.Lattices#CACHING_SQL_STATISTIC_PROVIDER 如果未设置，Calcite 将生成并执行 SQL 查询以查找真实值，并缓存结果。 另请参见：Lattice 格。 Tile 块 出现在 root.schemas.lattices.tiles 中。 dimensions: [ the_year, [t, quarter] ], measures: [ agg: sum, args: unit_sales , agg: sum, args: store_sales , agg: count ] YAML： dimensions: [ the_year, [t, quarter] ]measures:- agg: sum args: unit_sales- agg: sum args: store_sales- agg: count dimensions （字符串列表或字符串列表，必填，但可以为空）定义此 Tile 块的维度。每个维度都是 Lattice 格中的一列，就像一个 GROUP BY 子句。每个元素可以是字符串（Lattice 格内列的唯一标签）或字符串列表（由表别名和列名组成的对）； measures（Measure 元素的可选列表）表示应用于参数的聚合函数列表。如果未指定，则使用 Lattice 格的默认度量列表。 度量 出现在 root.schemas.lattices.defaultMeasures 和 root.schemas.lattices.tiles.measures 中。 JSON： agg: sum, args: [ unit_sales ] YAML： agg: sumargs: unit_sales agg 是聚合函数的名称（通常为 count、sum、min、max）； args （可选）是列标签（字符串），或零个或多个列标签的列表。 有效值为： 未指定：无参数； null：没有参数； 空列表：无参数； String：单个参数，格子列的名称； 列表：多个参数，每个参数一个列标签。 与点阵维度不同，度量不能以限定格式 @code [“table”, “column”] 指定。定义晶格时，请确保要用作度量的每一列在晶格内都有唯一的标签（如有必要，请使用 “@code AS label”），并在想要传递该列时使用该标签作为衡量参数。 写在最后 笔者因为工作原因接触到 Calcite，前期学习过程中，深感 Calcite 学习资料之匮乏，因此创建了 Calcite 从入门到精通知识星球，希望能够将学习过程中的资料和经验沉淀下来，为更多想要学习 Calcite 的朋友提供一些帮助。"},{"title":"操作系统适配器","path":"/wiki/calcite/os-adapter.html","content":"原文链接：https://calcite.apache.org/docs/os_adapter.html 概述 操作系统适配器允许你使用 SQL 查询访问操作系统和环境中的数据。 它旨在解决传统上使用 UNIX 命令管道解决的问题，但具有 SQL 的强大功能和类型安全性。 该适配器还包括一个名为 sqlsh 的包装器，允许你从喜爱的 shell 执行命令。 安全警告 操作系统适配器会启动进程，并且可能是一个安全漏洞。它包含在 Calcite 的 “plus” 模块中，默认情况下不启用。在安全敏感的情况下启用它之前，你必须仔细考虑。 兼容性 我们尝试在每个操作系统上支持所有表，并确保表具有相同的列。但我们严重依赖操作系统命令，这些命令差异很大。因此： 这些命令仅适用于 Linux 和 macOS（不适用于 Windows，即使使用 Cygwin）； vmstat 在 Linux 和 macOS 之间的列非常不同； files 和 ps 具有相同的列名，但语义不同； 其他命令基本相同。 一个简单的例子 每个 bash 黑客都知道要找到 3 个最大的文件，你需要输入： $ find . -type f -print0 | xargs -0 ls -l | sort -nr -k 5 | head -3-rw-r--r-- 1 jhyde jhyde 194877 Jul 16 16:10 ./validate/SqlValidatorImpl.java-rw-r--r-- 1 jhyde jhyde 73826 Jul 4 21:51 ./fun/SqlStdOperatorTable.java-rw-r--r-- 1 jhyde jhyde 39214 Jul 4 21:51 ./type/SqlTypeUtil.java 这实际上是一个关系操作管道，每个元组由空格分隔的字段行表示。如果我们能够将文件列表作为关系访问并在 SQL 查询中使用它会怎样？如果我们能够轻松地从 shell 执行该 SQL 查询会怎样？这就是 sqlsh 所做的： $ sqlsh select size, path from files where type = f order by size desc limit 3194877 validate/SqlValidatorImpl.java73826 fun/SqlStdOperatorTable.java39214 type/SqlTypeUtil.java sqlsh sqlsh 启动与 Calcite 的连接，其默认模式是操作系统适配器。 它使用 JAVA 词法模式，这意味着未加引号的表和列名称保持其编写时的大小写。这与 bash 等 shell 的行为一致。 必须小心处理 shell 元字符（如 *、、、( 和 )）。通常添加反斜杠就足够了。 表和命令 操作系统适配器包含以下表： cpu_info - CPU 信息（来自 oshi） cpu_time - CPU 时间（来自 oshi） du - 磁盘使用情况（基于 du 命令） files - 文件（基于 find 命令） git_commits - Git 提交（基于 git log） interface_addresses - 网络地址（来自 oshi） interface_details - 网络接口（来自 oshi） java_info - Java 信息（来自 oshi） memory_info - 内存（来自 oshi） mounts - 文件系统挂载（来自 oshi） os_version - 操作系统版本（来自 oshi） ps - 进程（基于 ps 命令） stdin - 标准输入 system_info - 系统信息（来自 oshi） vmstat - 虚拟内存（基于 vmstat 命令） 大多数表实现为表函数之上的视图。 添加新数据源很简单；欢迎贡献你的！ 示例：du 有多少类文件，总大小是多少？在 bash 中： $ du -ka . | grep \\.class$ | awk size+=$1 END print FNR, size4416 27960 在 sqlsh 中： $ sqlsh select count\\(\\*\\), sum\\(size_k\\) from du where path like %.class4416 27960 反斜杠是必需的，因为 (、*、) 和 ' 是 shell 元字符。 示例：files 有多少文件和目录？在 bash 中，你会使用 find： $ find . -printf %Y %p | grep /test/ | cut -d -f1 | sort | uniq -c 143 d 1336 f 在 sqlsh 中，使用 files 表： $ sqlsh select type, count\\(\\*\\) from files where path like %/test/% group by typed 143f 1336 示例：ps 哪些用户有正在运行的进程？在 sqlsh 中： $ sqlsh select distinct ps.\\`user\\` from psavahirootjhydesyslognobodydaemon ps. 限定符和反引号是必需的，因为 USER 是 SQL 保留字。 现在是一个 “前 N 个” 问题：哪些用户的进程最多？在 bash 中： $ ps aux | awk print $1 | sort | uniq -c | sort -nr | head -3 在 sqlsh 中： $ ./sqlsh select count\\(\\*\\), ps.\\`user\\` from ps group by ps.\\`user\\` order by 1 desc limit 3185 root69 jhyde2 avahi 示例：vmstat 我的内存如何？ $ ./sqlsh -o mysql select \\* from vmstat+--------+--------+----------+----------+----------+-----------+---------+---------+-------+-------+-----------+-----------+--------+--------+--------+--------+--------+| proc_r | proc_b | mem_swpd | mem_free | mem_buff | mem_cache | swap_si | swap_so | io_bi | io_bo | system_in | system_cs | cpu_us | cpu_sy | cpu_id | cpu_wa | cpu_st |+--------+--------+----------+----------+----------+-----------+---------+---------+-------+-------+-----------+-----------+--------+--------+--------+--------+--------+| 12 | 0 | 54220 | 5174424 | 402180 | 4402196 | 0 | 0 | 15 | 35 | 3 | 2 | 7 | 1 | 92 | 0 | 0 |+--------+--------+----------+----------+----------+-----------+---------+---------+-------+-------+-----------+-----------+--------+--------+--------+--------+--------+(1 row) 示例：explain 要找出表有哪些列，请使用 explain： $ sqlsh explain plan with type for select \\* from dusize_k BIGINT NOT NULL,path VARCHAR NOT NULL,size_b BIGINT NOT NULL 示例：git 每年有多少次提交和不同的作者？git_commits 表基于 git log 命令。 ./sqlsh select floor\\(commit_timestamp to year\\) as y, count\\(\\*\\), count\\(distinct author\\) from git_commits group by y order by 12012-01-01 00:00:00 180 62013-01-01 00:00:00 502 132014-01-01 00:00:00 679 362015-01-01 00:00:00 470 452016-01-01 00:00:00 465 672017-01-01 00:00:00 279 53 请注意，group by y 是可能的，因为 sqlsh 使用 Calcite 的宽松模式。 示例：stdin 打印标准输入，为每行添加一个编号。 $ (echo cats; echo and dogs) | cat -n - 1 cats 2 and dogs 在 sqlsh 中： $ (echo cats; echo and dogs) | ./sqlsh select \\* from stdin1 cats2 and dogs 示例：输出格式 -o 选项控制输出格式。 $ ./sqlsh -o mysql select min\\(size_k\\), max\\(size_k\\) from du+--------+--------+| EXPR$0 | EXPR$1 |+--------+--------+| 0 | 94312 |+--------+--------+(1 row) 格式选项： spaced - 字段之间有空格（默认） headers - 与 spaced 相同，但有标题 csv - 逗号分隔值 json - JSON，每行一个对象 mysql - 对齐的表，与 MySQL 使用的格式相同 示例：jps 提供所有当前 java 进程 pid 的显示。在 sqlsh 中： $ ./sqlsh select distinct jps.\\`pid\\`, jps.\\`info\\` from jps+--------+---------------------+| pid | info |+--------+---------------------+| 49457 | RemoteMavenServer || 48326 | KotlinCompileDaemon |+--------+---------------------+(1 row) 进一步工作 操作系统适配器是在 [CALCITE-1896] 中创建的，但尚未完成。 进一步工作的一些想法： 在未加引号的表名中允许 ‘-’ 和 ‘.’（以匹配典型的文件名） 允许序号字段引用，例如 ‘$3’。这对于没有命名字段的文件（例如 stdin）会有帮助，但即使字段有名称，你也可以使用它们。还有 ‘$0’ 表示整个输入行。 使用文件适配器，例如 select * from file.scott.emp 将使用文件适配器打开文件 scott/emp.csv 更多基于 git 的表，例如分支、标签、每次提交中更改的文件 wc 函数，例如 select path, lineCount from git_ls_files cross apply wc(path) 移动 sqlsh 命令，或者至少将其下面的 java 代码移动到 sqlline 中 写在最后 笔者因为工作原因接触到 Calcite，前期学习过程中，深感 Calcite 学习资料之匮乏，因此创建了 Calcite 从入门到精通知识星球，希望能够将学习过程中的资料和经验沉淀下来，为更多想要学习 Calcite 的朋友提供一些帮助。"},{"title":"Pig 适配器","path":"/wiki/calcite/pig-adapter.html","content":"原文链接：https://calcite.apache.org/docs/pig_adapter.html 概述 Pig 适配器允许你用 SQL 编写查询并使用 Apache Pig 执行它们。 一个简单的例子 让我们从一个简单的例子开始。首先，我们需要一个模型定义，如下所示。 version: 1.0, defaultSchema: SALES, schemas: [ name: PIG, type: custom, factory: org.apache.calcite.adapter.pig.PigSchemaFactory, tables: [ name: t, type: custom, factory: org.apache.calcite.adapter.pig.PigTableFactory, operand: file: data.txt, columns: [tc0, tc1] , name: s, type: custom, factory: org.apache.calcite.adapter.pig.PigTableFactory, operand: file: data2.txt, columns: [sc0, sc1] ] ] 现在，如果你编写 SQL 查询 select *from tjoin s on tc1 = sc0 Pig 适配器将生成 Pig Latin 脚本 t = LOAD data.txt USING PigStorage() AS (tc0:chararray, tc1:chararray);s = LOAD data2.txt USING PigStorage() AS (sc0:chararray, sc1:chararray);t = JOIN t BY tc1, s BY sc0; 然后使用 Pig 运行时执行它，通常是 Apache Hadoop 上的 MapReduce。 与 Piglet 的关系 Calcite 还有另一个名为 Piglet 的组件。它允许你用 Pig Latin 的子集编写查询，并使用任何适用的 Calcite 适配器执行它们。因此，Piglet 基本上与 Pig 适配器相反。 写在最后 笔者因为工作原因接触到 Calcite，前期学习过程中，深感 Calcite 学习资料之匮乏，因此创建了 Calcite 从入门到精通知识星球，希望能够将学习过程中的资料和经验沉淀下来，为更多想要学习 Calcite 的朋友提供一些帮助。"},{"title":"Redis 适配器","path":"/wiki/calcite/redis-adapter.html","content":"原文链接：https://calcite.apache.org/docs/redis_adapter.html Redis 是一个开源（BSD 许可）、内存中的数据结构存储，用作数据库、缓存和消息代理。它支持诸如字符串、哈希、列表、集合、具有范围查询的有序集合、位图、HyperLogLog、具有半径查询的地理空间索引和流等数据结构。Redis 具有内置复制、Lua 脚本、LRU 驱逐、事务和不同级别的磁盘持久性，并通过 Redis Sentinel 提供高可用性，以及通过 Redis Cluster 进行自动分区。 Calcite 的 Redis 适配器允许你使用 SQL 查询 Redis 中的数据，并将其与其他 Calcite 模式中的数据结合使用。 Redis 适配器允许查询存储在 Redis 中的实时数据。每个 Redis 键值对都显示为单行。可以通过使用表定义文件将行分解为单元格。支持 Redis string、hash、sets、zsets、list 值类型。 首先，我们需要一个模型定义。模型为 Calcite 创建 Redis 适配器实例提供了必要的参数。 下面给出一个模型文件的基本示例： version: 1.0, defaultSchema: foodmart, schemas: [ type: custom, name: foodmart, factory: org.apache.calcite.adapter.redis.RedisSchemaFactory, operand: host: localhost, port: 6379, database: 0, password: , tables: [ name: json_01, factory: org.apache.calcite.adapter.redis.RedisTableFactory, operand: dataFormat: json, fields: [ name: DEPTNO, type: varchar, mapping: DEPTNO , name: NAME, type: varchar, mapping: NAME ] , name: raw_01, factory: org.apache.calcite.adapter.redis.RedisTableFactory, operand: dataFormat: raw, fields: [ name: id, type: varchar, mapping: id , name: city, type: varchar, mapping: city , name: pop, type: int, mapping: pop ] , name: csv_01, factory: org.apache.calcite.adapter.redis.RedisTableFactory, operand: dataFormat: csv, keyDelimiter: :, fields: [ name: EMPNO, type: varchar, mapping: 0 , name: NAME, type: varchar, mapping: 1 ] ] ] 此文件存储为 redis/src/test/resources/redis-mix-model.json，因此你可以通过 sqlline 连接到 Redis，如下所示： $ ./sqllinesqlline !connect jdbc:calcite:model=redis/src/test/resources/redis-mix-model.json admin adminsqlline !tables+-----------+-------------+------------+--------------+---------+----------+------------+-----------+---------------------------+----------------+| TABLE_CAT | TABLE_SCHEM | TABLE_NAME | TABLE_TYPE | REMARKS | TYPE_CAT | TYPE_SCHEM | TYPE_NAME | SELF_REFERENCING_COL_NAME | REF_GENERATION |+-----------+-------------+------------+--------------+---------+----------+------------+-----------+---------------------------+----------------+| | foodmart | csv_01 | TABLE | | | | | | || | foodmart | json_01 | TABLE | | | | | | || | foodmart | raw_01 | TABLE | | | | | | |+-----------+-------------+------------+--------------+---------+----------+------------+-----------+---------------------------+----------------+sqlline Select a.DEPTNO, b.NAME from csv_01 a left join json_02 b on a.DEPTNO=b.DEPTNO;+--------+----------+| DEPTNO | NAME |+--------+----------+| 10 | Sales1 |+--------+----------+1 row selected (3.304 seconds) 此查询显示了 CSV 格式表 csv_01 和 JSON 格式表 json_02 中的连接查询结果。 以下是有关字段的一些详细信息： keyDelimiter 用于分割值，默认为冒号，分割后的值用于映射字段列。这仅适用于 CSV 格式。 format 键用于指定 Redis 中数据的格式。目前，它支持：csv、json 和 raw。raw 格式保持原始 Redis 键和值完整，仅使用一个字段键进行查询。下面不再描述详细信息。 mapping 的作用是将 Redis 的列映射到底层数据。由于 Redis 中没有列的概念，具体的映射方法根据格式而异。例如，对于 csv，我们知道 CSV 数据将在解析后形成。相应的列映射使用底层数组的索引（下标）。在上面的示例中，EMPNO 映射到索引 0，NAME 映射到索引 1，依此类推。 目前 Redis 适配器支持三种格式：raw、JSON 和 CSV。 示例：raw raw 格式保持原始 Redis 键值格式，只有一列 key： 127.0.0.1:6379 LPUSH raw_02 book1sqlline select * from raw_02;+-------+| key |+-------+| book2 || book1 |+-------+ 示例：JSON JSON 格式解析 Redis 字符串值，并使用映射将字段转换为多列。 127.0.0.1:6379 LPUSH json_02 DEPTNO:10,NAME:Sales1 模式包含映射： name: json_02, factory: org.apache.calcite.adapter.redis.RedisTableFactory, operand: dataFormat: json, fields: [ name: DEPTNO, type: varchar, mapping: DEPTNO , name: NAME, type: varchar, mapping: NAME ] sqlline select * from json_02;+--------+----------+| DEPTNO | NAME |+--------+----------+| 20 | Sales2 || 10 | Sales1 |+--------+----------+2 rows selected (0.014 seconds) 示例：CSV CSV 格式解析 Redis 字符串值，并结合 fields 中的映射将其组合成多列。默认分隔符为 :。 127.0.0.1:6379 LPUSH csv_02 10:Sales 模式包含映射： name: csv_02, factory: org.apache.calcite.adapter.redis.RedisTableFactory, operand: dataFormat: csv, keyDelimiter: :, fields: [ name: DEPTNO, type: varchar, mapping: 0 , name: NAME, type: varchar, mapping: 1 ] sqlline select * from csv_02;+--------+-------+| DEPTNO | NAME |+--------+-------+| 20 | Sales || 10 | Sales |+--------+-------+ 未来计划： 需要进一步完善更多 Redis 功能：例如 HyperLogLog 和 Pub/Sub。 写在最后 笔者因为工作原因接触到 Calcite，前期学习过程中，深感 Calcite 学习资料之匮乏，因此创建了 Calcite 从入门到精通知识星球，希望能够将学习过程中的资料和经验沉淀下来，为更多想要学习 Calcite 的朋友提供一些帮助。"},{"title":"参考指南","path":"/wiki/calcite/reference.html","content":"原文链接：https://calcite.apache.org/docs/reference.html 该页面描述了 Calcite 的默认 SQL 解析器识别的 SQL 方言。 语法 SQL 语法采用了 BNF 风格。 statement: setStatement | resetStatement | explain | describe | insert | update | merge | delete | querystatementList: statement [ ; statement ]* [ ; ]setStatement: [ ALTER SYSTEM | SESSION ] SET identifier = expressionresetStatement: [ ALTER SYSTEM | SESSION ] RESET identifier | [ ALTER SYSTEM | SESSION ] RESET ALLexplain: EXPLAIN PLAN [ WITH TYPE | WITH IMPLEMENTATION | WITHOUT IMPLEMENTATION ] [ EXCLUDING ATTRIBUTES | INCLUDING [ ALL ] ATTRIBUTES ] [ AS JSON | AS XML | AS DOT ] FOR query | insert | update | merge | delete describe: DESCRIBE DATABASE databaseName | DESCRIBE CATALOG [ databaseName . ] catalogName | DESCRIBE SCHEMA [ [ databaseName . ] catalogName ] . schemaName | DESCRIBE [ TABLE ] [ [ [ databaseName . ] catalogName . ] schemaName . ] tableName [ columnName ] | DESCRIBE [ STATEMENT ] query | insert | update | merge | delete insert: INSERT | UPSERT INTO tablePrimary [ ( column [, column ]* ) ] queryupdate: UPDATE tablePrimary SET assign [, assign ]* [ WHERE booleanExpression ]assign: identifier = expressionmerge: MERGE INTO tablePrimary [ [ AS ] alias ] USING tablePrimary ON booleanExpression [ WHEN MATCHED THEN UPDATE SET assign [, assign ]* ] [ WHEN NOT MATCHED THEN INSERT VALUES ( value [ , value ]* ) ]delete: DELETE FROM tablePrimary [ [ AS ] alias ] [ WHERE booleanExpression ]query: values | WITH [ RECURSIVE ] withItem [ , withItem ]* query | select | selectWithoutFrom | query UNION [ ALL | DISTINCT ] query | query EXCEPT [ ALL | DISTINCT ] query | query MINUS [ ALL | DISTINCT ] query | query INTERSECT [ ALL | DISTINCT ] query [ ORDER BY orderItem [, orderItem ]* ] [ LIMIT [ start, ] count | ALL ] [ OFFSET start ROW | ROWS ] [ FETCH FIRST | NEXT [ count ] ROW | ROWS ONLY ]withItem: name [ ( column [, column ]* ) ] AS ( query )orderItem: expression [ ASC | DESC ] [ NULLS FIRST | NULLS LAST ]select: SELECT [ hintComment ] [ STREAM ] [ ALL | DISTINCT ] * | projectItem [, projectItem ]* FROM tableExpression [ WHERE booleanExpression ] [ GROUP BY [ ALL | DISTINCT ] groupItem [, groupItem ]* ] [ HAVING booleanExpression ] [ WINDOW windowName AS windowSpec [, windowName AS windowSpec ]* ] [ QUALIFY booleanExpression ]selectWithoutFrom: SELECT [ ALL | DISTINCT ] * | projectItem [, projectItem ]* projectItem: expression [ [ AS ] columnAlias ] | tableAlias . *tableExpression: tableReference [, tableReference ]* | tableExpression [ NATURAL ] [ LEFT | RIGHT | FULL [ OUTER ] ] JOIN tableExpression [ joinCondition ] | tableExpression CROSS JOIN tableExpression | tableExpression [ CROSS | OUTER ] APPLY tableExpressionjoinCondition: ON booleanExpression | USING ( column [, column ]* )tableReference: tablePrimary [ FOR SYSTEM_TIME AS OF expression ] [ pivot ] [ unpivot ] [ matchRecognize ] [ [ AS ] alias [ ( columnAlias [, columnAlias ]* ) ] ]tablePrimary: [ [ catalogName . ] schemaName . ] tableName ( TABLE [ [ catalogName . ] schemaName . ] tableName ) | tablePrimary [ hintComment ] [ EXTEND ] ( columnDecl [, columnDecl ]* ) | [ LATERAL ] ( query ) | UNNEST ( expression ) [ WITH ORDINALITY ] | [ LATERAL ] TABLE ( [ SPECIFIC ] functionName ( expression [, expression ]* ) )columnDecl: column type [ NOT NULL ]hint: hintName | hintName ( hintOptions )hintOptions: hintKVOption [, hintKVOption ]* | optionName [, optionName ]* | optionValue [, optionValue ]*hintKVOption: optionName = stringLiteral | stringLiteral = stringLiteraloptionValue: stringLiteral | numericLiteralcolumnOrList: column | ( column [, column ]* )exprOrList: expr | ( expr [, expr ]* )pivot: PIVOT ( pivotAgg [, pivotAgg ]* FOR pivotList IN ( pivotExpr [, pivotExpr ]* ) )pivotAgg: agg ( [ ALL | DISTINCT ] value [, value ]* ) [ [ AS ] alias ]pivotList: columnOrListpivotExpr: exprOrList [ [ AS ] alias ]unpivot: UNPIVOT [ INCLUDING NULLS | EXCLUDING NULLS ] ( unpivotMeasureList FOR unpivotAxisList IN ( unpivotValue [, unpivotValue ]* ) )unpivotMeasureList: columnOrListunpivotAxisList: columnOrListunpivotValue: column [ AS literal ] | ( column [, column ]* ) [ AS ( literal [, literal ]* ) ]values: VALUES | VALUE expression [, expression ]*groupItem: expression | ( ) | ( expression [, expression ]* ) | CUBE ( expression [, expression ]* ) | ROLLUP ( expression [, expression ]* ) | GROUPING SETS ( groupItem [, groupItem ]* )window: windowName | windowSpecwindowSpec: ( [ windowName ] [ ORDER BY orderItem [, orderItem ]* ] [ PARTITION BY expression [, expression ]* ] [ RANGE numericOrIntervalExpression PRECEDING | FOLLOWING | ROWS numericExpression PRECEDING | FOLLOWING ] ) 在 insert 中，如果 INSERT 或 UPSERT 语句未指定目标列列表，则查询必须具有与目标表相同的列数，某些一致性级别除外。 在 merge 中，至少必须存在 WHEN MATCHED 和 WHEN NOT MATCHED 子句之一。 tablePrimary 只能在某些一致性级别中包含 EXTEND 子句；在这些相同的一致性级别中，insert 中的任何列都可以由 columnDecl 替换，这与将其包含在 EXTEND 子句中具有类似的效果。 在 orderItem 中，如果 expression 是正整数 n，则表示 SELECT 子句中的第 n 项。 在查询中，count 和 start 可以分别是无符号整数字面量或值为整数的动态参数。 聚合查询是在 SELECT 子句中包含 GROUP BY 或 HAVING 子句，或包含聚合函数的查询。在聚合查询的 SELECT、HAVING 和 ORDER BY 子句中，所有表达式必须是当前组内的常量（即：由 GROUP BY 子句定义的分组常量或常量）、或者是聚合函数，或者是常量和聚合函数的组合。聚合和分组函数只能出现在聚合查询中，并且只能出现在 SELECT、HAVING 或 ORDER BY 子句中。 标量子查询是指用作表达式的子查询。如果子查询没有返回行，则值为 NULL，如果它返回多于一行，则会报错。 IN、EXISTS、UNIQUE 和标量子查询，可以出现在任何可以出现表达式的位置（例如 JOIN 的 SELECT 子句、WHERE 子句、ON 子句，或作为聚合函数的参数）。 IN、EXISTS、UNIQUE 或标量子查询可以是相关的，即：它可以引用一个封闭查询中 FROM 子句的表。 GROUP BY DISTINCT 删除重复的分组集（例如：GROUP BY DISTINCT GROUPING SETS ((a), (a, b), (a)) 等价于 GROUP BY GROUPING SETS ((a), (a, b))），GROUP BY ALL 和 GROUP BY 是等价的。 selectWithoutFrom 等价于 VALUES，但它不是标准 SQL，并且仅在某些一致性级别中允许使用。 MINUS 等价于 EXCEPT，但不是标准 SQL，仅在某些一致性级别中允许使用。 CROSS APPLY 和 OUTER APPLY 仅允许在某些一致性级别中使用。 LIMIT start, count 等价于 LIMIT count OFFSET start，但仅在某些一致性级别中允许使用。 在某些一致性级别中，OFFSET start 可能发生在 LIMIT count 之前。 VALUE 与 VALUES 等效，但不是标准 SQL，并且仅在某些一致性级别中允许使用。 关键字 以下是 SQL 关键字的列表。保留的关键字使用粗体展示。 A, ABS, ABSENT, ABSOLUTE, ACTION, ADA, ADD, ADMIN, AFTER, ALL, ALLOCATE, ALLOW, ALTER, ALWAYS, AND, ANY, APPLY, ARE, ARRAY, ARRAY_AGG, ARRAY_CONCAT_AGG, ARRAY_MAX_CARDINALITY, AS, ASC, ASENSITIVE, ASSERTION, ASSIGNMENT, ASYMMETRIC, AT, ATOMIC, ATTRIBUTE, ATTRIBUTES, AUTHORIZATION, AVG, BEFORE, BEGIN, BEGIN_FRAME, BEGIN_PARTITION, BERNOULLI, BETWEEN, BIGINT, BINARY, BIT, BLOB, BOOLEAN, BOTH, BREADTH, BY, C, CALL, CALLED, CARDINALITY, CASCADE, CASCADED, CASE, CAST, CATALOG, CATALOG_NAME, CEIL, CEILING, CENTURY, CHAIN, CHAR, CHARACTER, CHARACTERISTICS, CHARACTERS, CHARACTER_LENGTH, CHARACTER_SET_CATALOG, CHARACTER_SET_NAME, CHARACTER_SET_SCHEMA, CHAR_LENGTH, CHECK, CLASSIFIER, CLASS_ORIGIN, CLOB, CLOSE, COALESCE, COBOL, COLLATE, COLLATION, COLLATION_CATALOG, COLLATION_NAME, COLLATION_SCHEMA, COLLECT, COLUMN, COLUMN_NAME, COMMAND_FUNCTION, COMMAND_FUNCTION_CODE, COMMIT, COMMITTED, CONDITION, CONDITIONAL, CONDITION_NUMBER, CONNECT, CONNECTION, CONNECTION_NAME, CONSTRAINT, CONSTRAINTS, CONSTRAINT_CATALOG, CONSTRAINT_NAME, CONSTRAINT_SCHEMA, CONSTRUCTOR, CONTAINS, CONTAINS_SUBSTR, CONTINUE, CONVERT, CORR, CORRESPONDING, COUNT, COVAR_POP, COVAR_SAMP, CREATE, CROSS, CUBE, CUME_DIST, CURRENT, CURRENT_CATALOG, CURRENT_DATE, CURRENT_DEFAULT_TRANSFORM_GROUP, CURRENT_PATH, CURRENT_ROLE, CURRENT_ROW, CURRENT_SCHEMA, CURRENT_TIME, CURRENT_TIMESTAMP, CURRENT_TRANSFORM_GROUP_FOR_TYPE, CURRENT_USER, CURSOR, CURSOR_NAME, CYCLE, DATA, DATABASE, DATE, DATETIME, DATETIME_DIFF, DATETIME_INTERVAL_CODE, DATETIME_INTERVAL_PRECISION, DATETIME_TRUNC, DATE_DIFF, DATE_TRUNC, DAY, DAYOFWEEK, DAYOFYEAR, DAYS, DEALLOCATE, DEC, DECADE, DECIMAL, DECLARE, DEFAULT, DEFAULTS, DEFERRABLE, DEFERRED, DEFINE, DEFINED, DEFINER, DEGREE, DELETE, DENSE_RANK, DEPTH, DEREF, DERIVED, DESC, DESCRIBE, DESCRIPTION, DESCRIPTOR, DETERMINISTIC, DIAGNOSTICS, DISALLOW, DISCONNECT, DISPATCH, DISTINCT, DOMAIN, DOT, DOUBLE, DOW, DOY, DROP, DYNAMIC, DYNAMIC_FUNCTION, DYNAMIC_FUNCTION_CODE, EACH, ELEMENT, ELSE, EMPTY, ENCODING, END, END-EXEC, END_FRAME, END_PARTITION, EPOCH, EQUALS, ERROR, ESCAPE, EVERY, EXCEPT, EXCEPTION, EXCLUDE, EXCLUDING, EXEC, EXECUTE, EXISTS, EXP, EXPLAIN, EXTEND, EXTERNAL, EXTRACT, FALSE, FETCH, FILTER, FINAL, FIRST, FIRST_VALUE, FLOAT, FLOOR, FOLLOWING, FOR, FOREIGN, FORMAT, FORTRAN, FOUND, FRAC_SECOND, FRAME_ROW, FREE, FRIDAY, FROM, FULL, FUNCTION, FUSION, G, GENERAL, GENERATED, GEOMETRY, GET, GLOBAL, GO, GOTO, GRANT, GRANTED, GROUP, GROUPING, GROUPS, GROUP_CONCAT, HAVING, HIERARCHY, HOLD, HOP, HOUR, HOURS, IDENTITY, IGNORE, ILIKE, IMMEDIATE, IMMEDIATELY, IMPLEMENTATION, IMPORT, IN, INCLUDE, INCLUDING, INCREMENT, INDICATOR, INITIAL, INITIALLY, INNER, INOUT, INPUT, INSENSITIVE, INSERT, INSTANCE, INSTANTIABLE, INT, INTEGER, INTERSECT, INTERSECTION, INTERVAL, INTO, INVOKER, IS, ISODOW, ISOLATION, ISOYEAR, JAVA, JOIN, JSON, JSON_ARRAY, JSON_ARRAYAGG, JSON_EXISTS, JSON_OBJECT, JSON_OBJECTAGG, JSON_QUERY, JSON_SCOPE, JSON_VALUE, K, KEY, KEY_MEMBER, KEY_TYPE, LABEL, LAG, LANGUAGE, LARGE, LAST, LAST_VALUE, LATERAL, LEAD, LEADING, LEFT, LENGTH, LEVEL, LIBRARY, LIKE, LIKE_REGEX, LIMIT, LN, LOCAL, LOCALTIME, LOCALTIMESTAMP, LOCATOR, LOWER, M, MAP, MATCH, MATCHED, MATCHES, MATCH_NUMBER, MATCH_RECOGNIZE, MAX, MAXVALUE, MEASURES, MEMBER, MERGE, MESSAGE_LENGTH, MESSAGE_OCTET_LENGTH, MESSAGE_TEXT, METHOD, MICROSECOND, MILLENNIUM, MILLISECOND, MIN, MINUS, MINUTE, MINUTES, MINVALUE, MOD, MODIFIES, MODULE, MONDAY, MONTH, MONTHS, MORE, MULTISET, MUMPS, NAME, NAMES, NANOSECOND, NATIONAL, NATURAL, NCHAR, NCLOB, NESTING, NEW, NEXT, NO, NONE, NORMALIZE, NORMALIZED, NOT, NTH_VALUE, NTILE, NULL, NULLABLE, NULLIF, NULLS, NUMBER, NUMERIC, OBJECT, OCCURRENCES_REGEX, OCTETS, OCTET_LENGTH, OF, OFFSET, OLD, OMIT, ON, ONE, ONLY, OPEN, OPTION, OPTIONS, OR, ORDER, ORDERING, ORDINAL, ORDINALITY, OTHERS, OUT, OUTER, OUTPUT, OVER, OVERLAPS, OVERLAY, OVERRIDING, PAD, PARAMETER, PARAMETER_MODE, PARAMETER_NAME, PARAMETER_ORDINAL_POSITION, PARAMETER_SPECIFIC_CATALOG, PARAMETER_SPECIFIC_NAME, PARAMETER_SPECIFIC_SCHEMA, PARTIAL, PARTITION, PASCAL, PASSING, PASSTHROUGH, PAST, PATH, PATTERN, PER, PERCENT, PERCENTILE_CONT, PERCENTILE_DISC, PERCENT_RANK, PERIOD, PERMUTE, PIVOT, PLACING, PLAN, PLI, PORTION, POSITION, POSITION_REGEX, POWER, PRECEDES, PRECEDING, PRECISION, PREPARE, PRESERVE, PREV, PRIMARY, PRIOR, PRIVILEGES, PROCEDURE, PUBLIC, QUALIFY, QUARTER, QUARTERS, RANGE, RANK, READ, READS, REAL, RECURSIVE, REF, REFERENCES, REFERENCING, REGR_AVGX, REGR_AVGY, REGR_COUNT, REGR_INTERCEPT, REGR_R2, REGR_SLOPE, REGR_SXX, REGR_SXY, REGR_SYY, RELATIVE, RELEASE, REPEATABLE, REPLACE, RESET, RESPECT, RESTART, RESTRICT, RESULT, RETURN, RETURNED_CARDINALITY, RETURNED_LENGTH, RETURNED_OCTET_LENGTH, RETURNED_SQLSTATE, RETURNING, RETURNS, REVOKE, RIGHT, RLIKE, ROLE, ROLLBACK, ROLLUP, ROUTINE, ROUTINE_CATALOG, ROUTINE_NAME, ROUTINE_SCHEMA, ROW, ROWS, ROW_COUNT, ROW_NUMBER, RUNNING, SAFE_CAST, SAFE_OFFSET, SAFE_ORDINAL, SATURDAY, SAVEPOINT, SCALAR, SCALE, SCHEMA, SCHEMA_NAME, SCOPE, SCOPE_CATALOGS, SCOPE_NAME, SCOPE_SCHEMA, SCROLL, SEARCH, SECOND, SECONDS, SECTION, SECURITY, SEEK, SELECT, SELF, SENSITIVE, SEPARATOR, SEQUENCE, SERIALIZABLE, SERVER, SERVER_NAME, SESSION, SESSION_USER, SET, SETS, SHOW, SIMILAR, SIMPLE, SIZE, SKIP, SMALLINT, SOME, SOURCE, SPACE, SPECIFIC, SPECIFICTYPE, SPECIFIC_NAME, SQL, SQLEXCEPTION, SQLSTATE, SQLWARNING, SQL_BIGINT, SQL_BINARY, SQL_BIT, SQL_BLOB, SQL_BOOLEAN, SQL_CHAR, SQL_CLOB, SQL_DATE, SQL_DECIMAL, SQL_DOUBLE, SQL_FLOAT, SQL_INTEGER, SQL_INTERVAL_DAY, SQL_INTERVAL_DAY_TO_HOUR, SQL_INTERVAL_DAY_TO_MINUTE, SQL_INTERVAL_DAY_TO_SECOND, SQL_INTERVAL_HOUR, SQL_INTERVAL_HOUR_TO_MINUTE, SQL_INTERVAL_HOUR_TO_SECOND, SQL_INTERVAL_MINUTE, SQL_INTERVAL_MINUTE_TO_SECOND, SQL_INTERVAL_MONTH, SQL_INTERVAL_SECOND, SQL_INTERVAL_YEAR, SQL_INTERVAL_YEAR_TO_MONTH, SQL_LONGVARBINARY, SQL_LONGVARCHAR, SQL_LONGVARNCHAR, SQL_NCHAR, SQL_NCLOB, SQL_NUMERIC, SQL_NVARCHAR, SQL_REAL, SQL_SMALLINT, SQL_TIME, SQL_TIMESTAMP, SQL_TINYINT, SQL_TSI_DAY, SQL_TSI_FRAC_SECOND, SQL_TSI_HOUR, SQL_TSI_MICROSECOND, SQL_TSI_MINUTE, SQL_TSI_MONTH, SQL_TSI_QUARTER, SQL_TSI_SECOND, SQL_TSI_WEEK, SQL_TSI_YEAR, SQL_VARBINARY, SQL_VARCHAR, SQRT, START, STATE, STATEMENT, STATIC, STDDEV_POP, STDDEV_SAMP, STREAM, STRING_AGG, STRUCTURE, STYLE, SUBCLASS_ORIGIN, SUBMULTISET, SUBSET, SUBSTITUTE, SUBSTRING, SUBSTRING_REGEX, SUCCEEDS, SUM, SUNDAY, SYMMETRIC, SYSTEM, SYSTEM_TIME, SYSTEM_USER, TABLE, TABLESAMPLE, TABLE_NAME, TEMPORARY, THEN, THURSDAY, TIES, TIME, TIMESTAMP, TIMESTAMPADD, TIMESTAMPDIFF, TIMESTAMP_DIFF, TIMESTAMP_TRUNC, TIMEZONE_HOUR, TIMEZONE_MINUTE, TIME_DIFF, TIME_TRUNC, TINYINT, TO, TOP_LEVEL_COUNT, TRAILING, TRANSACTION, TRANSACTIONS_ACTIVE, TRANSACTIONS_COMMITTED, TRANSACTIONS_ROLLED_BACK, TRANSFORM, TRANSFORMS, TRANSLATE, TRANSLATE_REGEX, TRANSLATION, TREAT, TRIGGER, TRIGGER_CATALOG, TRIGGER_NAME, TRIGGER_SCHEMA, TRIM, TRIM_ARRAY, TRUE, TRUNCATE, TRY_CAST, TUESDAY, TUMBLE, TYPE, UESCAPE, UNBOUNDED, UNCOMMITTED, UNCONDITIONAL, UNDER, UNION, UNIQUE, UNKNOWN, UNNAMED, UNNEST, UNPIVOT, UPDATE, UPPER, UPSERT, USAGE, USER, USER_DEFINED_TYPE_CATALOG, USER_DEFINED_TYPE_CODE, USER_DEFINED_TYPE_NAME, USER_DEFINED_TYPE_SCHEMA, USING, UTF16, UTF32, UTF8, VALUE, VALUES, VALUE_OF, VARBINARY, VARCHAR, VARYING, VAR_POP, VAR_SAMP, VERSION, VERSIONING, VIEW, WEDNESDAY, WEEK, WEEKS, WHEN, WHENEVER, WHERE, WIDTH_BUCKET, WINDOW, WITH, WITHIN, WITHOUT, WORK, WRAPPER, WRITE, XML, YEAR, YEARS, ZONE. (adsbygoogle = window.adsbygoogle || []).push({}); 赞助商 标识符 标识符是 SQL 查询中使用的表、列和其他元数据元素的名称。 不带引号的标识符（例如 emp）必须以字母开头，并且只能包含字母、数字和下划线。它们被隐式转换为大写。 带引号的标识符，例如 Employee Name ，以双引号开头和结尾。它们几乎可以包含任何字符，包括空格和其他标点符号。如果你希望在标识符中包含双引号，请使用另一个双引号对其进行转义，例如：An employee called Fred.。 在 Calcite 中，将标识符与引用对象的名称匹配是区分大小写的。但请记住，未加引号的标识符在匹配之前会隐式转换为大写，并且如果它引用的对象是使用未加引号的标识符作为其名称创建的，则其名称也将转换为大写。 数据类型 标量类型 数据类型 描述 范围和示例字面量 BOOLEAN 逻辑值 值：TRUE, FALSE, UNKNOWN TINYINT 1 字节有符号整数 范围是 -128 到 127 SMALLINT 2 字节有符号整数 范围为 -32768 至 32767 INTEGER, INT 4 字节有符号整数 范围为 -2147483648 至 2147483647 BIGINT 8 字节有符号整数 范围为 -9223372036854775808 至 9223372036854775807 DECIMAL(p, s) 定点数（即：小数点位置固定） 示例：123.45 和 DECIMAL ‘123.45’ 是相同的值，并且类型为 DECIMAL(5, 2) NUMERIC(p, s) 定点数（即：小数点位置固定） DECIMAL 的同义词 REAL 4 字节浮点数 6 位小数精度；示例：CAST(1.2 AS REAL)、CAST(‘Infinity’ AS REAL) DOUBLE 8 字节浮点数 15 位小数精度；示例：1.4E2、CAST(‘-Infinity’ AS DOUBLE)、CAST(‘NaN’ AS DOUBLE) FLOAT 8 字节浮点数 DOUBLE 的同义词 CHAR(n), CHARACTER(n) 定长字符串 ‘Hello’、‘’（空字符串）、_latin1’Hello’、n’Hello’、_UTF16’Hello’、‘Hello’ ‘there’（字面量分为多个部分）、e’Hello there’（字面量包含 C 风格的转义符） VARCHAR(n), CHARACTER VARYING(n) 变长字符串 作为 CHAR(n) BINARY(n) 固定宽度的二进制字符串 x’45F0AB’、x’‘（空二进制字符串）、x’AB’ ‘CD’（多部分二进制字符串字面量） VARBINARY(n), BINARY VARYING(n) 变长二进制字符串 作为 BINARY(n) DATE 日期 示例：DATE “1969-07-20” TIME 一天中的时间 示例：TIME “20:17:40” TIMESTAMP [ WITHOUT TIME ZONE ] 日期和时间 示例：TIMESTAMP ‘1969-07-20 20:17:40’ TIMESTAMP WITH LOCAL TIME ZONE 带有当地时区的日期和时间 示例：TIMESTAMP ‘1969-07-20 20:17:40 America/Los Angeles’ TIMESTAMP WITH TIME ZONE 带时区的日期和时间 示例：TIMESTAMP ‘1969-07-20 20:17:40 America/Los Angeles’ INTERVAL timeUnit [ TO timeUnit ] 日期时间间隔 示例：INTERVAL ‘1-5’ YEAR TO MONTH, INTERVAL ‘45’ DAY, INTERVAL ‘1 2:34:56.789’ DAY TO SECOND GEOMETRY 几何类型 示例： ST_GeomFromText(‘POINT (30 10)’) timeUnit 包含了以下可选值： timeUnit: MILLENNIUM | CENTURY | DECADE | YEAR | QUARTER | MONTH | WEEK | DOY | DOW | DAY | HOUR | MINUTE | SECOND | EPOCH 注意： DATE、TIME 和 TIMESTAMP 没有时区。对于这些类型，甚至没有隐式时区，例如 UTC（如 Java 中）或本地时区。由用户或应用程序提供时区。反过来，TIMESTAMP WITH LOCAL TIME ZONE 不会在内部存储时区，但它将依赖于提供的时区来提供正确的语义。 仅在某些一致性级别中才允许使用几何类型。 间隔字面量只能使用时间单位 YEAR、QUARTER、MONTH、WEEK、DAY、HOUR、MINUTE 和 SECOND。在某些一致性级别中，我们还允许使用复数形式：YEARS、QUARTERS、MONTHS、WEEKS、DAYS、HOURS、MINUTES 和 SECONDS。 非标量类型 类型 描述 示例字面量 ANY 所有类型的联合 UNKNOWN 未知类型的值，用作占位符 ROW 具有 1 列或多列的行 示例：row(f0 int null, f1 varchar) MAP 键值对集合 示例：(int, varchar) map MULTISET 可能包含重复项的无序集合 示例：int multiset ARRAY 可能包含重复项的有序连续集合 示例：varchar(10) array CURSOR 执行结果之上的游标 注意： 每个 ROW 列类型，都可以有一个可选的 [ NULL | NOT NULL ] 后缀，用来声明此列类型是否可为空，默认值是不可为空。 空间类型 空间数据使用字符串（众所周知的 text (WKT) 编码）或者二进制字符串进行表示（众所周知的 binary (WKB) 编码）。 在要使用字面量的地方，应用 ST_GeomFromText 函数，例如 ST_GeomFromText('POINT (30 10)') 。 数据类型 类型编码 WKT 中的示例 GEOMETRY 0 点、曲线、曲面、几何集合的泛化 POINT 1 ST_GeomFromText('POINT (30 10)') 是 2D 空间中的点； ST_GeomFromText('POINT Z(30 10 2)') 是 3D 空间中的点 CURVE 13 LINESTRING 的泛化 LINESTRING 2 ST_GeomFromText('LINESTRING (30 10, 10 30, 40 40)') SURFACE 14 多边形、多面体曲面的泛化 POLYGON 3 ST_GeomFromText('POLYGON ((30 10, 40 40, 20 40, 10 20, 30 10))') 是五边形； ST_GeomFromText('POLYGON ((35 10, 45 45, 15 40, 10 20, 35 10), (20 30, 35 35, 30 20, 20 30))') 是一个有四边形孔的五边形 POLYHEDRALSURFACE 15 GEOMETRYCOLLECTION 7 零个或多个 GEOMETRY 实例的集合；多点、多线、多多边形的概括 MULTIPOINT 4 ST_GeomFromText(‘MULTIPOINT ((10 40), (40 30), (20 20), (30 10))’) 等价于ST_GeomFromText(‘MULTIPOINT (10 40, 40 30, 20 20, 30 10)’) MULTICURVE - MULTILINESTRING 的泛化 MULTILINESTRING 5 ST_GeomFromText('MULTILINESTRING ((10 10, 20 20, 10 40), (40 40, 30 30, 40 20, 30 10))') MULTISURFACE - MULTIPOLYGON 的泛化 MULTIPOLYGON 6 ST_GeomFromText('MULTIPOLYGON (((30 20, 45 40, 10 40, 30 20)), ((15 5, 40 10, 10 20, 5 10, 15 5)))') 运算符和函数 运算符优先级 运算符优先级和结合性，从高到低。 运算符 结合性 . 左 :: 左 [ ]（集合元素） 左 + -（一元加、减） 右 * / % || 左 + - 左 BETWEEN, IN, LIKE, SIMILAR, OVERLAPS, CONTAINS 等 - = = = != = 左 IS NULL, IS FALSE, IS NOT TRUE 等 - NOT 右 AND 左 OR 左 注意：:: 、 = 是特定于方言的，但为了完整性起见在此表中显示。 比较运算符 运算符语法 描述 value1 = value2 等于 value1 value2 不等于 value1 != value2 不相等（仅在某些一致性级别） value1 value2 大于 value1 = value2 大于等于 value1 value2 小于 value1 = value2 小于等于 value1 = value2 两个值是否相等，将 null 值视为相同 value IS NULL 值是否为 null value IS NOT NULL 值是否不为 null value1 IS DISTINCT FROM value2 两个值是否不相等，将 null 值视为相同 value1 IS NOT DISTINCT FROM value2 两个值是否相等，将 null 值视为相同 value1 BETWEEN value2 AND value3 value1 是否大于等于 value2 且小于等于 value3 value1 NOT BETWEEN value2 AND value3 value1 是否小于 value2 并且大于 value3 string1 LIKE string2 [ ESCAPE string3 ] string1 是否与模式 string2 匹配 string1 NOT LIKE string2 [ ESCAPE string3 ] string1 是否与模式 string2 不匹配 string1 SIMILAR TO string2 [ ESCAPE string3 ] string1 是否与正则表达式 string2 匹配 string1 NOT SIMILAR TO string2 [ ESCAPE string3 ] string1 是否与正则表达式 string2 不匹配 value IN (value [, value ]*) value 是否等于列表中的值 value NOT IN (value [, value ]*) value 是否不等于列表中的每个值 value IN (sub-query) value 是否等于子查询返回的行 value NOT IN (sub-query) value 是否不等于子查询返回的每一行 value comparison SOME (sub-query or collection) 是否值比较 SOME 的子查询或集合至少返回一行 value comparison ANY (sub-query or collection) SOME 的同义词 value comparison ALL (sub-query or collection) 是否值比较 ALL 的子查询或集合返回所有值 EXISTS (sub-query) 子查询是否至少返回一行 UNIQUE (sub-query) 子查询返回的行是否唯一（忽略空值） comp: = | | | = | | = | = 逻辑运算符 运算符语法 描述 boolean1 OR boolean2 boolean1 为 TRUE 或者 boolean2 为 TRUE boolean1 AND boolean2 boolean1 为 TRUE 并且 boolean2 为 TRUE NOT boolean 布尔值是否不为 TRUE；如果布尔值未知，则返回 UNKNOWN boolean IS FALSE 布尔值是否为FALSE；如果布尔值未知则返回 FALSE boolean IS NOT FALSE 布尔值是否不为 FALSE；如果布尔值未知则返回 TRUE boolean IS TRUE 布尔值是否为 TRUE；如果布尔值未知则返回 FALSE boolean IS NOT TRUE 布尔值是否不为 TRUE；如果布尔值未知则返回 TRUE boolean IS UNKNOWN 布尔值是否未知 boolean IS NOT UNKNOWN 布尔值是否不为 UNKNOWN 算术运算符和函数 运算符语法 描述 + numeric 返回数字 - numeric 返回负数 numeric1 + numeric2 返回 numeric1 加 numeric2 numeric1 - numeric2 返回 numeric1 减去 numeric2 numeric1 * numeric2 返回 numeric1 乘以 numeric2 numeric1 / numeric2 返回 numeric1 除以 numeric2 numeric1 % numeric2 作为 MOD(numeric1, numeric2)（仅在某些一致性级别） POWER(numeric1, numeric2) 返回 numeric1 的 numeric2 次方 ABS(numeric) 返回数字的绝对值 MOD(numeric1, numeric2) 返回 numeric1 除以 numeric2 的余数（模）。仅当 numeric1 为负数时结果才为负数 SQRT(numeric) 返回数字的平方根 LN(numeric) 返回数值的自然对数（以 e 为底） LOG10(numeric) 返回 numeric 以 10 为底的对数 EXP(numeric) 返回 e 的数值次方 CEIL(numeric) 将 numeric 向上舍入，返回大于或等于 numeric 的最小整数 FLOOR(numeric) 将数字向下舍入，返回小于或等于数字的最大整数 RAND([seed]) 生成 0 到 1（含）之间的随机双精度数，可选择使用种子初始化随机数生成器 RAND_INTEGER([seed, ] numeric) 生成 0 到 numeric - 1（含）之间的随机整数，可选择使用种子初始化随机数生成器 ACOS(numeric) 返回数值的反余弦 ASIN(numeric) 返回数字的反正弦值 ATAN(numeric) 返回数值的反正切值 ATAN2(numeric, numeric) 返回数字坐标的反正切值 CBRT(numeric) 返回数字的立方根 COS(numeric) 返回数字的余弦值 COT(numeric) 返回数值的余切值 DEGREES(numeric) 将数值从弧度转换为度数 PI() 返回比任何其他值更接近 pi 的值 RADIANS(numeric) 将数值从度数转换为弧度 ROUND(numeric1 [, numeric2]) 将 numeric1 舍入到小数点右边可选的 numeric2（如果未指定 0）位 SIGN(numeric) 返回数字的符号 SIN(numeric) 返回数字的正弦值 TAN(numeric) 返回数字的正切值 TRUNCATE(numeric1 [, numeric2]) 将 numeric1 截断为可选的 numeric2（如果未指定 0）小数点右边的位置 字符串运算符和函数 运算符语法 描述 string || string 连接两个字符串 CHAR_LENGTH(string) 返回字符串中的字符数 CHARACTER_LENGTH(string) 像 CHAR_LENGTH(string) 一样，返回字符串中的字符数 UPPER(string) 返回转换为大写字母的字符串 LOWER(string) 返回转换为小写字母的字符串 POSITION(substring IN string) 返回 string 中 substring 第一次出现的位置 POSITION(substring IN string FROM integer) 返回从给定点开始在 string 中第一次出现 substring 的位置（非标准 SQL） TRIM( { BOTH | LEADING | TRAILING } string1 FROM string2) 从 string1 的开始/结束/两端删除仅包含 string1 中字符的最长字符串 OVERLAY(string1 PLACING string2 FROM integer [ FOR integer2 ]) 用 string2 替换 string1 的子字符串 SUBSTRING(string FROM integer) 返回从给定点开始的字符串的子字符串 SUBSTRING(string FROM integer FOR integer) 返回从给定点开始、具有给定长度的字符串子字符串。 INITCAP(string) 返回 string，其中每个单词的首字母转换为大写，其余字母转换为小写。单词是由非字母数字字符分隔的字母数字字符序列。 未实现： SUBSTRING(string FROM regexp FOR regexp) 二进制字符串运算符和函数 运算符语法 描述 binary || binary 连接两个二进制字符串 OCTET_LENGTH(binary) 返回二进制的字节数 POSITION(binary1 IN binary2) 返回 binary1 在 binary2 中第一次出现的位置 POSITION(binary1 IN binary2 FROM integer) 返回从给定点开始 binary1 在 binary2 中第一次出现的位置（非标准 SQL） OVERLAY(binary1 PLACING binary2 FROM integer [ FOR integer2 ]) 用 binary2 替换 binary1 的子字符串 SUBSTRING(binary FROM integer) 返回从给定点开始的 binary 子字符串 SUBSTRING(binary FROM integer FOR integer) 返回从给定点开始、具有给定长度的 binary 子字符串 日期/时间函数 运算符语法 描述 LOCALTIME 以 TIME 数据类型的值返回会话时区的当前日期和时间 LOCALTIME(precision) 以 TIME 数据类型的值返回会话时区中的当前日期和时间，精度为 precision 位 LOCALTIMESTAMP 以 TIMESTAMP 数据类型的值返回会话时区的当前日期和时间 LOCALTIMESTAMP(precision) 以 TIMESTAMP 数据类型的值返回会话时区中的当前日期和时间，精度为 precision 位 CURRENT_TIME 返回会话时区中的当前时间，数据类型为 TIMESTAMP WITH TIME ZONE CURRENT_DATE 以 DATE 数据类型的值返回会话时区的当前日期 CURRENT_TIMESTAMP 返回会话时区中的当前日期和时间，数据类型为 TIMESTAMP WITH TIME ZONE EXTRACT(timeUnit FROM datetime) 从日期时间值表达式中提取并返回指定日期时间字段的值 FLOOR(datetime TO timeUnit) 将日期时间向下舍入为时间单位 CEIL(datetime TO timeUnit) 将日期时间向上舍入为时间单位 YEAR(date) 等价于 EXTRACT(YEAR FROM date)，返回一个整数 QUARTER(date) 等价于 EXTRACT(QUARTER FROM date)，返回 1 到 4 之间的整数 MONTH(date) 等价于 EXTRACT(MONTH FROM date)。返回 1 到 12 之间的整数 WEEK(date) 等价于 EXTRACT(WEEK FROM date)。返回 1 到 53 之间的整数 DAYOFYEAR(date) 等价于 EXTRACT(DOY FROM date)。返回 1 到 366 之间的整数 DAYOFMONTH(date) 等价于 EXTRACT(DAY FROM date)。返回 1 到 31 之间的整数 DAYOFWEEK(date) 等价于 EXTRACT(DOW FROM date)。返回 1 到 7 之间的整数 HOUR(date) 等价于 EXTRACT(HOUR FROM date)。返回 0 到 23 之间的整数 MINUTE(date) 等价于 EXTRACT(MINUTE FROM date)。返回 0 到 59 之间的整数 SECOND(date) 等价于 EXTRACT(SECOND FROM date)。返回 0 到 59 之间的整数 TIMESTAMPADD(timeUnit, integer, datetime) 返回添加了（有符号）整数时间单位间隔的日期时间。等价于 datetime + INTERVAL 'integer' timeUnit TIMESTAMPDIFF(timeUnit, datetime, datetime2) 返回 datetime 和 datetime2 之间的 timeUnit 间隔数（有符号）。等价于 (datetime2 - datetime) timeUnit LAST_DAY(date) 以 DATE 数据类型的值返回月份最后一天的日期；例如，对于 DATE’2020-02-10’ 和 TIMESTAMP’2020-02-10 10:10:10’，它均返回 DATE’2020-02-29’ 在标准 SQL 中，对 niladic 函数（例如 CURRENT_DATE）的调用不接受括号。在某些一致性级别中，可以接受带括号的调用（例如 CURRENT_DATE()）。 未实现： CEIL(interval) FLOOR(interval) + interval - interval interval + interval interval - interval interval / interval 系统函数 运算符语法 描述 USER 等价于 CURRENT_USER CURRENT_USER 当前执行上下文的用户名 SESSION_USER 会话用户名 SYSTEM_USER 返回操作系统识别的当前数据存储用户的名称 CURRENT_PATH 返回一个字符串，表示当前查找范围以引用用户定义的例程和类型 CURRENT_ROLE 返回当前活动角色 CURRENT_SCHEMA 返回当前模式 条件函数和运算符 运算符语法 描述 CASE valueWHEN value1 [, value11 ]* THEN result1[ WHEN valueN [, valueN1 ]* THEN resultN ]*[ ELSE resultZ ]END 简单案例 CASEWHEN condition1 THEN result1[ WHEN conditionN THEN resultN ]*[ ELSE resultZ ]END 搜索案例 NULLIF(value, value) 如果值相同则返回 NULL。例如，NULLIF(5, 5) 返回NULL；NULLIF(5, 0) 返回 5。 COALESCE(value, value [, value ]*) 如果第一个值为 null，则提供一个值。 例如，COALESCE(NULL, 5) 返回 5。 类型转换 通常，表达式不能包含不同数据类型的值。例如，表达式不能将 5 乘以 10，然后添加 JULIAN。但是，Calcite 支持将值从一种数据类型隐式和显式转换为另一种数据类型。 隐式和显式类型转换 Calcite 建议你指定显式转换，而不是依赖隐式或自动转换，原因如下： 使用显式数据类型转换函数时，SQL 语句更容易理解； 隐式数据类型转换可能会对性能产生负面影响，尤其是当列值的数据类型转换为常量数据类型而不是与之相反时； 隐式转换取决于它发生的上下文，并且在每种情况下可能不会以相同的方式工作。例如，从日期时间值到 VARCHAR 值的隐式转换可能会返回意外的格式。 隐式转换的算法可能会在 Calcite 版本之间发生变化，显式转换的行为更可预测。 显式类型转换 运算符语法 描述 CAST(value AS type) 将值转换为给定类型。整数类型之间的转换会向 0 截断 CONVERT(string, charSet1, charSet2) 将字符串从 charSet1 转换为 charSet2 CONVERT(value USING transcodingName) 将值从一个基本字符集更改为 transcodingName TRANSLATE(value USING transcodingName) 将值从一个基本字符集更改为 transcodingName 将字符串转换为 BINARY 或 VARBINARY 类型会生成字符串字符集中字符串编码的字节列表。如果字符串的字符无法使用其字符集表示，则会产生运行时错误。 支持的数据类型语法： type: typeName [ collectionsTypeName ]*typeName: sqlTypeName | rowTypeName | compoundIdentifiersqlTypeName: char [ precision ] [ charSet ] | varchar [ precision ] [ charSet ] | DATE | time | timestamp | GEOMETRY | decimal [ precision [, scale] ] | BOOLEAN | integer | BINARY [ precision ] | varbinary [ precision ] | TINYINT | SMALLINT | BIGINT | REAL | double | FLOAT | ANY [ precision [, scale] ]collectionsTypeName: ARRAY | MULTISETrowTypeName: ROW ( fieldName1 fieldType1 [ NULL | NOT NULL ] [ , fieldName2 fieldType2 [ NULL | NOT NULL ] ]* )char: CHARACTER | CHARvarchar: char VARYING | VARCHARdecimal: DECIMAL | DEC | NUMERICinteger: INTEGER | INTvarbinary: BINARY VARYING | VARBINARYdouble: DOUBLE [ PRECISION ]time: TIME [ precision ] [ timeZone ]timestamp: TIMESTAMP [ precision ] [ timeZone ]charSet: CHARACTER SET charSetNametimeZone: WITHOUT TIME ZONE | WITH LOCAL TIME ZONE 隐式类型转换 当这种转换有意义时，Calcite 会自动将值从一种数据类型转换为另一种数据类型。下表是 Calcite 类型转换矩阵，该表显示了所有可能的转换，而不考虑转换的上下文。管理这些细节的规则遵循该表。 从 - 到 NULL BOOLEAN TINYINT SMALLINT INT BIGINT DECIMAL FLOAT OR REAL DOUBLE INTERVAL DATE TIME TIMESTAMP CHAR OR VARCHAR BINARY OR VARBINARY GEOMETRY ARRAY NULL i i i i i i i i i i i i i i i i x BOOLEAN x i x x x x x x x x x x x i x x x TINYINT x e i i i i i i i e x x e i x x x SMALLINT x e i i i i i i i e x x e i x x x INT x e i i i i i i i e x x e i x x x BIGINT x e i i i i i i i e x x e i x x x DECIMAL x e i i i i i i i e x x e i x x x FLOAT/REAL x e i i i i i i i x x x e i x x x DOUBLE x e i i i i i i i x x x e i x x x INTERVAL x x e e e e e x x i x x x e x x x DATE x x x x x x x x x x i x i i x x x TIME x x x x x x x x x x x i e i x x x TIMESTAMP x x e e e e e e e x i e i i x x x CHAR or VARCHAR x e i i i i i i i i i i i i i i i BINARY or VARBINARY x x x x x x x x x x e e e i i x x GEOMETRY x x x x x x x x x x x x x i x i x ARRAY x x x x x x x x x x x x x x x x i i：隐式转换 / e：显式转换 / x：不允许 转化条件和策略 集合运算（UNION、EXCEPT、INTERSECT）：比较各分支行的数据类型，找出各字段对的公共类型； 二进制算术表达式（+、-、、^、/、%）：将字符串操作数提升为另一个数字操作数的数据类型； 二进制比较（=、、=、、、=）：如果操作数为 STRING 和 TIMESTAMP，则提升为 TIMESTAMP；使 1 = true 和 0 = false 始终计算为 TRUE；如果有数字类型操作数，则为两个操作数找到共同的类型。 IN 子查询：比较 LHS 和 RHS 的类型，找出共同的类型；如果是结构体类型，则为每个字段找到更宽的类型； IN 表达式列表：比较每个表达式以找到共同类型； CASE WHEN 表达式或 COALESCE：找到 THEN 和 ELSE 操作数的共同更宽类型； 字符 + INTERVAL 或字符 - INTERVAL ：将字符提升为 TIMESTAMP； 内置函数：查找检查器中注册的类型系列，如果检查器规则允许，则查找系列默认类型； 用户定义函数（UDF）：根据 eval() 方法声明的参数类型进行强制转换； INSERT 和 UPDATE：如果两个字段的类型名称或精度（比例）不同，则将源字段强制转换为对应的目标表字段的类型。 注意： 以下情况的隐式类型强制将被忽略： 其中一个类型是 ANY； CHARACTER 类型中的类型强制始终被忽略，即从 CHAR(20) 到 VARCHAR(30)； 从一个数字到另一个具有更高优先级的类型强制将被忽略，即从 INT 到 LONG。 寻找相同类型的策略 如果操作符有预期的数据类型，则直接将其作为所需类型。（例如，UDF 会有 eval() 方法，该方法有反射参数类型）； 如果没有预期的数据类型，但已注册数据类型系列，则尝试将参数强制转换为系列的默认数据类型，即 String 系列将具有 VARCHAR 类型； 如果既未指定预期的数据类型也未指定系列，则尝试找到节点类型中最紧密的公共类型，即 INTEGER 和 DOUBLE 将返回 DOUBLE，这种情况下数字精度不会丢失； 如果没有找到最紧密的公共类型，则尝试找到更宽的类型，即 VARCHAR 和 INTEGER 将返回 INTEGER，在将小数扩展为小数时，我们允许一些精度损失，或者提升为 VARCHAR 类型。 值构造函数 运算符语法 描述 ROW (value [, value ]*) 根据值列表创建一行。 (value [, value ]* ) 根据值列表创建一行。 row ‘[’ index ‘]’ 返回行中特定位置的元素（从 1 开始的索引）。 row ‘[’ name ‘]’ 返回具有特定名称的行元素。 map ‘[’ key ‘]’ 返回具有特定键的映射元素。 array ‘[’ index ‘]’ 返回数组中特定位置的元素（从 1 开始的索引）。 ARRAY ‘[’ value [, value ]* ‘]’ 根据值列表创建一个数组。 MAP ‘[’ key, value [, key, value ]* ‘]’ 根据键值对列表创建映射。 查询值构造函数 运算符语法 描述 ARRAY (sub-query) 根据子查询的结果创建一个数组。示例：ARRAY(SELECT empno FROM emp ORDER BY empno) MAP (sub-query) 根据键值对子查询的结果创建映射。示例：MAP(SELECT empno, deptno FROM emp) MULTISET (sub-query) 从子查询的结果中创建一个多重集。示例：MULTISET(SELECT empno FROM emp) 集合函数 运算符语法 描述 ELEMENT(value) 返回数组或多集的唯一元素；如果集合为空，则返回 null；如果有多个元素，则抛出。 CARDINALITY(value) 返回数组或多集内的元素数量。 value MEMBER OF multiset 返回 value 是否是 multiset 的成员。 multiset IS A SET multiset 是否是一个集合（没有重复）。 multiset IS NOT A SET multiset 是否不是一个集合（有重复）。 multiset IS EMPTY multiset 是否包含零个元素。 multiset IS NOT EMPTY multiset 是否包含一个或多个元素。 multiset SUBMULTISET OF multiset2 multiset 是否是 multiset2 的子多集。 multiset NOT SUBMULTISET OF multiset2 multiset 是否不是 multiset2 的子多集。 multiset MULTISET UNION [ ALL | DISTINCT ] multiset2 返回并集 multiset 和 multiset2，如果指定了 DISTINCT（ALL 为默认值），则消除重复项。 multiset MULTISET INTERSECT [ ALL | DISTINCT ] multiset2 返回 multiset 和 multiset2 的交集，如果指定了 DISTINCT（ALL 是默认值），则消除重复项。 multiset MULTISET EXCEPT [ ALL | DISTINCT ] multiset2 返回 multiset 和 multiset2 的差异，如果指定了 DISTINCT（ALL 是默认值），则消除重复项。 另请参阅：UNNEST 关系运算符将集合转换为关系。 时间段谓词 运算符语法 描述 period1 CONTAINS datetime period1 包含 datetime period1 CONTAINS period2 period1 包含 period2 period1 OVERLAPS period2 period1 与 period2 重叠 period1 EQUALS period2 period1 等于 period2 period1 PRECEDES period2 period1 早于 period2 period1 IMMEDIATELY PRECEDES period2 period1 早于 period2 并没有间隔 period1 SUCCEEDS period2 period1 晚于 period2 period1 IMMEDIATELY SUCCEEDS period2 period1 晚于 period2 并没有间隔 其中 period1 和 period2 是时间段表达式： period: (datetime, datetime) | (datetime, interval) | PERIOD (datetime, datetime) | PERIOD (datetime, interval) JDBC 函数转义 数字 运算符语法 描述 {fn ABS(numeric)} 返回 numeric 的绝对值 {fn ACOS(numeric)} 返回 numeric 的反余弦 {fn ASIN(numeric)} 返回 numeric 的反正弦值 {fn ATAN(numeric)} 返回 numeric 的反正切 {fn ATAN2(numeric, numeric)} 返回 numeric 坐标的反正切 {fn CBRT(numeric)} 返回 numeric 的立方根 {fn CEILING(numeric)} 将 numeric 向上舍入，并返回大于或等于 numeric 的最小数字 {fn COS(numeric)} 返回 numeric 的余弦 {fn COT(numeric)} 返回 numeric 的余切 {fn DEGREES(numeric)} 将 numeric 从弧度转换为度 {fn EXP(numeric)} 返回 e 的 numeric 次方 {fn FLOOR(numeric)} 将 numeric 向下舍入，并返回小于或等于 numeric 的最大数字 {fn LOG(numeric)} 返回 numeric 的自然对数（底数 e） {fn LOG10(numeric)} 返回 numeric 的以 10 为底的对数 {fn MOD(numeric1, numeric2)} 返回 numeric1 除以 numeric2 的余数（模数），仅当 numeric1 为负数时，结果才为负数 {fn PI()} 返回一个比任何其他值都更接近 pi 的值 {fn POWER(numeric1, numeric2)} 返回 numeric1 的 numeric2 次幂 {fn RADIANS(numeric)} 将 numeric 从度数转换为弧度 {fn RAND(numeric)} 使用 numeric 作为种子值返回随机双精度值 {fn ROUND(numeric1, numeric2)} 将 numeric1 四舍五入为 numeric2 位，保留小数点后一位 {fn SIGN(numeric)} 返回 numeric 的符号 {fn SIN(numeric)} 返回 numeric 的正弦值 {fn SQRT(numeric)} 返回 numeric 的平方根 {fn TAN(numeric)} 返回 numeric 的正切 {fn TRUNCATE(numeric1, numeric2)} 将 numeric1 截断为 numeric2 位，保留小数点后一位 字符串 运算符语法 描述 {fn ASCII(string)} 返回 string 第一个字符的 ASCII 码；如果第一个字符是非 ASCII 字符，则返回其 Unicode 代码点；如果 string 为空，则返回 0 {fn CHAR(integer)} 返回 ASCII 码为 integer % 256 的字符，如果 integer 0，则返回 null {fn CONCAT(character, character)} 返回字符串的连接 {fn INSERT(string1, start, length, string2)} 将 string2 插入到 string1 中的插槽中 {fn LCASE(string)} 返回一个字符串，其中 string 中的所有字母字符都已转换为小写 {fn LENGTH(string)} 返回字符串中的字符数 {fn LOCATE(string1, string2 [, integer])} 返回 string1 在 string2 中第一次出现的位置。除非指定了 integer，否则将从 string2 的开头进行搜索。 {fn LEFT(string, length)} 返回 string 最左边的 length 个字符 {fn LTRIM(string)} 返回删除了前导空格字符的字符串 {fn REPLACE(string, search, replacement)} 返回一个字符串，其中 string 中出现的所有 search 均被 replacement 替换；如果 replacement 为空字符串，则删除出现的 search {fn REVERSE(string)} 返回字符顺序颠倒的字符串 {fn RIGHT(string, length)} 返回 string 最右边的 length 个字符 {fn RTRIM(string)} 返回删除了尾随空格字符的 string {fn SUBSTRING(string, offset, length)} 返回从 string 开始，由 length 个字符组成的字符串，起始于 offset 位置 {fn UCASE(string)} 返回一个字符串，其中 string 中的所有字母字符都已转换为大写 日期/时间 运算符语法 描述 {fn CURDATE()} 相当于 CURRENT_DATE {fn CURTIME()} 相当于 LOCALTIME {fn NOW()} 相当于LOCALTIMESTAMP {fn YEAR(date)} 相当于 EXTRACT(YEAR FROM date)。返回一个整数。 {fn QUARTER(date)} 相当于 EXTRACT(QUARTER FROM date)。返回 1 到 4 之间的整数。 {fn MONTH(date)} 相当于 EXTRACT(MONTH FROM date)。返回 1 到 12 之间的整数。 {fn WEEK(date)} 相当于 EXTRACT(WEEK FROM date)。返回 1 到 53 之间的整数。 {fn DAYOFYEAR(date)} 相当于 EXTRACT(DOY FROM date)。返回 1 到 366 之间的整数。 {fn DAYOFMONTH(date)} 相当于 EXTRACT(DAY FROM date)。返回 1 到 31 之间的整数。 {fn DAYOFWEEK(date)} 相当于 EXTRACT(DOW FROM date)。返回 1 到 7 之间的整数。 {fn HOUR(date)} 相当于 EXTRACT(HOUR FROM date)。返回 0 到 23 之间的整数。 {fn MINUTE(date)} 相当于 EXTRACT(MINUTE FROM date)。返回 0 到 59 之间的整数。 {fn SECOND(date)} 相当于 EXTRACT(SECOND FROM date)。返回 0 到 59 之间的整数。 {fn TIMESTAMPADD(timeUnit, count, datetime)} 将 count timeUnits 的间隔添加到日期时间 {fn TIMESTAMPDIFF(timeUnit, timestamp1, timestamp2)} 从 timestamp2 中减去 timestamp1 并以 timeUnits 形式返回结果 系统 运算符语法 描述 {fn DATABASE()} 相当于 CURRENT_CATALOG {fn IFNULL(value1, value2)} 如果 value1 为空，则返回 value2 {fn USER()} 相当于 CURRENT_USER 转换 运算符语法 描述 {fn CONVERT(value, type)} 将 值 转换为 类型 聚合函数 语法： aggregateCall: agg ( [ ALL | DISTINCT ] value [, value ]* ) [ WITHIN DISTINCT ( expression [, expression ]* ) ] [ WITHIN GROUP ( ORDER BY orderItem [, orderItem ]* ) ] [ FILTER ( WHERE condition ) ] | agg ( * ) [ FILTER (WHERE condition) ] 其中 agg 是下表中的运算符之一，或者是用户定义的聚合函数。 如果存在 FILTER，则聚合函数仅考虑条件计算结果为 TRUE 的行。 如果存在 DISTINCT，则重复的参数值在传递给聚合函数之前会被消除。 如果存在 WITHIN DISTINCT，则在传递给聚合函数之前，参数值在指定键的每个值内都会有所不同。 如果存在 WITHIN GROUP，则聚合函数会在聚合值之前根据 WITHIN GROUP 内的 ORDER BY 子句对输入行进行排序。WITHIN GROUP 仅允许用于假设集合函数（RANK、DENSE_RANK、PERCENT_RANK 和 CUME_DIST）、逆分布函数（PERCENTILE_CONT 和 PERCENTILE_DISC）和集合函数（COLLECT 和 LISTAGG）。 运算符语法 描述 ANY_VALUE( [ ALL | DISTINCT ] value) 返回所有输入值中的一个值；这在 SQL 标准中没有指定 ARG_MAX(value, comp) 返回组中 comp 的最大值 ARG_MIN(value, comp) 返回组中 comp 的最小值 APPROX_COUNT_DISTINCT(value [, value ]*) 返回 value 的不同值的近似数量；数据库可以使用近似值，但不需要 AVG( [ ALL | DISTINCT ] numeric) 返回所有输入值的平均值（算术平均值） BIT_AND( [ ALL | DISTINCT ] value) 返回所有非空输入值的按位与，如果没有则返回空；支持整数和二进制类型 BIT_OR( [ ALL | DISTINCT ] value) 返回所有非空输入值的按位或，如果没有则返回空；支持整数和二进制类型 BIT_XOR( [ ALL | DISTINCT ] value) 返回所有非空输入值的按位异或，若无则返回空；支持整数和二进制类型 COLLECT( [ ALL | DISTINCT ] value) 返回值的多集 COUNT(*) 返回输入行的数量 COUNT( [ ALL | DISTINCT ] value [, value ]*) 返回值不为空的输入行数（如果值为复合值，则完全不为空） COVAR_POP(numeric1, numeric2) 返回所有输入值对 (numeric1, numeric2) 的总体协方差 COVAR_SAMP(numeric1, numeric2) 返回所有输入值对 (numeric1, numeric2) 的样本协方差 EVERY(condition) 如果条件的所有值都为 TRUE，则返回 TRUE FUSION(multiset) 返回所有输入值的多重集的多重集并集 INTERSECTION(multiset) 返回所有输入值的多重集的多重集交集 LISTAGG( [ ALL | DISTINCT ] value [, separator]) 返回连接成字符串的值，以分隔符分隔（默认为‘，’） MAX( [ ALL | DISTINCT ] value) 返回所有输入值中的最大值 MIN( [ ALL | DISTINCT ] value) 返回所有输入值中的最小值 MODE(value) 返回所有输入值中出现频率最高的值 REGR_COUNT(numeric1, numeric2) 返回依赖表达式和独立表达式均不为空的行数 REGR_SXX(numeric1, numeric2) 返回线性回归模型中因变量表达式的平方和 REGR_SYY(numeric1, numeric2) 返回线性回归模型中独立表达式的平方和 SOME(condition) 如果条件中的一个或多个值为 TRUE，则返回 TRUE STDDEV( [ ALL | DISTINCT ] numeric) STDDEV_SAMP 的同义词 STDDEV_POP( [ ALL | DISTINCT ] numeric) 返回所有输入值的总体标准差 STDDEV_SAMP( [ ALL | DISTINCT ] numeric) 返回所有输入值的数字样本标准差 SUM( [ ALL | DISTINCT ] numeric) 返回所有输入值的数字总和 VAR_POP( [ ALL | DISTINCT ] value) 返回所有输入值的总体方差（总体标准差的平方） VAR_SAMP( [ ALL | DISTINCT ] numeric) 返回所有输入值的样本方差（样本标准差的平方） 未实现的： REGR_AVGX(numeric1, numeric2) REGR_AVGY(numeric1, numeric2) REGR_INTERCEPT(numeric1, numeric2) REGR_R2(numeric1, numeric2) REGR_SLOPE(numeric1, numeric2) REGR_SXY(numeric1, numeric2) 有序集聚合函数 语法与 aggregateCall 相同，但需要 WITHIN GROUP。 例如下面的： 分数 是 0 到 1 之间的数字文字（包括 0 和 1），代表百分比； 运算符语法 描述 PERCENTILE_CONT(fraction) WITHIN GROUP (ORDER BY orderItem) 根据列值的连续分布返回百分位数，如果需要，则在相邻的输入项之间进行插值 PERCENTILE_DISC(fraction) WITHIN GROUP (ORDER BY orderItem [, orderItem ]*) 根据列值的离散分布返回百分位数，返回排序中位置等于或超过指定分数的第一个输入值 窗口函数 语法： windowedAggregateCall: agg ( [ ALL | DISTINCT ] value [, value ]* ) [ RESPECT NULLS | IGNORE NULLS ] [ WITHIN GROUP ( ORDER BY orderItem [, orderItem ]* ) ] [ FILTER ( WHERE condition ) ] OVER window | agg ( * ) [ FILTER ( WHERE condition ) ] OVER window 其中 agg 是下表中的运算符之一，或者是用户定义的聚合函数。 DISTINCT、FILTER 和 WITHIN GROUP 与聚合函数的描述一致。 运算符语法 描述 COUNT(value [, value ]*) OVER window 返回 window 中 value 不为空的行数（如果 value 是复合的，则完全不为空） COUNT(*) OVER window 返回 window 中的行数 AVG(numeric) OVER window 返回 window 中所有值的 numeric 的平均值（算术平均值） SUM(numeric) OVER window 返回 window 中所有值的 numeric 之和 MAX(value) OVER window 返回 window 中所有值中 value 的最大值 MIN(value) OVER window 返回 window 中所有值中 value 的最小值 RANK() OVER window 返回当前行的排名（有间隙）；与其第一个对等行的 ROW_NUMBER 相同 DENSE_RANK() OVER window 返回当前行的排名，无间隙；该函数计算同组 ROW_NUMBER() OVER window 返回分区内当前行的编号，从 1 开始计数 FIRST_VALUE(value) OVER window 返回在窗口框架第一行计算的值 LAST_VALUE(value) OVER window 返回在窗口框架最后一行计算的值 LEAD(value, offset, default) OVER window 返回在分区内当前行之后 offset 行处求值的 value；如果没有这样的行，则返回 default。offset 和 default 都是针对当前行求值的。如果省略，offset 默认为 1，default 默认为 NULL LAG(value, offset, default) OVER window 返回在分区内当前行之前 offset 行处求值的 value；如果没有这样的行，则返回 default。offset 和 default 都是针对当前行求值的。如果省略，offset 默认为 1，default 默认为 NULL NTH_VALUE(value, nth) OVER window 返回在窗口框架的第 n 行计算的值 NTILE(value) OVER window 返回从 1 到 value 的整数，尽可能均匀地划分分区 注意： 你可以为 FIRST_VALUE、LAST_VALUE、NTH_VALUE、LEAD 和 LAG 函数指定空处理（IGNORE NULLS、RESPECT NULLS）。语法由解析器处理，但只有 RESPECT NULLS 在运行时实现。 未实现： COUNT(DISTINCT value [, value ]*) OVER window APPROX_COUNT_DISTINCT(value [, value ]*) OVER window PERCENT_RANK(value) OVER window CUME_DIST(value) OVER window 分组函数 运算符语法 描述 GROUPING(expression [, expression ]*) 返回给定分组表达式的位向量 GROUP_ID() 返回唯一标识分组键组合的整数 GROUPING_ID(expression [, expression ]*) GROUPING 的同义词 描述符 运算符语法 描述 DESCRIPTOR(name [, name ]*) DESCRIPTOR 作为函数中的参数出现，表示名称列表。名称的解释留给函数。 表函数 表函数出现在 FROM 子句中。 表函数可能具有通用表参数（即，创建表函数时未声明任何行类型），并且结果的行类型可能取决于输入表的行类型。此外，输入表按三个特征分类。第一个特征是语义。输入表具有行语义或集合语义，如下所示： 行语义意味着表函数的结果依赖于逐行； 集合语义意味着函数的结果取决于数据的分区方式。 第二个特性仅适用于具有集合语义的输入表，即即使输入表为空，表函数是否可以生成结果行。 如果表函数可以在空输入时生成结果行，则该表被称为空时保留； 另一种选择是空时修剪，这意味着如果输入表为空，结果将被修剪掉。 第三个特征是输入表是否支持传递列。传递列是一种机制，允许表函数将输入行的每一列复制到输出行的列中。 具有集合语义的输入表可以按一列或多列进行分区。具有集合语义的输入表可以按一列或多列进行排序。 注意： 具有行语义的输入表可能未被分区或排序； 多态表函数可能有多个输入表。但是，最多只有一个输入表可以具有行语义。 TUMBLE 在流式查询中，TUMBLE 根据时间戳列为关系的每一行分配一个窗口。分配的窗口由其开始和结束指定。所有分配的窗口都具有相同的长度，这就是为什么翻转有时被称为“固定窗口”。TUMBLE 表函数的第一个参数是通用表参数。输入表具有行语义并支持传递列。 运算符语法 描述 TUMBLE(data, DESCRIPTOR(timecol), size [, offset ]) 表示 timecol 大小间隔的滚动窗口，可选择在偏移处对齐。 下面是一个例子： SELECT * FROM TABLE( TUMBLE( TABLE orders, DESCRIPTOR(rowtime), INTERVAL 1 MINUTE));-- or with the named params-- note: the DATA param must be the firstSELECT * FROM TABLE( TUMBLE( DATA = TABLE orders, TIMECOL = DESCRIPTOR(rowtime), SIZE = INTERVAL 1 MINUTE)); 将一分钟范围的滚动窗口应用于订单表中的行。rowtime 是订单表的水印列，用于告知数据是否完整。 HOP 在流式查询中，HOP 会分配覆盖大小间隔内的行的窗口，并根据时间戳列移动每个滑动窗口。分配的窗口可能会重叠，因此有时跳跃被称为滑动窗口。HOP 表函数的第一个参数是通用表参数。输入表具有行语义并支持传递列。 运算符语法 描述 HOP(data, DESCRIPTOR(timecol), slide, size [, offset ]) 表示 timecol 的跳跃窗口，覆盖 size 间隔内的行，移动每个幻灯片并可选择在偏移处对齐。 下面是一个例子： SELECT * FROM TABLE( HOP( TABLE orders, DESCRIPTOR(rowtime), INTERVAL 2 MINUTE, INTERVAL 5 MINUTE));-- or with the named params-- note: the DATA param must be the firstSELECT * FROM TABLE( HOP( DATA = TABLE orders, TIMECOL = DESCRIPTOR(rowtime), SLIDE = INTERVAL 2 MINUTE, SIZE = INTERVAL 5 MINUTE)); 对订单表的行应用 5 分钟间隔大小的跳跃，每 2 分钟移动一次。rowtime 是订单表的水印列，用于指示数据的完整性。 SESSION 在流式查询中，SESSION 根据日期时间分配覆盖行的窗口。在会话窗口内，行之间的距离小于间隔。会话窗口按键应用。SESSION 表函数的第一个参数是通用表参数。输入表具有设置语义并支持传递列。此外，如果输入表为空，SESSION 表函数将不会生成结果行。 运算符语法 描述 session(data, DESCRIPTOR(timecol), DESCRIPTOR(key), size) 表示 timecol 大小为 interval 的会话窗口。会话窗口按键应用。 下面是一个例子： SELECT * FROM TABLE( SESSION( TABLE orders PARTITION BY product, DESCRIPTOR(rowtime), INTERVAL 20 MINUTE));-- or with the named params-- note: the DATA param must be the firstSELECT * FROM TABLE( SESSION( DATA = TABLE orders PARTITION BY product, TIMECOL = DESCRIPTOR(rowtime), SIZE = INTERVAL 20 MINUTE)); 对订单表中的行应用具有 20 分钟非活动间隔的会话。rowtime 是订单表中带水印的列，用于指示数据的完整性。会话按产品应用。 注意：Tumble、Hop 和 Session 窗口表函数将原始表中的每一行分配给一个窗口。输出表具有与原始表相同的所有列，以及两个附加列 window_start 和 window_end，分别表示窗口间隔的开始和结束。 分组窗口函数 警告：分组窗口函数已被弃用。 分组窗口函数出现在 GROUP BY 子句中，并定义一个表示包含多行的窗口的键值。 在某些窗口函数中，一行可能属于多个窗口。例如，如果使用 HOP(t, INTERVAL '2' HOUR, INTERVAL '1' HOUR) 对查询进行分组，则带有时间戳 10:15:00 的行将同时出现在 10:00 - 11:00 和 11:00 - 12:00 总计中。 运算符语法 描述 HOP(datetime, slide, size [, time ]) 表示日期时间的跳跃窗口，覆盖大小间隔内的行，移动每张幻灯片，并可选择在时间上对齐 SESSION(datetime, interval [, time ]) 表示日期时间间隔的会话窗口，可选择在时间上对齐 TUMBLE(datetime, interval [, time ]) 表示日期时间间隔的滚动窗口，可选择在时间上对齐 分组辅助函数 分组辅助函数允许您访问由分组窗口函数定义的窗口的属性。 运算符语法 描述 HOP_END(expression, slide, size [, time ]) 返回由 HOP 函数调用定义的窗口末尾的 expression 的值 HOP_START(expression, slide, size [, time ]) 返回由 HOP 函数调用定义的窗口开头的 expression 的值 SESSION_END(expression, interval [, time]) 返回由 SESSION 函数调用定义的窗口末尾的 expression 的值 SESSION_START(expression, interval [, time]) 返回由 SESSION 函数调用定义的窗口开头的 expression 的值 TUMBLE_END(expression, interval [, time ]) 返回由 TUMBLE 函数调用定义的窗口末尾的 expression 的值 TUMBLE_START(expression, interval [, time ]) 返回由 TUMBLE 函数调用定义的窗口开头的 expression 的值 空间函数 在以下内容中： geom 是一个 GEOMETRY； geomCollection 是一个 GEOMETRYCOLLECTION； point 是一个 POINT； lineString 是一个 LINESTRING； iMatrix 是 DE-9IM 交叉矩阵； distance、tolerance、segmentLengthFraction、offsetDistance 都是 double 类型； dimension、quadSegs、srid、zoom 是整数类型； layerType是一个字符串； gml 是包含地理标记语言 (GML) 的字符串； wkt 是包含 众所周知的文本 (WKT) 的字符串； wkb 是包含 众所周知的二进制 (WKB) 的二进制字符串。 在 C（代表兼容性）列中，o 表示该函数实现了 SQL 的 OpenGIS 简单功能实现规范，版本 1.2.1；p 表示该函数是 OpenGIS 的 PostGIS 扩展；h 表示该函数是 H2GIS 扩展。 TODO 几何转换函数（2D） C （兼容性） 运算符语法 描述 p ST_AsBinary(geom) ST_AsWKB 的同义词 p ST_AsEWKB(geom) ST_AsWKB 的同义词 p ST_AsEWKT(geom) 转换 GEOMETRY → EWKT p ST_AsGeoJSON(geom) 转换 GEOMETRY → GeoJSON p ST_AsGML(geom) 转换 GEOMETRY → GML p ST_AsText(geom) ST_AsWKT 的同义词 o ST_AsWKB(geom) 转换 GEOMETRY → WKB o ST_AsWKT(geom) 转换 GEOMETRY → WKT o ST_Force2D(geom) 3D GEOMETRY → 2D GEOMETRY o ST_GeomFromEWKB(wkb [, srid ]) ST_GeomFromWKB 的同义词 o ST_GeomFromEWKT(wkb [, srid ]) 转换 WKT → GEOMETRY o ST_GeomFromGeoJSON(json) 转换 GeoJSON → GEOMETRY o ST_GeomFromGML(wkb [, srid ]) 转换 GML → GEOMETRY o ST_GeomFromText(wkt [, srid ]) ST_GeomFromWKT 的同义词 o ST_GeomFromWKB(wkb [, srid ]) 转换 WKB → GEOMETRY o ST_GeomFromWKT(wkb [, srid ]) 转换 WKT → GEOMETRY o ST_LineFromText(wkt [, srid ]) 转换 WKT → LINESTRING o ST_LineFromWKB(wkt [, srid ]) 转换 WKT → LINESTRING o ST_MLineFromText(wkt [, srid ]) 转换 WKT → MULTILINESTRING o ST_MPointFromText(wkt [, srid ]) 转换 WKT → MULTIPOINT o ST_MPolyFromText(wkt [, srid ]) Converts WKT → MULTIPOLYGON o ST_PointFromText(wkt [, srid ]) 转换 WKT → POINT o ST_PointFromWKB(wkt [, srid ]) 转换 WKB → POINT o ST_PolyFromText(wkt [, srid ]) 转换 WKT → POLYGON o ST_PolyFromWKB(wkt [, srid ]) 转换 WKB → POLYGON p ST_ReducePrecision(geom, gridSize) 将 geom 的精度降低至提供的 gridSize h ST_ToMultiPoint(geom) 将 geom 的坐标（可能是 GEOMETRYCOLLECTION）转换为 MULTIPOINT h ST_ToMultiLine(geom) 将 geom 的坐标（可能是 GEOMETRYCOLLECTION）转换为 MULTILINESTRING h ST_ToMultiSegments(geom) 将 geom（可能是 GEOMETRYCOLLECTION）转换为存储在 MULTILINESTRING 中的一组不同段 未实现： ST_GoogleMapLink(geom [, layerType [, zoom ]]) GEOMETRY → Google map link ST_OSMMapLink(geom [, marker ]) GEOMETRY → OSM map link 几何转换函数（3D） C（兼容性） 运算符语法 描述 o ST_Force3D(geom) 2D GEOMETRY → 3D GEOMETRY 几何创建函数（2D） C（兼容性） 运算符语法 描述 h ST_BoundingCircle(geom) 返回 geom 的最小边界圆 h ST_Expand(geom, distance) 扩展 geom 的包络 h ST_Expand(geom, deltaX, deltaY) 扩展 geom 的包络 h ST_MakeEllipse(point, width, height) 构造一个椭圆 p ST_MakeEnvelope(xMin, yMin, xMax, yMax [, srid ]) 创建一个矩形 POLYGON h ST_MakeGrid(geom, deltaX, deltaY) 根据 geom 计算多边形的规则网格 h ST_MakeGridPoints(geom, deltaX, deltaY) 根据 geom 计算规则的点网格 o ST_MakeLine(point1 [, point ]*) 根据给定的 POINT（或 MULTIPOINT）创建线串 p ST_MakePoint(x, y [, z ]) ST_Point 的同义词 p ST_MakePolygon(lineString [, hole ]*) 使用给定的孔（需要封闭的 LINESTRING）从 lineString 创建 POLYGON h ST_MinimumDiameter(geom) 返回 geom 的最小直径 h ST_MinimumRectangle(geom) 返回包围 geom 的最小矩形 h ST_OctogonalEnvelope(geom) 返回 geom 的八边形包络 o ST_Point(x, y [, z ]) 根据两个或三个坐标构造一个点 未实现： ST_RingBuffer(geom, distance, bufferCount [, endCapStyle [, doDifference]]) 返回以 geom 为中心且缓冲区大小不断增加的 MULTIPOLYGON 缓冲区。 几何创建函数（3D） 未实现： ST Extrude(from, height [, flag]) 挤出几何图形； ST_Geometry Shadow(geom_point, height) 计算 geom 的阴影足迹； ST_GeometryShadow(geom, azimuth, elevation, height [, unify ]) 计算 geom 的阴影覆盖范围。 几何属性（2D） C（兼容性） 运算符语法 描述 o ST_Boundary(geom [, srid ]) 返回 geom 的边界 o ST_Centroid(geom) 返回 geom 的质心 o ST_CoordDim(geom) 返回 geom 坐标的维度 o ST_Dimension(geom) 返回 geom 的维度 o ST_Distance(geom1, geom2) 返回 geom1 和 geom2 之间的距离 h ST_ExteriorRing(geom) 返回 geom 的外环，如果 geom 不是多边形，则返回 null o ST_GeometryType(geom) 返回 geom 的类型 o ST_GeometryTypeCode(geom) 返回 geom 的 OGC SFS 类型代码 p ST_EndPoint(lineString) 返回 geom 的最后一个坐标 o ST_Envelope(geom [, srid ]) 返回 geom 的包络（可能是 GEOMETRYCOLLECTION）作为 GEOMETRY o ST_Extent(geom) Returns the minimum bounding box of geom (which may be a GEOMETRYCOLLECTION) h ST_GeometryN(geomCollection, n) 返回 geomCollection 的第 n 个 GEOMETRY h ST_InteriorRingN(geom) 返回 geom 的第 n 个内环，如果 geom 不是多边形，则返回 null h ST_IsClosed(geom) 返回 geom 是否为封闭的 LINESTRING 或 MULTILINESTRING o ST_IsEmpty(geom) 返回 geom 是否为空 o ST_IsRectangle(geom) 返回 geom 是否为矩形 h ST_IsRing(geom) 返回 geom 是否为封闭的简单线串或 MULTILINESTRING o ST_IsSimple(geom) 返回 geom 是否简单 o ST_IsValid(geom) 返回 geom 是否有效 h ST_NPoints(geom) 返回 geom 中的点数 h ST_NumGeometries(geom) 返回 geom 中的几何图形数量（如果不是 GEOMETRYCOLLECTION，则返回 1） h ST_NumInteriorRing(geom) ST_NumInteriorRings 的同义词 h ST_NumInteriorRings(geom) 返回 geom 的内部环的数量 h ST_NumPoints(geom) 返回 geom 中的点数 p ST_PointN(geom, n) 返回 geom 的第 n 个点 p ST_PointOnSurface(geom) 返回 geom 的内部或边界点 o ST_SRID(geom) 返回 geom 的 SRID 值，如果没有则返回 0 p ST_StartPoint(geom) 返回 geom 的第一个点 o ST_X(geom) 返回 geom 第一个坐标的 x 值 o ST_XMax(geom) 返回 geom 的最大 x 值 o ST_XMin(geom) 返回 geom 的最小 x 值 o ST_Y(geom) 返回 geom 第一个坐标的 y 值 o ST_YMax(geom) 返回 geom 的最大 y 值 o ST_YMin(geom) 返回 geom 的最小 y 值 未实现： ST_CompactnessRatio(polygon) 返回 polygon 面积除以其周长等于其周长的圆的面积的平方根； ST_Explode(query [, field Name]) 将查询的 field Name 列中的GEOMETRY COLLECTIONs分解为多个几何图形； ST_IsValidDetail(geom [, selfTouchValid ]) 返回有效细节作为对象数组； ST_IsValidReason(geom [, selfTouchValid ]) 返回文本说明 geom 是否有效，如果无效，则说明原因。 几何属性（3D） C（兼容性） 运算符语法 描述 p ST_Is3D(s) 返回 geom 是否至少有一个 z 坐标 o ST_Z(geom) 返回 geom 第一个坐标的 z 值 o ST_ZMax(geom) 返回 geom 的最大 z 值 o ST_ZMin(geom) 返回 geom 的最小 z 值 几何谓词 C（兼容性） 运算符语法 描述 o ST_Contains(geom1, geom2) 返回 geom1 是否包含 geom2 p ST_ContainsProperly(geom1, geom2) 返回 geom1 是否包含 geom2 但与其边界不相交 p ST_CoveredBy(geom1, geom2) 返回 geom1 中是否没有点位于 geom2 之外。 p ST_Covers(geom1, geom2) 返回 geom2 中是否没有点位于 geom1 之外 o ST_Crosses(geom1, geom2) 返回 geom1 是否与 geom2 相交 o ST_Disjoint(geom1, geom2) 返回 geom1 和 geom2 是否不相交 p ST_DWithin(geom1, geom2, distance) 返回 geom1 和 geom 是否在彼此的 距离 范围内 o ST_EnvelopesIntersect(geom1, geom2) 返回 geom1 的包络线是否与 geom2 的包络线相交 o ST_Equals(geom1, geom2) 返回 geom1 是否等于 geom2 o ST_Intersects(geom1, geom2) 返回 geom1 是否与 geom2 相交 o ST_Overlaps(geom1, geom2) 返回 geom1 是否与 geom2 重叠 o ST_Relate(geom1, geom2) 返回 geom1 和 geom2 的 DE-9IM 交集矩阵 o ST_Relate(geom1, geom2, iMatrix) 返回 geom1 和 geom2 是否通过给定的交集矩阵 iMatrix 相关 o ST_Touches(geom1, geom2) 返回 geom1 是否接触 geom2 o ST_Within(geom1, geom2) 返回 geom1 是否在 geom2 内 未实现： ST_OrderingEquals(geom1, geom2) 返回 geom1 是否等于 geom2 以及它们的坐标和组件几何图形以相同的顺序列出。 几何运算符（2D） 以下函数组合了 2D 几何图形。 C（兼容性） 运算符语法 描述 p ST_Buffer(geom, distance [, quadSegs, endCapStyle ]) 计算 geom 周围的缓冲区 p ST_Buffer(geom, distance [, bufferStyle ]) 计算 geom 周围的缓冲区 o ST_ConvexHull(geom) 计算包含 geom 中所有点的最小凸多边形 o ST_Difference(geom1, geom2) 计算两个几何图形之间的差异 o ST_SymDifference(geom1, geom2) 计算两个几何体之间的对称差异 o ST_Intersection(geom1, geom2) 计算 geom1 和 geom2 的交集 p ST_OffsetCurve(geom, distance, bufferStyle) 计算 linestring 的偏移线 o ST_Union(geom1, geom2) 计算 geom1 和 geom2 的并集 o ST_Union(geomCollection) 计算 geomCollection 中几何图形的并集 另请参考：ST_Union 聚合函数。 仿射变换函数（3D 和 2D） 以下函数可变换 2D 几何图形。 C（兼容性） 运算符语法 描述 o ST_Rotate(geom, angle [, origin | x, y]) 将 geom 绕 origin （或点 (x, y)）逆时针旋转 angle （以弧度为单位） o ST_Scale(geom, xFactor, yFactor) 通过将纵坐标乘以指定的比例因子来缩放 geom o ST_Translate(geom, x, y) 将 geom 平移向量 (x, y) 未实现： ST_Scale(geom, xFactor, yFactor [, zFactor ]) 通过将纵坐标乘以指定的比例因子来缩放 geom； ST_Translate(geom, x, y, [, z]) 平移 geom。 几何编辑功能（2D） 以下函数可修改 2D 几何图形。 C（兼容性） 运算符语法 描述 p ST_AddPoint(linestring, point [, index]) 将 point 添加到给定 index 处的 linestring（如果未指定 index，则添加到末尾） h ST_Densify(geom, tolerance) 通过沿线段插入额外的顶点来加密 geom h ST_FlipCoordinates(geom) 翻转 geom 的 X 和 Y 坐标 h ST_Holes(geom) 返回 geom 中的孔（可能是 GEOMETRYCOLLECTION） h ST_Normalize(geom) 将 geom 转换为正常形式 p ST_RemoveRepeatedPoints(geom [, tolerance]) 从 geom 中删除重复的坐标 h ST_RemoveHoles(geom) 移除 geom 的孔 p ST_RemovePoint(linestring, index) 删除 linestring 中给定 index 处的 point h ST_Reverse(geom) 反转 geom 坐标的顺序 未实现： ST_CollectionExtract(geom, dimension) 过滤 geom，返回具有给定 dimension (1 = point, 2 = line-string, 3 = polygon) 的成员的多重几何图形。 几何编辑功能（3D） 以下函数可修改 3D 几何图形。 C（兼容性） 运算符语法 描述 H ST_AddZ(geom, zToAdd) 将 zToAdd 添加到 geom 的 z 坐标 未实现： ST_Interpolate3DLine(geom) 返回带有 z 值插值的 geom，如果它不是线串或 MULTILINESTRING，则返回 null； ST_MultiplyZ(geom, zFactor) 返回 geom 的 z 值乘以 zFactor； ST_Reverse3DLine(geom [, sortOrder ]) 可能会根据其第一个和最后一个坐标的 z 值反转 geom； ST_UpdateZ(geom, newZ [, updateCondition ]) 更新 geom 的 z 值； ST_ZUpdateLineExtremities(geom, startZ, endZ [, interpolate ]) 更新 geom 的起始和终止 z 值。 几何测量功能（2D） 以下函数测量几何形状。 C（兼容性） 运算符语法 描述 o ST_Area(geom) 返回 geom 的面积（可能是 GEOMETRYCOLLECTION） h ST_ClosestCoordinate(point, geom) 返回最接近 point 的 geom 坐标 h ST_ClosestPoint(geom1, geom2) 返回 geom1 最接近 geom2 的点 h ST_FurthestCoordinate(geom, point) 返回距离点最远的几何的坐标 h ST_Length(geom) 返回 geom 的长度 h ST_LocateAlong(geom, segmentLengthFraction, offsetDistance) 返回一个 MULTIPOINT，其中包含位于 geom 线段 segmentLengthFraction 和 offsetDistance 处的点 h ST_LongestLine(geom1, geom2) 返回 geom1 和 geom2 点之间的二维最长线串 h ST_MaxDistance(geom1, geom2) 计算 geom1 和 geom2 之间的最大距离 h ST_Perimeter(polygon) 返回 polygon (可能是 MULTIPOLYGON) 的周长长度 h ST_ProjectPoint(point, lineString) 将 point 投影到 lineString 上（可能是 MULTILINESTRING） 几何测量功能（3D） 未实现： ST_3DArea(geom) 返回多边形的三维面积； ST_3DLength(geom) 返回线串的 3D 长度； ST_3DPerimeter(geom) 返回多边形或MULTIPOLYGON的三维周长； ST_SunPosition(point [, timestamp ]) 计算 point 和 timestamp 处的太阳位置（现在默认）。 几何处理功能（2D） 以下函数处理几何图形。 C（兼容性） 运算符语法 描述 o ST_LineMerge(geom) 合并线性组件的集合以形成最大长度的线串 o ST_MakeValid(geom) 将给定的无效几何图形转换为有效几何图形 o ST_Polygonize(geom) 从 geom 的边缘创建一个 MULTIPOLYGON o ST_PrecisionReducer(geom, n) 将 geom 的精度降低至 n 位小数 o ST_Simplify(geom, distance) 使用 Douglas-Peuker 算法 和 distance 容差来简化 geom o ST_SimplifyPreserveTopology(geom, distance) 简化 geom，保留其拓扑结构 o ST_Snap(geom1, geom2, tolerance) 将 geom1 和 geom2 对齐在一起 p ST_Split(geom, blade) 通过 blade 分割 geom 未实现： ST_LineIntersector(geom1, geom2) 将 geom1 （线串）与 geom2 分割； ST_LineMerge(geom) 合并线性组件的集合以形成最大长度的线串； ST_MakeValid(geom [, retainGeomDim [, retainDuplicateCoord [, retainCoordDim]]]) 使 geom 有效； ST_RingSideBuffer(geom, distance, bufferCount [, endCapStyle [, doDifference]]) 计算一侧的环形缓冲区； ST_SideBuffer(geom, distance [, bufferStyle ]) 计算一侧的单个缓冲区。 几何投影函数 由于 EPSG 数据集的使用条款有限制，因此它与 Proj4J 分开发布。为了使用 Apache Calcite 中的投影函数，用户必须在其依赖项中包含 EPSG 数据集。 C（兼容性） 运算符语法 描述 o ST_SetSRID(geom, srid) 返回具有新 SRID 的 geom 副本 o ST_Transform(geom, srid) 将 geom 从一个坐标参考系统 (CRS) 转换为 srid 指定的 CRS 三角函数 未实现： ST_Azimuth(point1, point2) 返回从 point1 到 point2 的线段的方位角。 地形函数 未实现： ST_TriangleAspect(geom) 返回三角形的纵横比； ST_TriangleContouring(query [, z1, z2, z3 ][, varArgs ]*) 将三角形按类别分割成更小的三角形； ST_TriangleDirection(geom) 计算三角形的最陡上升方向并将其以线串形式返回； ST_TriangleSlope(geom) 以百分比形式计算三角形的斜率； ST_Voronoi(geom [, outDimension [, envelopePolygon ]]) 创建 Voronoi 图。 三角测量函数 C（兼容性） 运算符语法 描述 h ST_ConstrainedDelaunay(geom [, flag]) 根据 geom 计算受约束的 Delaunay 三角剖分 h ST_Delaunay(geom [, flag]) 根据 geom 中的点计算 Delaunay 三角剖分 未实现： ST_Tessellate(polygon) 使用自适应三角形对多边形 (可能是 MULTIPOLYGON) 进行镶嵌。 几何聚合函数 C（兼容性） 运算符语法 描述 h ST_Accum(geom) 将 geom 累积到数组中 h ST_Collect(geom) 将 geom 收集到 GeometryCollection 中 h ST_Union(geom) 计算 geom 中几何的并集 JSON 函数 在以下内容中： jsonValue 是包含 JSON 值的字符串； path 是包含 JSON 路径表达式的字符串；path 的开头应指定模式标志 strict 或 lax。 查询函数 运算符语法 描述 JSON_EXISTS(jsonValue, path [ { TRUE | FALSE | UNKNOWN | ERROR } ON ERROR ] ) jsonValue 是否满足使用 JSON 路径表达式 path 描述的搜索条件 JSON_VALUE(jsonValue, path [ RETURNING type ] [ { ERROR | NULL | DEFAULT expr } ON EMPTY ] [ { ERROR | NULL | DEFAULT expr } ON ERROR ] ) 使用 JSON 路径表达式 path 从 jsonValue 中提取 SQL 标量 JSON_QUERY(jsonValue, path [ { WITHOUT [ ARRAY ] | WITH [ CONDITIONAL | UNCONDITIONAL ] [ ARRAY ] } WRAPPER ] [ { ERROR | NULL | EMPTY ARRAY | EMPTY OBJECT } ON EMPTY ] [ { ERROR | NULL | EMPTY ARRAY | EMPTY OBJECT } ON ERROR ] ) 使用 path JSON 路径表达式从 jsonValue 中提取 JSON 对象或 JSON 数组 注意： ON ERROR 和 ON EMPTY 子句定义当抛出错误或即将返回空值时函数的回退行为； ARRAY WRAPPER 子句定义如何在 JSON_QUERY 函数中表示 JSON 数组结果。以下示例比较了包装器行为。 示例数据： a: [1,2], b: [1,2], c: hi 比较： 运算符语法 $.A $.B $.C JSON_VALUE [1, 2] error hi JSON QUERY WITHOUT ARRAY WRAPPER error [1, 2] error JSON QUERY WITH UNCONDITIONAL ARRAY WRAPPER [ “[1,2]” ] [ [1,2] ] [ “hi” ] JSON QUERY WITH CONDITIONAL ARRAY WRAPPER [ “[1,2]” ] [1,2] [ “hi” ] 未实现： JSON_TABLE 构造函数 运算符语法 描述 JSON_OBJECT( jsonKeyVal [, jsonKeyVal ]* [ nullBehavior ] ) 使用一系列键值对构造 JSON 对象 JSON_OBJECTAGG( jsonKeyVal [ nullBehavior ] ) 使用键值对构建 JSON 对象的聚合函数 JSON_ARRAY( [ jsonVal [, jsonVal ]* ] [ nullBehavior ] ) 使用一系列值构造 JSON 数组 JSON_ARRAYAGG( jsonVal [ ORDER BY orderItem [, orderItem ]* ] [ nullBehavior ] ) 使用值构建 JSON 数组的聚合函数 jsonKeyVal: [ KEY ] name VALUE value [ FORMAT JSON ] | name : value [ FORMAT JSON ]jsonVal: value [ FORMAT JSON ]nullBehavior: NULL ON NULL | ABSENT ON NULL 注意： 标志 FORMAT JSON 表示值被格式化为 JSON 字符串。当使用 FORMAT JSON 时，该值应该从 JSON 字符串解析为 SQL 结构化值； ON NULL 子句定义 JSON 输出如何表示空值。JSON_OBJECT 和 JSON_OBJECTAGG 的默认空行为是 NULL ON NULL，而对于 JSON_ARRAY 和 JSON_ARRAYAGG，则为 ABSENT ON NULL； 如果提供了 ORDER BY 子句，JSON_ARRAYAGG 会在执行聚合之前将输入行按指定顺序排序。 比较运算符 运算符语法 描述 jsonValue IS JSON [ VALUE ] jsonValue 是否为 JSON 值 jsonValue IS NOT JSON [ VALUE ] jsonValue 是否不是 JSON 值 jsonValue IS JSON SCALAR jsonValue 是否是 JSON 标量值 jsonValue IS NOT JSON SCALAR jsonValue 是否不是 JSON 标量值 jsonValue IS JSON OBJECT jsonValue 是否为 JSON 对象 jsonValue IS NOT JSON OBJECT jsonValue 是否不是 JSON 对象 jsonValue IS JSON ARRAY jsonValue 是否为 JSON 数组 jsonValue IS NOT JSON ARRAY jsonValue 是否不是 JSON 数组 特定方言运算符 以下运算符不在 SQL 标准中，并且未在 Calcite 的默认运算符表中启用。仅当您的会话启用了额外的运算符表时，它们才可用于查询。 要启用操作员表，请设置 fun 连接字符串参数。 “C”（兼容性）列包含值： ‘*’ 代表所有库， ‘b’ 代表 Google BigQuery（连接字符串中为 ‘fun=bigquery’）， ‘c’ 代表 Apache Calcite（连接字符串中为 ‘fun=calcite’）， ‘h’ 代表 Apache Hive（连接字符串中为 ‘fun=hive’）， ‘m’ 代表 MySQL（连接字符串中为 ‘fun=mysql’）， ‘q’ 代表 Microsoft SQL Server（连接字符串中为 ‘fun=mssql’）， ‘o’ 代表 Oracle（连接字符串中为 ‘fun=oracle’）， ‘p’ 代表 PostgreSQL（连接字符串中为 ‘fun=postgresql’）， ‘s’ 代表 Apache Spark（连接字符串中为 ‘fun=spark’）。 一个操作符名称可能对应多种SQL方言，但语义不同。 BigQuery 的类型系统对类型和函数使用了容易混淆的不同名称： BigQuery 的 DATETIME 类型表示本地日期时间，与 Calcite 的 TIMESTAMP 类型相对应； BigQuery 的 TIMESTAMP 类型表示瞬间，与 Calcite 的 TIMESTAMP WITH LOCAL TIME ZONE 类型相对应； timestampLtz 参数（例如 DATE(timestampLtz) 中的参数）具有 Calcite 类型 TIMESTAMP WITH LOCAL TIME ZONE； TIMESTAMP(string) 函数旨在与 BigQuery 函数兼容，返回 Calcite TIMESTAMP WITH LOCAL TIME ZONE； 类似地，DATETIME(string) 返回 Calcite TIMESTAMP。 C（兼容性） 运算符语法 描述 p expr :: type 将 expr 转换为 type m expr1 = expr2 两个值是否相等，将空值视为相同，类似于IS NOT DISTINCT FROM * ACOSH(numeric) 返回 numeric 的反双曲余弦值 s ARRAY([expr [, expr ]*]) 在 Apache Spark 中构造一个数组。该函数允许用户使用 ARRAY() 创建一个空数组 s ARRAY_APPEND(array, element) 将一个元素附加到数组末尾并返回结果。元素的类型应与数组元素的类型相似。如果数组为空，则函数将返回空值。如果元素为空，则将空元素添加到数组末尾 s ARRAY_COMPACT(array) 从数组中删除空值 b ARRAY_CONCAT(array [, array ]*) 连接一个或多个数组。如果任何输入参数为“NULL”，则函数返回“NULL” s ARRAY_CONTAINS(array, element) 如果 数组 包含 元素，则返回 true s ARRAY_DISTINCT(array) 从保持元素排序的数组中删除重复的值 s ARRAY_EXCEPT(array1, array2) 返回 array1 中存在但不存在于 array2 中的元素数组，且无重复 s ARRAY_INSERT(array, pos, element) 将 元素 放入 数组 的索引 pos 中。数组索引从 1 开始，如果索引为负数，则从末尾开始。数组大小之上的索引将使用 NULL 元素附加到数组中，如果索引为负数，则将 NULL 元素添加到数组前面。 s ARRAY_INTERSECT(array1, array2) 返回 array1 和 array2 交集处的元素数组，不包含重复项 s ARRAY_JOIN(array, delimiter [, nullText ]) ARRAY_TO_STRING 的同义词 b ARRAY_LENGTH(array) CARDINALITY 的同义词 s ARRAY_MAX(array) 返回数组中的最大值 s ARRAY_MIN(array) 返回数组中的最小值 s ARRAY_POSITION(array, element) 返回数组中第一个元素的（从 1 开始）索引 s ARRAY_REMOVE(array, element) 从数组中删除所有等于元素的元素 s ARRAY_PREPEND(array, element) 将一个元素附加到数组的开头并返回结果。元素的类型应与数组元素的类型相似。如果数组为空，则函数将返回空。如果元素为空，则将空元素添加到数组的开头 s ARRAY_REPEAT(element, count) 返回包含元素 count 次的数组。 b ARRAY_REVERSE(array) 反转数组的元素 s ARRAY_SIZE(array) CARDINALITY 的同义词 b ARRAY_TO_STRING(array, delimiter [, nullText ]) 将 array 中元素的连接作为 STRING 返回，并以 delimiter 作为分隔符。如果使用 nullText 参数，则该函数会将数组中的任何 NULL 值替换为 nullText 的值。如果未使用 nullText 参数，则该函数会省略 NULL 值及其前面的分隔符。如果任何参数为 NULL，则返回 NULL s ARRAY_UNION(array1, array2) 返回 array1 和 array2 的并集元素数组，不包含重复元素 s ARRAYS_OVERLAP(array1, array2) 如果 array1 至少包含一个非空元素，并且该元素也存在于 *array2 中，则返回 true。如果两个数组没有共同元素，且两个数组都非空，并且其中一个数组包含一个空元素，则返回 null，否则返回 false s ARRAYS_ZIP(array [, array ]*) 返回合并的结构体数组，其中第 N 个结构体包含输入数组的所有第 N 个值 s SORT_ARRAY(array [, ascendingOrder]) 根据数组元素的自然顺序，按升序或降序对 数组 进行排序。如果未指定 ascendingOrder，则默认顺序为升序。空元素将按升序放置在返回数组的开头，或按降序放置在返回数组的末尾 * ASINH(numeric) 返回 numeric 的反双曲正弦值 * ATANH(numeric) 返回 numeric 的反双曲正切值 f BITAND_AGG(value) 相当于 BIT_AND(value) f BITOR_AGG(value) 相当于 BIT_OR(value) s BIT_LENGTH(binary) 返回二进制的位长度 s BIT_LENGTH(string) 返回 string 的位长度 s BIT_GET(value, position) 返回数字值指定位置的位（0 或 1）值。位置从右到左编号，从零开始。位置参数不能为负数 b CEIL(value) 与标准 CEIL(value) 类似，除非 value 是整数类型，否则返回类型为双精度 m s CHAR(integer) 返回 ASCII 码为 integer % 256 的字符，如果 integer 0，则返回 null b o p CHR(integer) 返回 UTF-8 代码为 整数 的字符 b CODE_POINTS_TO_BYTES(integers) 将整数（0 到 255 之间的整数数组，含 0 和 255）转换为字节；如果任何元素超出范围，则会引发错误 b CODE_POINTS_TO_STRING(integers) 将 整数（0 到 0xD7FF 之间或 0xE000 到 0x10FFFF 之间的整数数组）转换为字符串；如果任何元素超出范围，则会引发错误 o CONCAT(string, string) 连接两个字符串，仅当两个字符串参数都为空时才返回空，否则将空视为空字符串 b m CONCAT(string [, string ]*) 连接一个或多个字符串，如果任何参数为空，则返回空 p q CONCAT(string [, string ]*) 连接一个或多个字符串，null 被视为空字符串 m p CONCAT_WS(separator, str1 [, string ]*) 连接一个或多个字符串，仅当分隔符为空时才返回空，否则将空参数视为空字符串 q CONCAT_WS(separator, str1, str2 [, string ]*) 连接两个或多个字符串，需要至少 3 个参数（最多 254 个），将空参数视为空字符串 m COMPRESS(string) 使用 zlib 压缩来压缩字符串并将结果作为二进制字符串返回 b CONTAINS_SUBSTR(expression, string [ , json_scope = json_scope_value ]) 返回 string 是否作为子字符串存在于 expression 中。可选的 json_scope 参数指定如果 expression 为 JSON 格式，则搜索的范围。如果 expression 中存在 NULL 且不匹配，则返回 NULL q CONVERT(type, expression [ , style ]) 相当于 CAST(expression AS type)；忽略 style 操作数 p CONVERT_TIMEZONE(tz1, tz2, datetime) 将 datetime 的时区从 tz1 转换为 tz2 * COSH(numeric) 返回 numeric 的双曲余弦 * COTH(numeric) 返回 numeric 的双曲余切 * CSC(numeric) 以弧度返回 numeric 的余割 * CSCH(numeric) 返回 numeric 的双曲余割 b CURRENT_DATETIME([ timeZone ]) 从 timezone 返回当前时间作为时间戳 m DAYNAME(datetime) 返回连接语言环境中 datetime 中的星期几名称；例如，对于 DATE ‘2020-02-10’ 和 TIMESTAMP ‘2020-02-10 10:10:10’，它均返回‘星期日’ b DATE(timestamp) 从时间戳中提取日期 b DATE(timestampLtz) 从 timestampLtz（一个瞬间；BigQuery 的 TIMESTAMP 类型）中提取 DATE，假设为 UTC b DATE(timestampLtz, timeZone) 从 timeZone 中的 timestampLtz（瞬间；BigQuery 的 TIMESTAMP 类型）中提取 DATE b DATE(string) 相当于 CAST(string AS DATE) b DATE(year, month, day) 返回 year、month 和 day 的 DATE 值（均为 INTEGER 类型） p q DATEADD(timeUnit, integer, datetime) 相当于 TIMESTAMPADD(timeUnit, integer, datetime) p q DATEDIFF(timeUnit, datetime, datetime2) 相当于 TIMESTAMPDIFF(timeUnit, datetime, datetime2) q DATEPART(timeUnit, datetime) 相当于 EXTRACT(timeUnit FROM datetime) b DATETIME(date, time) 将日期和时间转换为时间戳 b DATETIME(date) 将日期转换为时间戳值（午夜） b DATETIME(date, timeZone) 将 date 转换为 TIMESTAMP 值（午夜），以 timeZone 为单位 b DATETIME(year, month, day, hour, minute, second) 为年、月、日、时、分、秒 创建时间戳（所有类型均为 INTEGER） b DATETIME_ADD(timestamp, interval) 返回在 timestamp 之后 interval 发生的 TIMESTAMP 值 b DATETIME_DIFF(timestamp, timestamp2, timeUnit) 返回 timestamp 和 timestamp2 之间的 timeUnit 的整数 b DATETIME_SUB(timestamp, interval) 返回在 timestamp 之前 interval 发生的 TIMESTAMP b DATETIME_TRUNC(timestamp, timeUnit) 将 timestamp 截断为 timeUnit 的粒度，四舍五入到单位的开头 b s DATE_FROM_UNIX_DATE(integer) 返回 1970-01-01 之后 整数 天的 DATE p DATE_PART(timeUnit, datetime) 相当于 EXTRACT(timeUnit FROM datetime) b DATE_ADD(date, interval) 返回在 date 之后 interval 发生的 DATE 值 b DATE_DIFF(date, date2, timeUnit) 返回 date 和 date2 之间的 timeUnit 的整数 b DATE_SUB(date, interval) 返回在 date 之前 interval 发生的 DATE 值 b DATE_TRUNC(date, timeUnit) 将 date 截断为 timeUnit 的粒度，四舍五入到单位的开头 o s DECODE(value, value1, result1 [, valueN, resultN ]* [, default ]) 将 value 与每个 valueN 值逐一进行比较；如果 value 等于 valueN，则返回相应的 resultN，否则返回 default，如果未指定 default，则返回 NULL p DIFFERENCE(string, string) 返回两个字符串的相似度度量，即它们的 SOUNDEX 值具有相同的字符位置数：如果 SOUNDEX 值相同，则返回 4；如果 SOUNDEX 值完全不同，则返回 0 f s ENDSWITH(string1, string2) 返回 string2 是否是 string1 的后缀 b p ENDS_WITH(string1, string2) 相当于 ENDSWITH(string1, string2) s EXISTS(array, func) 返回谓词 func 是否对 array 中的一个或多个元素成立 o EXISTSNODE(xml, xpath, [, namespaces ]) 确定使用指定的 xpath 遍历 XML 文档是否会产生任何节点。如果在 XPath 表达式匹配的元素或元素的文档片段上应用 XPath 遍历后没有剩余节点，则返回 0。如果剩余任何节点，则返回 1。可选命名空间值，用于指定前缀的默认映射或命名空间映射，在评估 XPath 表达式时使用。 o EXTRACT(xml, xpath, [, namespaces ]) 返回与 XPath 表达式匹配的元素的 XML 片段。可选的命名空间值，用于指定前缀的默认映射或命名空间映射，在评估 XPath 表达式时使用 m EXTRACTVALUE(xml, xpathExpr)) 返回 XPath 表达式匹配的元素或元素的子元素的第一个文本节点的文本。 h s FACTORIAL(integer) 返回integer的阶乘，integer的范围是[0, 20]。否则返回NULL h s FIND_IN_SET(matchStr, textStr) 返回逗号分隔的 textStr 中给定 matchStr 的索引（从 1 开始）。如果未找到给定的 matchStr 或 matchStr 包含逗号，则返回 0。例如，FIND_IN_SET(‘bc’, ‘a,bc,def’) 返回 2 b FLOOR(value) 与标准 FLOOR(value) 类似，除非 value 是整数类型，否则返回类型为双精度 b FORMAT_DATE(string, date) 根据指定的格式 string 格式化 date b FORMAT_DATETIME(string, timestamp) 根据指定的格式 string 格式化 timestamp h s FORMAT_NUMBER(value, decimalVal) 将数字 value 格式化为 ‘#,###,###.##’，四舍五入到小数位 decimalVal。如果 decimalVal 为 0，则结果没有小数点或小数部分 h s FORMAT_NUMBER(value, format) 将数字值格式化为 MySQL 的 FORMAT 格式，如‘#,###,###.##0.00’ b FORMAT_TIME(string, time) 根据指定的格式 string 格式化 time b FORMAT_TIMESTAMP(string timestamp) 根据指定的格式 string 格式化 timestamp s GETBIT(value, position) 相当于 BIT_GET(value, position) b o s GREATEST(expr [, expr ]*) 返回表达式中最大的一个 b h s IF(condition, value1, value2) 如果 condition 为 TRUE，则返回 value1，否则返回 value2 b s IFNULL(value1, value2) 相当于 NVL(value1, value2) p string1 ILIKE string2 [ ESCAPE string3 ] string1 是否与模式 string2 匹配，忽略大小写（类似于 LIKE） p string1 NOT ILIKE string2 [ ESCAPE string3 ] string1 是否与模式 string2 不匹配，忽略大小写（类似于 NOT LIKE） b o INSTR(string, substring [, from [, occurrence ] ]) 返回 string 中 substring 的位置，从 from （默认 1）开始搜索，直到找到 substring 的第 n 次 occurrence （默认 1） m INSTR(string, substring) 相当于 POSITION(子字符串 IN 字符串) b IS_INF(value) 返回值是否无限 b IS_NAN(value) 返回 value 是否为 NaN m JSON_TYPE(jsonValue) 返回一个字符串值，表示 jsonValue 的类型 m JSON_DEPTH(jsonValue) 返回一个整数值，表示 jsonValue 的深度 m JSON_PRETTY(jsonValue) 返回 jsonValue 的格式化打印 m JSON_LENGTH(jsonValue [, path ]) 返回一个整数，表示 jsonValue 的长度 m JSON_INSERT(jsonValue, path, val [, path, val ]*) 返回一个 JSON 文档，插入 jsonValue、path、val 的数据。 m JSON_KEYS(jsonValue [, path ]) 返回表示 JSON jsonValue 的键的字符串 m JSON_REMOVE(jsonValue, path [, path ]) 使用一系列 path 表达式从 jsonValue 中删除数据并返回结果 m JSON_REPLACE(jsonValue, path, val [, path, val ]*) 返回一个 JSON 文档，替换 jsonValue、path、val 的数据。 m JSON_SET(jsonValue, path, val [, path, val ]*) 返回一个 JSON 文档，其中包含 jsonValue、path、val 的数据。 m JSON_STORAGE_SIZE(jsonValue) 返回用于存储 jsonValue 二进制表示的字节数 b o s LEAST(expr [, expr ]* ) 返回表达式中的最小值 b m p s LEFT(string, length) 返回字符串最左边的长度个字符 f s LEN(string) 相当于 CHAR_LENGTH(string) b f s LENGTH(string) 相当于 CHAR_LENGTH(string) h s LEVENSHTEIN(string1, string2) 返回 string1 和 string2 之间的编辑距离 b LOG(numeric1 [, numeric2 ]) 返回以 numeric1 为底数 numeric2 的对数，如果 numeric2 不存在，则返回以 e 为底数 m s LOG2(numeric) 返回 numeric 的以 2 为底的对数 b o s LPAD(string, length [, pattern ]) 返回由 string 和 length 开头且带有 pattern 的字符串或字节值 b TO_BASE32(string) 将 string 转换为 base-32 编码形式并返回编码字符串 b FROM_BASE32(string) 以字符串形式返回 base-32 string 的解码结果 m TO_BASE64(string) 将 string 转换为 base-64 编码形式并返回编码字符串 b m FROM_BASE64(string) 以字符串形式返回 base-64 string 的解码结果 b TO_HEX(binary) 将 binary 转换为十六进制 varchar b FROM_HEX(varchar) 将十六进制编码的 varchar 转换为字节 b o s LTRIM(string) 返回从开头删除所有空格的 string s MAP() 返回空映射 s MAP(key, value [, key, value]*) 返回具有给定 key/value 对的映射 s MAP_CONCAT(map [, map]*) 连接一个或多个地图。如果任何输入参数为“NULL”，则函数返回“NULL”。请注意，calcite 使用的是 LAST_WIN 策略 s MAP_CONTAINS_KEY(map, key) 返回 map 是否包含 key s MAP_ENTRIES(map) 以数组形式返回 map 的条目，条目的顺序未定义 s MAP_KEYS(map) 以数组形式返回 map 的键，条目的顺序未定义。 s MAP_VALUES(map) 将 map 的值作为数组返回，条目的顺序未定义 s MAP_FROM_ARRAYS(array1, array2) 返回由 array1 和 array2 创建的映射。请注意，两个数组的长度应该相同，并且 calcite 使用 LAST_WIN 策略 s MAP_FROM_ENTRIES(arrayOfRows) 返回由具有两个字段的行数组创建的映射。请注意，一行中的字段数必须为 2。请注意，calcite 使用 LAST_WIN 策略 s STR_TO_MAP(string [, stringDelimiter [, keyValueDelimiter]]) 使用分隔符将 string 拆分为键/值对后返回映射。stringDelimiter 的默认分隔符为‘,’，keyValueDelimiter 的默认分隔符为‘:’。请注意，calcite 使用的是 LAST_WIN 策略 b m p s MD5(string) 计算 string 的 MD5 128 位校验和并将其作为十六进制字符串返回 m MONTHNAME(date) 返回连接的区域设置中 datetime 月份的名称；例如，对于 DATE ‘2020-02-10’ 和 TIMESTAMP ‘2020-02-10 10:10:10’，它均返回‘二月’ o s NVL(value1, value2) 如果 value1 不为空，则返回 value1，否则返回 value2 b OFFSET(index) 当索引一个数组时，将 index 包装在 OFFSET 中将返回基于 0 的 index 处的值；如果 index 超出范围，则会引发错误 b ORDINAL(index) 与 OFFSET 类似，但 index 从 1 开始 b PARSE_DATE(format, string) 使用 format 指定的格式将日期的 string 表示形式转换为 DATE 值 b PARSE_DATETIME(format, string) 使用 format 指定的格式将日期时间的 string 表示形式转换为 TIMESTAMP 值 b PARSE_TIME(format, string) 使用 format 指定的格式将时间的 string 表示形式转换为 TIME 值 b PARSE_TIMESTAMP(format, string[, timeZone]) 使用 format 指定的格式将时间戳的 string 表示形式转换为 timeZone 中的 TIMESTAMP WITH LOCAL TIME ZONE 值 h s PARSE_URL(urlString, partToExtract [, keyToExtract] ) 从 urlString 返回指定的 partToExtract。partToExtract 的有效值包括 HOST、PATH、QUERY、REF、PROTOCOL、AUTHORITY、FILE 和 USERINFO。keyToExtract 指定要提取哪个查询 b s POW(numeric1, numeric2) 返回 numeric1 的 numeric2 次方 p RANDOM() 生成 0 到 1 之间的随机双精度数（含 0 和 1） s REGEXP(string, regexp) 相当于 string1 RLIKE string2 b REGEXP_CONTAINS(string, regexp) 返回 string 是否与 regexp 部分匹配 b REGEXP_EXTRACT(string, regexp [, position [, occurrence]]) 返回 string 中与 regexp 匹配的子字符串，从 position（默认 1）开始搜索，直到找到第 n 次 occurrence（默认 1）。如果没有匹配，则返回 NULL b REGEXP_EXTRACT_ALL(string, regexp) 返回 string 中与 regexp 匹配的所有子字符串的数组。如果没有匹配，则返回一个空数组 b REGEXP_INSTR(string, regexp [, position [, occurrence [, occurrence_position]]]) 返回与 regexp 匹配的 string 中子字符串的最低 1 位置，从 position（默认 1）开始搜索，直到找到第 n 个 occurrence（默认 1）。将 indication_position（默认 0）设置为 1 将返回子字符串的结束位置 + 1。如果没有匹配，则返回 0 m o p s REGEXP_LIKE(string, regexp [, flags]) 相当于 string1 RLIKE string2，但带有一个可选的搜索标志参数。支持的标志包括：i：不区分大小写匹配c：区分大小写匹配n：区分换行符匹配s：不区分换行符匹配m：多行 b m o REGEXP_REPLACE(string, regexp, rep [, pos [, occurrence [, matchType]]]) 将 string 中与 regexp 匹配的所有子字符串替换为 expr 中起始 pos 处的 rep（如果省略，则默认为 1），occurrence 指定要搜索匹配的哪一次出现（如果省略，则默认为 1），matchType 指定如何执行匹配 b REGEXP_SUBSTR(string, regexp [, position [, occurrence]]) REGEXP_EXTRACT 的同义词 b m p s REPEAT(string, integer) 返回由重复 integer 次的 string 组成的字符串；如果 integer 小于 1，则返回空字符串 b m REVERSE(string) 返回字符顺序颠倒的字符串 b m p s RIGHT(string, length) 返回字符串最右边的长度个字符 h s string1 RLIKE string2 string1 是否与正则表达式模式 string2 匹配（类似于 LIKE，但使用 Java 正则表达式） h s string1 NOT RLIKE string2 string1 是否与正则表达式模式 string2 不匹配（类似于“NOT LIKE”，但使用 Java 正则表达式） b o s RPAD(string, length[, pattern ]) 返回由 string 附加到 length 并使用 pattern 组成的字符串或字节值 b o s RTRIM(string) 返回删除末尾所有空格的 string b SAFE_ADD(numeric1, numeric2) 返回 numeric1 + numeric2，溢出时返回 NULL。参数隐式转换为 BIGINT、DOUBLE 或 DECIMAL 类型之一 b SAFE_CAST(value AS type) 将 value 转换为 type，如果转换失败则返回 NULL b SAFE_DIVIDE(numeric1, numeric2) 返回 numeric1 / numeric2，如果溢出或 numeric2 为零，则返回 NULL。参数隐式转换为 BIGINT、DOUBLE 或 DECIMAL 类型之一 b SAFE_MULTIPLY(numeric1, numeric2) 返回 numeric1 * numeric2，或溢出时返回 NULL。参数隐式转换为 BIGINT、DOUBLE 或 DECIMAL 类型之一 b SAFE_NEGATE(numeric) 返回 numeric * -1，或溢出时返回 NULL。参数隐式转换为 BIGINT、DOUBLE 或 DECIMAL 类型之一 b SAFE_OFFSET(index) 与 OFFSET 类似，但如果 index 超出范围，则返回 null b SAFE_ORDINAL(index) 与 OFFSET 类似，但 index 从 1 开始，并且如果 index 超出范围则返回 null b SAFE_SUBTRACT(numeric1, numeric2) 返回 numeric1 - numeric2，或溢出时返回 NULL。参数隐式转换为 BIGINT、DOUBLE 或 DECIMAL 类型之一 * SEC(numeric) 以弧度返回 numeric 的正割 * SECH(numeric) 返回 numeric 的双曲正割 b m p s SHA1(string) 计算 string 的 SHA-1 哈希值并将其作为十六进制字符串返回 b p SHA256(string) 计算 string 的 SHA-256 哈希值并将其作为十六进制字符串返回 b p SHA512(string) 计算 string 的 SHA-512 哈希值并将其作为十六进制字符串返回 * SINH(numeric) 返回 numeric 的双曲正弦值 b m o p SOUNDEX(string) 返回 string 的语音表示；如果 string 使用多字节编码（如 UTF-8）进行编码，则抛出 s SOUNDEX(string) 返回 string 的语音表示；如果 string 使用多字节编码（如 UTF-8）编码，则返回原始 string m s SPACE(integer) 返回一个由 整数 空格组成的字符串；如果 整数 小于 1，则返回一个空字符串 b SPLIT(string [, delimiter ]) 返回以 delimiter 分隔的 string 字符串数组（如果省略，则默认为逗号）。如果 string 为空，则返回一个空数组，否则，如果 delimiter 为空，则返回一个包含原始 string 的数组。 f s STARTSWITH(string1, string2) 返回 string2 是否是 string1 的前缀 b p STARTS_WITH(string1, string2) 相当于 STARTSWITH(string1, string2) m STRCMP(string, string) 如果两个字符串相同则返回 0，如果第一个参数小于第二个参数则返回 -1，如果第二个参数小于第一个参数则返回 1 b p STRPOS(string, substring) 相当于 POSITION(子字符串 IN 字符串) b m o p SUBSTR(string, position [, substringLength ]) 返回 string 的一部分，从字符 position 开始，长度为 substringLength 个字符。SUBSTR 使用输入字符集定义的字符计算长度 * TANH(numeric) 返回 numeric 的双曲正切 b TIME(hour, minute, second) 返回 TIME 值 小时、分钟、秒（所有类型为 INTEGER） b TIME(timestamp) 从 timestamp（本地时间；BigQuery 的 DATETIME 类型）中提取时间 b TIME(instant) 从 timestampLtz（一个瞬间；BigQuery 的 TIMESTAMP 类型）中提取时间，假设为 UTC b TIME(instant, timeZone) 从 timeZone 中的 timestampLtz（瞬间；BigQuery 的 TIMESTAMP 类型）中提取时间 b TIMESTAMP(string) 相当于 CAST(string AS TIMESTAMP WITH LOCAL TIME ZONE) b TIMESTAMP(string, timeZone) 相当于 CAST(string AS TIMESTAMP WITH LOCAL TIME ZONE)，转换为 timeZone b TIMESTAMP(date) 将日期转换为带有本地时区的时间戳值（午夜） b TIMESTAMP(date, timeZone) 将 date 转换为带有本地时区的时间戳值（午夜），以 timeZone 为单位 b TIMESTAMP(timestamp) 将 timestamp 转换为带有本地时区的时间戳，假设为 UTC b TIMESTAMP(timestamp, timeZone) 将 timestamp 转换为 timeZone 中的带有本地时区的时间戳 b TIMESTAMP_ADD(timestamp, interval) 返回在 timestamp 之后 interval 发生的 TIMESTAMP 值 b TIMESTAMP_DIFF(timestamp, timestamp2, timeUnit) 返回 timestamp 和 timestamp2 之间的 timeUnit 的整数。相当于 TIMESTAMPDIFF(timeUnit, timestamp2, timestamp) 和 (timestamp - timestamp2) timeUnit b s TIMESTAMP_MICROS(integer) 返回 1970-01-01 00:00:00 之后 整数 微秒的 TIMESTAMP b s TIMESTAMP_MILLIS(integer) 返回 1970-01-01 00:00:00 之后 整数 毫秒的 TIMESTAMP b s TIMESTAMP_SECONDS(integer) 返回 1970-01-01 00:00:00 之后 整数 秒的 TIMESTAMP b TIMESTAMP_SUB(timestamp, interval) 返回 timestamp 之前 interval 的 TIMESTAMP 值 b TIMESTAMP_TRUNC(timestamp, timeUnit) 将 timestamp 截断为 timeUnit 的粒度，四舍五入到单位的开头 b TIME_ADD(time, interval) 将间隔添加到时间，与任何时区无关 b TIME_DIFF(time, time2, timeUnit) 返回 time 和 time2 之间的 timeUnit 的整数 b TIME_SUB(time, interval) 返回 time 之前 interval 的 TIME 值 b TIME_TRUNC(time, timeUnit) 将 time 截断为 timeUnit 的粒度，四舍五入到单位的开头 m o p TO_CHAR(timestamp, format) 使用格式 format 将 timestamp 转换为字符串 b TO_CODE_POINTS(string) 将 string 转换为表示代码点或扩展 ASCII 字符值的整数数组 o p TO_DATE(string, format) 使用格式 format 将 string 转换为日期 注意： Calcite 没有 Redshift 库，因此改用 Postgres 库。函数 DATEADD、DATEDIFF 在 Redshift 中实现，而不是 Postgres，但它们仍然出现在 Calcite 的 Postgres 库中； 函数 DATEADD、DATEDIFF、DATE_PART 需要 Babel 解析器； 如果参数为 null，则 JSON_TYPE / JSON_DEPTH / JSON_PRETTY / JSON_STORAGE_SIZE 返回 null； 如果第一个参数为 null，则 JSON_LENGTH / JSON_KEYS / JSON_REMOVE 返回 null； JSON_TYPE 通常返回一个大写字符串标志，指示 JSON 输入的类型。目前支持的类型标志有： INTEGER STRING FLOAT DOUBLE LONG BOOLEAN DATE OBJECT ARRAY NULL JSON_DEPTH 定义 JSON 值的深度如下： 空数组、空对象或标量值的深度为 1； 仅包含深度为 1 的元素的非空数组或仅包含深度为 1 的成员值的非空对象深度为 2； 否则，JSON 文档的深度大于 2。 JSON_LENGTH 定义 JSON 值的长度如下： 标量值的长度为 1； 数组或对象的长度是其包含的元素数。 特定方言的聚合函数。 C（兼容性） 运算符语法 描述 c AGGREGATE(m) 在当前 GROUP BY 键的上下文中计算度量 m b p ARRAY_AGG( [ ALL | DISTINCT ] value [ RESPECT NULLS | IGNORE NULLS ] [ ORDER BY orderItem [, orderItem ]* ] ) 将值收集到数组中 b p ARRAY_CONCAT_AGG( [ ALL | DISTINCT ] value [ ORDER BY orderItem [, orderItem ]* ] ) 将数组连接成数组 p s BOOL_AND(condition) EVERY 的同义词 p s BOOL_OR(condition) SOME 的同义词 b COUNTIF(condition) 返回条件为 TRUE 的行数；相当于 COUNT(*) FILTER (WHERE 条件) m GROUP_CONCAT( [ ALL | DISTINCT ] value [, value ]* [ ORDER BY orderItem [, orderItem ]* ] [ SEPARATOR separator ] ) MySQL 特定的 LISTAGG 变体 b LOGICAL_AND(condition) EVERY 的同义词 b LOGICAL_OR(condition) SOME 的同义词 s MAX_BY(value, comp) ARG_MAX 的同义词 s MIN_BY(value, comp) ARG_MIN 的同义词 b PERCENTILE_CONT(value, fraction [ RESPECT NULLS | IGNORE NULLS ] ) OVER windowSpec 标准 PERCENTILE_CONT 的同义词，其中 PERCENTILE_CONT(value,fraction)OVER(ORDER BY value) 相当于标准 PERCENTILE_CONT(fraction)WITHIN GROUP(ORDER BY value) b PERCENTILE_DISC(value, fraction [ RESPECT NULLS | IGNORE NULLS ] ) OVER windowSpec 标准 PERCENTILE_DISC 的同义词，其中 PERCENTILE_DISC(value,fraction)OVER(ORDER BY value) 相当于标准 PERCENTILE_DISC(fraction)WITHIN GROUP(ORDER BY value) b p STRING_AGG( [ ALL | DISTINCT ] value [, separator] [ ORDER BY orderItem [, orderItem ]* ] ) LISTAGG 的同义词 用法示例： JSON_TYPE 示例 SQL SELECT JSON_TYPE(v) AS c1, JSON_TYPE(JSON_VALUE(v, lax $.b ERROR ON ERROR)) AS c2, JSON_TYPE(JSON_VALUE(v, strict $.a[0] ERROR ON ERROR)) AS c3, JSON_TYPE(JSON_VALUE(v, strict $.a[1] ERROR ON ERROR)) AS c4FROM (VALUES (a: [10, true],b: [10, true])) AS t(v)LIMIT 10; 结果 C1 C2 C3 C4 OBJECT ARRAY INTEGER BOOLEAN JSON_DEPTH 示例 SQL SELECT JSON_DEPTH(v) AS c1, JSON_DEPTH(JSON_VALUE(v, lax $.b ERROR ON ERROR)) AS c2, JSON_DEPTH(JSON_VALUE(v, strict $.a[0] ERROR ON ERROR)) AS c3, JSON_DEPTH(JSON_VALUE(v, strict $.a[1] ERROR ON ERROR)) AS c4FROM (VALUES (a: [10, true],b: [10, true])) AS t(v)LIMIT 10; 结果 C1 C2 C3 C4 3 2 1 1 JSON_LENGTH 示例 SQL SELECT JSON_LENGTH(v) AS c1, JSON_LENGTH(v, lax $.a) AS c2, JSON_LENGTH(v, strict $.a[0]) AS c3, JSON_LENGTH(v, strict $.a[1]) AS c4FROM (VALUES (a: [10, true])) AS t(v)LIMIT 10; 结果 C1 C2 C3 C4 1 2 1 1 JSON_INSERT 示例 SQL SELECT JSON_INSERT(v, $.a, 10, $.c, [1]) AS c1, JSON_INSERT(v, $, 10, $.c, [1]) AS c2FROM (VALUES (a: [10, true])) AS t(v)LIMIT 10; 结果 C1 C2 {“a”：1，“b”：[2]，“c”：“[1]”} {“a”：1，“b”：[2]，“c”：“[1]”} JSON_KEYS 示例 SQL SELECT JSON_KEYS(v) AS c1, JSON_KEYS(v, lax $.a) AS c2, JSON_KEYS(v, lax $.b) AS c2, JSON_KEYS(v, strict $.a[0]) AS c3, JSON_KEYS(v, strict $.a[1]) AS c4FROM (VALUES (a: [10, true],b: c: 30)) AS t(v)LIMIT 10; 结果 C1 C2 C3 C4 C5 [“a”, “b”] NULL [“c”] NULL NULL JSON_REMOVE 示例 SQL SELECT JSON_REMOVE(v, $[1]) AS c1FROM (VALUES ([a, [b, c], d])) AS t(v)LIMIT 10; 结果 C1 [“a”, “d”] JSON_REPLACE 示例 SQL SELECTJSON_REPLACE(v, $.a, 10, $.c, [1]) AS c1,JSON_REPLACE(v, $, 10, $.c, [1]) AS c2FROM (VALUES (\\a\\: 1,\\b\\:[2])) AS t(v)limit 10; 结果 C1 C2 {“a”：1，“b”：[2]，“c”：“[1]”} {“a”:1 , “b”:[2] , “c”:“[1]”}”) JSON_SET 示例 SQL SELECTJSON_SET(v, $.a, 10, $.c, [1]) AS c1,JSON_SET(v, $, 10, $.c, [1]) AS c2FROM (VALUES (\\a\\: 1,\\b\\:[2])) AS t(v)limit 10; 结果 C1 C2 {“a”：10，“b”：[2]} 10 JSON_STORAGE_SIZE 示例 SQL SELECTJSON_STORAGE_SIZE([100, \\sakila\\, [1, 3, 5], 425.05]) AS c1,JSON_STORAGE_SIZE(\\a\\: 10, \\b\\: \\a\\, \\c\\: \\[1, 3, 5, 7]\\) AS c2,JSON_STORAGE_SIZE(\\a\\: 10, \\b\\: \\xyz\\, \\c\\: \\[1, 3, 5, 7]\\) AS c3,JSON_STORAGE_SIZE([100, \\json\\, [[10, 20, 30], 3, 5], 425.05]) AS c4limit 10; 结果 C1 C2 C3 C4 29 35 37 36 解码示例 SQL SELECT DECODE(f1, 1, aa, 2, bb, 3, cc, 4, dd, ee) as c1, DECODE(f2, 1, aa, 2, bb, 3, cc, 4, dd, ee) as c2, DECODE(f3, 1, aa, 2, bb, 3, cc, 4, dd, ee) as c3, DECODE(f4, 1, aa, 2, bb, 3, cc, 4, dd, ee) as c4, DECODE(f5, 1, aa, 2, bb, 3, cc, 4, dd, ee) as c5FROM (VALUES (1, 2, 3, 4, 5)) AS t(f1, f2, f3, f4, f5); 结果 C1 C2 C3 C4 C5 aa bb cc dd ee 翻译示例 SQL SELECT TRANSLATE(Aa*Bb*CcD*d, */%, _) as c1, TRANSLATE(Aa/Bb/CcD/d, */%, _) as c2, TRANSLATE(Aa Bb CcD d, */%, _) as c3, TRANSLATE(Aa%Bb%CcD%d, */%, _) as c4FROM (VALUES (true)) AS t(f0); 结果 C1 C2 C3 C4 Aa_Bb_CcD_d Aa_Bb_CcD_d Aa_Bb_CcD_d Aa_Bb_CcD_d 高阶函数 lambdaExpression: parameters - expressionparameters: ( [ identifier [, identifier ] ] ) | identifier 高阶函数未包含在 SQL 标准中，因此所有函数也将在方言特定运算符中列出。 带有 lambda 参数的函数示例为 EXISTS。 用户定义函数 Calcite 是可扩展的。您可以使用用户代码定义每种函数。对于每种函数，通常有几种定义函数的方法，从方便到高效不等。 要实现标量函数，有 3 个选项： 创建一个具有公共静态 eval 方法的类，并注册该类； 创建一个具有公共非静态 eval 方法和无参数公共构造函数的类，并注册该类； 创建一个具有一个或多个公共静态方法的类，并注册每个类/方法组合。 要实现聚合函数，有 2 个选项： 创建一个具有公共静态 init、add 和 result 方法的类，并注册该类； 创建一个具有公共非静态 init、add 和 result 方法以及无参数的公共构造函数的类，并注册该类。 可选地，向类添加一个公共 merge 方法；这允许 Calcite 生成合并小计的代码。 可选地，让您的类实现 SqlSplittableAggFunction 接口；这允许 Calcite 跨多个聚合阶段分解函数，从汇总表中汇总，并通过连接推送它。 要实现 table 函数，有 3 个选项： 创建一个具有静态 eval 方法的类，该方法返回 ScannableTable 或 QueryableTable，并注册该类； 创建一个具有非静态 eval 方法的类，该方法返回 ScannableTable 或 QueryableTable，并注册该类； 创建一个具有一个或多个公共静态方法的类，这些方法返回 ScannableTable 或 QueryableTable，并注册每个类/方法组合。 要实现 table 宏，有 3 个选项： 创建一个具有静态 eval 方法的类，该方法返回 TranslatableTable，并注册该类； 创建一个具有非静态 eval 方法的类，该方法返回 TranslatableTable，并注册该类； 创建一个具有一个或多个公共静态方法的类，该方法返回 TranslatableTable，并注册每个类/方法组合。 Calcite 从实现函数的 Java 方法的参数和返回类型推断出函数的参数类型和结果类型。此外，您可以使用 Parameter 注释指定每个参数的名称和可选性。 使用命名参数和可选参数调用函数 通常，调用函数时，需要按顺序指定其所有参数。但如果函数有很多参数，尤其是当您想随着时间的推移添加更多参数时，这可能会成为一个问题。 为了解决这个问题，SQL 标准允许您按名称传递参数，并定义可选参数（即，如果未指定参数，则使用默认值）。 假设您有一个函数 f，声明如下伪语法： FUNCTION f( INTEGER a, INTEGER b DEFAULT NULL, INTEGER c, INTEGER d DEFAULT NULL, INTEGER e DEFAULT NULL) RETURNS INTEGER 该函数的所有参数都有名称，并且参数 b、d 和 e 的默认值为 NULL，因此是可选的（在 Calcite 中，NULL 是可选参数唯一允许的默认值；这可能会在未来发生变化）。 调用带有可选参数的函数时，您可以省略列表末尾的可选参数，或者对任何可选参数使用 DEFAULT 关键字。以下是一些示例： f(1, 2, 3, 4, 5) 按顺序为每个参数提供一个值； f(1, 2, 3, 4) 省略 e，获取其默认值 NULL； f(1, DEFAULT, 3) 省略 d 和 e，并指定使用默认值 b； f(1, DEFAULT, 3, DEFAULT, DEFAULT) 与上一个示例具有相同的效果； f(1, 2) 不合法，因为 c 不是可选的； f(1, 2, DEFAULT, 4) 不合法，因为 c 不是可选的。 您可以使用 = 语法按名称指定参数。如果一个参数被命名，则所有参数都必须被命名。参数可以位于任何其他参数中，但不得多次指定任何参数，并且您需要为每个非可选参数提供一个值。以下是一些示例： f(c = 3, d = 1, a = 0) 等同于 f(0, NULL, 3, 1, NULL); f(c = 3, d = 1) 不合法，因为您没有为 a 指定值，并且 a 不是可选的。 SQL Hint 提示是给优化器的指令。编写 SQL 时，您可能知道优化器不知道的数据信息。提示使您能够做出通常由优化器做出的决策。 规划器执行器：没有完美的规划器，因此实施提示以允许用户更好地控制执行是有意义的。例如：“永远不要将此子查询与其他子查询合并”（/*+ no_merge */）；“将这些表视为前导表”（/*+ leading */）以影响连接顺序等； 附加元数据/统计信息：某些统计信息（如“用于扫描的表索引”或“某些 shuffle 键的倾斜信息”）对于查询而言是动态的，使用提示配置它们会非常方便，因为我们从规划器获得的规划元数据通常不太准确； 运算符资源约束：在许多情况下，我们会为执行运算符提供默认的资源配置，即最小并行度、内存（消耗资源的 UDF）、特殊资源需求（GPU 或 SSD 磁盘）……使用每个查询（而不是作业）的提示来分析资源会非常灵活。 语法 Calcite 支持两个位置的提示： 查询提示：紧跟在 SELECT 关键字之后； 表提示：紧跟在引用的表名之后。 例如： SELECT /*+ hint1, hint2(a=1, b=2) */...FROM tableName /*+ hint3(5, x) */JOIN tableName /*+ hint4(c=id), hint5 */... 语法如下： hintComment: /*+ hint [, hint ]* */hint: hintName | hintName ( optionKey = optionVal [, optionKey = optionVal ]* ) | hintName ( hintOption [, hintOption ]* )optionKey: simpleIdentifier | stringLiteraloptionVal: stringLiteralhintOption: simpleIdentifier | numericLiteral | stringLiteral 它在 Calcite 中处于实验阶段，尚未完全实现，我们已实现的内容包括： 解析器支持上述语法； RelHint 表示提示项； 在 sql-to-rel 转换和规划器规划期间传播提示的机制。 我们尚未添加任何内置提示项，如果我们认为提示足够稳定，我们会引入更多。 MATCH_RECOGNIZE MATCH_RECOGNIZE 是一个 SQL 扩展，用于识别复杂事件处理 (CEP) 中的事件序列。 它在 Calcite 中处于实验阶段，尚未完全实现。 语法 matchRecognize: MATCH_RECOGNIZE ( [ PARTITION BY expression [, expression ]* ] [ ORDER BY orderItem [, orderItem ]* ] [ MEASURES measureColumn [, measureColumn ]* ] [ ONE ROW PER MATCH | ALL ROWS PER MATCH ] [ AFTER MATCH skip ] PATTERN ( pattern ) [ WITHIN intervalLiteral ] [ SUBSET subsetItem [, subsetItem ]* ] DEFINE variable AS condition [, variable AS condition ]* )skip: SKIP TO NEXT ROW | SKIP PAST LAST ROW | SKIP TO FIRST variable | SKIP TO LAST variable | SKIP TO variablesubsetItem: variable = ( variable [, variable ]* )measureColumn: expression AS aliaspattern: patternTerm [ | patternTerm ]*patternTerm: patternFactor [ patternFactor ]*patternFactor: patternPrimary [ patternQuantifier ]patternPrimary: variable | $ | ^ | ( [ pattern ] ) | - pattern - | PERMUTE ( pattern [, pattern ]* )patternQuantifier: * | *? | + | +? | ? | ?? | [ minRepeat ], [ maxRepeat ] [?] | repeat intervalLiteral: INTERVAL string timeUnit [ TO timeUnit ] 在 patternQuantifier 中，repeat 是正整数，minRepeat 和 maxRepeat 是非负整数。 DDL 扩展 DDL 扩展仅在 calcite-server 模块中可用。要启用，请在类路径中包含 calcite-server.jar，并将 parserFactory=org.apache.calcite.sql.parser.ddl.SqlDdlParserImpl#FACTORY 添加到 JDBC 连接字符串（请参阅连接字符串属性 parserFactory）。 ddlStatement: createSchemaStatement | createForeignSchemaStatement | createTableStatement | createTableLikeStatement | createViewStatement | createMaterializedViewStatement | createTypeStatement | createFunctionStatement | dropSchemaStatement | dropForeignSchemaStatement | dropTableStatement | dropViewStatement | dropMaterializedViewStatement | dropTypeStatement | dropFunctionStatementcreateSchemaStatement: CREATE [ OR REPLACE ] SCHEMA [ IF NOT EXISTS ] namecreateForeignSchemaStatement: CREATE [ OR REPLACE ] FOREIGN SCHEMA [ IF NOT EXISTS ] name ( TYPE type | LIBRARY com.example.calcite.ExampleSchemaFactory ) [ OPTIONS ( option [, option ]* ) ]option: name literalcreateTableStatement: CREATE TABLE [ IF NOT EXISTS ] name [ ( tableElement [, tableElement ]* ) ] [ AS query ]createTableLikeStatement: CREATE TABLE [ IF NOT EXISTS ] name LIKE sourceTable [ likeOption [, likeOption ]* ]likeOption: INCLUDING | EXCLUDING DEFAULTS | GENERATED | ALL createTypeStatement: CREATE [ OR REPLACE ] TYPE name AS baseType | ( attributeDef [, attributeDef ]* ) attributeDef: attributeName type [ COLLATE collation ] [ NULL | NOT NULL ] [ DEFAULT expression ]tableElement: columnName type [ columnGenerator ] [ columnConstraint ] | columnName | tableConstraintcolumnGenerator: DEFAULT expression | [ GENERATED ALWAYS ] AS ( expression ) VIRTUAL | STORED columnConstraint: [ CONSTRAINT name ] [ NOT ] NULLtableConstraint: [ CONSTRAINT name ] CHECK ( expression ) | PRIMARY KEY ( columnName [, columnName ]* ) | UNIQUE ( columnName [, columnName ]* ) createViewStatement: CREATE [ OR REPLACE ] VIEW name [ ( columnName [, columnName ]* ) ] AS querycreateMaterializedViewStatement: CREATE MATERIALIZED VIEW [ IF NOT EXISTS ] name [ ( columnName [, columnName ]* ) ] AS querycreateFunctionStatement: CREATE [ OR REPLACE ] FUNCTION [ IF NOT EXISTS ] name AS classNameLiteral [ USING usingFile [, usingFile ]* ]usingFile: JAR | FILE | ARCHIVE filePathLiteraldropSchemaStatement: DROP SCHEMA [ IF EXISTS ] namedropForeignSchemaStatement: DROP FOREIGN SCHEMA [ IF EXISTS ] namedropTableStatement: DROP TABLE [ IF EXISTS ] namedropViewStatement: DROP VIEW [ IF EXISTS ] namedropMaterializedViewStatement: DROP MATERIALIZED VIEW [ IF EXISTS ] namedropTypeStatement: DROP TYPE [ IF EXISTS ] namedropFunctionStatement: DROP FUNCTION [ IF EXISTS ] name 在 createTableStatement 中，如果指定 AS query，则可以省略 tableElement 列表，也可以省略任何 tableElement 的数据类型，在这种情况下，它只会重命名基础列。 在 columnGenerator 中，如果没有为生成的列指定 VIRTUAL 或 STORED，则 VIRTUAL 为默认值。 在 createFunctionStatement 和 usingFile 中，classNameLiteral 和 filePathLiteral 是字符文字。 为用户定义类型声明对象 在架构中定义并安装对象类型后，您可以使用它在任何 SQL 块中声明对象。例如，您可以使用对象类型指定属性、列、变量、绑定变量、记录字段、表元素、形式参数或函数结果的数据类型。在运行时，将创建对象类型的实例；也就是说，实例化该类型的对象。每个对象可以保存不同的值。 例如，我们可以声明类型 address_typ 和 employee_typ： CREATE TYPE address_typ AS ( street VARCHAR(30), city VARCHAR(20), state CHAR(2), postal_code VARCHAR(6));CREATE TYPE employee_typ AS ( employee_id DECIMAL(6), first_name VARCHAR(20), last_name VARCHAR(25), email VARCHAR(25), phone_number VARCHAR(20), hire_date DATE, job_id VARCHAR(10), salary DECIMAL(8,2), commission_pct DECIMAL(2,2), manager_id DECIMAL(6), department_id DECIMAL(4), address address_typ); 使用这些类型，您可以按如下方式实例化对象： employee_typ(315, Francis, Logan, FLOGAN, 555.777.2222, DATE 2004-05-01, SA_MAN, 11000, .15, 101, 110, address_typ(376 Mission, San Francisco, CA, 94222)) 写在最后 笔者因为工作原因接触到 Calcite，前期学习过程中，深感 Calcite 学习资料之匮乏，因此创建了 Calcite 从入门到精通知识星球，希望能够将学习过程中的资料和经验沉淀下来，为更多想要学习 Calcite 的朋友提供一些帮助。"},{"title":"空间","path":"/wiki/calcite/spatial.html","content":"原文链接：https://calcite.apache.org/docs/spatial.html Calcite 的目标是实现 《OpenGIS 简单功能实现规范 1.2.1 版》中的 SQL，这个规范是由 PostGIS 和 H2GIS 等空间数据库实现的标准。 我们还旨在添加对空间索引和其他形式的查询优化的优化器支持。 介绍 空间数据库是针对表示几何空间中定义的对象的数据，进行存储和查询优化的数据库。 Calcite 对空间数据的支持包括： GEOMETRY 数据类型和子类型，包括 POINT，LINESTRING 和 POLYGON； 空间函数（以 ST_ 作为前缀。我们已经实现了 OpenGIS 规范中 150 个中的大约 35 个）。 并且在某些时候还包括了查询重写以使用空间索引。 启用空间支持 虽然 GEOMETRY 数据类型是内置的，但默认情况下不启用这些功能。你需要在 JDBC 连接字符串中添加 fun=spatial 才能启用这些功能。例如，sqlline： $ ./sqlline !connect jdbc:calcite:fun=spatial sa SELECT ST_PointFromText(POINT(-71.064544 42.28787));+-------------------------------+| EXPR$0 |+-------------------------------+| x:-71.064544,y:42.28787 |+-------------------------------+1 row selected (0.323 seconds) 查询重写 一种重写类使用了希尔伯特空间填充曲线。假设表格具有表示点位置的列 x 和 y，以及表示该点沿曲线距离的列 h 。然后，涉及到固定点 (x, y) 距离的谓词可以被转换为涉及 h 的范围谓词。 假设我们有一张包含餐馆位置的表： CREATE TABLE Restaurants ( INT id NOT NULL PRIMARY KEY, VARCHAR(30) name, VARCHAR(20) cuisine, INT x NOT NULL, INT y NOT NULL, INT h NOT NULL DERIVED (ST_Hilbert(x, y)))SORT KEY (h); 优化器要求 h 是点 (x, y) 在希尔伯特曲线上的位置，并且还要求表按 h 排序。 DDL 语法中的 DERIVED 和 SORT KEY 子句是为了本示例的目的而新增的，但具有 CHECK 约束的聚簇表同样可以正常工作。 这个查询 SELECT *FROM RestaurantsWHERE ST_DWithin(ST_Point(x, y), ST_Point(10.0, 20.0), 6) 可以被重写为 SELECT *FROM RestaurantsWHERE (h BETWEEN 36496 AND 36520 OR h BETWEEN 36456 AND 36464 OR h BETWEEN 33252 AND 33254 OR h BETWEEN 33236 AND 33244 OR h BETWEEN 33164 AND 33176 OR h BETWEEN 33092 AND 33100 OR h BETWEEN 33055 AND 33080 OR h BETWEEN 33050 AND 33053 OR h BETWEEN 33033 AND 33035)AND ST_DWithin(ST_Point(x, y), ST_Point(10.0, 20.0), 6) 重写的查询包含 h 上的范围集合，后面跟上原始的 ST_DWithin 谓词。范围谓词会被首先评估，并且它的速度非常快，因为表是按 h 排序的。 这是完整的转换集： 描述 表达 测试恒定矩形（X，X2，Y，Y2）是否包含点（a，b）。重写以使用希尔伯特指数。 ST_Contains(ST_Rectangle(X, X2, Y, Y2), ST_Point(a, b)))h BETWEEN C1 AND C2OR …OR h BETWEEN C2kC_{2k}C2k​ AND C2k+1C_{2k+1}C2k+1​ 测试常量几何图形 G 是否包含点 (a, b)。重写为使用常量几何形状的边界框，该边界框也是常量，然后重写为希尔伯特范围，如上所述。 ST_Contains(ST_Envelope(G), ST_Point(a, b))ST_Contains(ST_Rectangle(X, X2, Y, Y2), ST_Point(a, b))) 测试点 (a, b) 是否位于常量点 (X, Y) 周围的缓冲区内。前面的特例，因为缓冲区是一个常量几何形状。 ST_Contains(ST_Buffer(ST_Point(a, b), D), ST_Point(X, Y)) 测试点（a，b）是否在恒定点（X，Y）的恒定距离 D 内。首先，转换为缓冲区，然后使用之前的重写来获取常量几何形状。 ST_DWithin(ST_Point(a, b), ST_Point(X, Y), D))ST_Contains(ST_Buffer(ST_Point(X, Y), D), ST_Point(a, b)) 测试恒定点（X，Y）是否在点（a，b）的恒定距离 D 内。反转调用 ST_DWithin 的参数，然后使用之前的重写。 ST_DWithin(ST_Point(X, Y), ST_Point(a, b), D))T_Contains(ST_Buffer(ST_Point(X, Y), D), ST_Point(a, b)) 上面的 a 和 b 是变量， X 、 X2 、 Y 、 Y2 和 G 是常量。 许多重写是不精确的：在某些点上谓词会返回 false，但重写的谓词会返回 true。例如，重写可能会将点是否在圆中的测试转换为该点是否在圆的边界正方形中的测试。这些重写值得执行，因为它们应用起来要快得多，并且通常允许对希尔伯特索引进行范围扫描。但为了安全起见，方解石应用原始谓词来消除误报。 致谢 Calcite 的 OpenGIS 实现使用了 JTS 拓扑套件。感谢从 JTS 社区获得的帮助。 在开发此功能时，我们广泛使用了 PostGIS 文档和测试以及 H2GIS 文档，并在规范不清楚时将两者作为参考实现进行查阅。感谢这些很棒的项目。 写在最后 笔者因为工作原因接触到 Calcite，前期学习过程中，深感 Calcite 学习资料之匮乏，因此创建了 Calcite 从入门到精通知识星球，希望能够将学习过程中的资料和经验沉淀下来，为更多想要学习 Calcite 的朋友提供一些帮助。"},{"title":"流式查询","path":"/wiki/calcite/stream.html","content":"原文链接：https://calcite.apache.org/docs/stream.html Calcite 扩展了 SQL 和关系代数以支持流式查询。 介绍 流是持续、永久流动的记录集合。与表不同，它们通常不存储在磁盘上，而是通过网络流动并在内存中短暂保存。 流是对表的补充，因为它们代表企业现在和未来正在发生的事情，而表则代表过去。将流归档到表中是很常见的操作。 与表一样，你通常希望使用基于关系代数的高级语言来查询流，根据模式进行验证，并进行优化以利用可用的资源和算法。 Calcite 的 SQL 是标准 SQL 的扩展，而不是另一种类 SQL 语言。这种区别很重要，原因如下： 对于任何了解常规 SQL 的人来说，流式 SQL 都很容易学习； 语义很清晰，因为我们的目标是在流上产生相同的结果，就像表中存在相同的数据一样； 你可以编写组合流和表（或流的历史记录，基本上是内存中的表）的查询； 许多现有工具可以生成标准 SQL。 如果不使用 STREAM 关键字，则会返回常规标准 SQL。 示例模式 我们的流式 SQL 示例使用以下模式： Orders (rowtime, productId, orderId, units)——一个流和一张表； Products (rowtime, productId, name)——一张表； Shipments (rowtime, orderId)——一个流。 (adsbygoogle = window.adsbygoogle || []).push({}); 赞助商 一个简单的查询 让我们从最简单的流式查询开始： SELECT STREAM *FROM Orders; rowtime | productId | orderId | units----------+-----------+---------+------- 10:17:00 | 30 | 5 | 4 10:17:05 | 10 | 6 | 1 10:18:05 | 20 | 7 | 2 10:18:07 | 30 | 8 | 20 11:02:00 | 10 | 9 | 6 11:04:00 | 10 | 10 | 1 11:09:30 | 40 | 11 | 12 11:24:11 | 10 | 12 | 4 此查询从 Orders 流中读取所有列和行。与任何流式查询一样，它永远不会终止。每当记录到达 Orders 时，它就会输出一条记录。 输入 Control-C 终止查询。 STREAM 关键字是流式 SQL 中的主要扩展。它告诉系统你对新输入的订单感兴趣，而不是现有订单。如下查询： SELECT *FROM Orders; rowtime | productId | orderId | units----------+-----------+---------+------- 08:30:00 | 10 | 1 | 3 08:45:10 | 20 | 2 | 1 09:12:21 | 10 | 3 | 10 09:27:44 | 30 | 4 | 24 records returned. 也有效，但会打印出所有现有订单，然后终止。我们将其称为关系查询，而不是流式查询。它具有传统的 SQL 语义。 Orders 很特殊，因为它既有流又有表。如果您尝试在表上运行流式查询，或在流上运行关系查询，Calcite 会给出错误： SELECT * FROM Shipments;ERROR: Cannot convert stream SHIPMENTS to a tableSELECT STREAM * FROM Products;ERROR: Cannot convert table PRODUCTS to a stream 过滤行 就像在常规 SQL 中一样，您使用 WHERE 子句来过滤行： SELECT STREAM *FROM OrdersWHERE units 3; rowtime | productId | orderId | units----------+-----------+---------+------- 10:17:00 | 30 | 5 | 4 10:18:07 | 30 | 8 | 20 11:02:00 | 10 | 9 | 6 11:09:30 | 40 | 11 | 12 11:24:11 | 10 | 12 | 4 投影表达式 在 SELECT 语句中使用表达式来选择要返回的列或计算表达式： SELECT STREAM rowtime, An order for || units || || CASE units WHEN 1 THEN unit ELSE units END || of product # || productId AS descriptionFROM Orders; rowtime | description----------+--------------------------------------- 10:17:00 | An order for 4 units of product #30 10:17:05 | An order for 1 unit of product #10 10:18:05 | An order for 2 units of product #20 10:18:07 | An order for 20 units of product #30 11:02:00 | An order by 6 units of product #10 11:04:00 | An order by 1 unit of product #10 11:09:30 | An order for 12 units of product #40 11:24:11 | An order by 4 units of product #10 我们建议您始终在 SELECT 子句中包含 rowtime 列。在每个流和流查询中拥有排序的时间戳使得稍后可以进行高级计算，例如 GROUP BY 和 JOIN 。 滚动窗口（Tumbling windows） 有多种方法可以计算流上的聚合函数。它们的差异是： 每行输入多少行？ 每个传入值是否出现在一个或多个总计中？ 什么定义了窗口，即构成给定输出行的行集？ 结果是流还是关系？ Calcite 有多种窗口类型： 滚动窗口 tumbling windows（GROUP BY）； 跳跃窗口 hopping window（多 GROUP BY）； 滑动窗口 sliding window（window 函数）； 级联窗口 cascading window（window 函数）。 下图显示了使用它们的查询类型： 首先，我们来看一个滚动窗口，它由流式的 GROUP BY 定义。如下是示例 SQL： SELECT STREAM CEIL(rowtime TO HOUR) AS rowtime, productId, COUNT(*) AS c, SUM(units) AS unitsFROM OrdersGROUP BY CEIL(rowtime TO HOUR), productId; rowtime | productId | c | units----------+-----------+---------+------- 11:00:00 | 30 | 2 | 24 11:00:00 | 10 | 1 | 1 11:00:00 | 20 | 1 | 7 12:00:00 | 10 | 3 | 11 12:00:00 | 40 | 1 | 12 结果是一个流。在 11 点钟，Calcite 会为自 10 点钟以来有订单的每个 productId 分组计数，时间戳为 11 点钟。 12点，它会统计11:00到12:00之间发生的订单。每个输入行仅被包含在一个输出的分组中。 Calcite 如何知道 10:00:00 的统计已在 11:00:00 完成，以便可以输出它们？因为 Calcite 知道 rowtime 正在增加，并且也知道 CEIL(rowtime TO HOUR) 在增加。因此，一旦它在 11:00:00 或之后看到了一行，它就永远不会看到对 10:00:00 统计有影响的数据行。 递增或递减的列或表达式被称为单调的。 如果列或表达式的值稍微乱序，并且流具有声明特定值将永远不会再次出现的机制（例如标点符号或水印），则该列或表达式被称为准单调。 如果 GROUP BY 子句中没有单调或准单调表达式，Calcite 就无法取得进展，并且不允许查询： SELECT STREAM productId, COUNT(*) AS c, SUM(units) AS unitsFROM OrdersGROUP BY productId;ERROR: Streaming aggregation requires at least one monotonic expression in GROUP BY clause 单调和准单调列需要在模式中声明。当记录进入流时，单调性被强制执行，并且从该流读取的查询假定了数据具有单调性。我们建议你为每个流提供一个名为 rowtime 的时间戳列，但你也可以将其他列声明为单调的，例如 orderId 。 我们在下面讨论标点符号、水印和其他取得进展的方法。 改进的滚动窗口 前面的滚动窗口示例很容易编写，因为窗口为一小时。对于不是整个时间单位的间隔，例如 2 小时或 2 小时 17 分钟，你不能使用 CEIL ，并且表达式会变得更加复杂。 Calcite 支持滚动窗口的替代语法： SELECT STREAM TUMBLE_END(rowtime, INTERVAL 1 HOUR) AS rowtime, productId, COUNT(*) AS c, SUM(units) AS unitsFROM OrdersGROUP BY TUMBLE(rowtime, INTERVAL 1 HOUR), productId; rowtime | productId | c | units----------+-----------+---------+------- 11:00:00 | 30 | 2 | 24 11:00:00 | 10 | 1 | 1 11:00:00 | 20 | 1 | 7 12:00:00 | 10 | 3 | 11 12:00:00 | 40 | 1 | 12 如你所见，它返回与上一个查询相同的结果。 TUMBLE 函数会返回一个分组键，该键在最终输出的统计结果中会保持相同； TUMBLE_END 函数采用相同的参数并返回该窗口结束的时间；此外还有一个 TUMBLE_START 函数。 TUMBLE 有一个可选参数来对齐窗口。在以下示例中，我们使用 30 分钟间隔和 0:12 作为对齐时间，因此查询会在每小时过去 12 和 42 分钟输出结果： SELECT STREAM TUMBLE_END(rowtime, INTERVAL 30 MINUTE, TIME 0:12) AS rowtime, productId, COUNT(*) AS c, SUM(units) AS unitsFROM OrdersGROUP BY TUMBLE(rowtime, INTERVAL 30 MINUTE, TIME 0:12), productId; rowtime | productId | c | units----------+-----------+---------+------- 10:42:00 | 30 | 2 | 24 10:42:00 | 10 | 1 | 1 10:42:00 | 20 | 1 | 7 11:12:00 | 10 | 2 | 7 11:12:00 | 40 | 1 | 12 11:42:00 | 10 | 1 | 4 跳跃窗口（Hopping windows） 跳跃窗口是滚动窗口的泛化，它允许数据在窗口中保留的时间长于输出间隔。 例如，以下查询输出时间戳为 11:00 的行，其中包含从 08:00 到 11:00 的数据（如果我们比较迂腐的话，则为 10:59.9），而时间戳为 12:00 的行包含从 09:00 到 12:00 的数据。 SELECT STREAM HOP_END(rowtime, INTERVAL 1 HOUR, INTERVAL 3 HOUR) AS rowtime, COUNT(*) AS c, SUM(units) AS unitsFROM OrdersGROUP BY HOP(rowtime, INTERVAL 1 HOUR, INTERVAL 3 HOUR); rowtime | c | units----------+----------+------- 11:00:00 | 4 | 27 12:00:00 | 8 | 50 在此查询中，由于保留期是输出期的 3 倍，因此每个输入行正好贡献 3 个输出行。想象一下 HOP 函数为输入行生成一组分组键的集合，并将其值放入每个分组键的累加器中。例如， HOP(10:18:00, INTERVAL '1' HOUR, INTERVAL '3') 生成 3 个时间段。 [08:00, 09:00) [09:00, 10:00) [10:00, 11:00) 这为那些对内置函数 HOP 和 TUMBLE 不满意的用户提供了允许用户定义分区函数的可能性。 我们可以构建复杂的复杂表达式，例如指数衰减的移动平均线： SELECT STREAM HOP_END(rowtime), productId, SUM(unitPrice * EXP((rowtime - HOP_START(rowtime)) SECOND / INTERVAL 1 HOUR)) / SUM(EXP((rowtime - HOP_START(rowtime)) SECOND / INTERVAL 1 HOUR))FROM OrdersGROUP BY HOP(rowtime, INTERVAL 1 SECOND, INTERVAL 1 HOUR), productId 它会输出： 11:00:00 处的一行包含 [10:00:00, 11:00:00) 中的行； 11:00:01 处的行包含 [10:00:01, 11:00:01) 中的行。 该表达式对最近订单的权重比对旧订单的权重更大。将窗口从 1 小时延长到 2 小时或 1 年实际上对结果的准确性没有影响（但会使用更多的内存和计算）。 请注意，我们在聚合函数 ( SUM ) 中使用 HOP_START ，因为它是一个对于统计中所有行而言都是恒定的值。对于典型的聚合函数（ SUM 、 COUNT 等），这是不允许的。 如果您熟悉 GROUPING SETS ，您可能会注意到分区函数可以被视为 GROUPING SETS 的泛化，因为它们允许输入行参与多个统计。 GROUPING SETS 的辅助函数，例如 GROUPING() 和 GROUP_ID ，可以在聚合函数内部使用，因此 HOP_START 并不奇怪和 HOP_END 可以以相同的方式使用。 分组集 GROUPING SETS 对于流式查询有效，前提是每个分组集都包含单调或准单调表达式。 CUBE 和 ROLLUP 对于流式查询无效，因为它们将生成至少一个聚合所有内容的分组集（如 GROUP BY () ）。 聚合后过滤 与标准 SQL 中一样，你可以应用 HAVING 子句来过滤流 GROUP BY 输出的行： SELECT STREAM TUMBLE_END(rowtime, INTERVAL 1 HOUR) AS rowtime, productIdFROM OrdersGROUP BY TUMBLE(rowtime, INTERVAL 1 HOUR), productIdHAVING COUNT(*) 2 OR SUM(units) 10; rowtime | productId----------+----------- 10:00:00 | 30 11:00:00 | 10 11:00:00 | 40 子查询、视图和 SQL 的闭包属性 前面的 HAVING 查询可以使用子查询上的 WHERE 子句来表示： SELECT STREAM rowtime, productIdFROM ( SELECT TUMBLE_END(rowtime, INTERVAL 1 HOUR) AS rowtime, productId, COUNT(*) AS c, SUM(units) AS su FROM Orders GROUP BY TUMBLE(rowtime, INTERVAL 1 HOUR), productId)WHERE c 2 OR su 10; rowtime | productId----------+----------- 10:00:00 | 30 11:00:00 | 10 11:00:00 | 40 HAVING 是在 SQL 早期引入的，当时需要一种方法来在聚合后执行过滤器。 （回想一下， WHERE 在行进入 GROUP BY 子句之前过滤行。） 从那时起，SQL 就成为一种数学封闭语言，这意味着您可以对表执行的任何操作也可以对查询执行。 SQL的闭包特性非常强大。它不仅使 HAVING 过时（或者至少将其简化为语法糖），而且使视图成为可能： CREATE VIEW HourlyOrderTotals (rowtime, productId, c, su) AS SELECT TUMBLE_END(rowtime, INTERVAL 1 HOUR), productId, COUNT(*), SUM(units) FROM Orders GROUP BY TUMBLE(rowtime, INTERVAL 1 HOUR), productId;SELECT STREAM rowtime, productIdFROM HourlyOrderTotalsWHERE c 2 OR su 10; rowtime | productId----------+----------- 10:00:00 | 30 11:00:00 | 10 11:00:00 | 40 FROM 子句中的子查询有时称为内联视图，但实际上，它们比视图更基本。视图只是一种方便的方法，通过给片段命名并将它们存储在元数据存储库中，将 SQL 分割成可管理的块。 许多人发现嵌套查询和视图在流上比在关系上更有用。流式查询是所有连续运行的运算符管道，并且这些管道通常会变得很长。嵌套查询和视图有助于表达和管理这些管道。 顺便说一句， WITH 子句可以完成与子查询或视图相同的功能： WITH HourlyOrderTotals (rowtime, productId, c, su) AS ( SELECT TUMBLE_END(rowtime, INTERVAL 1 HOUR), productId, COUNT(*), SUM(units) FROM Orders GROUP BY TUMBLE(rowtime, INTERVAL 1 HOUR), productId)SELECT STREAM rowtime, productIdFROM HourlyOrderTotalsWHERE c 2 OR su 10; rowtime | productId----------+----------- 10:00:00 | 30 11:00:00 | 10 11:00:00 | 40 流和关系之间的转换 回顾一下 HourlyOrderTotals 视图的定义。视图是流还是关系？ 它不包含 STREAM 关键字，因此它是一个关系。然而，它是一种可以转换为流的关系。 您可以在关系查询和流查询中使用它： # A relation; will query the historic Orders table.# Returns the largest number of product #10 ever sold in one hour.SELECT max(su)FROM HourlyOrderTotalsWHERE productId = 10;# A stream; will query the Orders stream.# Returns every hour in which at least one product #10 was sold.SELECT STREAM rowtimeFROM HourlyOrderTotalsWHERE productId = 10; 这种方法不限于视图和子查询。按照 CQL[1] 中规定的方法，流式 SQL 中的每个查询都被定义为关系查询，并使用最顶层 SELECT 中的 STREAM 关键字转换为流。 如果 STREAM 关键字出现在子查询或视图定义中，则它不起作用。 在查询准备时，Calcite 会确定查询中引用的关系是否可以转换为流或历史关系。 有时，流会提供其部分历史记录（例如 Apache Kafka[2] 主题中最近 24 小时的数据），但不是全部。在运行时，Calcite 会确定是否有足够的历史记录来运行查询，如果没有，则给出错误。 饼图问题：流上的关系查询 需要将流转换为关系的一种特殊情况发生在我所说的饼图问题中。想象一下，您需要编写一个带有图表的网页，如下所示，该图表总结了过去一小时内每种产品的订单数量。 但是 Orders 流只包含一些记录，而不是一个小时的统计。我们需要对流的历史记录运行关系查询： SELECT productId, count(*)FROM OrdersWHERE rowtime BETWEEN current_timestamp - INTERVAL 1 HOUR AND current_timestamp; 如果 Orders 流的历史记录被写入到 Orders 表，我们就可以返回查询，尽管这样成本很高。如果我们能够告诉系统将一小时的统计具体化到一个表中，随着流的流动不断维护它，并自动重写查询以使用该表，那就更好了。 排序 ORDER BY 的故事与 GROUP BY 类似。语法看起来与常规 SQL 类似，但 Calcite 必须确保它能够及时提供结果。因此，它需要在 ORDER BY 键的前面有一个单调的表达式。 SELECT STREAM CEIL(rowtime TO hour) AS rowtime, productId, orderId, unitsFROM OrdersORDER BY CEIL(rowtime TO hour) ASC, units DESC; rowtime | productId | orderId | units----------+-----------+---------+------- 10:00:00 | 30 | 8 | 20 10:00:00 | 30 | 5 | 4 10:00:00 | 20 | 7 | 2 10:00:00 | 10 | 6 | 1 11:00:00 | 40 | 11 | 12 11:00:00 | 10 | 9 | 6 11:00:00 | 10 | 12 | 4 11:00:00 | 10 | 10 | 1 大多数查询将按照插入的顺序返回结果，因为引擎使用流算法，但您不应该依赖它。例如，考虑一下： SELECT STREAM *FROM OrdersWHERE productId = 10UNION ALLSELECT STREAM *FROM OrdersWHERE productId = 30; rowtime | productId | orderId | units----------+-----------+---------+------- 10:17:05 | 10 | 6 | 1 10:17:00 | 30 | 5 | 4 10:18:07 | 30 | 8 | 20 11:02:00 | 10 | 9 | 6 11:04:00 | 10 | 10 | 1 11:24:11 | 10 | 12 | 4 productId = 30 的行显然是无序的，可能是因为 Orders 流在 productId 上分区，并且分区流在不同时间发送数据。 如果您需要特定的顺序，请添加明确的 ORDER BY ： SELECT STREAM *FROM OrdersWHERE productId = 10UNION ALLSELECT STREAM *FROM OrdersWHERE productId = 30ORDER BY rowtime; rowtime | productId | orderId | units----------+-----------+---------+------- 10:17:00 | 30 | 5 | 4 10:17:05 | 10 | 6 | 1 10:18:07 | 30 | 8 | 20 11:02:00 | 10 | 9 | 6 11:04:00 | 10 | 10 | 1 11:24:11 | 10 | 12 | 4 Calcite 可能会通过使用 rowtime 合并来实现 UNION ALL ，这只是效率稍低一些。 您只需要在最外面的查询中添加 ORDER BY 即可。例如，如果您需要在 UNION ALL 之后执行 GROUP BY ，Calcite 将隐式添加 ORDER BY ，以使 GROUP BY 算法成为可能。 表构造函数 VALUES 子句创建一个包含给定行集的内联表。 VALUES 子句不允许流式传输。行集永远不会改变，因此流永远不会返回任何行。 SELECT STREAM * FROM (VALUES (1, abc));ERROR: Cannot stream VALUES 滑动窗口（Sliding windows） 标准 SQL 具有所谓的分析函数，可以在 SELECT 子句中使用。与 GROUP BY 不同，它们不会折叠记录。每输入一条记录，就会输出一条记录。但聚合函数是基于多行的窗口。 让我们看一个例子。 SELECT STREAM rowtime, productId, units, SUM(units) OVER (ORDER BY rowtime RANGE INTERVAL 1 HOUR PRECEDING) unitsLastHourFROM Orders; 该功能毫不费力即可提供强大的功能。根据多窗口使用规范，您可以在 SELECT 子句中使用多个函数。 以下示例返回过去 10 分钟平均订单大小大于上周平均订单大小的订单。 SELECT STREAM *FROM ( SELECT STREAM rowtime, productId, units, AVG(units) OVER product (RANGE INTERVAL 10 MINUTE PRECEDING) AS m10, AVG(units) OVER product (RANGE INTERVAL 7 DAY PRECEDING) AS d7 FROM Orders WINDOW product AS ( ORDER BY rowtime PARTITION BY productId))WHERE m10 d7; 为了简洁起见，这里我们使用这样的语法：使用 WINDOW 子句部分定义窗口，然后在每个 OVER 子句中细化窗口。如果你愿意，你还可以在 WINDOW 子句中定义所有窗口，或内联所有窗口。 但真正的力量超越了语法。在幕后，该查询维护两个表，并使用 FIFO 队列在小计中添加和删除值。但是您可以访问这些表，而无需在查询中引入联接。 窗口聚合语法的一些其他功能： 你可以根据行数定义窗口； 该窗口可以引用尚未到达的行（流将等待，直到他们到达）； 你可以计算与顺序相关的函数，例如 RANK 和中位数。 级联窗口（Cascading windows） 如果我们想要一个为每条记录返回结果的查询（如滑动窗口），但在固定时间段重置总计（如滚动窗口），该怎么办？这种模式称为级联窗口。这是一个例子： SELECT STREAM rowtime, productId, units, SUM(units) OVER (PARTITION BY FLOOR(rowtime TO HOUR)) AS unitsSinceTopOfHourFROM Orders; 它看起来类似于滑动窗口查询，但单调表达式出现在窗口的 PARTITION BY 子句中。随着行时间从 10:59:59 移动到 11:00:00， FLOOR(rowtime TO HOUR) 从 10:00:00 更改为 11:00:00，会开始一个新分区。在新的小时到达的第一行将开始新的总计；第二行的总计由两行组成，依此类推。 Calcite 知道旧分区将永远不会再次使用，因此会从其内部存储中删除该分区的所有统计。 使用级联和滑动窗口的分析函数可以组合在同一个查询中。 流和表关联 涉及流的关联有两种：流到表关联和流到流关联。 如果表的内容没有改变，那么流到表的关联就很简单。此查询通过每种产品的标价丰富了订单流： SELECT STREAM o.rowtime, o.productId, o.orderId, o.units, p.name, p.unitPriceFROM Orders AS oJOIN Products AS p ON o.productId = p.productId; rowtime | productId | orderId | units | name | unitPrice----------+-----------+---------+-------+ -------+----------- 10:17:00 | 30 | 5 | 4 | Cheese | 17 10:17:05 | 10 | 6 | 1 | Beer | 0.25 10:18:05 | 20 | 7 | 2 | Wine | 6 10:18:07 | 30 | 8 | 20 | Cheese | 17 11:02:00 | 10 | 9 | 6 | Beer | 0.25 11:04:00 | 10 | 10 | 1 | Beer | 0.25 11:09:30 | 40 | 11 | 12 | Bread | 100 11:24:11 | 10 | 12 | 4 | Beer | 0.25 如果表发生变化会发生什么？例如，假设产品 10 的单价在 11:00 增加到 0.35。 11:00之前下的订单应采用旧价格，11:00之后下的订单应采用新价格。 实现此目的的一种方法是使用一个表来保存每个版本的开始和结束有效日期，在以下示例中为 ProductVersions ： SELECT STREAM *FROM Orders AS oJOIN ProductVersions AS p ON o.productId = p.productId AND o.rowtime BETWEEN p.startDate AND p.endDate rowtime | productId | orderId | units | productId1 | name | unitPrice----------+-----------+---------+-------+ -----------+--------+----------- 10:17:00 | 30 | 5 | 4 | 30 | Cheese | 17 10:17:05 | 10 | 6 | 1 | 10 | Beer | 0.25 10:18:05 | 20 | 7 | 2 | 20 | Wine | 6 10:18:07 | 30 | 8 | 20 | 30 | Cheese | 17 11:02:00 | 10 | 9 | 6 | 10 | Beer | 0.35 11:04:00 | 10 | 10 | 1 | 10 | Beer | 0.35 11:09:30 | 40 | 11 | 12 | 40 | Bread | 100 11:24:11 | 10 | 12 | 4 | 10 | Beer | 0.35 实现此目的的另一种方法是使用具有时间支持的数据库（能够查找过去任何时刻的数据库内容），并且系统需要知道 rowtime Orders 流的列对应于 Products 表的事务时间戳。 对于许多应用程序来说，不值得花费时间支持或版本化表的成本和精力。应用程序可以接受查询在重播时给出不同的结果：在此示例中，在重播时，产品 10 的所有订单都被分配了较晚的单价 0.35。 流和流关联 如果连接条件以某种方式使得两个流彼此保持有限距离，则关联两个流是有意义的。在以下查询中，发货日期在订单日期的一小时内： SELECT STREAM o.rowtime, o.productId, o.orderId, s.rowtime AS shipTimeFROM Orders AS oJOIN Shipments AS s ON o.orderId = s.orderId AND s.rowtime BETWEEN o.rowtime AND o.rowtime + INTERVAL 1 HOUR; rowtime | productId | orderId | shipTime----------+-----------+---------+---------- 10:17:00 | 30 | 5 | 10:55:00 10:17:05 | 10 | 6 | 10:20:00 11:02:00 | 10 | 9 | 11:58:00 11:24:11 | 10 | 12 | 11:44:00 请注意，相当多的订单没有出现，因为它们在一小时内没有发货。当系统收到时间戳为 11:24:11 的订单 10 时，它已经从哈希表中删除了时间戳为 10:18:07 的订单 8（含）之前的订单。 正如您所看到的，将两个流的单调或准单调列关联在一起的锁定步骤对于系统取得进展是必要的。如果它不能推断出锁定步骤，它将拒绝执行查询。 数据管理语言（DML） 不仅查询语句能够支持流操作，而且 DML 语句（（ INSERT 、 UPDATE 、 DELETE 以及它们衍生的 UPSERT 和 REPLACE）也支持流操作。 DML 很有用，因为它允许你具体化流或基于流的表，因此在经常使用值时可以节省精力。 考虑流应用程序通常由查询管道组成，每个查询将输入流转换为输出流。管道的组件可以是视图： CREATE VIEW LargeOrders ASSELECT STREAM * FROM Orders WHERE units 1000; 或标准的 INSERT 声明： INSERT INTO LargeOrdersSELECT STREAM * FROM Orders WHERE units 1000; 它们看起来很相似，并且在这两种情况下，管道中的下一步都可以从 LargeOrders 读取，而不必担心它是如何填充的。效率上有区别：无论有多少个消费者， INSERT 语句都做同样的工作。而视图的工作与消费者的数量成正比，特别是如果没有消费者，则该视图不起作用。 其他形式的 DML 对流也有意义。例如，以下标准的 UPSERT 语句维护一个表，该表具体化了最后一小时订单的统计： UPSERT INTO OrdersSummarySELECT STREAM productId, COUNT(*) OVER lastHour AS cFROM OrdersWINDOW lastHour AS ( PARTITION BY productId ORDER BY rowtime RANGE INTERVAL 1 HOUR PRECEDING) 标点符号（Punctuation） 即使单调键中没有足够的值来推出结果，标点符号[3]也允许流查询取得进展（我更喜欢术语是行时间边界，水印[4]是一个相关概念，但出于这些目的，标点符号就足够了）。 如果流启用了标点符号，则它可能无法排序，但仍然可以排序。因此，出于语义目的，按照排序流进行工作就足够了。 顺便说一句，如果无序流是 t 排序的（即每条记录保证在其时间戳的 t 秒内到达）或 k 排序的（即每条记录保证不超过k 个位置乱序）。因此，对这些流的查询可以与对带有标点符号的流的查询类似地进行规划。 而且，我们经常希望聚合不基于时间但仍然单调的属性。 一支球队在获胜状态和失败状态之间转换的次数就是这样一个单调属性。系统需要自己弄清楚聚合这样的属性是安全的；标点符号不添加任何额外信息。 我想到了优化器的一些元数据（成本指标）： 该流是否根据给定的属性（或多个属性）排序？ 是否可以根据给定属性对流进行排序？ （对于有限关系，答案始终是是；对于流，它取决于标点符号的存在或属性和排序键之间的链接）； 为了执行这种排序，我们需要引入什么延迟？ 执行该排序的成本是多少（CPU、内存等）？ 我们在 BuiltInMetadata.Collation 中已经有了(1)。对于 (2)，对于有限关系，答案始终为真。但我们需要为流实现 (2)、(3) 和 (4)。 流的状态 并非本文中的所有概念都已在 Calcite 中实现。其他的可能在 Calcite 中实现，但不能在 SamzaSQL [5] [6] 等特定适配器中实现。 已实现 流式的 SELECT 、 WHERE 、 GROUP BY 、 HAVING 、 UNION ALL 、 ORDER BY； FLOOR 和 CEIL 函数； 单调性； 不允许流式的 VALUES。 未实现 本文档中提供了以下功能，就好像方解石支持它们一样，但实际上它（尚未）不支持。完全支持意味着参考实现支持该功能（包括负面情况）并且 TCK 对其进行了测试。 流到流 JOIN； 流到表 JOIN； 流式视图； 带有 ORDER BY 的流式 UNION ALL （需要合并）； 流式关系查询； 流式窗口聚合（滑动和级联窗口）； 检查子查询和视图中的 STREAM 是否被忽略； 检查流式 ORDER BY 不能有 OFFSET 或 LIMIT； 有限的历史——在运行时，检查是否有足够的历史记录来运行查询； 准单调性； HOP 和 TUMBLE （以及辅助 HOP_START 、 HOP_END 、 TUMBLE_START 、 TUMBLE_END ）功能。 文档中待办事项 重新访问是否可以流式执行 VALUES； OVER 子句定义流上的窗口； 考虑是否在流式查询中允许 CUBE 和 ROLLUP ，并了解某些级别的聚合永远不会完成（因为它们没有单调表达式），因此永远不会被输出； 修复 UPSERT 示例以删除过去一小时内未出现的产品记录； 输出到多个流的DML；也许是标准 REPLACE 语句的扩展。 功能 以下函数不存在于标准 SQL 中，但在流 SQL 中定义。 标量函数： FLOOR(dateTime TO intervalType) 将日期、时间或时间戳值向下舍入为给定的间隔类型； CEIL(dateTime TO intervalType) 将日期、时间或时间戳值四舍五入到给定的间隔类型。 分区函数： HOP(t, emit, retain) 返回作为跳跃窗口一部分的行的组键集合； HOP(t, emit, retain, align) 返回作为具有给定对齐方式的跳跃窗口一部分的行的组键的集合； TUMBLE(t, emit) 返回作为滚动窗口一部分的行的组键； TUMBLE(t, emit, align) 返回作为具有给定对齐方式的翻滚窗口一部分的行的组键。 TUMBLE(t, e) 相当于 TUMBLE(t, e, TIME '00:00:00') 。 TUMBLE(t, e, a) 相当于 HOP(t, e, e, a) 。 HOP(t, e, r) 相当于 HOP(t, e, r, TIME '00:00:00') 。 写在最后 笔者因为工作原因接触到 Calcite，前期学习过程中，深感 Calcite 学习资料之匮乏，因此创建了 Calcite 从入门到精通知识星球，希望能够将学习过程中的资料和经验沉淀下来，为更多想要学习 Calcite 的朋友提供一些帮助。 参考文档 Arvind Arasu, Shivnath Babu, and Jennifer Widom (2003) The CQL Continuous Query Language: Semantic Foundations and Query Execution. ↩︎ Apache Kafka. ↩︎ Peter A. Tucker, David Maier, Tim Sheard, and Leonidas Fegaras (2003) Exploiting Punctuation Semantics in Continuous Data Streams. ↩︎ Tyler Akidau, Alex Balikov, Kaya Bekiroglu, Slava Chernyak, Josh Haberman, Reuven Lax, Sam McVeety, Daniel Mills, Paul Nordstrom, and Sam Whittle (2013) MillWheel: Fault-Tolerant Stream Processing at Internet Scale. ↩︎ Apache Samza. ↩︎ SamzaSQL. ↩︎"},{"title":"教程","path":"/wiki/calcite/tutorial.html","content":"原文链接：https://calcite.apache.org/docs/tutorial.html 这是一个分步骤教程，它展示了如何构建和连接 Calcite。它使用一个简单的适配器，使得 CSV 文件目录看起来像是一个包含表的模式。Calcite 则完成了剩余工作，并提供了一个完整的 SQL 接口。 calcite-example-csv 是一个功能齐全的 Calcite 适配器，它可以读取 CSV 格式的文本文件。同时值得注意的是，几百行 Java 代码就足以提供完整的 SQL 查询功能。 CSV 也可以作为构建其他数据格式适配器的模板。尽管代码行数不多，但它涵盖了几个重要的概念： 使用 SchemaFactory 和 Schema 接口的用户自定义模式； 在 JSON 格式的模型文件中声明模式； 在 JSON 格式的模型文件中声明视图； 使用 Table 接口的用户自定义表； 确定表的记录类型； Table 的简单实现——使用 ScannableTable 接口，直接枚举所有行； 更高级的实现——实现 FilterableTable，可以根据简单的谓词过滤掉行； Table 的高级实现——使用 TranslatableTable 的规划器规则转换为关系运算符； 下载和构建 你需要 Java（版本 8、9 或 10）和 Git。 $ git clone https://github.com/apache/calcite.git$ cd calcite/example/csv$ ./sqlline 首次查询 现在让我们使用 sqlline 连接到 Calcite，sqlline 是一个包含在 Calcite 项目中的 SQL shell 功能。 $ ./sqllinesqlline !connect jdbc:calcite:model=src/test/resources/model.json admin admin 如果你运行的是 Windows，则命令为 sqlline.bat。 执行一个元数据查询： sqlline !tables+-----------+-------------+------------+--------------+---------+----------+------------+-----------+---------------------------+----------------+| TABLE_CAT | TABLE_SCHEM | TABLE_NAME | TABLE_TYPE | REMARKS | TYPE_CAT | TYPE_SCHEM | TYPE_NAME | SELF_REFERENCING_COL_NAME | REF_GENERATION |+-----------+-------------+------------+--------------+---------+----------+------------+-----------+---------------------------+----------------+| | SALES | DEPTS | TABLE | | | | | | || | SALES | EMPS | TABLE | | | | | | || | SALES | SDEPTS | TABLE | | | | | | || | metadata | COLUMNS | SYSTEM TABLE | | | | | | || | metadata | TABLES | SYSTEM TABLE | | | | | | |+-----------+-------------+------------+--------------+---------+----------+------------+-----------+---------------------------+----------------+ JDBC 专家们注意：sqlline 的 !tables 命令只是在背后执行了 DatabaseMetaData.getTables() 方法。它也提供了其他命令，可以用来查询 JDBC 元数据，例如 !columns 和 !describe。 正如你看见的，系统中有 5 张表： EMPS，DEPTS 和 SDEPTS 表在当前 SALES 模式中，COLUMNS 和 TABLES 表在系统 metadata 模式中。系统表始终存在于 Calcite 中，而其他表则由模式的具体实现提供。在这个场景下，EMPS、DEPTS 和 SDEPTS 表是基于 resources/sales 目录下的 EMPS.csv.gz、DEPTS.csv 和 SDEPTS.csv 文件。 让我们对这些表执行一些查询，来展示 Calcite 提供的 SQL 完整实现。首先，进行表扫描： sqlline SELECT * FROM emps;+-------+-------+--------+--------+---------------+-------+------+---------+---------+------------+| EMPNO | NAME | DEPTNO | GENDER | CITY | EMPID | AGE | SLACKER | MANAGER | JOINEDAT |+-------+-------+--------+--------+---------------+-------+------+---------+---------+------------+| 100 | Fred | 10 | | | 30 | 25 | true | false | 1996-08-03 || 110 | Eric | 20 | M | San Francisco | 3 | 80 | | false | 2001-01-01 || 110 | John | 40 | M | Vancouver | 2 | null | false | true | 2002-05-03 || 120 | Wilma | 20 | F | | 1 | 5 | | true | 2005-09-07 || 130 | Alice | 40 | F | Vancouver | 2 | null | false | true | 2007-01-01 |+-------+-------+--------+--------+---------------+-------+------+---------+---------+------------+ 再进行关联和分组查询： sqlline SELECT d.name, COUNT(*). . . . FROM emps AS e JOIN depts AS d ON e.deptno = d.deptno. . . . GROUP BY d.name;+------------+---------+| NAME | EXPR$1 |+------------+---------+| Sales | 1 || Marketing | 2 |+------------+---------+ 最后，VALUES 运算符会返回一个单行，这是测试表达式和 SQL 内置函数的快捷方法： sqlline VALUES CHAR_LENGTH(Hello, || world!);+---------+| EXPR$0 |+---------+| 13 |+---------+ Calcite 有许多其他 SQL 特性。我们没有时间在这里介绍它们。你可以再写一些查询来进行实验。 (adsbygoogle = window.adsbygoogle || []).push({}); 赞助商 模式发现 那么，Calcite 是如何发现这些表的呢？记住，Calcite 内核对 CSV 文件一无所知（作为一个没有存储层的数据库，Calcite 不了解任何文件格式）。Calcite 知道这些表，完全是因为我们告诉它去执行 calcite-example-csv 项目中的代码。 发现过程包含了几个步骤。首先，我们基于模型文件中的模式工厂类定义了一个模式。然后，模式工厂创建了一个模式，并且这个模式创建一些表，每个表都知道通过扫描 CSV 文件来获取数据。最后，在 Calcite 解析完查询并生成使用这些表的执行计划后，Calcite 会在执行查询时，调用这些表来读取数据。现在让我们更详细地了解这些步骤。 在 JDBC 连接字符串上，我们以 JSON 格式给出了模型的路径。下面是模型的内容： version: 1.0, defaultSchema: SALES, schemas: [ name: SALES, type: custom, factory: org.apache.calcite.adapter.csv.CsvSchemaFactory, operand: directory: sales ] 模型定义了一个名为 SALES 的单模式。这个模式由插件类 org.apache.calcite.adapter.csv.CsvSchemaFactory 提供支持，它是 calcite-example-csv 项目的一部分，并实现了 Calcite SchemaFactory 接口。它的 create 方法，通过从模型文件中传入的 directory 参数，实例化了模式： public Schema create(SchemaPlus parentSchema, String name, MapString, Object operand) final String directory = (String) operand.get(directory); final File base = (File) operand.get(ModelHandler.ExtraOperand.BASE_DIRECTORY.camelName); File directoryFile = new File(directory); if (base != null !directoryFile.isAbsolute()) directoryFile = new File(base, directory); String flavorName = (String) operand.get(flavor); CsvTable.Flavor flavor; if (flavorName == null) flavor = CsvTable.Flavor.SCANNABLE; else flavor = CsvTable.Flavor.valueOf(flavorName.toUpperCase(Locale.ROOT)); return new CsvSchema(directoryFile, flavor); 在模型的驱动下，模式工厂实例化了一个名为 SALES 的单模式。这个模式是 org.apache.calcite.adapter.csv.CsvSchema 的一个实例， 并实现了 Calcite Schema 接口。 模式的一项工作是生成一系列的表（它还可以生成子模式和表函数，但这些是高级功能，calcite-example-csv 不支持它们）。这些表实现了 Calcite Table 接口。CsvSchema 生成的表是 CsvTable 及其子类的实例。 下面是 CsvSchema 的相关代码，它重写了 AbstractSchema 基类中的 getTableMap() 方法。 private MapString, Table createTableMap() // Look for files in the directory ending in .csv, .csv.gz, .json, // .json.gz. final Source baseSource = Sources.of(directoryFile); File[] files = directoryFile.listFiles((dir, name) - final String nameSansGz = trim(name, .gz); return nameSansGz.endsWith(.csv) || nameSansGz.endsWith(.json); ); if (files == null) System.out.println(directory + directoryFile + not found); files = new File[0]; // Build a map from table name to table; each file becomes a table. final ImmutableMap.BuilderString, Table builder = ImmutableMap.builder(); for (File file : files) Source source = Sources.of(file); Source sourceSansGz = source.trim(.gz); final Source sourceSansJson = sourceSansGz.trimOrNull(.json); if (sourceSansJson != null) final Table table = new JsonScannableTable(source); builder.put(sourceSansJson.relative(baseSource).path(), table); final Source sourceSansCsv = sourceSansGz.trimOrNull(.csv); if (sourceSansCsv != null) final Table table = createTable(source); builder.put(sourceSansCsv.relative(baseSource).path(), table); return builder.build();/** * Creates different sub-type of table based on the flavor attribute. */private Table createTable(Source source) switch (flavor) case TRANSLATABLE: return new CsvTranslatableTable(source, null); case SCANNABLE: return new CsvScannableTable(source, null); case FILTERABLE: return new CsvFilterableTable(source, null); default: throw new AssertionError(Unknown flavor + this.flavor); 这个模式扫描目录并查找所有具有适当扩展名的文件，并为它们创建表。在这种场景下，目录是 sales ，目录下包含了文件 EMPS.csv.gz、DEPTS.csv 和 SDEPTS.csv，这些文件对应表 EMPS、DEPTS 和 SDEPTS。 模式中的表和视图 注意，我们不需要在模型中定义任何表，模式自动生成了这些表。除了这些自动创建的表之外，你还可以使用模式中的 tables 属性，定义额外的表。让我们看看，如何创建一个重要且有用的表类型，即视图。 当你在写一个查询时，视图看起来就像一个表，但它不存储数据。它通过执行查询获取结果。在查询语句被计划执行时，视图将会被展开，因此查询优化器通常可以执行优化，例如，删除那些在最终结果中未使用的 SELECT 子句表达式。 下面是一个定义视图的模式： version: 1.0, defaultSchema: SALES, schemas: [ name: SALES, type: custom, factory: org.apache.calcite.adapter.csv.CsvSchemaFactory, operand: directory: sales , tables: [ name: FEMALE_EMPS, type: view, sql: SELECT * FROM emps WHERE gender = F ] ] type: view 这行将 FEMALE_EMPS 标记为视图，而不是常规表或自定义表。JSON 并不能简单地书写长字符串，因此 Calcite 支持另一种可选的语法。如果你的视图有很长的 SQL 语句，你可以将单个字符串改为多行列表： name: FEMALE_EMPS, type: view, sql: [ SELECT * FROM emps, WHERE gender = F ] 现在，我们已经定义了一个视图，我们可以像使用表一样，在查询中使用它： sqlline SELECT e.name, d.name FROM female_emps AS e JOIN depts AS d on e.deptno = d.deptno;+--------+------------+| NAME | NAME |+--------+------------+| Wilma | Marketing |+--------+------------+ 自定义表 自定义表是那些由用户自定义的代码驱动的表。他们不需要存在于自定义模式中。 在 model-with-custom-table.json 模型文件中，有一个自定义表的例子： version: 1.0, defaultSchema: CUSTOM_TABLE, schemas: [ name: CUSTOM_TABLE, tables: [ name: EMPS, type: custom, factory: org.apache.calcite.adapter.csv.CsvTableFactory, operand: file: sales/EMPS.csv.gz, flavor: scannable ] ] 我们可以使用常规的方式查询自定义表： sqlline !connect jdbc:calcite:model=src/test/resources/model-with-custom-table.json admin adminsqlline SELECT empno, name FROM custom_table.emps;+--------+--------+| EMPNO | NAME |+--------+--------+| 100 | Fred || 110 | Eric || 110 | John || 120 | Wilma || 130 | Alice |+--------+--------+ 这个模式是一个常规模式，包含了一个由 org.apache.calcite.adapter.csv.CsvTableFactory 提供支持的自定义表，它实现了 Calcite TableFactory 接口。它的 create 方法，根据从模型文件中传入的 file 参数，实例化了 CsvScannableTable： public CsvTable create(SchemaPlus schema, String name, MapString, Object operand, @Nullable RelDataType rowType) String fileName = (String) operand.get(file); final File base = (File) operand.get(ModelHandler.ExtraOperand.BASE_DIRECTORY.camelName); final Source source = Sources.file(base, fileName); final RelProtoDataType protoRowType = rowType != null ? RelDataTypeImpl.proto(rowType) : null; return new CsvScannableTable(source, protoRowType); 实现自定义表，通常是实现自定义模式的一个更简单方法。这两种方法可能最终都会创建类似的 Table 接口实现，但对于自定义表，你不需要实现元数据发现。CsvTableFactory 创建一个 CsvScannableTable，就像 CsvSchema 所做的那样，但表的实现不会扫描文件系统来查找 .csv 文件。 自定义表需要模型的开发者做更多的工作，需要明确指定每个表及其文件，但也给开发者提供了更多的控制权，例如，为每个表提供不同的参数。 模型中的注释 模型可以使用 /* ... */ 和 // 语法来包含注释： version:1.0, /* Multi-line comment. */ defaultSchema:CUSTOM_TABLE, // Single-line comment. schemas:[ .. ] 注释不是标准的 JSON，而是一种无害的扩展。 使用优化器规则优化查询 到目前为止，我们看到的表实现都是可以接受的，只要表不包含大量数据。但是，如果你的客户的表有一百列以及一百万行，你肯定更愿意看到系统在每个查询时，不要检索出所有的数据。你可能希望 Calcite 与适配器协商，并找到一种更有效的数据访问方式。 这种协商就是查询优化的一种简单形式。Calcite 通过添加优化器规则来支持查询优化。优化器规则在查询解析树中查找模式（例如某种表解析树顶部的投影），并使用一组新的优化节点来替换树中匹配的节点。 优化器规则像模式和表一样，也是可扩展的。因此，如果你有一个想要通过 SQL 访问的数据存储，你可以首先定义自定义表或模式，然后定义一些规则来提高访问的效率。 让我们通过一个实战来加深理解，使用优化器规则访问 CSV 文件中的部分列。下面有两个非常相似的模式，我们执行相同的查询： sqlline !connect jdbc:calcite:model=src/test/resources/model.json admin adminsqlline explain plan for select name from emps;+-----------------------------------------------------+| PLAN |+-----------------------------------------------------+| EnumerableCalc(expr#0..9=[inputs], NAME=[$t1]) || EnumerableTableScan(table=[[SALES, EMPS]]) |+-----------------------------------------------------+sqlline !connect jdbc:calcite:model=src/test/resources/smart.json admin adminsqlline explain plan for select name from emps;+-----------------------------------------------------+| PLAN |+-----------------------------------------------------+| CsvTableScan(table=[[SALES, EMPS]], fields=[[1]]) |+-----------------------------------------------------+ 是什么导致了执行计划的差异？让我们跟着证据的线索走。在 smart.json 模型文件中，只有一行： flavor: translatable 这个配置会使用 flavor = TRANSLATABLE 来创建 CsvSchema，它的 createTable 方法创建了 CsvTranslatableTable 而不是 CsvScannableTable。 CsvTranslatableTable 实现了 TranslatableTable.toRel() 方法，用来创建 CsvTableScan。表扫描是查询操作树的叶子节点。通常实现是 EnumerableTableScan，但我们创建了一个独特的子类型，它将导致规则触发。 下面是完整的规则实现： public class CsvProjectTableScanRule extends RelRuleCsvProjectTableScanRule.Config /** * Creates a CsvProjectTableScanRule. */ protected CsvProjectTableScanRule(Config config) super(config); @Override public void onMatch(RelOptRuleCall call) final LogicalProject project = call.rel(0); final CsvTableScan scan = call.rel(1); int[] fields = getProjectFields(project.getProjects()); if (fields == null) // Project contains expressions more complex than just field references. return; call.transformTo(new CsvTableScan(scan.getCluster(), scan.getTable(), scan.csvTable, fields)); private static int[] getProjectFields(ListRexNode exps) final int[] fields = new int[exps.size()]; for (int i = 0; i exps.size(); i++) final RexNode exp = exps.get(i); if (exp instanceof RexInputRef) fields[i] = ((RexInputRef) exp).getIndex(); else return null; // not a simple projection return fields; /** * Rule configuration. */ @Value.Immutable(singleton = false) public interface Config extends RelRule.Config Config DEFAULT = ImmutableCsvProjectTableScanRule.Config.builder().withOperandSupplier(b0 - b0.operand(LogicalProject.class).oneInput(b1 - b1.operand(CsvTableScan.class).noInputs())).build(); @Override default CsvProjectTableScanRule toRule() return new CsvProjectTableScanRule(this); 规则的默认实例驻留在 CsvRules 的持有类中： public abstract class CsvRules public static final CsvProjectTableScanRule PROJECT_SCAN = CsvProjectTableScanRule.Config.DEFAULT.toRule(); 在默认配置类中（Config 接口中的 DEFAULT 字段），对 withOperandSupplier 方法的调用声明了关系表达式的匹配模式，这个匹配模式会导致规则的触发。如果优化器发现 LogicalProject 的唯一输入是一个没有输入的 CsvTableScan，它将调用这个规则。 规则的变体是可能存在的。例如，不同的规则实例可能会在 CsvTableScan 上匹配到 EnumerableProject。 onMatch 方法生成一个新的关系表达式，并调用 RelOptRuleCall.transformTo() 来表明规则已经成功触发。 查询优化过程 有很多关于 Calcite 查询优化器是多么巧妙的说法，但是我们不会在这里谈论它。巧妙是设计用来减轻你的负担——优化器规则的开发者。 首先，Calcite 不会按照指定的顺序触发规则。查询优化过程按照分支树的众多分支执行，就像下棋程序检查许多可能的位移顺序一样。如果规则 A 和 B 都匹配了查询操作树的给定部分，则 Calcite 可以同时触发。 其次，Calcite 基于成本在多个计划中进行选择，但成本模型并不能阻止规则的触发，这个操作在短期内看起来似乎代价更大。 许多优化器都有一个线性优化方案。如上所述，在面对规则 A 和规则 B 这样的选择时，线性优化器需要立即选择。它可能有诸如将规则 A 应用于整棵树，然后将规则 B 应用于整棵树之类的策略，或者使用基于成本的策略，应用代价最小的规则。 Calcite 不需要进行这样的妥协。这使得组合各种规则集合变得简单。如果你想要将识别物化视图的规则与从 CSV 和 JDBC 源系统读取数据的规则结合起来，你只要将所有规则的集合提供给 Calcite 并告诉它去执行即可。 Calcite 确实使用了成本模型。成本模型决定最终使用哪个计划，有时会修剪搜索树以防止搜索空间爆炸，但它从不强迫你在规则 A 和规则 B 之间进行选择。这点很重要，因为它避免了陷入在搜索空间中不是全局最佳的局部最小值。 此外，如你所想，成本模型是可插拔的，它所依赖的表和查询操作统计也是可插拔的，但那些都是后面的主题。 JDBC 适配器 JDBC 适配器将 JDBC 数据源中的模式映射为 Calcite 模式。 例如，下面这个模式从 MySQL foodmart 数据库中读取： version: 1.0, defaultSchema: FOODMART, schemas: [ name: FOODMART, type: custom, factory: org.apache.calcite.adapter.jdbc.JdbcSchema$Factory, operand: jdbcDriver: com.mysql.jdbc.Driver, jdbcUrl: jdbc:mysql://localhost/foodmart, jdbcUser: foodmart, jdbcPassword: foodmart ] FoodMart 数据库，使用过 Mondrian OLAP 引擎的人应该比较熟悉，因为它是 Mondrian 的主要测试数据集。要加载数据集，请按照 Mondrian 安装说明 进行操作。 JDBC 适配器将尽可能多的处理下推到源系统，包括转换语法、数据类型和内置函数。如果 Calcite 查询是基于单个 JDBC 数据库的表，原则上整个查询应该转到数据库上执行。如果表是来自多个 JDBC 数据源，或者 JDBC 和非 JDBC 的混合数据源，Calcite 将尽可能使用最有效的分布式查询方法。 克隆 JDBC 适配器 克隆 JDBC 适配器会创建一个混合数据库。数据来自 JDBC 数据库，但在第一次访问每个表时会将数据读入内存表。Calcite 基于这些内存表获取查询结果，内存表实际上是数据库的缓存。 例如，以下模型从 MySQL foodmart 数据库读取表： version: 1.0, defaultSchema: FOODMART_CLONE, schemas: [ name: FOODMART_CLONE, type: custom, factory: org.apache.calcite.adapter.clone.CloneSchema$Factory, operand: jdbcDriver: com.mysql.jdbc.Driver, jdbcUrl: jdbc: mysql: //localhost/foodmart, jdbcUser: foodmart, jdbcPassword: foodmart ] 另一种技巧是在现有模式之上构建克隆模式。你可以使用 source 属性来引用模型中之前定义的模式，就像下面这样： version: 1.0, defaultSchema: FOODMART_CLONE, schemas: [ name: FOODMART, type: custom, factory: org.apache.calcite.adapter.jdbc.JdbcSchema$Factory, operand: jdbcDriver: com.mysql.jdbc.Driver, jdbcUrl: jdbc: mysql: //localhost/foodmart, jdbcUser: foodmart, jdbcPassword: foodmart , name: FOODMART_CLONE, type: custom, factory: org.apache.calcite.adapter.clone.CloneSchema$Factory, operand: source: FOODMART ] 你可以使用这种方法在任何类型的模式基础上创建克隆模式，不仅仅是 JDBC。 克隆适配器并不是万能的。我们计划开发更复杂的缓存策略，以及更完整和更高效的内存表实现，但现在克隆 JDBC 适配器展示了什么是可行的，并允许我们去尝试初始实现。 更多主题 还有很多其他方法来扩展 Calcite，但是这些在教程中没有涉及。适配器规范描述了所有涉及到的 API。 写在最后 笔者因为工作原因接触到 Calcite，前期学习过程中，深感 Calcite 学习资料之匮乏，因此创建了 Calcite 从入门到精通知识星球，希望能够将学习过程中的资料和经验沉淀下来，为更多想要学习 Calcite 的朋友提供一些帮助。"},{"title":"课程简介与关系模型","path":"/wiki/cmu_15_445/course-introduction-and-the-relational-model.html","content":"课程大纲 基础内容 Relational Databases Storage Execution Concurrency Control Recovery 高级话题 Distributed Databases Potpourri（大杂烩） 课程项目 课程项目采用 C++17 开发，需要自行学习 C++17 的编程知识。 所有课程项目都会使用 BusTub 学术 DBMS，它的主要架构如下： Disk-Oriented Storage：面向硬盘存储； Volcano-style Query Processing：Volcano 风格的查询处理器； Pluggable APIs：可插拔的 API； Currently does not support SQL：目前不支持 SQL。 (adsbygoogle = window.adsbygoogle || []).push({}); 赞助商 数据库学术研究 Database Group Meetings：https://db.cs.cmu.edu Advanced DBMS Developer Meetings：https://github.com/cmu-db/terrier 什么是数据库 什么是数据库？ Organized collection of inter-related data that models some aspect of the real-world. Databases are core the component of most computer applications. 数据库是以某种方式去进行关联的数据集合，可以对现实世界的某些方面进行建模，数据库不是那些随机分布在电脑上的零散文件，数据库中的数据之间通常都有某些共同的主题。它是大多数计算机应用的核心组件。 数据库示例 Create a database that models a digital music store to keep track of artists and albums. 创建一个模拟数字音乐商店的数据库，以跟踪艺术家和专辑。 Things we need store（我们需要存储如下信息）: Information about Artists（艺术家的信息） What Albums those Artists released（艺术家发表的专辑） 那我们的数据库将如何存储这些信息呢？Store our database as comma-separated value（我们可以将数据库中的信息存储在 CSV 文件中） (CSV) files that we manage in our own code. Use a separate file per entity（每一个实体使用一个单独的文件，例如：Artist 和 Album） The application has to parse the files each time they want to read/update records.（应用程序需要在读取或者更新记录时解析这些文件） Create a database that models a digital music store. 创建一个数据库，为数字音乐商店建模。 Example: Get the year that Ice Cube went solo. 示例：找出 Ice Cube 单飞的年份。我们可以写一个 python 程序，遍历文件中的所有行，通过 parse 函数将行记录解析成数据，然后判断数组的第一个值是否等于 Ice Cube，相等则返回第二个值。 通过这种方式查找数据会存在一些问题，这些问题也是我们想要构建一个数据库管理系统的动机。具体问题如下： DATA INTEGRITY（数据完整性） How do we ensure that the artist is the same for each album entry? 我们如何保证 artist 艺术家信息和 album 专辑中的艺术家信息相同？例如：Ice Cube 拼写错误，或者 Ice Cube 修改了自己的名字，album 表如何保证数据的一致？数据库中使用外键保证？ What if somebody overwrites the album year with an invalid string? 我们该如何保证对不同类型数据的存储是有效的，例如 year 输入了非法值 How do we store that there are multiple artists on an album? 如果一个专辑是多个艺术家创作的，我们该如何存储？ IMPLEMENTATION（实现） How do you find a particular record? 如何去查找一条具体的记录？如果有 10 亿条数据，for 循环的方式就无法高效处理。 What if we now want to create a new application that uses the same database? 如何实现数据库逻辑的复用，示例中是使用 python 实现的数据库查询逻辑，其他语言无法复用。 What if two threads try to write to the same file at the same time? 如果两个线程同时尝试写入同一个数据文件，该如何处理？如果不进行特殊处理，很可能会出现记录的覆盖，第一个线程修改的内容将会丢失。 DURABILITY（持久性） What if the machine crashes while our program is updating a record? 当程序正在更新一条记录时，此时程序或者机器宕机了，记录是更新完了，还是只更新了一半？我该如何推断它的正确状态呢？ What if we want to replicate the database on multiple machines for high availability? 因为机器的不可靠性，我们会考虑将数据库文件复制到不同的机器上以保证高可用。如果一台机器崩溃了，还可以使用备库提供服务。 数据库管理系统 基于以上存在的各种问题，我们需要有一个通用的数据库解决方案，也就是数据库管理系统 DBMS。那么什么是数据库管理系统呢？ A DBMS is software that allows applications to store and analyze information in a database. 数据库管理系统是一种专业软件，它允许应用程序在不关系底层实现的情况下，对数据库中的信息进行存储和分析。 A general-purpose DBMS is designed to allow the definition, creation, querying, update, and administration of databases. 本课程就是要设计一个通用的 DMBS，即设计用于允许应用程序来对数据库进行定义、创建、查询、更新以及管理。我们的主要目标是实现基于硬盘的数据库管理系统，当然也存在其他各种形式的数据库，例如内存数据库等。 早期数据库管理系统 Database applications were difficult to build and maintain. 数据库应用非常难以构建和维护。 Tight coupling between logical and physical layers. 逻辑层和物理层之间紧密耦合。 You have to (roughly) know what queries your app would execute before you deployed the database. 在部署数据库之前，你必须(粗略地)知道应用程序将执行哪些查询。 早期的数据库，你需要结合业务场景，通过数据库的 API 告诉数据库，我需要基于哈希表或者基于树的存储结构，当我们要销毁数据时，同样需要根据选择的存储结构，调用不同的数据库 API 进行操作。 关系模型 Ted Codd 发现了这个问题，为了避免人们重复地进行编码和重构，Ted Codd 提出了关系模型——A Relational Model of Data for Large Shared Data Banks。 Database abstraction to avoid this maintenance（Ted Codd 提出了关系模型的三要素，数据库抽象以避免这些人工维护）: Store database in simple data structures. 将数据库转换为简单的数据结构进行存储，即关系（将所有表存储在数据库中以建立关系，表与表之间建立关系）； Access data through high-level language. 通过高级语言访问数据库。 Physical storage left up to implementation. 大型数据库的物理存储策略取决于数据库管理系统实现，存储结构对应用程序透明。逻辑层使用 SQL，物理存储层由数据库管理系统实现，这样实现了逻辑层和物理层的完全解耦，即使需要更换存储结构，应用程序也可以仍然使用同样的 SQL 访问。 数据模型 A data model is collection of concepts for describing the data in a database. 数据模型是描述数据库中数据的概念的集合。 A schema is a description of a particular collection of data, using a given data model. 模式是使用给定数据模型对特定数据集合的描述。 Relational——关系模型，大部分的 DBMS 都采用了关系模型，本课程重点内容。 Key/Value——NoSQL Graph——NoSQL Document——NoSQL Column-family——NoSQL Array / Matrix——数组和矩阵模型，通常会在机器学习中使用 Hierarchical——层次数据模型（很古老的模型） Network——网络数据模型（很古老的模型） 关系模型三要素 关系数据模型包含了三个部分： Structure（关系结构）: The definition of relations and their contents. 关系及其内容的定义。 Integrity（数据完整性约束）: Ensure the database’s contents satisfy constraints. 保证数据库内容满足约束条件。 Manipulation（操纵）: How to access and modify a database’s contents. 如何访问和修改数据库的内容。 关系模型中涉及到的关系和元祖的概念如下： A relation is unordered set that contain the relationship of attributes that represent entities. 关系是一组无序的元素或记录，这些元素或记录的属性用来表示实体。 A tuple is a set of attribute values (also known as its domain) in the relation. 元组是关系中属性值的集合，通常用元组来表示关系模型中的一条记录。 Values are (normally) atomic/scalar. Ted Codd 提出的关系模型中，值(通常)是原子/标量，不能是数组也不能是嵌套对象，但是随着关系模型的发展，也支持了数组和 JSON 对象存储。 The special value NULL is a member of every domain. 每个元组中包含了一个特殊值 NULL。 n-ary 关系其实就是一张表上有 n 列。在课程中，会使用到 relation 和 table 这两个术语，实际上他们是一回事。 关系模型之主键 A relation’s primary key uniquely identifies a single tuple. 关系中的主键能够唯一标识一个元组。 Some DBMSs automatically create an internal primary key if you don’t define one. 如果你没有定义主键，一些 DBMS 会自动创建一个内部的主键。 Auto-generation of unique integer primary keys: 自动生成唯一的整数主键的方式。 SEQUENCE (SQL:2003) AUTO_INCREMENT (MySQL) 为了方便唯一定位一条记录，我们为 Artist 表增加了一个 id 作为主键。 关系模型之外键 A foreign key specifies that an attribute from one relation has to map to a tuple in another relation. 外键是指将一个元组中的属性映射到另外一个元祖中的属性上，可以用来维护不同关系之间的数据一致性。 假设前面的示例中，我们想要在专辑表存储多位艺术家的名字，我们可以尝试将 artist 改成 artists，但是这就违背了原子性的要求，即每个字段必须只有一个值，如果存储多个值，使用中也会带来很多不便。 为了解决这个问题，我们可以考虑增加一张 ArtistAlbum 表，用来存储 Artist 和 Album 的映射关系，通过 ArtistAlbum 表，可以建立多对多的关系。另外，为了保证 ArtistAlbum 表的数据和主表的一致性，需要将对应字段设置成外键，保证数据的一致性。 数据操作语言 DML How to store and retrieve information from a database. 如何从数据库存储及获取数据，通常有两种方式可以实现。 Procedural（过程式方式，关系代数 Relational Algebra，本课程的重点）: The query specifies the (high-level) strategy the DBMS should use to find the desired result（查询指定高级别的策略，指导 DBMS 去查找想要的结果）. Non-Procedural（非过程式方式 关系演算 Relational Calculus）: The query specifies only what data is wanted and not how to find it. 查询只指定想要什么数据，不提供查找的方式（声明式方式，SQL 就是一种声明式语言）。 关系代数 Fundamental operations to retrieve and manipulate tuples in a relation. 检索和操作关系中的元组的基本操作。 Based on set algebra. 基于集合的代数。 Each operator takes one or more relations as its inputs and outputs a new relation. 每个操作符以一个或多个关系作为其输入，并输出一个新的关系。 We can “chain” operators together to create more complex operations. 我们可以将操作符“链接”在一起，以创建更复杂的操作。 Ted Codd 提出了关系代数的七种基础运算符，这些运算是检索记录所必须的基础操作。值得一提的是，这种代数是基于集合的，这种集合是数据的无需列表或无需集合，里面的元素是可以重复的。 关系代数之 SELECT Choose a subset of the tuples from a relation that satisfies a selection predicate. 从满足选择谓词的关系中选择元组的子集。 Predicate acts as a filter to retain only tuples that fulfill its qualifying requirement. Predicate充当过滤器，只保留满足其限定要求的元组。 Can combine multiple predicates using conjunctions / disjunctions. 可以使用连词/析取来组合多个谓词。conjunctions / disjunctions 如何理解？conjunctions 表示 and，可以用 ∧ 表示，disjunctions 表示 or，可以用 ∨ 表示。 下面是选择操作符的示例，我们可以单独使用 a_id = ‘a2’ 对结果集进行过滤，过滤之后可以得到一个结构和原始表格一致的新的结果集。此外，我们还可以组合多个谓词，来实现更复杂的选择逻辑。 SELECT * FROM R WHERE a_id=a2 AND b_id102; 关系代数之 Projection Generate a relation with tuples that contains only the specified attributes. 生成只包含指定属性的元组关系。 Can rearrange attributes’ ordering. 可以重新排列属性的顺序。 Can manipulate the values. 可以操作值。 下面的示例展示了先进行选择，再进行投影的操作，我们可以按照自己想要的顺序指定投影，可以在投影操作中进行运算，例如 b_id - 100，然后生成一个新的关系。 SELECT b_id-100, a_id FROM R WHERE a_id = a2; 关系代数之 UNION Generate a relation that contains all tuples that appear in either only one or both input relations. 生成一个关系，其中包含只出现在一个或两个输入关系中的所有元组。 Syntax: (R ∪ S) 当你想对两个关系进行 UNION 操作时，这两个关系必须具有相同的属性和相同的类型。 (SELECT * FROM R) UNION ALL (SELECT * FROM S); 关系代数之 INTERSECTION Generate a relation that contains only the tuples that appear in both of the input relations. 生成一个只包含两个输入关系中出现的元组的关系。 Syntax: (R ∩ S) 和 UNION 运算一样，INTERSECTION 运算也要求两个关系必须具有相同的属性和相同的类型。 (SELECT * FROM R) INTERSECT (SELECT * FROM S); 关系代数之 DIFFERENCE Generate a relation that contains only the tuples that appear in the first and not the second of the input relations. 生成一个只包含在输入关系的第一个而不是第二个中出现的元组的关系。 Syntax: (R – S) (SELECT * FROM R) EXCEPT (SELECT * FROM S); 关系代数之 PRODUCT Generate a relation that contains all possible combinations of tuples from the input relations. 从输入关系中生成一个包含所有可能的元组组合的关系。 Product 积运算，也叫笛卡尔积。 Syntax: (R × S) 在 SQL 中，我们可以使用 CROSS JOIN，或者不写任何 JOIN 时，使用的就是笛卡尔积。 SELECT * FROM R CROSS JOIN S;SELECT * FROM R, S; 关系代数之 JOIN Generate a relation that contains all tuples that are a combination of two tuples (one from each input relation) with a common value(s) for one or more attributes. 生成一个包含所有元组的关系，这些元组是两个元组(每个输入关系一个)的组合，具有一个或多个属性的公共值。 这里所说的 JOIN 指的是自然连接，而不是我们一般所说的 JOIN，自然连接会根据两个元组中相同名称，相同类型的属性进行关联。 Syntax: (R ⋈ S) 在 SQL 中，我们可以使用 NATURAL JOIN 进行关联，NATURAL JOIN 会自动根据相同名称的字段进行关联。 SELECT * FROM R NATURAL JOIN S; 关系代数之 EXTRA OPERATORS Ted Codd 提出基础的关系模型运算符之后，后人又结合新的数据库发展提出了其他的运算符，主要包括如下的运算符。 关于 DBMS 一些观点 Relational algebra still defines the high-level steps of how to compute a query. 关系代数仍然定义了如何计算查询的高级步骤。 假设我们要对 R 和 S 进行 Join，我可以先对 R 和 S 进行自然连接，然后再使用 b_id = 102 条件进行过滤。也可以先对 S 表进行过滤，再使用 R 表和过滤结果进行自然连接。这两者的效率相差非常大。 A better approach is to state the high-level answer that you want the DBMS to compute. 更好的方法是声明希望DBMS计算的高级答案。 Retrieve the joined tuples from R and S where b_id equals 102. 从 R 和 S 中检索b_id = 102的连接元组。 对于应用程序来说，更好的方法是通过 SQL 声明我们要获取的结果，而不去关心数据库如何执行，具体的执行逻辑交给 DBMS 实现，根据不同的情况进行选择。 关系模型之查询 The relational model is independent of any query language implementation. 关系模型独立于任何查询语言实现。Ted Codd 提出关系模型时，甚至还没有 SQL 语言，后来 IBM 提出了 SQL 语言，才逐步成为事实标准。 SQL is the de facto standard. SQL是事实上的标准。 for line in file:\trecord = parse(line)\tif Ice Cube == record[0]: print int(record[1])SELECT year FROM artists WHERE name = Ice Cube; 课程总结 Databases are ubiquitous. 数据库无处不在。 Relational algebra defines the primitives for processing queries on a relational database. 关系代数定义处理关系数据库查询的原语。 We will see relational algebra again when we talk about query optimization + execution. 在讨论查询优化 + 执行时，我们将再次看到关系代数。 思考：数据库是如何将 SQL 转换为关系代数，以及如何执行的？ 参考资料 2019 年秋季版 CMU 数据库 15-445/645 中文翻译版-01-课程简介与关系模型-01 2019 年秋季版 CMU 数据库 15-445/645 中文翻译版-01-课程简介与关系模型-02 2019 年秋季版 CMU 数据库 15-445/645 中文翻译版-01-课程简介与关系模型-03 2019 年秋季版 CMU 数据库 15-445/645 中文翻译版-01-课程简介与关系模型-04 课程 Slides 课程 Notes Database System Concepts 6th Edition Chapters 1-2, 6"},{"title":"课程资料","path":"/wiki/cmu_15_445/index.html","content":"CMU 15-445/645 学习笔记中主要内容为 2025 年春季课程，笔记中也可能存在最新课程内容，这些新的内容会在笔记中进行说明。下面是课程学习中参考的一些资料，收集整理如下，如有发现更好的参考资料，欢迎大家留言补充。 官方资料 CMU 15-445/645 (Spring 2025) DATABASE SYSTEMS 课程主页 Database Courses 网络资料 2019 年秋季版 CMU 数据库 15-445/645 中文翻译版 CMU 15-445: Database Systems 自学指南 CMU 15-445/645 Intro to Database Systems 数据库导论"},{"title":"Join 算法","path":"/wiki/cmu_15_445/joins-algorithms.html","content":"Join 算法 为什么需要 Join 算法 为了避免不必要的数据信息重复，通常我们会在关系型数据库中，对表进行规范化处理，即：将不同的信息拆分为不同表进行存储。例如：我们会设计订单表 t_order，以及订单明细表 t_order_item，每个订单表 t_order 会有多个订单明细 t_order_item，如果我们想查询所有关于 Andy 的订单信息，这时候就需要将这 2 张表进行 join，获取到所有数据。 因此，我们需要使用 Join 来完成类似的需求，Join 可以在不丢失任何数据的情况下，对原始的元组信息 Tuple 进行重组，查询出我们需要的数据。 Join 运算符介绍 本节课程主要关注使用等值条件的内连接 Inner，暂时只讨论 2 张表关联的情况。通过这些基础的 Join 运算，后续我们稍作调整就可以支持其他类型的 Join 运算，对于多路连接运算（多张表关联），我们会在高级课程 15-721 中介绍。 通常，我们会将数据量更小的表作为执行计划树中的左表（或者叫外表），优化器会尝试估算 2 张表的数据量，然后生成对应的执行计划。如下是一个典型的 Join 执行计划树，所有运算符按照树形结构排列，数据从叶子节点流向父节点，最终所有数据汇总到根节点，根节点输出的就是查询的结果。 Join 查询计划 Join 运算符在执行时，需要考虑当前的节点需要向父节点输出哪些数据，上图中的箭头表示了数据输出。此外，Join 运算符还需要考虑执行的成本，如何判断一种 Join 算法比另一种更好？ Join 运算符输出结果 针对第一个问题——当前的节点需要向父节点输出哪些数据，我们参考上图的示例，对于所有属于 R 中的元组，我们使用 r 表示（r ∈ R ），而属于 S 中的元组，我们使用 s 表示（s ∈ S ）。我们根据 Join 关联条件 R.id = S.id 判断不同的元组组合是否满足条件，满足关联条件的数据会输出到父节点。 Join 输出的内容不是固定不变的，它可能会受查询处理模型、存储模型以及查询计划树的影响。例如，当存储模型是基于行的存储模型，或者基于列的存储模型时，Join 输出的元组数据可能是基于行或者列存储的数据。此外，根据查询计划树对于数据的要求，Join 运算符输出的可能是部分属性，而非关联表的全部属性。 参考资料 课程 Slides 课程 Notes 课程 Videos Database System Concepts 7th Edition Chapter 15.4 - 15.6 2019 年秋季版 CMU 数据库 15-445/645 中文翻译版 - Join 算法 - 01 2019 年秋季版 CMU 数据库 15-445/645 中文翻译版 - Join 算法 - 02 2019 年秋季版 CMU 数据库 15-445/645 中文翻译版 - Join 算法 - 03 2019 年秋季版 CMU 数据库 15-445/645 中文翻译版 - Join 算法 - 04"},{"title":"课程资料","path":"/wiki/cmu_15_721/index.html","content":"TODO"},{"title":"课程资料","path":"/wiki/cmu_15_799/index.html","content":"CMU 15-799 学习笔记中主要内容为 2025 年春季课程，笔记中也可能存在后续更新的课程内容，这些新的内容会在笔记中进行说明。下面是课程学习中参考的一些资料，收集整理如下，如有发现更好的参考资料，欢迎大家留言补充。 官方资料 CMU 15-799 (SPRING 2025) Special Topics in Database Systems 课程主页 Database Courses 网络资料 CMU 15-799: Special Topics in Database Systems 自学指南"},{"title":"查询优化简介","path":"/wiki/cmu_15_799/intro_to_query_optimization.html","content":"课程目标 15-799 课程主要介绍数据库查询优化器的现代实践，以及查询优化器相关的系统编程。通过课程的学习，可以掌握如下 3 部分内容： 查询优化器实现； 编写正确且高效的代码； 适当的文档 + 测试； 当前数据库的需求非常大，每个公司都会遇到数据库相关的问题，而查询优化器是区分不同数据库能力的关键点，因此学习查询优化器非常有价值，能够帮助大家更好地理解数据库并解决问题。 本课程包含了如下 2 个实战项目，项目 1 需要独立完成，项目 2 则需要团队合作： Project #1 - Query Optimizer Evaluation：动手体验流行的查询优化器，项目中你将使用 Apache Calcite 来优化 SQL 查询，然后在 Calcite（通过枚举适配器）和 DuckDB 上执行 SQL 查询； Project #2 - 根据示例项目，选择一个能够团队参与的项目（待定）。 诞生的背景 SQL 是一种声明性的查询语句，用户通常只需要通过 SQL 语句（如下展示），告诉数据库管理系统（DBMS）查询哪些数据，而不需要告诉它如何完成这些任务。 SELECT DISTINCT enameFROM Emp E\tJOIN Dept D ON E.did = D.didWHERE D.dname = Toy 对于一个给定的查询，DBMS 会尝试找到一个正确并且高效的执行计划，这也是本门课程的目标，即：正确（correct）、高效（best cost）地执行 SQL。 正确执行 SQL 是一个基本前提，满足了这个基本前提后，我们会更加关注 SQL 执行的效率。因此，我们需要通过代价（cost）这样的指标，来表示 SQL 执行计划的效率，从而可以对不同执行计划进行比较。 那么，查询优化和现实世界有什么样的联系呢？假设我们有 2 张表，分别是员工表（Emp）和部门表（Dept），我们使用如下的 SQL 语句，尝试从 Toy 部门查找到该部门下所有员工的不同名字。 SELECT DISTINCT enameFROM Emp E\tJOIN Dept D ON E.did = D.didWHERE D.dname = Toy 为了能够有效地执行这条 SQL，我们会在 Catalog 中记录列索引，包括：聚簇索引（Clustered Index）和非聚簇索引（Unclustered Index），以及表的记录数和页数。除了这些信息，Catalog 中还会记录额外的元数据，或者表中内容的摘要，这些信息会用来确定基数估计（Cardinality Estimation）、谓词选择性（Predicate Selectivity）。 按照 SQL 字面意思，我们将 SQL 语句翻译为上图所示的执行计划，最底部我们对 Emp 和 Dept 进行扫描，然后对他们进行笛卡尔积运算。从图中可以看到，笛卡尔积运算需要进行大量的读写 IO 操作，并且后续的选择操作（包括：Emp.did = Dept.did 连接条件、dname = 'Toy' 过滤条件 ）需要重新读回全部写入数据。过滤操作完成后，我们会对最终的查询结果进行投影，得到我们需要的员工名字。 因此，如果我们直接将 SQL 按照字面意思翻译为执行计划，那么整个执行过程大约需要 200 万次 IO 操作，这个查询成本显然是非常高的。那么，我们可以让这条 SQL 执行地更高效吗？为了实现这个目标，首先可以假设 I/Os 是成本指标中的一项，我们可以根据 I/Os 大小，来简单确定某个执行计划是否是最优的，降低 I/Os 可以直接提升 SQL 执行效率。 前文生成的执行计划采用的是笛卡尔积 Join，这会产生大量的 I/Os 操作，为了减少 I/Os 成本，可以将关联条件 E.did = D.did 下推到 Join 中，这样就可以使用其他更高效的 Join 运算符，例如：Page Nested-Loop Join，下图展示了 Page Nested-Loop Join 的执行计划，由于提前过滤 Join 关联条件，整个执行计划只需要 5 万 4 千次 I/Os。 优化器的职责就是识别出哪些执行计划效率低，并将这些低效率的执行计划优化为语义等价且高效的执行计划。优化器可能会进行更进一步地优化，例如：使用 Sort Merge Join 来替换 Page Nested-Loop Join，这种操作本质上改变了 Join 运算符的物理运算符。 除了 Join 运算符层面的优化，优化器还可以对查询执行模型进行优化，例如我们使用物化模型（Materialization Model）进行查询，每个运算符都需要执行完所有操作，并将数据写入到临时文件中，然后再从临时文件中读取出来，这种执行模型没有采用流水线执行方式（No Pipelining）。我们可以尝试将执行模型切换为向量化执行模型（Vectorization Model），它可以充分利用流水线的优势，无需完成运算符的所有数据计算，只需要向上传递一个元组向量，整体的 I/Os 可以下降一半。 前面我们讨论的主要是如何对 Join 运算符进行优化，除了 Join 优化外，还可以将谓词下推到 Join 运算符之下，从而大幅度减少 I/Os。dname = 'Toy' 是 Dept 表的谓词，如果在 Join 之后做谓词过滤，则会导致 Join 的计算量特别大，因此优化器会将谓词下推到 Join 运算符之下，这样优化后的 I/Os 可以降低到 37。 经过这些优化，我们可以看出优化器的重要价值，从最开始的 200 万次 I/Os，一直减少到 37 次 I/Os，SQL 执行的性能也因此大幅度提升。以上展示的还只是一个简单的 SQL Case，对于复杂的 CTE，嵌套子查询，优化器带来的性能提升将会更高。 (adsbygoogle = window.adsbygoogle || []).push({}); 赞助商 优化器介绍 DBMS 概览 下图展示了 DBMS 的整体架构，应用程序通过 SQL 访问 DBMS，首先通过 SQL 解析器（Parser）对 SQL 字符串进行处理，不同的数据库会有不同的 SQL 方言，SQL 解析器将会识别出这些方言，然后生成抽象语法树（Abstract Syntax Tree）来表示 SQL。 下一个阶段，会将抽象语法树输入到绑定器（Binder）中，绑定器也叫分析器（Resolver），它负责查看抽象语法树中的标记（Token），并将这些标记转换为数据库对象，例如：对于给定的表名，它的内部标识符或对象 ID 是什么？如果 SQL 引用了不存在的表，绑定器可能会抛出异常。为了完成 SQL 绑定，每个数据库都会维护系统目录（System Catalog），它是关于数据库的数据库，内部存储了表的元数据信息，例如：表包含哪些列，列的类型，是否有物化视图、虚拟视图或触发器等。 绑定器会生成初始的逻辑执行计划，这个执行计划像是对抽象语法树的精确翻译，并且采用了关系代数进行表示，逻辑执行计划没有具体说明要如何执行。优化器（Optimizer）接收到逻辑执行计划后，内部会基于代价模型（Cost Model）计算代价，计算代价的过程中，需要从系统目录中获取统计信息（包括：行数、列值的分布特征），然后采用一些计算公式，估算出某个运算符的执行代价，然后选择代价最小的执行计划。最终，优化器会生成代价最小的物理执行计划，物理执行计划中声明了要如何执行，执行器（Executor）根据物理执行计划就可以完成 SQL 执行。 查询优化器 查询优化器负责根据输入的逻辑执行计划，生成对应的物理执行计划。查询优化器的目标主要包含以下几点： 从一个巨大的搜索空间中，寻找出高效的执行计划； 准确区分出一个执行计划，是否比另外一个执行计划更好； 高效地查找搜索空间，找出代价最低的物理执行计划； 理想情况下，不管查询表达式如何书写，查询优化器都要能够生成出最佳的执行计划（复杂的 SQL 场景下，找出最优执行计划的过程就需要消耗大量时间，因此通常会找出相对高效的执行计划）。 逻辑计划 VS 物理计划 物理执行计划由物理运算符组成，物理运算符则定义了具体的执行策略，例如：访问路径、Join 算法等。物理运算符还会依赖他们处理数据的物理格式，例如：排序、压缩等。需要注意的是，逻辑运算符和物理运算符并不是 1:1 对应的，例如：逻辑运算符 LogicalScan 可以转换为 TableScan 或 IndexScan，逻辑运算符 LogicalJoin 可以转换为 NestedLoopJoin、SortMergeJoin 或 HashJoin。 课程主题 如下是 15-799 查询优化课程包含的主题： 搜索策略（Search Strategies） 枚举 / 转换（Enumeration / Transformations） 并行化（Parallelization） 统计 / 汇总（Statistics / Summarization） 基数估计 / 参数化（Cardinality Estimation / Parameterization） 自适应 / 反馈机制（Adaptivity / Feedback Mechanisms） 现实世界的实现（Real-world Implementations） 搜索策略 启发式规则： 重写查询以消除（猜测的，或基于经验的）低效率的运算符； 例如：始终先进行选择，或尽可能早地下推投影； 这些技术可能需要检查目录（Catalog），但不需要检查数据。 基于代价的搜索： 使用模型估算执行计划的成本； 列举查询的多个等效计划，并选择成本最低的计划。 自上而下 VS 自下而上 自上而下优化： 从查询所需的结果开始，然后沿着树向下查找能够实现该目标的最佳计划； 例如：Volcano、Cascades。 自下而上优化： 从零开始，然后制定计划，以实现你想要的结果。自下而上优化是边进行，边构建所需的运算符； 例如：System R、Starburst。 参考资料 EQOP Book (Chapter 1) An Overview of Query Optimization in Relational Systems (S. Chaudhuri, PODS 1998) (Optional) 课程 Slides 课程 Video"},{"title":"前言","path":"/wiki/jdbc/preface.html","content":"原文链接：https://download.oracle.com/otndocs/jcp/jdbc-4_2-mrel2-spec/ This document supersedes and consolidates the content of these predecessor specifications: ■ “JDBC: A Java SQL API” ■ “JDBC 2.1 API” ■ “JDBC 2.0 Standard Extension API” ■ “JDBC 3.0 Specification” This document introduces a range of new features for the JDBC API and is combined with various specification improvements that focus on features introduced in or before the JDBC 3.0 API. Where possible, any adjustment to the JDBC 3.0 API is marked for easy identification - look for the JDBC 4.2 API demarcation for specific features introduced in this revised and updated specification. Readers can also download the API specification (JavadocTM API and comments) for a complete and precise definition of JDBC classes and interfaces. This documentation is available from the download page at https://jcp.org/en/jsr/detail?id=221 Typographic Conventions Submitting Feedback"},{"title":"课程资料","path":"/wiki/tsinghua_database/index.html","content":"数据库管理系统课程学习过程中参考了如下资料，如有其他推荐的参考资料，欢迎大家留言补充。 课程资料 数据库管理系统课程主页 数据库管理系统 B 站视频教程 HuaDB 数据库内核课程实验文档"},{"title":"查询优化","path":"/wiki/tsinghua_database/query-optimization.html","content":"查询优化的流程 TODO"}]